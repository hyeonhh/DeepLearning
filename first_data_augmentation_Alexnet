{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":109264,"sourceType":"datasetVersion","datasetId":56828,"isSourceIdPinned":false},{"sourceId":998277,"sourceType":"datasetVersion","datasetId":547506,"isSourceIdPinned":false}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 필요한 라이브러리 불러오기(import)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import DataLoader, Dataset, random_split,Subset\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import v2\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import ConcatDataset\n\nfrom sklearn import decomposition\nfrom PIL import Image\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:00:07.825617Z","iopub.execute_input":"2025-12-04T02:00:07.826432Z","iopub.status.idle":"2025-12-04T02:00:07.833141Z","shell.execute_reply.started":"2025-12-04T02:00:07.826403Z","shell.execute_reply":"2025-12-04T02:00:07.831878Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"akash2sharma/tiny-imagenet\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:58:27.612780Z","iopub.execute_input":"2025-11-27T10:58:27.613119Z","iopub.status.idle":"2025-11-27T10:58:28.041396Z","shell.execute_reply.started":"2025-11-27T10:58:27.613082Z","shell.execute_reply":"2025-11-27T10:58:28.040565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_root = '/kaggle/input/tiny-imagenet/tiny-imagenet-200/train'\n\nall_pixels = []\n\nfor class_id in os.listdir(train_root):\n    class_dir = os.path.join(train_root,class_id,\"images\")\n    if not os.path.isdir(class_dir):\n        continue\n\n    for fname in os.listdir(class_dir):\n        if not fname.lower().endswith((\".jpg\",\".jpeg\",\".png\")):\n            contitnue\n        path = os.path.join(class_dir,fname)\n        img = Image.open(path).convert(\"RGB\")\n        arr = np.array(img,dtype=np.float32) / 255.0\n        pixels = arr.reshape(-1,3)\n        all_pixels.append(pixels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T07:42:10.895972Z","iopub.execute_input":"2025-12-03T07:42:10.896267Z","iopub.status.idle":"2025-12-03T07:53:46.692126Z","shell.execute_reply.started":"2025-12-03T07:42:10.896248Z","shell.execute_reply":"2025-12-03T07:53:46.690812Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"len(all_pixels) # 100000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:00:25.964057Z","iopub.execute_input":"2025-12-04T02:00:25.964375Z","iopub.status.idle":"2025-12-04T02:00:25.991901Z","shell.execute_reply.started":"2025-12-04T02:00:25.964353Z","shell.execute_reply":"2025-12-04T02:00:25.990383Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2174850425.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_pixels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'all_pixels' is not defined"],"ename":"NameError","evalue":"name 'all_pixels' is not defined","output_type":"error"}],"execution_count":4},{"cell_type":"markdown","source":"# Transform 만들기 ","metadata":{}},{"cell_type":"code","source":"# 사용자 정의 transform 만들기\n\n# eigen_vector = []\n# eigen_value = [] \ndef getEigenFactor():\n    \n        img_array = np.array(all_pixels).astype(np.float32)\n        # print(f'(img_array.dtype) : {(img_array.dtype)}') # uint8 -> float32 \n    \n        H,W,C= img_array.shape\n        # print(f'img shape : {img_array.shape}')\n    \n        img_array = img_array.reshape(-1,3) \n        pca = decomposition.PCA(n_components=3)\n        pca.fit(img_array)\n        pca_result = pca.transform(img_array)\n          \n        # 주성분 벡터(고유벡터) , 각 행이 [R,G,B]\n        eigen_vector = pca.components_\n        # print(f'eigen_vector : {eigen_vector}')\n    \n        \n        # 분산(고유값)\n        eigen_value = pca.explained_variance_\n        # print(f'eigen_value : {eigen_value}')\n        \n        return eigen_vector, eigen_value\n\n#eigen_vector , eigen_value = getEigenFactor()\n\nclass AugmentDataPCA():\n\n  def __init__(self):\n    self.std = 0.1\n\n  def __call__(self,img):\n\n    img_arr = np.array(img).astype(np.float32)\n   # print(f'img_arr :{img_arr.shape}') # (224, 224, 3)\n    H,W,C = img_arr.shape\n    # eigen_vector, eigen_value = getEigenFactor()\n\n    alpha = np.random.randn(3).astype(np.float32) * self.std\n    # print(f'alpha : {alpha}')\n      \n    delta = eigen_value * alpha \n    # print(f'delta : {delta}')\n\n    color_shift = eigen_vector @ delta # principal_components와 delta.T와의 내적합을 구해준다.\n    # print(f'color_shift : {color_shift}')\n    augmented_img = img_arr + color_shift # img_array에 채널별 계산값을 더해준다.\n      \n    # print(f'augmented_img shape : {augmented_img.shape}') # 50176, 3\n    augmented_img = augmented_img.reshape(H,W,C)\n    # print(f'augmented_img shape : {augmented_img.shape}') # 50176, 3\n\n    augmented_img = np.clip(augmented_img, 0,255).astype(np.uint8)\n\n    return Image.fromarray(augmented_img)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:00:31.245472Z","iopub.execute_input":"2025-12-04T02:00:31.245792Z","iopub.status.idle":"2025-12-04T02:00:31.254429Z","shell.execute_reply.started":"2025-12-04T02:00:31.245768Z","shell.execute_reply":"2025-12-04T02:00:31.252936Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"eigen_value\neigen_vector","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:00:40.324484Z","iopub.execute_input":"2025-12-04T02:00:40.325316Z","iopub.status.idle":"2025-12-04T02:00:40.332455Z","shell.execute_reply.started":"2025-12-04T02:00:40.325281Z","shell.execute_reply":"2025-12-04T02:00:40.331457Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array([[ 0.6132164 ,  0.58443445,  0.53141505],\n       [-0.6817293 ,  0.05173409,  0.729773  ],\n       [-0.39901224,  0.80978984, -0.43015045]], dtype=float32)"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# PCA 결과 저장 & 불러오기","metadata":{}},{"cell_type":"code","source":"# PCA 결과 저장하기\nimport numpy as np\n\nnp.savez(\"pca_result.npz\",\n         eigen_vector = eigen_vector,\n        eigen_value = eigen_value\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T07:58:35.474855Z","iopub.execute_input":"2025-12-03T07:58:35.475233Z","iopub.status.idle":"2025-12-03T07:58:35.482261Z","shell.execute_reply.started":"2025-12-03T07:58:35.475209Z","shell.execute_reply":"2025-12-03T07:58:35.480887Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"pca = np.load('/kaggle/working/pca_result.npz')\neigen_vector = pca['eigen_vector']\neigen_value = pca['eigen_value']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:00:35.667558Z","iopub.execute_input":"2025-12-04T02:00:35.667881Z","iopub.status.idle":"2025-12-04T02:00:35.676071Z","shell.execute_reply.started":"2025-12-04T02:00:35.667835Z","shell.execute_reply":"2025-12-04T02:00:35.675072Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize(size =(256,256)),\n    transforms.CenterCrop(size= (224,224)),\n    AugmentDataPCA(),\n    transforms.ToTensor(),\n   # 전체 데이터의 각 채널별 평균 빼주기 \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nflip_transform = transforms.Compose([\n    transforms.Resize(size =(256,256)),\n    transforms.CenterCrop(size= (224,224)),\n    transforms.RandomHorizontalFlip(p=1.0),\n    AugmentDataPCA(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]\n)\n\ntest_transform = transforms.Compose([\n    transforms.Resize(size =(256,256)),\n    transforms.TenCrop(224),#10개 crop을 tuple로 반환    \n    # (10,3,224,224)형태로 stack\n    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n    transforms.Lambda(lambda crops: torch.stack([\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(c) for c in crops\n    ]))\n    ]\n)\n\n\ntrain_dir = '/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/'\nval_dir = '/kaggle/input/tiny-imagenet/tiny-imagenet-200/val/'\nval_reorg_dir='/kaggle/working/tiny-imagenet/val_reorganized'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:00:42.440762Z","iopub.execute_input":"2025-12-04T02:00:42.441378Z","iopub.status.idle":"2025-12-04T02:00:42.450663Z","shell.execute_reply.started":"2025-12-04T02:00:42.441350Z","shell.execute_reply":"2025-12-04T02:00:42.449814Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from IPython.utils.path import shutil\nimport os\nimport pandas as pd\n\nval_img_dir = os.path.join(val_dir, 'images')\nval_annotations = os.path.join(val_dir, 'val_annotations.txt')\n\nval_reorg_dir =  '/kaggle/working/tiny-imagenet/val_reorganized'\n\n# annotation 파일 읽기\n\nval_df = pd.read_csv(val_annotations, sep='\\t', header=None,names=['filename','class_id','x','y','w','h'])\n\nfor _,row in val_df.iterrows():\n    filename = row['filename']\n    class_id = row['class_id']\n\n    # 클래스 폴더 생성\n    class_folder = os.path.join(val_reorg_dir, class_id)\n    os.makedirs(class_folder, exist_ok=True)\n\n    # 이미지 복사\n    src = os.path.join(val_img_dir, filename)\n    dst = os.path.join(class_folder, filename)\n\n    shutil.copy(src,dst)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T06:11:03.591786Z","iopub.execute_input":"2025-12-01T06:11:03.592072Z","iopub.status.idle":"2025-12-01T06:12:40.017148Z","shell.execute_reply.started":"2025-12-01T06:11:03.592051Z","shell.execute_reply":"2025-12-01T06:12:40.016513Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 데이터 불러오기 ","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 128\ntrain_dataset = datasets.ImageFolder(train_dir, transform=transform)\ntrain_flip_dataset = datasets.ImageFolder(train_dir, transform=flip_transform)\n\nval_dataset = datasets.ImageFolder(val_reorg_dir, transform = test_transform)\n\nfull_train_dataset = ConcatDataset([train_dataset, train_flip_dataset])\n\ntrain_dataloader = DataLoader(full_train_dataset, batch_size = BATCH_SIZE, shuffle = True)#  shuffle: 에포크마다 배치 순서를 섞을지 여부\n\nVAL_BATCH_SIZE = 32\nval_dataloader = DataLoader(val_dataset, batch_size= VAL_BATCH_SIZE, shuffle = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:38:22.326770Z","iopub.execute_input":"2025-12-04T02:38:22.328053Z","iopub.status.idle":"2025-12-04T02:38:33.685968Z","shell.execute_reply.started":"2025-12-04T02:38:22.328019Z","shell.execute_reply":"2025-12-04T02:38:33.684594Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2739592503.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_flip_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflip_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_reorg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         samples = self.make_dataset(\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;31m# is potentially overridden and thus could have a different logic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The class_to_idx parameter cannot be None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         return make_dataset(\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_empty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfnames\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"print(len(val_dataloader.dataset))# 200000\nlen(val_dataset) # 10000\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:38:37.527622Z","iopub.execute_input":"2025-12-04T02:38:37.528077Z","iopub.status.idle":"2025-12-04T02:38:37.535164Z","shell.execute_reply.started":"2025-12-04T02:38:37.528049Z","shell.execute_reply":"2025-12-04T02:38:37.534351Z"}},"outputs":[{"name":"stdout","text":"10000\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"10000"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# print(val_dataset[0][0].shape) # torch.Size([10, 3, 224, 224])\n   \n\n# torch.Size([3, 224, 224])\n# tiny dataset : torch.Size([3, 64, 64]) -> torch.Size([3, 224, 224])로 수정\n\n\nplt.imshow(train_dataloader.dataset[0][0].permute(1,2,0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:38:41.894015Z","iopub.execute_input":"2025-12-04T02:38:41.894349Z","iopub.status.idle":"2025-12-04T02:38:42.418892Z","shell.execute_reply.started":"2025-12-04T02:38:41.894326Z","shell.execute_reply":"2025-12-04T02:38:42.417889Z"}},"outputs":[{"name":"stderr","text":"Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7cfb4750e490>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"# Tiny버전 모델 정의하기","metadata":{}},{"cell_type":"code","source":"class Alexnet_Tiny(nn.Module):\n    def __init__(self):\n        super().__init__()\n        dropout = nn.Dropout(p=0.5),\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64,kernel_size=(11,11),stride=4,padding=2)\n        nn.init.normal_(self.conv1.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv1.bias,val=0)\n        self.lrn1 = nn.LocalResponseNorm(size = 5, alpha=10**(-4),beta=0.75,k = 2)\n\n\n        self.conv2= nn.Conv2d(in_channels=64, out_channels=192, kernel_size=(5,5),padding = 2)\n        nn.init.normal_(self.conv2.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv2.bias,val=1)\n        self.lrn2 = nn.LocalResponseNorm(size = 5, alpha=10**(-4),beta=0.75,k = 2)\n\n        self.conv3= nn.Conv2d(in_channels=192, out_channels=384, kernel_size=(3,3),padding=1)\n        nn.init.normal_(self.conv3.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv3.bias,val=0)\n        \n\n        self.conv4= nn.Conv2d(in_channels=384, out_channels=256, kernel_size=(3,3),padding=1)\n        nn.init.normal_(self.conv4.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv4.bias,val=1)\n\n        self.conv5= nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3),padding = 1)\n        nn.init.normal_(self.conv5.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv5.bias,val=1)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((6,6))\n        self.fc1= nn.Linear(in_features=256*6*6, out_features=4096)\n        nn.init.constant_(self.fc1.bias,val=1)\n        self.fc2 = nn.Linear(in_features = 4096, out_features=4096)\n        nn.init.constant_(self.fc2.bias,val=1)\n        self.fc3 = nn.Linear(in_features = 4096, out_features=200)\n        nn.init.constant_(self.fc3.bias,val=0)\n        nn.init.normal_(self.fc1.weight, mean=0.0, std=0.01)\n        nn.init.normal_(self.fc2.weight, mean=0.0, std=0.01)\n        nn.init.normal_(self.fc3.weight, mean=0.0, std=0.01)\n\n\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            self.fc1,\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            self.fc2,\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            self.fc3\n        )\n\n\n\n    def forward(self,x):\n        # ---- Conv1 ----\n        # ReLU\n\n        # print(f'x : {x.shape}')\n        out = F.relu(self.conv1(x))\n        #  Response Normalization 적용하기\n        lrn = nn.LocalResponseNorm(size = 5, alpha=10**(-4),beta=0.75,k = 2)\n        out = self.lrn1(out)\n        # MaxPooling 적용 (window = 3, stride = 2)\n        out = F.max_pool2d(input = out, kernel_size=3, stride=2)\n\n       # print(f'conv1 후 : {out.shape}')\n\n\n\n        # ---- Conv2 ----\n        out = F.relu(self.conv2(out))\n        #  Response Normalization 적용하기\n        out = self.lrn2(out)\n\n        # MaxPooling 적용 (window = 3, stride = 2)\n        out = F.max_pool2d(input = out,kernel_size=3, stride=2)\n\n        # print(f'conv2 후 : {out.shape}')\n\n\n        # ---- Conv3 ----\n        out = F.relu(self.conv3(out))\n\n        # print(f'conv3 후 : {out.shape}')\n\n\n        # ---- Conv4 ----\n        out = F.relu(self.conv4(out))\n        # print(f'conv4 후 : {out.shape}')\n\n\n        # ---- Conv5 ----\n        out = F.relu(self.conv5(out))\n        out = F.max_pool2d(input = out, kernel_size = 3, stride= 2 )\n\n        # print(f'conv5 후 : {out.shape}')\n\n\n        # ---- Fully Connected layer ----\n        logits = self.classifier(out)\n\n        # print(f'logits : {logits.shape}')\n\n        # ---- softmax ----\n        #probs = F.softmax(logits, dim=1)\n\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:38:45.554069Z","iopub.execute_input":"2025-12-04T02:38:45.554417Z","iopub.status.idle":"2025-12-04T02:38:45.571833Z","shell.execute_reply.started":"2025-12-04T02:38:45.554393Z","shell.execute_reply":"2025-12-04T02:38:45.570739Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class Alexnet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        dropout = nn.Dropout(p=0.5),\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96,kernel_size=(11,11),stride=4,padding=2)\n        nn.init.normal_(self.conv1.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv1.bias,val=0)\n\n        self.conv2= nn.Conv2d(in_channels=96, out_channels=256, kernel_size=(5,5),padding = 2)\n        nn.init.normal_(self.conv2.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv2.bias,val=1)\n        self.conv3= nn.Conv2d(in_channels=256, out_channels=384, kernel_size=(3,3),padding=1)\n        nn.init.normal_(self.conv3.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv3.bias,val=0)\n\n        self.conv4= nn.Conv2d(in_channels=384, out_channels=384, kernel_size=(3,3),padding=1)\n        nn.init.normal_(self.conv4.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv4.bias,val=1)\n\n        self.conv5= nn.Conv2d(in_channels=384, out_channels=256, kernel_size=(3,3),padding = 1)\n        nn.init.normal_(self.conv5.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv5.bias,val=1)\n\n        self.fc1= nn.Linear(in_features=256*6*6, out_features=4096)\n        nn.init.constant_(self.fc1.bias,val=1)\n        self.fc2 = nn.Linear(in_features = 4096, out_features=4096)\n        nn.init.constant_(self.fc2.bias,val=1)\n        self.fc3 = nn.Linear(in_features = 4096, out_features=200)\n        nn.init.constant_(self.fc3.bias,val=0)\n\n\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            self.fc1,\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            self.fc2,\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            self.fc3\n        )\n\n\n\n    def forward(self,x):\n        # ---- Conv1 ----\n        # ReLU\n\n        print(f'x : {x.shape}')\n        out = F.relu(self.conv1(x))\n        #  Response Normalization 적용하기\n        lrn = nn.LocalResponseNorm(size = 5, alpha=10**(-4),beta=0.75,k = 2)\n        out = lrn(out)\n        # MaxPooling 적용 (window = 3, stride = 2)\n        out = F.max_pool2d(input = out, kernel_size=3, stride=2)\n\n        print(f'conv1 후 : {out.shape}')\n\n\n\n        # ---- Conv2 ----\n        out = F.relu(self.conv2(out))\n        #  Response Normalization 적용하기\n        out = lrn(out)\n\n        # MaxPooling 적용 (window = 3, stride = 2)\n        out = F.max_pool2d(input = out,kernel_size=3, stride=2)\n\n        print(f'conv2 후 : {out.shape}')\n\n\n        # ---- Conv3 ----\n        out = F.relu(self.conv3(out))\n\n        print(f'conv3 후 : {out.shape}')\n\n\n        # ---- Conv4 ----\n        out = F.relu(self.conv4(out))\n        print(f'conv4 후 : {out.shape}')\n\n\n        # ---- Conv5 ----\n        out = F.relu(self.conv5(out))\n        out = F.max_pool2d(input = out, kernel_size = 3, stride= 2 )\n\n        print(f'conv5 후 : {out.shape}')\n\n\n        # ---- Fully Connected layer ----\n        logits = self.classifier(out)\n\n        print(f'logits : {logits.shape}')\n\n        # ---- softmax ----\n        probs = F.softmax(logits, dim=1)\n\n        return logits, probs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T11:20:14.844860Z","iopub.execute_input":"2025-11-27T11:20:14.845155Z","iopub.status.idle":"2025-11-27T11:20:14.859359Z","shell.execute_reply.started":"2025-11-27T11:20:14.845135Z","shell.execute_reply":"2025-11-27T11:20:14.858595Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# # tpu용\n# device = xm.xla_device() \n\nmodel = Alexnet_Tiny().to(device)# 먼저 모델을 디바이스로 보내기 \n\nEPOCH = 30\n\nLEARNING_RATE = 1e-4 # validation loss에 따라 조절하기\n\nloss_fn = torch.nn.CrossEntropyLoss()\n#optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9) # 그 다음 optimizer 만들기 \n# # 또는\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n#schedular = StepLR(optimizer=optimizer, step_size=1, gamma=0.0001)\n\nwriter = SummaryWriter()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:49:33.777435Z","iopub.execute_input":"2025-12-04T02:49:33.777774Z","iopub.status.idle":"2025-12-04T02:49:34.727135Z","shell.execute_reply.started":"2025-12-04T02:49:33.777750Z","shell.execute_reply":"2025-12-04T02:49:34.726164Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def train_model(model, data,label):\n  \n  model.train()\n\n  running_loss = 0.0\n  last_loss = 0.0\n  true_prediction = 0\n\n\n  data ,label = data.to(device),label.to(device)\n\n  optimizer.zero_grad()\n\n  logits = model(data)\n  loss = loss_fn(logits,label)\n\n \n  loss.backward()\n  optimizer.step()\n\n # 정확도 계산\n  label, logits = label.cpu().detach().numpy(), logits.cpu().detach().numpy()\n\n  label_predict = np.argmax(logits,axis=1)\n\n  true_prediction = np.sum(label == label_predict)\n  train_loss = loss.item()\n\n  return train_loss , true_prediction\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:38:58.873608Z","iopub.execute_input":"2025-12-04T02:38:58.874798Z","iopub.status.idle":"2025-12-04T02:38:58.880759Z","shell.execute_reply.started":"2025-12-04T02:38:58.874765Z","shell.execute_reply":"2025-12-04T02:38:58.879793Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def validate(model,data,label):\n\n    model.eval()\n\n    running_loss = 0.0\n    last_loss= 0.0\n    true_prediction = 0\n\n    # print(f'val - before view : {data.shape}')\n\n    B,flip,C,H,W = data.shape\n    \n    data = data.view(-1, C,H,W)\n    # print(f'val after view : {data.shape}')\n\n    data ,label = data.to(device),  label.to(device)\n    logits = model(data) # bx10 , num_classes\n    # print(f'logits1.shape : {logits.shape}')\n    logits = logits.view(B,flip,-1)\n    # print(f'logits2.shape : {logits.shape}')\n\n    logits_mean = logits.mean(1)\n\n    \n    # print(f'logits_mean.shape : {logits_mean.shape}')\n\n    loss = loss_fn(logits_mean, label)\n\n    label = label.to('cpu').detach().numpy()\n\n    logits_mean = logits_mean.to('cpu').detach().numpy()\n\n    # print(f'logits : {logits}')\n\n    prob_label = np.argmax(logits_mean,axis = 1)\n    print(f'VAL : 예측라벨 : {prob_label}, 정답 {label}')\n\n    true_prediction = np.sum(label == prob_label)\n    val_loss = loss.item()\n    \n    return val_loss , true_prediction\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:39:01.292888Z","iopub.execute_input":"2025-12-04T02:39:01.293223Z","iopub.status.idle":"2025-12-04T02:39:01.300744Z","shell.execute_reply.started":"2025-12-04T02:39:01.293198Z","shell.execute_reply":"2025-12-04T02:39:01.299584Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def valAugmentation(img):\n    crop_size = 224\n    five_crop = v2.TenCrop(crop_size)\n    ten_cropped_img = five_crop(img)\n\n    plt.imshow(ten_cropped_img)\n    return ten_cropped_img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:27:56.581088Z","iopub.execute_input":"2025-11-30T13:27:56.581678Z","iopub.status.idle":"2025-11-30T13:27:56.585458Z","shell.execute_reply.started":"2025-11-30T13:27:56.581654Z","shell.execute_reply":"2025-11-30T13:27:56.584729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = torch.rand(3,3)\nprint(test)\nprint(test.sum())\nprint(test.sum(dim=[0]))\n\nprint(test.size(0))\nprint(test.sum(dim=[0])/test.size(0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T12:54:45.420874Z","iopub.execute_input":"2025-11-26T12:54:45.421624Z","iopub.status.idle":"2025-11-26T12:54:45.435192Z","shell.execute_reply.started":"2025-11-26T12:54:45.421597Z","shell.execute_reply":"2025-11-26T12:54:45.434598Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_img_value = torch.zeros(3)\ntotal_pixels =  0\n\nfor batch,(data,label) in enumerate(val_dataloader):\n\n    # print(data.shape) # torch.Size([120, 3, 224, 224])\n\n    total_img_value += data.sum(dim=[0,2,3]) # dim을 한꺼번에 모두 더함 \n    total_pixels += data.size(0) * data.size(2) * data.size(3)\n\nmean = total_img_value/total_pixels\n\nmean","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:18:37.723789Z","iopub.execute_input":"2025-11-26T06:18:37.724518Z","iopub.status.idle":"2025-11-26T06:19:34.811753Z","shell.execute_reply.started":"2025-11-26T06:18:37.724490Z","shell.execute_reply":"2025-11-26T06:19:34.810982Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def unnormalize(img, mean, std):\n    img = img.clone()\n    for t, m, s in zip(img, mean, std):\n        t.mul_(s).add_(m)\n    return img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:39:56.259128Z","iopub.execute_input":"2025-11-26T08:39:56.259678Z","iopub.status.idle":"2025-11-26T08:39:56.263674Z","shell.execute_reply.started":"2025-11-26T08:39:56.259655Z","shell.execute_reply":"2025-11-26T08:39:56.262934Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_dataset.transform)\n\nimg = unnormalize(train_dataset[0][0], [0.4777, 0.4520, 0.4032], [1.0,1.0,1.0])\n\n# 또는\nprint(img.shape, img.min(), img.max())\n\nplt.imshow(img.permute(1,2,0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T06:05:03.632319Z","iopub.execute_input":"2025-12-01T06:05:03.632927Z","iopub.status.idle":"2025-12-01T06:05:03.652761Z","shell.execute_reply.started":"2025-12-01T06:05:03.632880Z","shell.execute_reply":"2025-12-01T06:05:03.651854Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for data, label in train_dataloader:\n    print(label.min(), label.max())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T12:56:11.014193Z","iopub.execute_input":"2025-11-26T12:56:11.014501Z","iopub.status.idle":"2025-11-26T12:56:18.984007Z","shell.execute_reply.started":"2025-11-26T12:56:11.014462Z","shell.execute_reply":"2025-11-26T12:56:18.982965Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Early Stopping 구현하기 \n\n- 4시간 넘게 돌렸는데 val accuracy가 좋아지지 않았다 - early stopping 추가해서 더이상 올라가지 않으면 종료하기 ","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self,patience = 10, verbose=False, delta=0,path = 'checkpoing.pt',trace_func=print):\n        self.patience= patience\n        self.verbose = verbose # True이면 개선 시마다 메시지 출력 \n        self.delta = delta # 개선으로 인정할 최소 변화량 \n        self.path = path\n        self.trace_func = trace_func\n\n        self.best_val_accuracy = None\n        self.early_stop = False\n        self.val_accuracy_max = - np.inf\n        self.counter = 0\n    def __call__(self, val_accuracy, model):\n        if np.isnan(val_accuracy):\n            self.trace_func(\"Validation accuracy is NaN. Ignoring this epoch\")\n            return \n        if self.best_val_accuracy is None :\n            self.best_val_accuracy = val_accuracy\n            self.save_checkpoint(self.best_val_accuracy, model)\n        elif val_accuracy >  self.best_val_accuracy + self.delta:\n            self.best_val_accuracy = val_accuracy\n            self.save_checkpoint(val_accuracy,model)\n            self.counter = 0 \n        else:\n            self.counter += 1\n            self.trace_func(f'EarlyStopping counter : {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n    def save_checkpoint(self,val_accuracy, model):\n        if self.verbose:\n            self.trace_func(f'Validation accuracy increased {self.best_val_accuracy} -> {val_accuracy}. Saving Model...' \n                           )\n            torch.save(model.state_dict(),self.path)\n            self.best_val_accuracy = val_accuracy\n\nearly_stopping = EarlyStopping(patience = 15,verbose = True, delta = 1e-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T02:20:22.224849Z","iopub.execute_input":"2025-11-29T02:20:22.225647Z","iopub.status.idle":"2025-11-29T02:20:22.232198Z","shell.execute_reply.started":"2025-11-29T02:20:22.225610Z","shell.execute_reply":"2025-11-29T02:20:22.231426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_epoch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:02:11.565554Z","iopub.execute_input":"2025-12-03T06:02:11.565805Z","iopub.status.idle":"2025-12-03T06:02:11.589460Z","shell.execute_reply.started":"2025-12-03T06:02:11.565785Z","shell.execute_reply":"2025-12-03T06:02:11.588771Z"}},"outputs":[{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mstart_epoch\u001b[49m\n","\u001b[31mNameError\u001b[39m: name 'start_epoch' is not defined"],"ename":"NameError","evalue":"name 'start_epoch' is not defined","output_type":"error"}],"execution_count":75},{"cell_type":"code","source":"batch_train_losses = []\nbatch_val_losses = []\nbatch_train_accuracies = []\nbatch_val_accuracies = []\n\n\nepoch_train_losses = []\nepoch_val_losses = []\nepoch_train_accuracies = []\nepoch_val_accuracies = []\n\n\n\nstart_epoch = 0\n\nfor epoch in range(start_epoch, EPOCH):\n\n      epoch_train_loss = 0.0\n      epoch_train_accuracy = 0.0\n      epoch_true_prediction= 0\n      epoch_total = 0\n\n      epoch_val_loss = 0.0\n      epoch_val_accuracy = 0.0\n      epoch_val_true_prediction= 0\n      epoch_val_total = 0\n\n      for batch , (train_data,train_label) in enumerate(train_dataloader):\n\n        global_step = epoch * len(train_dataloader) + batch\n        # 총 batch가 200번 반복됨 , 에포크 5니까 총 1000번 반복됨.\n        print(f'Epoch : {epoch}, batch {batch}')\n        train_loss , batch_true_prediction = train_model(model,train_data,train_label)\n\n        batch_size = train_label.shape[0]\n\n        epoch_train_loss += train_loss\n        epoch_true_prediction += batch_true_prediction\n        epoch_total += batch_size\n\n        # 각 배치별 스칼라 기록\n        batch_train_accuracy = batch_true_prediction / batch_size\n\n        batch_train_losses.append(train_loss)\n        batch_train_accuracies.append(batch_train_accuracy)\n\n        print(f'(Train) Batch {batch} Loss : {train_loss}, 맞은 개수 : {batch_true_prediction}')\n        writer.add_scalar(\"Batch별 Loss/Train\", train_loss, global_step)\n        writer.add_scalar(\"Batch별 Accuracy/Train\", batch_train_accuracy,global_step)\n\n      # 각 에포크별 스칼라 기록\n      epoch_train_losses.append(epoch_train_loss / len(train_dataloader))#배치의 개수로 나누면 됨\n      epoch_train_accuracies.append(epoch_true_prediction / epoch_total)\n    \n      writer.add_scalar(\"Epoch별 Loss/Train\", epoch_train_loss / len(train_dataloader), epoch)\n      writer.add_scalar(\"Epoch별 Accuracy/Train\", epoch_true_prediction / epoch_total,epoch)\n      print(f'epoch {epoch} Loss/Train :{epoch_train_loss / len(train_dataloader)} ')\n      print(f'epoch {epoch} Accuracy/Train : {epoch_true_prediction / epoch_total}')\n\n\n    # 정확도 추가하자\n      with torch.no_grad():\n        for batch,(val_data,val_label) in enumerate(val_dataloader):\n          # print(\"data shape:\", val_data.shape, \"label shape:\", val_label.shape)\n          global_step = epoch * len(val_dataloader) + batch\n          val_loss , val_true_predction = validate(model,val_data,val_label)\n          batch_size = val_label.shape[0]\n          epoch_val_loss+= val_loss\n          epoch_val_true_prediction += val_true_predction\n          epoch_val_total += batch_size\n\n          print(f'(VAL) Batch {batch} Loss : {val_loss}, accuracy: {val_true_predction/batch_size}')\n          batch_val_losses.append(val_loss)\n          batch_val_accuracies.append(val_true_predction/batch_size)\n          writer.add_scalar(\"Batch별 Loss/Validate\", val_loss, global_step)\n          writer.add_scalar(\"Batch별 Accuracy/Validate Batch\", val_true_predction/batch_size,global_step)\n\n        epoch_val_losses.append(epoch_val_loss/len(val_dataloader))\n        epoch_val_accuracies.append(epoch_val_true_prediction/epoch_val_total)\n\n       # early_stopping(val_accuracy = epoch_val_true_prediction/epoch_val_total, model = model)\n     \n        writer.add_scalar(\"Epoch별 Loss/Validate\", epoch_val_loss/len(val_dataloader), epoch)\n        writer.add_scalar(\"Epoch별 Accuracy/Validate\", epoch_val_true_prediction/epoch_val_total,epoch)\n        print(f'epoch {epoch} Loss/Validate :{epoch_val_loss/len(val_dataloader)} ')\n        print(f'epoch {epoch} Accuracy/Validate : {epoch_val_true_prediction/epoch_val_total}')\n        \n        torch.save(\n    {'epoch' : EPOCH,\n    'model_state_dict':model.state_dict(),\n    'optimizer' : optimizer.state_dict()}\n   ,'alexnet_ckpt.pth')\n\n        # if early_stopping.early_stop:\n        #     print(f'Early Stopped : {epoch} ')\n        #     break\n\n\nwriter.flush()\n\n\n%load_ext tensorboard\n%tensorboard --logdir=runs\n\nwriter.close()\n\n# https://ysg2997.tistory.com/16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:00:26.549071Z","iopub.execute_input":"2025-12-04T03:00:26.549407Z","iopub.status.idle":"2025-12-04T13:21:34.335027Z","shell.execute_reply.started":"2025-12-04T03:00:26.549384Z","shell.execute_reply":"2025-12-04T13:21:34.332077Z"}},"outputs":[{"name":"stdout","text":"Epoch : 0, batch 0\n(Train) Batch 0 Loss : 23.17424774169922, 맞은 개수 : 0\nEpoch : 0, batch 1\n(Train) Batch 1 Loss : 21.252201080322266, 맞은 개수 : 1\nEpoch : 0, batch 2\n(Train) Batch 2 Loss : 18.903522491455078, 맞은 개수 : 0\nEpoch : 0, batch 3\n(Train) Batch 3 Loss : 15.76911735534668, 맞은 개수 : 0\nEpoch : 0, batch 4\n(Train) Batch 4 Loss : 11.717905044555664, 맞은 개수 : 2\nEpoch : 0, batch 5\n(Train) Batch 5 Loss : 8.05810832977295, 맞은 개수 : 2\nEpoch : 0, batch 6\n(Train) Batch 6 Loss : 5.857138633728027, 맞은 개수 : 0\nEpoch : 0, batch 7\n(Train) Batch 7 Loss : 5.691567897796631, 맞은 개수 : 0\nEpoch : 0, batch 8\n(Train) Batch 8 Loss : 5.843633651733398, 맞은 개수 : 0\nEpoch : 0, batch 9\n(Train) Batch 9 Loss : 6.052900314331055, 맞은 개수 : 0\nEpoch : 0, batch 10\n(Train) Batch 10 Loss : 5.818406581878662, 맞은 개수 : 2\nEpoch : 0, batch 11\n(Train) Batch 11 Loss : 5.880307674407959, 맞은 개수 : 0\nEpoch : 0, batch 12\n(Train) Batch 12 Loss : 6.080780982971191, 맞은 개수 : 0\nEpoch : 0, batch 13\n(Train) Batch 13 Loss : 5.670884609222412, 맞은 개수 : 1\nEpoch : 0, batch 14\n(Train) Batch 14 Loss : 5.954147815704346, 맞은 개수 : 1\nEpoch : 0, batch 15\n(Train) Batch 15 Loss : 5.992873191833496, 맞은 개수 : 1\nEpoch : 0, batch 16\n(Train) Batch 16 Loss : 5.657070636749268, 맞은 개수 : 0\nEpoch : 0, batch 17\n(Train) Batch 17 Loss : 6.109907627105713, 맞은 개수 : 0\nEpoch : 0, batch 18\n(Train) Batch 18 Loss : 5.914217948913574, 맞은 개수 : 0\nEpoch : 0, batch 19\n(Train) Batch 19 Loss : 5.868527412414551, 맞은 개수 : 1\nEpoch : 0, batch 20\n(Train) Batch 20 Loss : 5.97918176651001, 맞은 개수 : 1\nEpoch : 0, batch 21\n(Train) Batch 21 Loss : 5.741337776184082, 맞은 개수 : 0\nEpoch : 0, batch 22\n(Train) Batch 22 Loss : 6.007114410400391, 맞은 개수 : 1\nEpoch : 0, batch 23\n(Train) Batch 23 Loss : 5.64945650100708, 맞은 개수 : 1\nEpoch : 0, batch 24\n(Train) Batch 24 Loss : 5.737310409545898, 맞은 개수 : 1\nEpoch : 0, batch 25\n(Train) Batch 25 Loss : 5.607895851135254, 맞은 개수 : 0\nEpoch : 0, batch 26\n(Train) Batch 26 Loss : 5.576611518859863, 맞은 개수 : 1\nEpoch : 0, batch 27\n(Train) Batch 27 Loss : 5.88409948348999, 맞은 개수 : 1\nEpoch : 0, batch 28\n(Train) Batch 28 Loss : 5.640453338623047, 맞은 개수 : 0\nEpoch : 0, batch 29\n(Train) Batch 29 Loss : 5.733673572540283, 맞은 개수 : 0\nEpoch : 0, batch 30\n(Train) Batch 30 Loss : 5.545273303985596, 맞은 개수 : 0\nEpoch : 0, batch 31\n(Train) Batch 31 Loss : 5.6935529708862305, 맞은 개수 : 0\nEpoch : 0, batch 32\n(Train) Batch 32 Loss : 5.515109062194824, 맞은 개수 : 0\nEpoch : 0, batch 33\n(Train) Batch 33 Loss : 5.699384689331055, 맞은 개수 : 0\nEpoch : 0, batch 34\n(Train) Batch 34 Loss : 5.675596237182617, 맞은 개수 : 1\nEpoch : 0, batch 35\n(Train) Batch 35 Loss : 5.641446113586426, 맞은 개수 : 0\nEpoch : 0, batch 36\n(Train) Batch 36 Loss : 5.6701579093933105, 맞은 개수 : 0\nEpoch : 0, batch 37\n(Train) Batch 37 Loss : 5.491871356964111, 맞은 개수 : 1\nEpoch : 0, batch 38\n(Train) Batch 38 Loss : 5.51460599899292, 맞은 개수 : 1\nEpoch : 0, batch 39\n(Train) Batch 39 Loss : 5.5481367111206055, 맞은 개수 : 1\nEpoch : 0, batch 40\n(Train) Batch 40 Loss : 5.548766136169434, 맞은 개수 : 1\nEpoch : 0, batch 41\n(Train) Batch 41 Loss : 5.582382678985596, 맞은 개수 : 0\nEpoch : 0, batch 42\n(Train) Batch 42 Loss : 5.457205772399902, 맞은 개수 : 0\nEpoch : 0, batch 43\n(Train) Batch 43 Loss : 5.607367515563965, 맞은 개수 : 1\nEpoch : 0, batch 44\n(Train) Batch 44 Loss : 5.643713474273682, 맞은 개수 : 0\nEpoch : 0, batch 45\n(Train) Batch 45 Loss : 5.592901706695557, 맞은 개수 : 1\nEpoch : 0, batch 46\n(Train) Batch 46 Loss : 5.514814376831055, 맞은 개수 : 0\nEpoch : 0, batch 47\n(Train) Batch 47 Loss : 5.550537586212158, 맞은 개수 : 0\nEpoch : 0, batch 48\n(Train) Batch 48 Loss : 5.6253662109375, 맞은 개수 : 1\nEpoch : 0, batch 49\n(Train) Batch 49 Loss : 5.692572116851807, 맞은 개수 : 1\nEpoch : 0, batch 50\n(Train) Batch 50 Loss : 5.549252510070801, 맞은 개수 : 0\nEpoch : 0, batch 51\n(Train) Batch 51 Loss : 5.504478931427002, 맞은 개수 : 1\nEpoch : 0, batch 52\n(Train) Batch 52 Loss : 5.558463096618652, 맞은 개수 : 1\nEpoch : 0, batch 53\n(Train) Batch 53 Loss : 5.418807029724121, 맞은 개수 : 1\nEpoch : 0, batch 54\n(Train) Batch 54 Loss : 5.498901844024658, 맞은 개수 : 1\nEpoch : 0, batch 55\n(Train) Batch 55 Loss : 5.543200969696045, 맞은 개수 : 0\nEpoch : 0, batch 56\n(Train) Batch 56 Loss : 5.49278450012207, 맞은 개수 : 1\nEpoch : 0, batch 57\n(Train) Batch 57 Loss : 5.52252721786499, 맞은 개수 : 0\nEpoch : 0, batch 58\n(Train) Batch 58 Loss : 5.538049697875977, 맞은 개수 : 0\nEpoch : 0, batch 59\n(Train) Batch 59 Loss : 5.43898868560791, 맞은 개수 : 0\nEpoch : 0, batch 60\n(Train) Batch 60 Loss : 5.504266738891602, 맞은 개수 : 1\nEpoch : 0, batch 61\n(Train) Batch 61 Loss : 5.456200122833252, 맞은 개수 : 1\nEpoch : 0, batch 62\n(Train) Batch 62 Loss : 5.48444938659668, 맞은 개수 : 1\nEpoch : 0, batch 63\n(Train) Batch 63 Loss : 5.541037082672119, 맞은 개수 : 0\nEpoch : 0, batch 64\n(Train) Batch 64 Loss : 5.419292449951172, 맞은 개수 : 0\nEpoch : 0, batch 65\n(Train) Batch 65 Loss : 5.407003402709961, 맞은 개수 : 1\nEpoch : 0, batch 66\n(Train) Batch 66 Loss : 5.354437828063965, 맞은 개수 : 0\nEpoch : 0, batch 67\n(Train) Batch 67 Loss : 5.423878192901611, 맞은 개수 : 0\nEpoch : 0, batch 68\n(Train) Batch 68 Loss : 5.341892719268799, 맞은 개수 : 1\nEpoch : 0, batch 69\n(Train) Batch 69 Loss : 5.464715003967285, 맞은 개수 : 1\nEpoch : 0, batch 70\n(Train) Batch 70 Loss : 5.503091335296631, 맞은 개수 : 0\nEpoch : 0, batch 71\n(Train) Batch 71 Loss : 5.471121788024902, 맞은 개수 : 1\nEpoch : 0, batch 72\n(Train) Batch 72 Loss : 5.465041160583496, 맞은 개수 : 0\nEpoch : 0, batch 73\n(Train) Batch 73 Loss : 5.551694393157959, 맞은 개수 : 0\nEpoch : 0, batch 74\n(Train) Batch 74 Loss : 5.484241485595703, 맞은 개수 : 2\nEpoch : 0, batch 75\n(Train) Batch 75 Loss : 5.424915313720703, 맞은 개수 : 1\nEpoch : 0, batch 76\n(Train) Batch 76 Loss : 5.494002819061279, 맞은 개수 : 1\nEpoch : 0, batch 77\n(Train) Batch 77 Loss : 5.521159648895264, 맞은 개수 : 0\nEpoch : 0, batch 78\n(Train) Batch 78 Loss : 5.465662479400635, 맞은 개수 : 1\nEpoch : 0, batch 79\n(Train) Batch 79 Loss : 5.45064640045166, 맞은 개수 : 0\nEpoch : 0, batch 80\n(Train) Batch 80 Loss : 5.40025520324707, 맞은 개수 : 0\nEpoch : 0, batch 81\n(Train) Batch 81 Loss : 5.419976711273193, 맞은 개수 : 2\nEpoch : 0, batch 82\n(Train) Batch 82 Loss : 5.436689853668213, 맞은 개수 : 1\nEpoch : 0, batch 83\n(Train) Batch 83 Loss : 5.561222553253174, 맞은 개수 : 0\nEpoch : 0, batch 84\n(Train) Batch 84 Loss : 5.539236068725586, 맞은 개수 : 0\nEpoch : 0, batch 85\n(Train) Batch 85 Loss : 5.4195709228515625, 맞은 개수 : 1\nEpoch : 0, batch 86\n(Train) Batch 86 Loss : 5.402413368225098, 맞은 개수 : 1\nEpoch : 0, batch 87\n(Train) Batch 87 Loss : 5.376094818115234, 맞은 개수 : 0\nEpoch : 0, batch 88\n(Train) Batch 88 Loss : 5.4009246826171875, 맞은 개수 : 1\nEpoch : 0, batch 89\n(Train) Batch 89 Loss : 5.4114089012146, 맞은 개수 : 1\nEpoch : 0, batch 90\n(Train) Batch 90 Loss : 5.42638635635376, 맞은 개수 : 2\nEpoch : 0, batch 91\n(Train) Batch 91 Loss : 5.4210357666015625, 맞은 개수 : 0\nEpoch : 0, batch 92\n(Train) Batch 92 Loss : 5.336655616760254, 맞은 개수 : 0\nEpoch : 0, batch 93\n(Train) Batch 93 Loss : 5.342074871063232, 맞은 개수 : 0\nEpoch : 0, batch 94\n(Train) Batch 94 Loss : 5.416034698486328, 맞은 개수 : 1\nEpoch : 0, batch 95\n(Train) Batch 95 Loss : 5.422868251800537, 맞은 개수 : 0\nEpoch : 0, batch 96\n(Train) Batch 96 Loss : 5.321555137634277, 맞은 개수 : 0\nEpoch : 0, batch 97\n(Train) Batch 97 Loss : 5.438262462615967, 맞은 개수 : 0\nEpoch : 0, batch 98\n(Train) Batch 98 Loss : 5.496871471405029, 맞은 개수 : 0\nEpoch : 0, batch 99\n(Train) Batch 99 Loss : 5.376394748687744, 맞은 개수 : 0\nEpoch : 0, batch 100\n(Train) Batch 100 Loss : 5.375890254974365, 맞은 개수 : 0\nEpoch : 0, batch 101\n(Train) Batch 101 Loss : 5.3744587898254395, 맞은 개수 : 1\nEpoch : 0, batch 102\n(Train) Batch 102 Loss : 5.419722557067871, 맞은 개수 : 0\nEpoch : 0, batch 103\n(Train) Batch 103 Loss : 5.40521764755249, 맞은 개수 : 0\nEpoch : 0, batch 104\n(Train) Batch 104 Loss : 5.370269775390625, 맞은 개수 : 1\nEpoch : 0, batch 105\n(Train) Batch 105 Loss : 5.428705215454102, 맞은 개수 : 0\nEpoch : 0, batch 106\n(Train) Batch 106 Loss : 5.396923065185547, 맞은 개수 : 0\nEpoch : 0, batch 107\n(Train) Batch 107 Loss : 5.408125877380371, 맞은 개수 : 0\nEpoch : 0, batch 108\n(Train) Batch 108 Loss : 5.408549785614014, 맞은 개수 : 2\nEpoch : 0, batch 109\n(Train) Batch 109 Loss : 5.382303237915039, 맞은 개수 : 1\nEpoch : 0, batch 110\n(Train) Batch 110 Loss : 5.399098873138428, 맞은 개수 : 1\nEpoch : 0, batch 111\n(Train) Batch 111 Loss : 5.402471542358398, 맞은 개수 : 0\nEpoch : 0, batch 112\n(Train) Batch 112 Loss : 5.4646687507629395, 맞은 개수 : 0\nEpoch : 0, batch 113\n(Train) Batch 113 Loss : 5.355365753173828, 맞은 개수 : 1\nEpoch : 0, batch 114\n(Train) Batch 114 Loss : 5.365084648132324, 맞은 개수 : 1\nEpoch : 0, batch 115\n(Train) Batch 115 Loss : 5.423216342926025, 맞은 개수 : 0\nEpoch : 0, batch 116\n(Train) Batch 116 Loss : 5.366313934326172, 맞은 개수 : 0\nEpoch : 0, batch 117\n(Train) Batch 117 Loss : 5.37753438949585, 맞은 개수 : 1\nEpoch : 0, batch 118\n(Train) Batch 118 Loss : 5.3980937004089355, 맞은 개수 : 0\nEpoch : 0, batch 119\n(Train) Batch 119 Loss : 5.419668197631836, 맞은 개수 : 0\nEpoch : 0, batch 120\n(Train) Batch 120 Loss : 5.419948577880859, 맞은 개수 : 0\nEpoch : 0, batch 121\n(Train) Batch 121 Loss : 5.3530964851379395, 맞은 개수 : 1\nEpoch : 0, batch 122\n(Train) Batch 122 Loss : 5.36026668548584, 맞은 개수 : 2\nEpoch : 0, batch 123\n(Train) Batch 123 Loss : 5.33325719833374, 맞은 개수 : 1\nEpoch : 0, batch 124\n(Train) Batch 124 Loss : 5.392271995544434, 맞은 개수 : 0\nEpoch : 0, batch 125\n(Train) Batch 125 Loss : 5.391888618469238, 맞은 개수 : 0\nEpoch : 0, batch 126\n(Train) Batch 126 Loss : 5.456073760986328, 맞은 개수 : 0\nEpoch : 0, batch 127\n(Train) Batch 127 Loss : 5.398449897766113, 맞은 개수 : 0\nEpoch : 0, batch 128\n(Train) Batch 128 Loss : 5.4223246574401855, 맞은 개수 : 0\nEpoch : 0, batch 129\n(Train) Batch 129 Loss : 5.399048805236816, 맞은 개수 : 0\nEpoch : 0, batch 130\n(Train) Batch 130 Loss : 5.424072742462158, 맞은 개수 : 0\nEpoch : 0, batch 131\n(Train) Batch 131 Loss : 5.348300457000732, 맞은 개수 : 0\nEpoch : 0, batch 132\n(Train) Batch 132 Loss : 5.40672492980957, 맞은 개수 : 1\nEpoch : 0, batch 133\n(Train) Batch 133 Loss : 5.377475261688232, 맞은 개수 : 0\nEpoch : 0, batch 134\n(Train) Batch 134 Loss : 5.348398685455322, 맞은 개수 : 0\nEpoch : 0, batch 135\n(Train) Batch 135 Loss : 5.357632637023926, 맞은 개수 : 0\nEpoch : 0, batch 136\n(Train) Batch 136 Loss : 5.421844482421875, 맞은 개수 : 0\nEpoch : 0, batch 137\n(Train) Batch 137 Loss : 5.373007297515869, 맞은 개수 : 0\nEpoch : 0, batch 138\n(Train) Batch 138 Loss : 5.335115909576416, 맞은 개수 : 1\nEpoch : 0, batch 139\n(Train) Batch 139 Loss : 5.420253753662109, 맞은 개수 : 0\nEpoch : 0, batch 140\n(Train) Batch 140 Loss : 5.352143287658691, 맞은 개수 : 0\nEpoch : 0, batch 141\n(Train) Batch 141 Loss : 5.333004951477051, 맞은 개수 : 2\nEpoch : 0, batch 142\n(Train) Batch 142 Loss : 5.3705315589904785, 맞은 개수 : 0\nEpoch : 0, batch 143\n(Train) Batch 143 Loss : 5.379708290100098, 맞은 개수 : 1\nEpoch : 0, batch 144\n(Train) Batch 144 Loss : 5.394279479980469, 맞은 개수 : 1\nEpoch : 0, batch 145\n(Train) Batch 145 Loss : 5.330226421356201, 맞은 개수 : 0\nEpoch : 0, batch 146\n(Train) Batch 146 Loss : 5.365714073181152, 맞은 개수 : 1\nEpoch : 0, batch 147\n(Train) Batch 147 Loss : 5.348169803619385, 맞은 개수 : 1\nEpoch : 0, batch 148\n(Train) Batch 148 Loss : 5.394547939300537, 맞은 개수 : 0\nEpoch : 0, batch 149\n(Train) Batch 149 Loss : 5.410656929016113, 맞은 개수 : 0\nEpoch : 0, batch 150\n(Train) Batch 150 Loss : 5.279481887817383, 맞은 개수 : 1\nEpoch : 0, batch 151\n(Train) Batch 151 Loss : 5.381810188293457, 맞은 개수 : 0\nEpoch : 0, batch 152\n(Train) Batch 152 Loss : 5.333927154541016, 맞은 개수 : 0\nEpoch : 0, batch 153\n(Train) Batch 153 Loss : 5.333795547485352, 맞은 개수 : 0\nEpoch : 0, batch 154\n(Train) Batch 154 Loss : 5.3837127685546875, 맞은 개수 : 0\nEpoch : 0, batch 155\n(Train) Batch 155 Loss : 5.384934425354004, 맞은 개수 : 1\nEpoch : 0, batch 156\n(Train) Batch 156 Loss : 5.368234634399414, 맞은 개수 : 0\nEpoch : 0, batch 157\n(Train) Batch 157 Loss : 5.32562780380249, 맞은 개수 : 2\nEpoch : 0, batch 158\n(Train) Batch 158 Loss : 5.357053756713867, 맞은 개수 : 1\nEpoch : 0, batch 159\n(Train) Batch 159 Loss : 5.352228164672852, 맞은 개수 : 1\nEpoch : 0, batch 160\n(Train) Batch 160 Loss : 5.318620681762695, 맞은 개수 : 1\nEpoch : 0, batch 161\n(Train) Batch 161 Loss : 5.337534427642822, 맞은 개수 : 0\nEpoch : 0, batch 162\n(Train) Batch 162 Loss : 5.349856376647949, 맞은 개수 : 0\nEpoch : 0, batch 163\n(Train) Batch 163 Loss : 5.375617027282715, 맞은 개수 : 0\nEpoch : 0, batch 164\n(Train) Batch 164 Loss : 5.273279666900635, 맞은 개수 : 1\nEpoch : 0, batch 165\n(Train) Batch 165 Loss : 5.398133277893066, 맞은 개수 : 1\nEpoch : 0, batch 166\n(Train) Batch 166 Loss : 5.331461429595947, 맞은 개수 : 3\nEpoch : 0, batch 167\n(Train) Batch 167 Loss : 5.362196445465088, 맞은 개수 : 1\nEpoch : 0, batch 168\n(Train) Batch 168 Loss : 5.290100574493408, 맞은 개수 : 1\nEpoch : 0, batch 169\n(Train) Batch 169 Loss : 5.352024078369141, 맞은 개수 : 2\nEpoch : 0, batch 170\n(Train) Batch 170 Loss : 5.374126434326172, 맞은 개수 : 0\nEpoch : 0, batch 171\n(Train) Batch 171 Loss : 5.346955299377441, 맞은 개수 : 1\nEpoch : 0, batch 172\n(Train) Batch 172 Loss : 5.308071613311768, 맞은 개수 : 0\nEpoch : 0, batch 173\n(Train) Batch 173 Loss : 5.323876857757568, 맞은 개수 : 1\nEpoch : 0, batch 174\n(Train) Batch 174 Loss : 5.381499767303467, 맞은 개수 : 0\nEpoch : 0, batch 175\n(Train) Batch 175 Loss : 5.351707458496094, 맞은 개수 : 2\nEpoch : 0, batch 176\n(Train) Batch 176 Loss : 5.371336936950684, 맞은 개수 : 0\nEpoch : 0, batch 177\n(Train) Batch 177 Loss : 5.393956661224365, 맞은 개수 : 0\nEpoch : 0, batch 178\n(Train) Batch 178 Loss : 5.3296990394592285, 맞은 개수 : 2\nEpoch : 0, batch 179\n(Train) Batch 179 Loss : 5.31796932220459, 맞은 개수 : 1\nEpoch : 0, batch 180\n(Train) Batch 180 Loss : 5.3485236167907715, 맞은 개수 : 0\nEpoch : 0, batch 181\n(Train) Batch 181 Loss : 5.367213726043701, 맞은 개수 : 1\nEpoch : 0, batch 182\n(Train) Batch 182 Loss : 5.339174747467041, 맞은 개수 : 0\nEpoch : 0, batch 183\n(Train) Batch 183 Loss : 5.337400436401367, 맞은 개수 : 1\nEpoch : 0, batch 184\n(Train) Batch 184 Loss : 5.352436065673828, 맞은 개수 : 0\nEpoch : 0, batch 185\n(Train) Batch 185 Loss : 5.320767402648926, 맞은 개수 : 2\nEpoch : 0, batch 186\n(Train) Batch 186 Loss : 5.325722694396973, 맞은 개수 : 0\nEpoch : 0, batch 187\n(Train) Batch 187 Loss : 5.379293918609619, 맞은 개수 : 1\nEpoch : 0, batch 188\n(Train) Batch 188 Loss : 5.326747417449951, 맞은 개수 : 1\nEpoch : 0, batch 189\n(Train) Batch 189 Loss : 5.336854934692383, 맞은 개수 : 2\nEpoch : 0, batch 190\n(Train) Batch 190 Loss : 5.3044328689575195, 맞은 개수 : 3\nEpoch : 0, batch 191\n(Train) Batch 191 Loss : 5.378526210784912, 맞은 개수 : 1\nEpoch : 0, batch 192\n(Train) Batch 192 Loss : 5.358618259429932, 맞은 개수 : 1\nEpoch : 0, batch 193\n(Train) Batch 193 Loss : 5.351293087005615, 맞은 개수 : 0\nEpoch : 0, batch 194\n(Train) Batch 194 Loss : 5.323053359985352, 맞은 개수 : 1\nEpoch : 0, batch 195\n(Train) Batch 195 Loss : 5.370251655578613, 맞은 개수 : 0\nEpoch : 0, batch 196\n(Train) Batch 196 Loss : 5.306234359741211, 맞은 개수 : 1\nEpoch : 0, batch 197\n(Train) Batch 197 Loss : 5.308798313140869, 맞은 개수 : 0\nEpoch : 0, batch 198\n(Train) Batch 198 Loss : 5.32846212387085, 맞은 개수 : 1\nEpoch : 0, batch 199\n(Train) Batch 199 Loss : 5.328518390655518, 맞은 개수 : 1\nEpoch : 0, batch 200\n(Train) Batch 200 Loss : 5.324193477630615, 맞은 개수 : 1\nEpoch : 0, batch 201\n(Train) Batch 201 Loss : 5.35130500793457, 맞은 개수 : 0\nEpoch : 0, batch 202\n(Train) Batch 202 Loss : 5.352428913116455, 맞은 개수 : 1\nEpoch : 0, batch 203\n(Train) Batch 203 Loss : 5.312074661254883, 맞은 개수 : 1\nEpoch : 0, batch 204\n(Train) Batch 204 Loss : 5.346882343292236, 맞은 개수 : 0\nEpoch : 0, batch 205\n(Train) Batch 205 Loss : 5.369465351104736, 맞은 개수 : 0\nEpoch : 0, batch 206\n(Train) Batch 206 Loss : 5.317834377288818, 맞은 개수 : 0\nEpoch : 0, batch 207\n(Train) Batch 207 Loss : 5.298417091369629, 맞은 개수 : 4\nEpoch : 0, batch 208\n(Train) Batch 208 Loss : 5.292255878448486, 맞은 개수 : 2\nEpoch : 0, batch 209\n(Train) Batch 209 Loss : 5.342205047607422, 맞은 개수 : 2\nEpoch : 0, batch 210\n(Train) Batch 210 Loss : 5.373660564422607, 맞은 개수 : 2\nEpoch : 0, batch 211\n(Train) Batch 211 Loss : 5.362518310546875, 맞은 개수 : 1\nEpoch : 0, batch 212\n(Train) Batch 212 Loss : 5.324522495269775, 맞은 개수 : 0\nEpoch : 0, batch 213\n(Train) Batch 213 Loss : 5.339039325714111, 맞은 개수 : 0\nEpoch : 0, batch 214\n(Train) Batch 214 Loss : 5.292243480682373, 맞은 개수 : 4\nEpoch : 0, batch 215\n(Train) Batch 215 Loss : 5.282215595245361, 맞은 개수 : 1\nEpoch : 0, batch 216\n(Train) Batch 216 Loss : 5.291794300079346, 맞은 개수 : 0\nEpoch : 0, batch 217\n(Train) Batch 217 Loss : 5.3269524574279785, 맞은 개수 : 2\nEpoch : 0, batch 218\n(Train) Batch 218 Loss : 5.307528495788574, 맞은 개수 : 0\nEpoch : 0, batch 219\n(Train) Batch 219 Loss : 5.308115482330322, 맞은 개수 : 1\nEpoch : 0, batch 220\n(Train) Batch 220 Loss : 5.358718395233154, 맞은 개수 : 0\nEpoch : 0, batch 221\n(Train) Batch 221 Loss : 5.243240833282471, 맞은 개수 : 1\nEpoch : 0, batch 222\n(Train) Batch 222 Loss : 5.304477214813232, 맞은 개수 : 1\nEpoch : 0, batch 223\n(Train) Batch 223 Loss : 5.318670272827148, 맞은 개수 : 2\nEpoch : 0, batch 224\n(Train) Batch 224 Loss : 5.294919490814209, 맞은 개수 : 1\nEpoch : 0, batch 225\n(Train) Batch 225 Loss : 5.243404388427734, 맞은 개수 : 2\nEpoch : 0, batch 226\n(Train) Batch 226 Loss : 5.321990489959717, 맞은 개수 : 0\nEpoch : 0, batch 227\n(Train) Batch 227 Loss : 5.324706554412842, 맞은 개수 : 0\nEpoch : 0, batch 228\n(Train) Batch 228 Loss : 5.349634170532227, 맞은 개수 : 0\nEpoch : 0, batch 229\n(Train) Batch 229 Loss : 5.320699691772461, 맞은 개수 : 1\nEpoch : 0, batch 230\n(Train) Batch 230 Loss : 5.301280498504639, 맞은 개수 : 3\nEpoch : 0, batch 231\n(Train) Batch 231 Loss : 5.298919677734375, 맞은 개수 : 0\nEpoch : 0, batch 232\n(Train) Batch 232 Loss : 5.326830863952637, 맞은 개수 : 1\nEpoch : 0, batch 233\n(Train) Batch 233 Loss : 5.333221435546875, 맞은 개수 : 0\nEpoch : 0, batch 234\n(Train) Batch 234 Loss : 5.31451416015625, 맞은 개수 : 0\nEpoch : 0, batch 235\n(Train) Batch 235 Loss : 5.333870887756348, 맞은 개수 : 0\nEpoch : 0, batch 236\n(Train) Batch 236 Loss : 5.333499431610107, 맞은 개수 : 1\nEpoch : 0, batch 237\n(Train) Batch 237 Loss : 5.285765171051025, 맞은 개수 : 3\nEpoch : 0, batch 238\n(Train) Batch 238 Loss : 5.2614216804504395, 맞은 개수 : 0\nEpoch : 0, batch 239\n(Train) Batch 239 Loss : 5.291079044342041, 맞은 개수 : 0\nEpoch : 0, batch 240\n(Train) Batch 240 Loss : 5.3176116943359375, 맞은 개수 : 0\nEpoch : 0, batch 241\n(Train) Batch 241 Loss : 5.2296366691589355, 맞은 개수 : 3\nEpoch : 0, batch 242\n(Train) Batch 242 Loss : 5.258029460906982, 맞은 개수 : 2\nEpoch : 0, batch 243\n(Train) Batch 243 Loss : 5.288463592529297, 맞은 개수 : 1\nEpoch : 0, batch 244\n(Train) Batch 244 Loss : 5.290678024291992, 맞은 개수 : 0\nEpoch : 0, batch 245\n(Train) Batch 245 Loss : 5.305225372314453, 맞은 개수 : 2\nEpoch : 0, batch 246\n(Train) Batch 246 Loss : 5.29489278793335, 맞은 개수 : 1\nEpoch : 0, batch 247\n(Train) Batch 247 Loss : 5.302708148956299, 맞은 개수 : 0\nEpoch : 0, batch 248\n(Train) Batch 248 Loss : 5.358961582183838, 맞은 개수 : 1\nEpoch : 0, batch 249\n(Train) Batch 249 Loss : 5.2873358726501465, 맞은 개수 : 2\nEpoch : 0, batch 250\n(Train) Batch 250 Loss : 5.3031158447265625, 맞은 개수 : 3\nEpoch : 0, batch 251\n(Train) Batch 251 Loss : 5.242675304412842, 맞은 개수 : 1\nEpoch : 0, batch 252\n(Train) Batch 252 Loss : 5.283951759338379, 맞은 개수 : 1\nEpoch : 0, batch 253\n(Train) Batch 253 Loss : 5.273850440979004, 맞은 개수 : 2\nEpoch : 0, batch 254\n(Train) Batch 254 Loss : 5.239251613616943, 맞은 개수 : 0\nEpoch : 0, batch 255\n(Train) Batch 255 Loss : 5.329060077667236, 맞은 개수 : 1\nEpoch : 0, batch 256\n(Train) Batch 256 Loss : 5.275345325469971, 맞은 개수 : 3\nEpoch : 0, batch 257\n(Train) Batch 257 Loss : 5.325050354003906, 맞은 개수 : 0\nEpoch : 0, batch 258\n(Train) Batch 258 Loss : 5.260985851287842, 맞은 개수 : 1\nEpoch : 0, batch 259\n(Train) Batch 259 Loss : 5.281891822814941, 맞은 개수 : 1\nEpoch : 0, batch 260\n(Train) Batch 260 Loss : 5.237648963928223, 맞은 개수 : 0\nEpoch : 0, batch 261\n(Train) Batch 261 Loss : 5.220395565032959, 맞은 개수 : 1\nEpoch : 0, batch 262\n(Train) Batch 262 Loss : 5.292970180511475, 맞은 개수 : 0\nEpoch : 0, batch 263\n(Train) Batch 263 Loss : 5.266419887542725, 맞은 개수 : 0\nEpoch : 0, batch 264\n(Train) Batch 264 Loss : 5.284580707550049, 맞은 개수 : 1\nEpoch : 0, batch 265\n(Train) Batch 265 Loss : 5.272511005401611, 맞은 개수 : 2\nEpoch : 0, batch 266\n(Train) Batch 266 Loss : 5.294478893280029, 맞은 개수 : 1\nEpoch : 0, batch 267\n(Train) Batch 267 Loss : 5.257821559906006, 맞은 개수 : 1\nEpoch : 0, batch 268\n(Train) Batch 268 Loss : 5.2815937995910645, 맞은 개수 : 1\nEpoch : 0, batch 269\n(Train) Batch 269 Loss : 5.2490763664245605, 맞은 개수 : 0\nEpoch : 0, batch 270\n(Train) Batch 270 Loss : 5.2657575607299805, 맞은 개수 : 1\nEpoch : 0, batch 271\n(Train) Batch 271 Loss : 5.304015636444092, 맞은 개수 : 1\nEpoch : 0, batch 272\n(Train) Batch 272 Loss : 5.218388080596924, 맞은 개수 : 1\nEpoch : 0, batch 273\n(Train) Batch 273 Loss : 5.195964336395264, 맞은 개수 : 1\nEpoch : 0, batch 274\n(Train) Batch 274 Loss : 5.264069557189941, 맞은 개수 : 2\nEpoch : 0, batch 275\n(Train) Batch 275 Loss : 5.238401412963867, 맞은 개수 : 2\nEpoch : 0, batch 276\n(Train) Batch 276 Loss : 5.300673961639404, 맞은 개수 : 1\nEpoch : 0, batch 277\n(Train) Batch 277 Loss : 5.273096084594727, 맞은 개수 : 1\nEpoch : 0, batch 278\n(Train) Batch 278 Loss : 5.2998366355896, 맞은 개수 : 2\nEpoch : 0, batch 279\n(Train) Batch 279 Loss : 5.267038345336914, 맞은 개수 : 1\nEpoch : 0, batch 280\n(Train) Batch 280 Loss : 5.259826183319092, 맞은 개수 : 3\nEpoch : 0, batch 281\n(Train) Batch 281 Loss : 5.207355499267578, 맞은 개수 : 2\nEpoch : 0, batch 282\n(Train) Batch 282 Loss : 5.241418838500977, 맞은 개수 : 0\nEpoch : 0, batch 283\n(Train) Batch 283 Loss : 5.146239280700684, 맞은 개수 : 3\nEpoch : 0, batch 284\n(Train) Batch 284 Loss : 5.249122619628906, 맞은 개수 : 1\nEpoch : 0, batch 285\n(Train) Batch 285 Loss : 5.254032611846924, 맞은 개수 : 0\nEpoch : 0, batch 286\n(Train) Batch 286 Loss : 5.210073947906494, 맞은 개수 : 2\nEpoch : 0, batch 287\n(Train) Batch 287 Loss : 5.220774173736572, 맞은 개수 : 3\nEpoch : 0, batch 288\n(Train) Batch 288 Loss : 5.291208267211914, 맞은 개수 : 1\nEpoch : 0, batch 289\n(Train) Batch 289 Loss : 5.1990580558776855, 맞은 개수 : 2\nEpoch : 0, batch 290\n(Train) Batch 290 Loss : 5.217228889465332, 맞은 개수 : 2\nEpoch : 0, batch 291\n(Train) Batch 291 Loss : 5.196162700653076, 맞은 개수 : 0\nEpoch : 0, batch 292\n(Train) Batch 292 Loss : 5.198800086975098, 맞은 개수 : 1\nEpoch : 0, batch 293\n(Train) Batch 293 Loss : 5.2882771492004395, 맞은 개수 : 1\nEpoch : 0, batch 294\n(Train) Batch 294 Loss : 5.163407802581787, 맞은 개수 : 3\nEpoch : 0, batch 295\n(Train) Batch 295 Loss : 5.245553970336914, 맞은 개수 : 0\nEpoch : 0, batch 296\n(Train) Batch 296 Loss : 5.148327827453613, 맞은 개수 : 1\nEpoch : 0, batch 297\n(Train) Batch 297 Loss : 5.169419288635254, 맞은 개수 : 1\nEpoch : 0, batch 298\n(Train) Batch 298 Loss : 5.192564010620117, 맞은 개수 : 1\nEpoch : 0, batch 299\n(Train) Batch 299 Loss : 5.183448314666748, 맞은 개수 : 1\nEpoch : 0, batch 300\n(Train) Batch 300 Loss : 5.282643795013428, 맞은 개수 : 1\nEpoch : 0, batch 301\n(Train) Batch 301 Loss : 5.231761455535889, 맞은 개수 : 1\nEpoch : 0, batch 302\n(Train) Batch 302 Loss : 5.215520858764648, 맞은 개수 : 2\nEpoch : 0, batch 303\n(Train) Batch 303 Loss : 5.286147117614746, 맞은 개수 : 3\nEpoch : 0, batch 304\n(Train) Batch 304 Loss : 5.252302646636963, 맞은 개수 : 0\nEpoch : 0, batch 305\n(Train) Batch 305 Loss : 5.2919793128967285, 맞은 개수 : 1\nEpoch : 0, batch 306\n(Train) Batch 306 Loss : 5.175694465637207, 맞은 개수 : 0\nEpoch : 0, batch 307\n(Train) Batch 307 Loss : 5.25986385345459, 맞은 개수 : 2\nEpoch : 0, batch 308\n(Train) Batch 308 Loss : 5.248953819274902, 맞은 개수 : 2\nEpoch : 0, batch 309\n(Train) Batch 309 Loss : 5.253384590148926, 맞은 개수 : 0\nEpoch : 0, batch 310\n(Train) Batch 310 Loss : 5.160303592681885, 맞은 개수 : 0\nEpoch : 0, batch 311\n(Train) Batch 311 Loss : 5.174511432647705, 맞은 개수 : 0\nEpoch : 0, batch 312\n(Train) Batch 312 Loss : 5.275818824768066, 맞은 개수 : 0\nEpoch : 0, batch 313\n(Train) Batch 313 Loss : 5.169538497924805, 맞은 개수 : 1\nEpoch : 0, batch 314\n(Train) Batch 314 Loss : 5.176680088043213, 맞은 개수 : 0\nEpoch : 0, batch 315\n(Train) Batch 315 Loss : 5.1951584815979, 맞은 개수 : 1\nEpoch : 0, batch 316\n(Train) Batch 316 Loss : 5.221306324005127, 맞은 개수 : 3\nEpoch : 0, batch 317\n(Train) Batch 317 Loss : 5.319553852081299, 맞은 개수 : 2\nEpoch : 0, batch 318\n(Train) Batch 318 Loss : 5.19861364364624, 맞은 개수 : 1\nEpoch : 0, batch 319\n(Train) Batch 319 Loss : 5.303884506225586, 맞은 개수 : 0\nEpoch : 0, batch 320\n(Train) Batch 320 Loss : 5.196797847747803, 맞은 개수 : 2\nEpoch : 0, batch 321\n(Train) Batch 321 Loss : 5.233586311340332, 맞은 개수 : 0\nEpoch : 0, batch 322\n(Train) Batch 322 Loss : 5.214173793792725, 맞은 개수 : 1\nEpoch : 0, batch 323\n(Train) Batch 323 Loss : 5.191414833068848, 맞은 개수 : 0\nEpoch : 0, batch 324\n(Train) Batch 324 Loss : 5.2155232429504395, 맞은 개수 : 2\nEpoch : 0, batch 325\n(Train) Batch 325 Loss : 5.208508014678955, 맞은 개수 : 1\nEpoch : 0, batch 326\n(Train) Batch 326 Loss : 5.271021842956543, 맞은 개수 : 2\nEpoch : 0, batch 327\n(Train) Batch 327 Loss : 5.2401885986328125, 맞은 개수 : 1\nEpoch : 0, batch 328\n(Train) Batch 328 Loss : 5.218626976013184, 맞은 개수 : 0\nEpoch : 0, batch 329\n(Train) Batch 329 Loss : 5.21451997756958, 맞은 개수 : 2\nEpoch : 0, batch 330\n(Train) Batch 330 Loss : 5.137745380401611, 맞은 개수 : 0\nEpoch : 0, batch 331\n(Train) Batch 331 Loss : 5.178962230682373, 맞은 개수 : 1\nEpoch : 0, batch 332\n(Train) Batch 332 Loss : 5.196102619171143, 맞은 개수 : 0\nEpoch : 0, batch 333\n(Train) Batch 333 Loss : 5.174273490905762, 맞은 개수 : 1\nEpoch : 0, batch 334\n(Train) Batch 334 Loss : 5.194706916809082, 맞은 개수 : 1\nEpoch : 0, batch 335\n(Train) Batch 335 Loss : 5.21950101852417, 맞은 개수 : 1\nEpoch : 0, batch 336\n(Train) Batch 336 Loss : 5.1861252784729, 맞은 개수 : 4\nEpoch : 0, batch 337\n(Train) Batch 337 Loss : 5.26749324798584, 맞은 개수 : 1\nEpoch : 0, batch 338\n(Train) Batch 338 Loss : 5.244602680206299, 맞은 개수 : 2\nEpoch : 0, batch 339\n(Train) Batch 339 Loss : 5.132620811462402, 맞은 개수 : 2\nEpoch : 0, batch 340\n(Train) Batch 340 Loss : 5.2237396240234375, 맞은 개수 : 1\nEpoch : 0, batch 341\n(Train) Batch 341 Loss : 5.107566833496094, 맞은 개수 : 2\nEpoch : 0, batch 342\n(Train) Batch 342 Loss : 5.119329929351807, 맞은 개수 : 1\nEpoch : 0, batch 343\n(Train) Batch 343 Loss : 5.224451541900635, 맞은 개수 : 1\nEpoch : 0, batch 344\n(Train) Batch 344 Loss : 5.201086044311523, 맞은 개수 : 0\nEpoch : 0, batch 345\n(Train) Batch 345 Loss : 5.202779769897461, 맞은 개수 : 1\nEpoch : 0, batch 346\n(Train) Batch 346 Loss : 5.259398937225342, 맞은 개수 : 2\nEpoch : 0, batch 347\n(Train) Batch 347 Loss : 5.205618381500244, 맞은 개수 : 4\nEpoch : 0, batch 348\n(Train) Batch 348 Loss : 5.179443836212158, 맞은 개수 : 0\nEpoch : 0, batch 349\n(Train) Batch 349 Loss : 5.235124588012695, 맞은 개수 : 1\nEpoch : 0, batch 350\n(Train) Batch 350 Loss : 5.18388557434082, 맞은 개수 : 2\nEpoch : 0, batch 351\n(Train) Batch 351 Loss : 5.2464776039123535, 맞은 개수 : 1\nEpoch : 0, batch 352\n(Train) Batch 352 Loss : 5.165343761444092, 맞은 개수 : 2\nEpoch : 0, batch 353\n(Train) Batch 353 Loss : 5.242271900177002, 맞은 개수 : 0\nEpoch : 0, batch 354\n(Train) Batch 354 Loss : 5.190332889556885, 맞은 개수 : 0\nEpoch : 0, batch 355\n(Train) Batch 355 Loss : 5.2121500968933105, 맞은 개수 : 0\nEpoch : 0, batch 356\n(Train) Batch 356 Loss : 5.138610363006592, 맞은 개수 : 2\nEpoch : 0, batch 357\n(Train) Batch 357 Loss : 5.156254768371582, 맞은 개수 : 2\nEpoch : 0, batch 358\n(Train) Batch 358 Loss : 5.158669471740723, 맞은 개수 : 2\nEpoch : 0, batch 359\n(Train) Batch 359 Loss : 5.189346790313721, 맞은 개수 : 1\nEpoch : 0, batch 360\n(Train) Batch 360 Loss : 5.200218200683594, 맞은 개수 : 2\nEpoch : 0, batch 361\n(Train) Batch 361 Loss : 5.194117069244385, 맞은 개수 : 1\nEpoch : 0, batch 362\n(Train) Batch 362 Loss : 5.222799777984619, 맞은 개수 : 0\nEpoch : 0, batch 363\n(Train) Batch 363 Loss : 5.1361775398254395, 맞은 개수 : 0\nEpoch : 0, batch 364\n(Train) Batch 364 Loss : 5.132511615753174, 맞은 개수 : 2\nEpoch : 0, batch 365\n(Train) Batch 365 Loss : 5.2372026443481445, 맞은 개수 : 1\nEpoch : 0, batch 366\n(Train) Batch 366 Loss : 5.24979829788208, 맞은 개수 : 0\nEpoch : 0, batch 367\n(Train) Batch 367 Loss : 5.202479839324951, 맞은 개수 : 1\nEpoch : 0, batch 368\n(Train) Batch 368 Loss : 5.100356578826904, 맞은 개수 : 3\nEpoch : 0, batch 369\n(Train) Batch 369 Loss : 5.17751407623291, 맞은 개수 : 6\nEpoch : 0, batch 370\n(Train) Batch 370 Loss : 5.217438697814941, 맞은 개수 : 0\nEpoch : 0, batch 371\n(Train) Batch 371 Loss : 5.189693450927734, 맞은 개수 : 1\nEpoch : 0, batch 372\n(Train) Batch 372 Loss : 5.265493869781494, 맞은 개수 : 2\nEpoch : 0, batch 373\n(Train) Batch 373 Loss : 5.147439956665039, 맞은 개수 : 1\nEpoch : 0, batch 374\n(Train) Batch 374 Loss : 5.17314338684082, 맞은 개수 : 1\nEpoch : 0, batch 375\n(Train) Batch 375 Loss : 5.149872779846191, 맞은 개수 : 3\nEpoch : 0, batch 376\n(Train) Batch 376 Loss : 5.146082401275635, 맞은 개수 : 2\nEpoch : 0, batch 377\n(Train) Batch 377 Loss : 5.275738716125488, 맞은 개수 : 2\nEpoch : 0, batch 378\n(Train) Batch 378 Loss : 5.177761554718018, 맞은 개수 : 1\nEpoch : 0, batch 379\n(Train) Batch 379 Loss : 5.112618923187256, 맞은 개수 : 0\nEpoch : 0, batch 380\n(Train) Batch 380 Loss : 5.1546630859375, 맞은 개수 : 3\nEpoch : 0, batch 381\n(Train) Batch 381 Loss : 5.21828556060791, 맞은 개수 : 1\nEpoch : 0, batch 382\n(Train) Batch 382 Loss : 5.211453437805176, 맞은 개수 : 1\nEpoch : 0, batch 383\n(Train) Batch 383 Loss : 5.137509346008301, 맞은 개수 : 1\nEpoch : 0, batch 384\n(Train) Batch 384 Loss : 5.187103271484375, 맞은 개수 : 1\nEpoch : 0, batch 385\n(Train) Batch 385 Loss : 5.215342044830322, 맞은 개수 : 3\nEpoch : 0, batch 386\n(Train) Batch 386 Loss : 5.190576076507568, 맞은 개수 : 1\nEpoch : 0, batch 387\n(Train) Batch 387 Loss : 5.140425682067871, 맞은 개수 : 5\nEpoch : 0, batch 388\n(Train) Batch 388 Loss : 5.163251876831055, 맞은 개수 : 6\nEpoch : 0, batch 389\n(Train) Batch 389 Loss : 5.15598201751709, 맞은 개수 : 2\nEpoch : 0, batch 390\n(Train) Batch 390 Loss : 5.252060890197754, 맞은 개수 : 0\nEpoch : 0, batch 391\n(Train) Batch 391 Loss : 5.230677127838135, 맞은 개수 : 0\nEpoch : 0, batch 392\n(Train) Batch 392 Loss : 5.2088823318481445, 맞은 개수 : 0\nEpoch : 0, batch 393\n(Train) Batch 393 Loss : 5.278390407562256, 맞은 개수 : 1\nEpoch : 0, batch 394\n(Train) Batch 394 Loss : 5.172718048095703, 맞은 개수 : 1\nEpoch : 0, batch 395\n(Train) Batch 395 Loss : 5.159453868865967, 맞은 개수 : 3\nEpoch : 0, batch 396\n(Train) Batch 396 Loss : 5.0805535316467285, 맞은 개수 : 1\nEpoch : 0, batch 397\n(Train) Batch 397 Loss : 5.245410442352295, 맞은 개수 : 1\nEpoch : 0, batch 398\n(Train) Batch 398 Loss : 5.1824493408203125, 맞은 개수 : 2\nEpoch : 0, batch 399\n(Train) Batch 399 Loss : 5.269239902496338, 맞은 개수 : 1\nEpoch : 0, batch 400\n(Train) Batch 400 Loss : 5.179002285003662, 맞은 개수 : 1\nEpoch : 0, batch 401\n(Train) Batch 401 Loss : 5.229918956756592, 맞은 개수 : 1\nEpoch : 0, batch 402\n(Train) Batch 402 Loss : 5.2198309898376465, 맞은 개수 : 0\nEpoch : 0, batch 403\n(Train) Batch 403 Loss : 5.131080150604248, 맞은 개수 : 1\nEpoch : 0, batch 404\n(Train) Batch 404 Loss : 5.178518772125244, 맞은 개수 : 1\nEpoch : 0, batch 405\n(Train) Batch 405 Loss : 5.105010986328125, 맞은 개수 : 2\nEpoch : 0, batch 406\n(Train) Batch 406 Loss : 5.2049760818481445, 맞은 개수 : 0\nEpoch : 0, batch 407\n(Train) Batch 407 Loss : 5.167862892150879, 맞은 개수 : 1\nEpoch : 0, batch 408\n(Train) Batch 408 Loss : 5.27318811416626, 맞은 개수 : 1\nEpoch : 0, batch 409\n(Train) Batch 409 Loss : 5.266414165496826, 맞은 개수 : 0\nEpoch : 0, batch 410\n(Train) Batch 410 Loss : 5.189176559448242, 맞은 개수 : 0\nEpoch : 0, batch 411\n(Train) Batch 411 Loss : 5.141226291656494, 맞은 개수 : 1\nEpoch : 0, batch 412\n(Train) Batch 412 Loss : 5.1873087882995605, 맞은 개수 : 2\nEpoch : 0, batch 413\n(Train) Batch 413 Loss : 5.225002288818359, 맞은 개수 : 0\nEpoch : 0, batch 414\n(Train) Batch 414 Loss : 5.200476169586182, 맞은 개수 : 2\nEpoch : 0, batch 415\n(Train) Batch 415 Loss : 5.221316814422607, 맞은 개수 : 0\nEpoch : 0, batch 416\n(Train) Batch 416 Loss : 5.150699615478516, 맞은 개수 : 1\nEpoch : 0, batch 417\n(Train) Batch 417 Loss : 5.195684909820557, 맞은 개수 : 3\nEpoch : 0, batch 418\n(Train) Batch 418 Loss : 5.232848167419434, 맞은 개수 : 2\nEpoch : 0, batch 419\n(Train) Batch 419 Loss : 5.121639728546143, 맞은 개수 : 4\nEpoch : 0, batch 420\n(Train) Batch 420 Loss : 5.168319225311279, 맞은 개수 : 2\nEpoch : 0, batch 421\n(Train) Batch 421 Loss : 5.179529666900635, 맞은 개수 : 1\nEpoch : 0, batch 422\n(Train) Batch 422 Loss : 5.137844085693359, 맞은 개수 : 1\nEpoch : 0, batch 423\n(Train) Batch 423 Loss : 5.139016151428223, 맞은 개수 : 1\nEpoch : 0, batch 424\n(Train) Batch 424 Loss : 5.179258346557617, 맞은 개수 : 0\nEpoch : 0, batch 425\n(Train) Batch 425 Loss : 5.1224870681762695, 맞은 개수 : 1\nEpoch : 0, batch 426\n(Train) Batch 426 Loss : 5.058620452880859, 맞은 개수 : 1\nEpoch : 0, batch 427\n(Train) Batch 427 Loss : 5.175546169281006, 맞은 개수 : 2\nEpoch : 0, batch 428\n(Train) Batch 428 Loss : 5.128211975097656, 맞은 개수 : 0\nEpoch : 0, batch 429\n(Train) Batch 429 Loss : 5.224081516265869, 맞은 개수 : 1\nEpoch : 0, batch 430\n(Train) Batch 430 Loss : 5.1903581619262695, 맞은 개수 : 0\nEpoch : 0, batch 431\n(Train) Batch 431 Loss : 5.08656120300293, 맞은 개수 : 3\nEpoch : 0, batch 432\n(Train) Batch 432 Loss : 5.10184383392334, 맞은 개수 : 2\nEpoch : 0, batch 433\n(Train) Batch 433 Loss : 5.110187530517578, 맞은 개수 : 1\nEpoch : 0, batch 434\n(Train) Batch 434 Loss : 5.049910068511963, 맞은 개수 : 2\nEpoch : 0, batch 435\n(Train) Batch 435 Loss : 5.217525482177734, 맞은 개수 : 1\nEpoch : 0, batch 436\n(Train) Batch 436 Loss : 5.1425862312316895, 맞은 개수 : 0\nEpoch : 0, batch 437\n(Train) Batch 437 Loss : 5.09521484375, 맞은 개수 : 0\nEpoch : 0, batch 438\n(Train) Batch 438 Loss : 5.22901725769043, 맞은 개수 : 2\nEpoch : 0, batch 439\n(Train) Batch 439 Loss : 5.232034206390381, 맞은 개수 : 1\nEpoch : 0, batch 440\n(Train) Batch 440 Loss : 5.144633769989014, 맞은 개수 : 3\nEpoch : 0, batch 441\n(Train) Batch 441 Loss : 5.14989709854126, 맞은 개수 : 1\nEpoch : 0, batch 442\n(Train) Batch 442 Loss : 5.139383316040039, 맞은 개수 : 1\nEpoch : 0, batch 443\n(Train) Batch 443 Loss : 5.069500923156738, 맞은 개수 : 0\nEpoch : 0, batch 444\n(Train) Batch 444 Loss : 5.174090385437012, 맞은 개수 : 0\nEpoch : 0, batch 445\n(Train) Batch 445 Loss : 5.190952301025391, 맞은 개수 : 0\nEpoch : 0, batch 446\n(Train) Batch 446 Loss : 5.141451835632324, 맞은 개수 : 2\nEpoch : 0, batch 447\n(Train) Batch 447 Loss : 5.07064962387085, 맞은 개수 : 1\nEpoch : 0, batch 448\n(Train) Batch 448 Loss : 5.153870105743408, 맞은 개수 : 1\nEpoch : 0, batch 449\n(Train) Batch 449 Loss : 5.216222763061523, 맞은 개수 : 1\nEpoch : 0, batch 450\n(Train) Batch 450 Loss : 5.173070430755615, 맞은 개수 : 2\nEpoch : 0, batch 451\n(Train) Batch 451 Loss : 5.2477850914001465, 맞은 개수 : 0\nEpoch : 0, batch 452\n(Train) Batch 452 Loss : 5.280214309692383, 맞은 개수 : 0\nEpoch : 0, batch 453\n(Train) Batch 453 Loss : 5.187613010406494, 맞은 개수 : 0\nEpoch : 0, batch 454\n(Train) Batch 454 Loss : 5.21251106262207, 맞은 개수 : 1\nEpoch : 0, batch 455\n(Train) Batch 455 Loss : 5.177212238311768, 맞은 개수 : 3\nEpoch : 0, batch 456\n(Train) Batch 456 Loss : 5.16963005065918, 맞은 개수 : 1\nEpoch : 0, batch 457\n(Train) Batch 457 Loss : 5.131698131561279, 맞은 개수 : 3\nEpoch : 0, batch 458\n(Train) Batch 458 Loss : 5.122422218322754, 맞은 개수 : 0\nEpoch : 0, batch 459\n(Train) Batch 459 Loss : 5.128798961639404, 맞은 개수 : 2\nEpoch : 0, batch 460\n(Train) Batch 460 Loss : 5.213481903076172, 맞은 개수 : 1\nEpoch : 0, batch 461\n(Train) Batch 461 Loss : 5.14858865737915, 맞은 개수 : 3\nEpoch : 0, batch 462\n(Train) Batch 462 Loss : 5.139172077178955, 맞은 개수 : 1\nEpoch : 0, batch 463\n(Train) Batch 463 Loss : 5.10597038269043, 맞은 개수 : 1\nEpoch : 0, batch 464\n(Train) Batch 464 Loss : 5.091787338256836, 맞은 개수 : 1\nEpoch : 0, batch 465\n(Train) Batch 465 Loss : 5.099804878234863, 맞은 개수 : 1\nEpoch : 0, batch 466\n(Train) Batch 466 Loss : 5.176746368408203, 맞은 개수 : 0\nEpoch : 0, batch 467\n(Train) Batch 467 Loss : 5.142717361450195, 맞은 개수 : 4\nEpoch : 0, batch 468\n(Train) Batch 468 Loss : 5.125973224639893, 맞은 개수 : 1\nEpoch : 0, batch 469\n(Train) Batch 469 Loss : 5.217039585113525, 맞은 개수 : 1\nEpoch : 0, batch 470\n(Train) Batch 470 Loss : 5.226966857910156, 맞은 개수 : 0\nEpoch : 0, batch 471\n(Train) Batch 471 Loss : 5.050499439239502, 맞은 개수 : 2\nEpoch : 0, batch 472\n(Train) Batch 472 Loss : 5.171012878417969, 맞은 개수 : 2\nEpoch : 0, batch 473\n(Train) Batch 473 Loss : 5.19473123550415, 맞은 개수 : 1\nEpoch : 0, batch 474\n(Train) Batch 474 Loss : 5.162530899047852, 맞은 개수 : 1\nEpoch : 0, batch 475\n(Train) Batch 475 Loss : 5.241361141204834, 맞은 개수 : 1\nEpoch : 0, batch 476\n(Train) Batch 476 Loss : 5.120198726654053, 맞은 개수 : 3\nEpoch : 0, batch 477\n(Train) Batch 477 Loss : 5.170837879180908, 맞은 개수 : 2\nEpoch : 0, batch 478\n(Train) Batch 478 Loss : 5.068944454193115, 맞은 개수 : 1\nEpoch : 0, batch 479\n(Train) Batch 479 Loss : 5.162822723388672, 맞은 개수 : 1\nEpoch : 0, batch 480\n(Train) Batch 480 Loss : 5.1491923332214355, 맞은 개수 : 4\nEpoch : 0, batch 481\n(Train) Batch 481 Loss : 5.183592319488525, 맞은 개수 : 2\nEpoch : 0, batch 482\n(Train) Batch 482 Loss : 5.109683513641357, 맞은 개수 : 1\nEpoch : 0, batch 483\n(Train) Batch 483 Loss : 5.125502109527588, 맞은 개수 : 3\nEpoch : 0, batch 484\n(Train) Batch 484 Loss : 5.0762224197387695, 맞은 개수 : 2\nEpoch : 0, batch 485\n(Train) Batch 485 Loss : 5.261363983154297, 맞은 개수 : 4\nEpoch : 0, batch 486\n(Train) Batch 486 Loss : 5.173330307006836, 맞은 개수 : 1\nEpoch : 0, batch 487\n(Train) Batch 487 Loss : 5.110289096832275, 맞은 개수 : 3\nEpoch : 0, batch 488\n(Train) Batch 488 Loss : 5.2147016525268555, 맞은 개수 : 3\nEpoch : 0, batch 489\n(Train) Batch 489 Loss : 5.138161659240723, 맞은 개수 : 0\nEpoch : 0, batch 490\n(Train) Batch 490 Loss : 5.104063987731934, 맞은 개수 : 2\nEpoch : 0, batch 491\n(Train) Batch 491 Loss : 5.115958213806152, 맞은 개수 : 2\nEpoch : 0, batch 492\n(Train) Batch 492 Loss : 5.20829963684082, 맞은 개수 : 2\nEpoch : 0, batch 493\n(Train) Batch 493 Loss : 5.146421909332275, 맞은 개수 : 2\nEpoch : 0, batch 494\n(Train) Batch 494 Loss : 5.161889553070068, 맞은 개수 : 4\nEpoch : 0, batch 495\n(Train) Batch 495 Loss : 5.077866077423096, 맞은 개수 : 3\nEpoch : 0, batch 496\n(Train) Batch 496 Loss : 5.135223388671875, 맞은 개수 : 4\nEpoch : 0, batch 497\n(Train) Batch 497 Loss : 5.1171417236328125, 맞은 개수 : 2\nEpoch : 0, batch 498\n(Train) Batch 498 Loss : 5.1606621742248535, 맞은 개수 : 1\nEpoch : 0, batch 499\n(Train) Batch 499 Loss : 5.147312164306641, 맞은 개수 : 0\nEpoch : 0, batch 500\n(Train) Batch 500 Loss : 5.159409523010254, 맞은 개수 : 1\nEpoch : 0, batch 501\n(Train) Batch 501 Loss : 5.164257526397705, 맞은 개수 : 1\nEpoch : 0, batch 502\n(Train) Batch 502 Loss : 5.112813472747803, 맞은 개수 : 0\nEpoch : 0, batch 503\n(Train) Batch 503 Loss : 5.125522136688232, 맞은 개수 : 2\nEpoch : 0, batch 504\n(Train) Batch 504 Loss : 5.098186016082764, 맞은 개수 : 4\nEpoch : 0, batch 505\n(Train) Batch 505 Loss : 5.120030879974365, 맞은 개수 : 3\nEpoch : 0, batch 506\n(Train) Batch 506 Loss : 5.158294677734375, 맞은 개수 : 1\nEpoch : 0, batch 507\n(Train) Batch 507 Loss : 5.127513408660889, 맞은 개수 : 2\nEpoch : 0, batch 508\n(Train) Batch 508 Loss : 5.128769397735596, 맞은 개수 : 1\nEpoch : 0, batch 509\n(Train) Batch 509 Loss : 5.072903633117676, 맞은 개수 : 2\nEpoch : 0, batch 510\n(Train) Batch 510 Loss : 5.19581413269043, 맞은 개수 : 1\nEpoch : 0, batch 511\n(Train) Batch 511 Loss : 5.178890228271484, 맞은 개수 : 3\nEpoch : 0, batch 512\n(Train) Batch 512 Loss : 5.18285608291626, 맞은 개수 : 0\nEpoch : 0, batch 513\n(Train) Batch 513 Loss : 5.14998197555542, 맞은 개수 : 3\nEpoch : 0, batch 514\n(Train) Batch 514 Loss : 5.091492176055908, 맞은 개수 : 0\nEpoch : 0, batch 515\n(Train) Batch 515 Loss : 5.086672782897949, 맞은 개수 : 1\nEpoch : 0, batch 516\n(Train) Batch 516 Loss : 5.102671146392822, 맞은 개수 : 1\nEpoch : 0, batch 517\n(Train) Batch 517 Loss : 5.151272773742676, 맞은 개수 : 2\nEpoch : 0, batch 518\n(Train) Batch 518 Loss : 5.090660095214844, 맞은 개수 : 2\nEpoch : 0, batch 519\n(Train) Batch 519 Loss : 5.113041877746582, 맞은 개수 : 1\nEpoch : 0, batch 520\n(Train) Batch 520 Loss : 5.062291145324707, 맞은 개수 : 2\nEpoch : 0, batch 521\n(Train) Batch 521 Loss : 5.057635307312012, 맞은 개수 : 3\nEpoch : 0, batch 522\n(Train) Batch 522 Loss : 5.0308613777160645, 맞은 개수 : 2\nEpoch : 0, batch 523\n(Train) Batch 523 Loss : 5.154976844787598, 맞은 개수 : 2\nEpoch : 0, batch 524\n(Train) Batch 524 Loss : 5.1525044441223145, 맞은 개수 : 4\nEpoch : 0, batch 525\n(Train) Batch 525 Loss : 5.205955982208252, 맞은 개수 : 1\nEpoch : 0, batch 526\n(Train) Batch 526 Loss : 5.157235145568848, 맞은 개수 : 4\nEpoch : 0, batch 527\n(Train) Batch 527 Loss : 5.077891826629639, 맞은 개수 : 2\nEpoch : 0, batch 528\n(Train) Batch 528 Loss : 5.138069152832031, 맞은 개수 : 0\nEpoch : 0, batch 529\n(Train) Batch 529 Loss : 5.215089321136475, 맞은 개수 : 1\nEpoch : 0, batch 530\n(Train) Batch 530 Loss : 5.133890628814697, 맞은 개수 : 2\nEpoch : 0, batch 531\n(Train) Batch 531 Loss : 5.140168190002441, 맞은 개수 : 3\nEpoch : 0, batch 532\n(Train) Batch 532 Loss : 5.154160499572754, 맞은 개수 : 1\nEpoch : 0, batch 533\n(Train) Batch 533 Loss : 5.1288676261901855, 맞은 개수 : 4\nEpoch : 0, batch 534\n(Train) Batch 534 Loss : 5.133664131164551, 맞은 개수 : 2\nEpoch : 0, batch 535\n(Train) Batch 535 Loss : 5.1031270027160645, 맞은 개수 : 2\nEpoch : 0, batch 536\n(Train) Batch 536 Loss : 5.164240837097168, 맞은 개수 : 2\nEpoch : 0, batch 537\n(Train) Batch 537 Loss : 5.135260581970215, 맞은 개수 : 2\nEpoch : 0, batch 538\n(Train) Batch 538 Loss : 5.235809326171875, 맞은 개수 : 1\nEpoch : 0, batch 539\n(Train) Batch 539 Loss : 5.114213466644287, 맞은 개수 : 1\nEpoch : 0, batch 540\n(Train) Batch 540 Loss : 5.1589508056640625, 맞은 개수 : 3\nEpoch : 0, batch 541\n(Train) Batch 541 Loss : 5.128343105316162, 맞은 개수 : 3\nEpoch : 0, batch 542\n(Train) Batch 542 Loss : 5.097065448760986, 맞은 개수 : 1\nEpoch : 0, batch 543\n(Train) Batch 543 Loss : 5.1002960205078125, 맞은 개수 : 1\nEpoch : 0, batch 544\n(Train) Batch 544 Loss : 5.1652092933654785, 맞은 개수 : 3\nEpoch : 0, batch 545\n(Train) Batch 545 Loss : 5.132890701293945, 맞은 개수 : 3\nEpoch : 0, batch 546\n(Train) Batch 546 Loss : 5.069249629974365, 맞은 개수 : 2\nEpoch : 0, batch 547\n(Train) Batch 547 Loss : 5.096134662628174, 맞은 개수 : 3\nEpoch : 0, batch 548\n(Train) Batch 548 Loss : 5.159403324127197, 맞은 개수 : 1\nEpoch : 0, batch 549\n(Train) Batch 549 Loss : 5.2048659324646, 맞은 개수 : 1\nEpoch : 0, batch 550\n(Train) Batch 550 Loss : 5.132789611816406, 맞은 개수 : 0\nEpoch : 0, batch 551\n(Train) Batch 551 Loss : 5.1264729499816895, 맞은 개수 : 0\nEpoch : 0, batch 552\n(Train) Batch 552 Loss : 5.173480033874512, 맞은 개수 : 3\nEpoch : 0, batch 553\n(Train) Batch 553 Loss : 5.073827266693115, 맞은 개수 : 2\nEpoch : 0, batch 554\n(Train) Batch 554 Loss : 5.126616954803467, 맞은 개수 : 0\nEpoch : 0, batch 555\n(Train) Batch 555 Loss : 5.015588760375977, 맞은 개수 : 2\nEpoch : 0, batch 556\n(Train) Batch 556 Loss : 5.098439693450928, 맞은 개수 : 3\nEpoch : 0, batch 557\n(Train) Batch 557 Loss : 5.1595306396484375, 맞은 개수 : 0\nEpoch : 0, batch 558\n(Train) Batch 558 Loss : 5.190700054168701, 맞은 개수 : 1\nEpoch : 0, batch 559\n(Train) Batch 559 Loss : 5.152773857116699, 맞은 개수 : 2\nEpoch : 0, batch 560\n(Train) Batch 560 Loss : 5.1098432540893555, 맞은 개수 : 3\nEpoch : 0, batch 561\n(Train) Batch 561 Loss : 5.081993579864502, 맞은 개수 : 2\nEpoch : 0, batch 562\n(Train) Batch 562 Loss : 5.095180034637451, 맞은 개수 : 1\nEpoch : 0, batch 563\n(Train) Batch 563 Loss : 5.154299736022949, 맞은 개수 : 3\nEpoch : 0, batch 564\n(Train) Batch 564 Loss : 5.183907985687256, 맞은 개수 : 3\nEpoch : 0, batch 565\n(Train) Batch 565 Loss : 5.16439151763916, 맞은 개수 : 1\nEpoch : 0, batch 566\n(Train) Batch 566 Loss : 5.254978656768799, 맞은 개수 : 0\nEpoch : 0, batch 567\n(Train) Batch 567 Loss : 5.1271796226501465, 맞은 개수 : 5\nEpoch : 0, batch 568\n(Train) Batch 568 Loss : 4.925473213195801, 맞은 개수 : 4\nEpoch : 0, batch 569\n(Train) Batch 569 Loss : 5.211071491241455, 맞은 개수 : 1\nEpoch : 0, batch 570\n(Train) Batch 570 Loss : 5.117329120635986, 맞은 개수 : 4\nEpoch : 0, batch 571\n(Train) Batch 571 Loss : 5.077213764190674, 맞은 개수 : 5\nEpoch : 0, batch 572\n(Train) Batch 572 Loss : 4.973786354064941, 맞은 개수 : 4\nEpoch : 0, batch 573\n(Train) Batch 573 Loss : 5.189388275146484, 맞은 개수 : 1\nEpoch : 0, batch 574\n(Train) Batch 574 Loss : 5.211150169372559, 맞은 개수 : 3\nEpoch : 0, batch 575\n(Train) Batch 575 Loss : 5.117242336273193, 맞은 개수 : 4\nEpoch : 0, batch 576\n(Train) Batch 576 Loss : 5.044909954071045, 맞은 개수 : 4\nEpoch : 0, batch 577\n(Train) Batch 577 Loss : 5.1314897537231445, 맞은 개수 : 2\nEpoch : 0, batch 578\n(Train) Batch 578 Loss : 5.147861480712891, 맞은 개수 : 0\nEpoch : 0, batch 579\n(Train) Batch 579 Loss : 5.116164684295654, 맞은 개수 : 3\nEpoch : 0, batch 580\n(Train) Batch 580 Loss : 5.072533130645752, 맞은 개수 : 2\nEpoch : 0, batch 581\n(Train) Batch 581 Loss : 5.132546424865723, 맞은 개수 : 3\nEpoch : 0, batch 582\n(Train) Batch 582 Loss : 5.108523845672607, 맞은 개수 : 2\nEpoch : 0, batch 583\n(Train) Batch 583 Loss : 5.244551658630371, 맞은 개수 : 0\nEpoch : 0, batch 584\n(Train) Batch 584 Loss : 5.125694274902344, 맞은 개수 : 1\nEpoch : 0, batch 585\n(Train) Batch 585 Loss : 5.138500690460205, 맞은 개수 : 1\nEpoch : 0, batch 586\n(Train) Batch 586 Loss : 5.053755760192871, 맞은 개수 : 3\nEpoch : 0, batch 587\n(Train) Batch 587 Loss : 5.156135559082031, 맞은 개수 : 2\nEpoch : 0, batch 588\n(Train) Batch 588 Loss : 5.13050651550293, 맞은 개수 : 1\nEpoch : 0, batch 589\n(Train) Batch 589 Loss : 5.106862545013428, 맞은 개수 : 1\nEpoch : 0, batch 590\n(Train) Batch 590 Loss : 5.078387260437012, 맞은 개수 : 1\nEpoch : 0, batch 591\n(Train) Batch 591 Loss : 5.132217884063721, 맞은 개수 : 1\nEpoch : 0, batch 592\n(Train) Batch 592 Loss : 5.090112686157227, 맞은 개수 : 1\nEpoch : 0, batch 593\n(Train) Batch 593 Loss : 5.066592693328857, 맞은 개수 : 1\nEpoch : 0, batch 594\n(Train) Batch 594 Loss : 5.101693153381348, 맞은 개수 : 3\nEpoch : 0, batch 595\n(Train) Batch 595 Loss : 5.199952125549316, 맞은 개수 : 0\nEpoch : 0, batch 596\n(Train) Batch 596 Loss : 5.0600152015686035, 맞은 개수 : 3\nEpoch : 0, batch 597\n(Train) Batch 597 Loss : 5.135869979858398, 맞은 개수 : 2\nEpoch : 0, batch 598\n(Train) Batch 598 Loss : 5.0831193923950195, 맞은 개수 : 2\nEpoch : 0, batch 599\n(Train) Batch 599 Loss : 5.100086688995361, 맞은 개수 : 1\nEpoch : 0, batch 600\n(Train) Batch 600 Loss : 5.178111553192139, 맞은 개수 : 2\nEpoch : 0, batch 601\n(Train) Batch 601 Loss : 5.065310955047607, 맞은 개수 : 2\nEpoch : 0, batch 602\n(Train) Batch 602 Loss : 5.090835094451904, 맞은 개수 : 2\nEpoch : 0, batch 603\n(Train) Batch 603 Loss : 5.147469520568848, 맞은 개수 : 5\nEpoch : 0, batch 604\n(Train) Batch 604 Loss : 5.118422508239746, 맞은 개수 : 0\nEpoch : 0, batch 605\n(Train) Batch 605 Loss : 5.014242649078369, 맞은 개수 : 3\nEpoch : 0, batch 606\n(Train) Batch 606 Loss : 5.054698467254639, 맞은 개수 : 3\nEpoch : 0, batch 607\n(Train) Batch 607 Loss : 5.069952964782715, 맞은 개수 : 0\nEpoch : 0, batch 608\n(Train) Batch 608 Loss : 5.161794185638428, 맞은 개수 : 0\nEpoch : 0, batch 609\n(Train) Batch 609 Loss : 5.074658393859863, 맞은 개수 : 0\nEpoch : 0, batch 610\n(Train) Batch 610 Loss : 5.097046852111816, 맞은 개수 : 4\nEpoch : 0, batch 611\n(Train) Batch 611 Loss : 5.089657783508301, 맞은 개수 : 1\nEpoch : 0, batch 612\n(Train) Batch 612 Loss : 5.110536575317383, 맞은 개수 : 4\nEpoch : 0, batch 613\n(Train) Batch 613 Loss : 5.037737846374512, 맞은 개수 : 1\nEpoch : 0, batch 614\n(Train) Batch 614 Loss : 5.085742950439453, 맞은 개수 : 5\nEpoch : 0, batch 615\n(Train) Batch 615 Loss : 5.064055442810059, 맞은 개수 : 0\nEpoch : 0, batch 616\n(Train) Batch 616 Loss : 5.070970058441162, 맞은 개수 : 1\nEpoch : 0, batch 617\n(Train) Batch 617 Loss : 5.135051727294922, 맞은 개수 : 2\nEpoch : 0, batch 618\n(Train) Batch 618 Loss : 5.18658971786499, 맞은 개수 : 0\nEpoch : 0, batch 619\n(Train) Batch 619 Loss : 5.0450873374938965, 맞은 개수 : 4\nEpoch : 0, batch 620\n(Train) Batch 620 Loss : 5.046494007110596, 맞은 개수 : 3\nEpoch : 0, batch 621\n(Train) Batch 621 Loss : 5.1603827476501465, 맞은 개수 : 5\nEpoch : 0, batch 622\n(Train) Batch 622 Loss : 5.070486545562744, 맞은 개수 : 3\nEpoch : 0, batch 623\n(Train) Batch 623 Loss : 5.075272083282471, 맞은 개수 : 1\nEpoch : 0, batch 624\n(Train) Batch 624 Loss : 4.9418206214904785, 맞은 개수 : 4\nEpoch : 0, batch 625\n(Train) Batch 625 Loss : 5.144639015197754, 맞은 개수 : 1\nEpoch : 0, batch 626\n(Train) Batch 626 Loss : 4.9809088706970215, 맞은 개수 : 2\nEpoch : 0, batch 627\n(Train) Batch 627 Loss : 4.949430465698242, 맞은 개수 : 2\nEpoch : 0, batch 628\n(Train) Batch 628 Loss : 5.055196285247803, 맞은 개수 : 4\nEpoch : 0, batch 629\n(Train) Batch 629 Loss : 5.2663984298706055, 맞은 개수 : 2\nEpoch : 0, batch 630\n(Train) Batch 630 Loss : 5.083347320556641, 맞은 개수 : 1\nEpoch : 0, batch 631\n(Train) Batch 631 Loss : 5.02902364730835, 맞은 개수 : 4\nEpoch : 0, batch 632\n(Train) Batch 632 Loss : 5.1210479736328125, 맞은 개수 : 1\nEpoch : 0, batch 633\n(Train) Batch 633 Loss : 5.05638313293457, 맞은 개수 : 2\nEpoch : 0, batch 634\n(Train) Batch 634 Loss : 5.137038230895996, 맞은 개수 : 2\nEpoch : 0, batch 635\n(Train) Batch 635 Loss : 5.053955554962158, 맞은 개수 : 3\nEpoch : 0, batch 636\n(Train) Batch 636 Loss : 5.0352396965026855, 맞은 개수 : 3\nEpoch : 0, batch 637\n(Train) Batch 637 Loss : 5.084964752197266, 맞은 개수 : 3\nEpoch : 0, batch 638\n(Train) Batch 638 Loss : 5.026080131530762, 맞은 개수 : 1\nEpoch : 0, batch 639\n(Train) Batch 639 Loss : 4.954955577850342, 맞은 개수 : 3\nEpoch : 0, batch 640\n(Train) Batch 640 Loss : 5.209219932556152, 맞은 개수 : 1\nEpoch : 0, batch 641\n(Train) Batch 641 Loss : 5.075221061706543, 맞은 개수 : 3\nEpoch : 0, batch 642\n(Train) Batch 642 Loss : 5.057612419128418, 맞은 개수 : 2\nEpoch : 0, batch 643\n(Train) Batch 643 Loss : 5.080608367919922, 맞은 개수 : 1\nEpoch : 0, batch 644\n(Train) Batch 644 Loss : 5.091421127319336, 맞은 개수 : 0\nEpoch : 0, batch 645\n(Train) Batch 645 Loss : 5.064079284667969, 맞은 개수 : 4\nEpoch : 0, batch 646\n(Train) Batch 646 Loss : 5.015624046325684, 맞은 개수 : 7\nEpoch : 0, batch 647\n(Train) Batch 647 Loss : 4.938961505889893, 맞은 개수 : 2\nEpoch : 0, batch 648\n(Train) Batch 648 Loss : 5.066201210021973, 맞은 개수 : 1\nEpoch : 0, batch 649\n(Train) Batch 649 Loss : 5.015866756439209, 맞은 개수 : 1\nEpoch : 0, batch 650\n(Train) Batch 650 Loss : 5.037352085113525, 맞은 개수 : 0\nEpoch : 0, batch 651\n(Train) Batch 651 Loss : 5.02891731262207, 맞은 개수 : 1\nEpoch : 0, batch 652\n(Train) Batch 652 Loss : 5.135230541229248, 맞은 개수 : 2\nEpoch : 0, batch 653\n(Train) Batch 653 Loss : 5.006844520568848, 맞은 개수 : 3\nEpoch : 0, batch 654\n(Train) Batch 654 Loss : 5.103931427001953, 맞은 개수 : 0\nEpoch : 0, batch 655\n(Train) Batch 655 Loss : 5.1909990310668945, 맞은 개수 : 3\nEpoch : 0, batch 656\n(Train) Batch 656 Loss : 5.092574596405029, 맞은 개수 : 2\nEpoch : 0, batch 657\n(Train) Batch 657 Loss : 5.038536071777344, 맞은 개수 : 4\nEpoch : 0, batch 658\n(Train) Batch 658 Loss : 5.035813808441162, 맞은 개수 : 2\nEpoch : 0, batch 659\n(Train) Batch 659 Loss : 5.104424476623535, 맞은 개수 : 1\nEpoch : 0, batch 660\n(Train) Batch 660 Loss : 5.198561668395996, 맞은 개수 : 1\nEpoch : 0, batch 661\n(Train) Batch 661 Loss : 5.077439308166504, 맞은 개수 : 2\nEpoch : 0, batch 662\n(Train) Batch 662 Loss : 5.0974955558776855, 맞은 개수 : 0\nEpoch : 0, batch 663\n(Train) Batch 663 Loss : 5.0780415534973145, 맞은 개수 : 1\nEpoch : 0, batch 664\n(Train) Batch 664 Loss : 5.12241268157959, 맞은 개수 : 0\nEpoch : 0, batch 665\n(Train) Batch 665 Loss : 5.058175563812256, 맞은 개수 : 1\nEpoch : 0, batch 666\n(Train) Batch 666 Loss : 5.07511568069458, 맞은 개수 : 0\nEpoch : 0, batch 667\n(Train) Batch 667 Loss : 4.983745098114014, 맞은 개수 : 2\nEpoch : 0, batch 668\n(Train) Batch 668 Loss : 5.080216884613037, 맞은 개수 : 3\nEpoch : 0, batch 669\n(Train) Batch 669 Loss : 5.098550319671631, 맞은 개수 : 2\nEpoch : 0, batch 670\n(Train) Batch 670 Loss : 5.177924156188965, 맞은 개수 : 3\nEpoch : 0, batch 671\n(Train) Batch 671 Loss : 5.134948253631592, 맞은 개수 : 2\nEpoch : 0, batch 672\n(Train) Batch 672 Loss : 4.956711769104004, 맞은 개수 : 4\nEpoch : 0, batch 673\n(Train) Batch 673 Loss : 5.1033854484558105, 맞은 개수 : 2\nEpoch : 0, batch 674\n(Train) Batch 674 Loss : 5.108952522277832, 맞은 개수 : 5\nEpoch : 0, batch 675\n(Train) Batch 675 Loss : 5.108400344848633, 맞은 개수 : 3\nEpoch : 0, batch 676\n(Train) Batch 676 Loss : 5.075249195098877, 맞은 개수 : 3\nEpoch : 0, batch 677\n(Train) Batch 677 Loss : 5.100459098815918, 맞은 개수 : 1\nEpoch : 0, batch 678\n(Train) Batch 678 Loss : 5.121907711029053, 맞은 개수 : 0\nEpoch : 0, batch 679\n(Train) Batch 679 Loss : 5.142388343811035, 맞은 개수 : 2\nEpoch : 0, batch 680\n(Train) Batch 680 Loss : 5.019293308258057, 맞은 개수 : 0\nEpoch : 0, batch 681\n(Train) Batch 681 Loss : 5.141111373901367, 맞은 개수 : 3\nEpoch : 0, batch 682\n(Train) Batch 682 Loss : 5.068619728088379, 맞은 개수 : 2\nEpoch : 0, batch 683\n(Train) Batch 683 Loss : 5.0462751388549805, 맞은 개수 : 1\nEpoch : 0, batch 684\n(Train) Batch 684 Loss : 5.0074872970581055, 맞은 개수 : 6\nEpoch : 0, batch 685\n(Train) Batch 685 Loss : 4.915024757385254, 맞은 개수 : 4\nEpoch : 0, batch 686\n(Train) Batch 686 Loss : 5.0907063484191895, 맞은 개수 : 0\nEpoch : 0, batch 687\n(Train) Batch 687 Loss : 5.03938102722168, 맞은 개수 : 2\nEpoch : 0, batch 688\n(Train) Batch 688 Loss : 4.9290289878845215, 맞은 개수 : 3\nEpoch : 0, batch 689\n(Train) Batch 689 Loss : 5.137666702270508, 맞은 개수 : 2\nEpoch : 0, batch 690\n(Train) Batch 690 Loss : 5.139672756195068, 맞은 개수 : 7\nEpoch : 0, batch 691\n(Train) Batch 691 Loss : 5.102021217346191, 맞은 개수 : 1\nEpoch : 0, batch 692\n(Train) Batch 692 Loss : 5.053870677947998, 맞은 개수 : 2\nEpoch : 0, batch 693\n(Train) Batch 693 Loss : 5.043826103210449, 맞은 개수 : 4\nEpoch : 0, batch 694\n(Train) Batch 694 Loss : 5.001615524291992, 맞은 개수 : 1\nEpoch : 0, batch 695\n(Train) Batch 695 Loss : 5.069291591644287, 맞은 개수 : 4\nEpoch : 0, batch 696\n(Train) Batch 696 Loss : 4.975707530975342, 맞은 개수 : 1\nEpoch : 0, batch 697\n(Train) Batch 697 Loss : 5.139949798583984, 맞은 개수 : 1\nEpoch : 0, batch 698\n(Train) Batch 698 Loss : 5.088052272796631, 맞은 개수 : 2\nEpoch : 0, batch 699\n(Train) Batch 699 Loss : 5.047831058502197, 맞은 개수 : 4\nEpoch : 0, batch 700\n(Train) Batch 700 Loss : 5.138889312744141, 맞은 개수 : 0\nEpoch : 0, batch 701\n(Train) Batch 701 Loss : 5.068469047546387, 맞은 개수 : 3\nEpoch : 0, batch 702\n(Train) Batch 702 Loss : 5.054941177368164, 맞은 개수 : 2\nEpoch : 0, batch 703\n(Train) Batch 703 Loss : 5.11445951461792, 맞은 개수 : 1\nEpoch : 0, batch 704\n(Train) Batch 704 Loss : 5.05936336517334, 맞은 개수 : 2\nEpoch : 0, batch 705\n(Train) Batch 705 Loss : 5.030117034912109, 맞은 개수 : 0\nEpoch : 0, batch 706\n(Train) Batch 706 Loss : 5.079616546630859, 맞은 개수 : 1\nEpoch : 0, batch 707\n(Train) Batch 707 Loss : 5.063032150268555, 맞은 개수 : 2\nEpoch : 0, batch 708\n(Train) Batch 708 Loss : 5.13716459274292, 맞은 개수 : 3\nEpoch : 0, batch 709\n(Train) Batch 709 Loss : 5.044639587402344, 맞은 개수 : 1\nEpoch : 0, batch 710\n(Train) Batch 710 Loss : 5.26383113861084, 맞은 개수 : 0\nEpoch : 0, batch 711\n(Train) Batch 711 Loss : 4.971559524536133, 맞은 개수 : 2\nEpoch : 0, batch 712\n(Train) Batch 712 Loss : 5.100652694702148, 맞은 개수 : 1\nEpoch : 0, batch 713\n(Train) Batch 713 Loss : 5.058140754699707, 맞은 개수 : 3\nEpoch : 0, batch 714\n(Train) Batch 714 Loss : 5.076671600341797, 맞은 개수 : 0\nEpoch : 0, batch 715\n(Train) Batch 715 Loss : 5.0665998458862305, 맞은 개수 : 1\nEpoch : 0, batch 716\n(Train) Batch 716 Loss : 5.068580150604248, 맞은 개수 : 2\nEpoch : 0, batch 717\n(Train) Batch 717 Loss : 4.9899001121521, 맞은 개수 : 2\nEpoch : 0, batch 718\n(Train) Batch 718 Loss : 5.07621431350708, 맞은 개수 : 2\nEpoch : 0, batch 719\n(Train) Batch 719 Loss : 5.087586879730225, 맞은 개수 : 1\nEpoch : 0, batch 720\n(Train) Batch 720 Loss : 5.1127142906188965, 맞은 개수 : 2\nEpoch : 0, batch 721\n(Train) Batch 721 Loss : 5.0837082862854, 맞은 개수 : 2\nEpoch : 0, batch 722\n(Train) Batch 722 Loss : 5.056900978088379, 맞은 개수 : 1\nEpoch : 0, batch 723\n(Train) Batch 723 Loss : 5.100620269775391, 맞은 개수 : 1\nEpoch : 0, batch 724\n(Train) Batch 724 Loss : 5.096478462219238, 맞은 개수 : 4\nEpoch : 0, batch 725\n(Train) Batch 725 Loss : 5.0546464920043945, 맞은 개수 : 1\nEpoch : 0, batch 726\n(Train) Batch 726 Loss : 5.049577713012695, 맞은 개수 : 5\nEpoch : 0, batch 727\n(Train) Batch 727 Loss : 5.015296459197998, 맞은 개수 : 5\nEpoch : 0, batch 728\n(Train) Batch 728 Loss : 4.992577075958252, 맞은 개수 : 2\nEpoch : 0, batch 729\n(Train) Batch 729 Loss : 5.161248207092285, 맞은 개수 : 1\nEpoch : 0, batch 730\n(Train) Batch 730 Loss : 5.1089982986450195, 맞은 개수 : 2\nEpoch : 0, batch 731\n(Train) Batch 731 Loss : 5.069866180419922, 맞은 개수 : 2\nEpoch : 0, batch 732\n(Train) Batch 732 Loss : 5.00596809387207, 맞은 개수 : 4\nEpoch : 0, batch 733\n(Train) Batch 733 Loss : 5.015167236328125, 맞은 개수 : 2\nEpoch : 0, batch 734\n(Train) Batch 734 Loss : 5.05874490737915, 맞은 개수 : 0\nEpoch : 0, batch 735\n(Train) Batch 735 Loss : 5.065144062042236, 맞은 개수 : 0\nEpoch : 0, batch 736\n(Train) Batch 736 Loss : 5.208673477172852, 맞은 개수 : 2\nEpoch : 0, batch 737\n(Train) Batch 737 Loss : 5.054643630981445, 맞은 개수 : 2\nEpoch : 0, batch 738\n(Train) Batch 738 Loss : 5.063279151916504, 맞은 개수 : 3\nEpoch : 0, batch 739\n(Train) Batch 739 Loss : 5.021998882293701, 맞은 개수 : 3\nEpoch : 0, batch 740\n(Train) Batch 740 Loss : 5.155842304229736, 맞은 개수 : 1\nEpoch : 0, batch 741\n(Train) Batch 741 Loss : 4.967457294464111, 맞은 개수 : 5\nEpoch : 0, batch 742\n(Train) Batch 742 Loss : 5.114982604980469, 맞은 개수 : 2\nEpoch : 0, batch 743\n(Train) Batch 743 Loss : 5.128188610076904, 맞은 개수 : 2\nEpoch : 0, batch 744\n(Train) Batch 744 Loss : 5.083956241607666, 맞은 개수 : 2\nEpoch : 0, batch 745\n(Train) Batch 745 Loss : 5.094784259796143, 맞은 개수 : 2\nEpoch : 0, batch 746\n(Train) Batch 746 Loss : 5.033104419708252, 맞은 개수 : 2\nEpoch : 0, batch 747\n(Train) Batch 747 Loss : 5.1549391746521, 맞은 개수 : 0\nEpoch : 0, batch 748\n(Train) Batch 748 Loss : 5.082097053527832, 맞은 개수 : 0\nEpoch : 0, batch 749\n(Train) Batch 749 Loss : 5.11329460144043, 맞은 개수 : 2\nEpoch : 0, batch 750\n(Train) Batch 750 Loss : 5.14542818069458, 맞은 개수 : 0\nEpoch : 0, batch 751\n(Train) Batch 751 Loss : 5.026103496551514, 맞은 개수 : 2\nEpoch : 0, batch 752\n(Train) Batch 752 Loss : 5.009697914123535, 맞은 개수 : 3\nEpoch : 0, batch 753\n(Train) Batch 753 Loss : 5.0027594566345215, 맞은 개수 : 2\nEpoch : 0, batch 754\n(Train) Batch 754 Loss : 5.003901481628418, 맞은 개수 : 3\nEpoch : 0, batch 755\n(Train) Batch 755 Loss : 5.1161274909973145, 맞은 개수 : 1\nEpoch : 0, batch 756\n(Train) Batch 756 Loss : 5.106777191162109, 맞은 개수 : 2\nEpoch : 0, batch 757\n(Train) Batch 757 Loss : 5.09597635269165, 맞은 개수 : 3\nEpoch : 0, batch 758\n(Train) Batch 758 Loss : 5.062490940093994, 맞은 개수 : 2\nEpoch : 0, batch 759\n(Train) Batch 759 Loss : 5.082089900970459, 맞은 개수 : 0\nEpoch : 0, batch 760\n(Train) Batch 760 Loss : 4.966484546661377, 맞은 개수 : 2\nEpoch : 0, batch 761\n(Train) Batch 761 Loss : 4.968012809753418, 맞은 개수 : 1\nEpoch : 0, batch 762\n(Train) Batch 762 Loss : 4.934686660766602, 맞은 개수 : 7\nEpoch : 0, batch 763\n(Train) Batch 763 Loss : 4.950395584106445, 맞은 개수 : 2\nEpoch : 0, batch 764\n(Train) Batch 764 Loss : 5.021838665008545, 맞은 개수 : 3\nEpoch : 0, batch 765\n(Train) Batch 765 Loss : 5.106507301330566, 맞은 개수 : 3\nEpoch : 0, batch 766\n(Train) Batch 766 Loss : 4.976352691650391, 맞은 개수 : 4\nEpoch : 0, batch 767\n(Train) Batch 767 Loss : 5.171919822692871, 맞은 개수 : 1\nEpoch : 0, batch 768\n(Train) Batch 768 Loss : 5.144944190979004, 맞은 개수 : 5\nEpoch : 0, batch 769\n(Train) Batch 769 Loss : 5.08374547958374, 맞은 개수 : 2\nEpoch : 0, batch 770\n(Train) Batch 770 Loss : 5.179748058319092, 맞은 개수 : 1\nEpoch : 0, batch 771\n(Train) Batch 771 Loss : 5.053234577178955, 맞은 개수 : 0\nEpoch : 0, batch 772\n(Train) Batch 772 Loss : 5.027511119842529, 맞은 개수 : 3\nEpoch : 0, batch 773\n(Train) Batch 773 Loss : 5.054901123046875, 맞은 개수 : 2\nEpoch : 0, batch 774\n(Train) Batch 774 Loss : 5.039307117462158, 맞은 개수 : 1\nEpoch : 0, batch 775\n(Train) Batch 775 Loss : 5.028732776641846, 맞은 개수 : 2\nEpoch : 0, batch 776\n(Train) Batch 776 Loss : 5.060301303863525, 맞은 개수 : 1\nEpoch : 0, batch 777\n(Train) Batch 777 Loss : 5.001052379608154, 맞은 개수 : 3\nEpoch : 0, batch 778\n(Train) Batch 778 Loss : 5.063164234161377, 맞은 개수 : 2\nEpoch : 0, batch 779\n(Train) Batch 779 Loss : 5.077858924865723, 맞은 개수 : 2\nEpoch : 0, batch 780\n(Train) Batch 780 Loss : 5.0297770500183105, 맞은 개수 : 3\nEpoch : 0, batch 781\n(Train) Batch 781 Loss : 5.095841884613037, 맞은 개수 : 3\nEpoch : 0, batch 782\n(Train) Batch 782 Loss : 5.13254976272583, 맞은 개수 : 2\nEpoch : 0, batch 783\n(Train) Batch 783 Loss : 5.013645648956299, 맞은 개수 : 3\nEpoch : 0, batch 784\n(Train) Batch 784 Loss : 5.046934604644775, 맞은 개수 : 1\nEpoch : 0, batch 785\n(Train) Batch 785 Loss : 4.977553367614746, 맞은 개수 : 1\nEpoch : 0, batch 786\n(Train) Batch 786 Loss : 4.998658657073975, 맞은 개수 : 7\nEpoch : 0, batch 787\n(Train) Batch 787 Loss : 5.059337615966797, 맞은 개수 : 2\nEpoch : 0, batch 788\n(Train) Batch 788 Loss : 4.938967704772949, 맞은 개수 : 5\nEpoch : 0, batch 789\n(Train) Batch 789 Loss : 5.055568218231201, 맞은 개수 : 2\nEpoch : 0, batch 790\n(Train) Batch 790 Loss : 5.038412570953369, 맞은 개수 : 5\nEpoch : 0, batch 791\n(Train) Batch 791 Loss : 5.054689884185791, 맞은 개수 : 1\nEpoch : 0, batch 792\n(Train) Batch 792 Loss : 4.957587242126465, 맞은 개수 : 4\nEpoch : 0, batch 793\n(Train) Batch 793 Loss : 5.169442176818848, 맞은 개수 : 3\nEpoch : 0, batch 794\n(Train) Batch 794 Loss : 5.1916913986206055, 맞은 개수 : 1\nEpoch : 0, batch 795\n(Train) Batch 795 Loss : 5.0016913414001465, 맞은 개수 : 3\nEpoch : 0, batch 796\n(Train) Batch 796 Loss : 5.024844169616699, 맞은 개수 : 2\nEpoch : 0, batch 797\n(Train) Batch 797 Loss : 5.123143672943115, 맞은 개수 : 2\nEpoch : 0, batch 798\n(Train) Batch 798 Loss : 5.048945903778076, 맞은 개수 : 2\nEpoch : 0, batch 799\n(Train) Batch 799 Loss : 5.169946193695068, 맞은 개수 : 4\nEpoch : 0, batch 800\n(Train) Batch 800 Loss : 5.065587997436523, 맞은 개수 : 2\nEpoch : 0, batch 801\n(Train) Batch 801 Loss : 4.979576110839844, 맞은 개수 : 1\nEpoch : 0, batch 802\n(Train) Batch 802 Loss : 5.052839756011963, 맞은 개수 : 4\nEpoch : 0, batch 803\n(Train) Batch 803 Loss : 4.967128276824951, 맞은 개수 : 4\nEpoch : 0, batch 804\n(Train) Batch 804 Loss : 4.8831586837768555, 맞은 개수 : 3\nEpoch : 0, batch 805\n(Train) Batch 805 Loss : 4.897895336151123, 맞은 개수 : 3\nEpoch : 0, batch 806\n(Train) Batch 806 Loss : 5.054224967956543, 맞은 개수 : 5\nEpoch : 0, batch 807\n(Train) Batch 807 Loss : 5.116278171539307, 맞은 개수 : 4\nEpoch : 0, batch 808\n(Train) Batch 808 Loss : 4.934681415557861, 맞은 개수 : 7\nEpoch : 0, batch 809\n(Train) Batch 809 Loss : 5.092490196228027, 맞은 개수 : 3\nEpoch : 0, batch 810\n(Train) Batch 810 Loss : 5.179086685180664, 맞은 개수 : 2\nEpoch : 0, batch 811\n(Train) Batch 811 Loss : 5.042512893676758, 맞은 개수 : 6\nEpoch : 0, batch 812\n(Train) Batch 812 Loss : 5.078131198883057, 맞은 개수 : 2\nEpoch : 0, batch 813\n(Train) Batch 813 Loss : 4.985065460205078, 맞은 개수 : 7\nEpoch : 0, batch 814\n(Train) Batch 814 Loss : 4.995050430297852, 맞은 개수 : 2\nEpoch : 0, batch 815\n(Train) Batch 815 Loss : 5.009203910827637, 맞은 개수 : 6\nEpoch : 0, batch 816\n(Train) Batch 816 Loss : 5.06972599029541, 맞은 개수 : 0\nEpoch : 0, batch 817\n(Train) Batch 817 Loss : 4.988706588745117, 맞은 개수 : 6\nEpoch : 0, batch 818\n(Train) Batch 818 Loss : 5.069628715515137, 맞은 개수 : 5\nEpoch : 0, batch 819\n(Train) Batch 819 Loss : 5.045776844024658, 맞은 개수 : 1\nEpoch : 0, batch 820\n(Train) Batch 820 Loss : 4.965461254119873, 맞은 개수 : 4\nEpoch : 0, batch 821\n(Train) Batch 821 Loss : 4.895585060119629, 맞은 개수 : 6\nEpoch : 0, batch 822\n(Train) Batch 822 Loss : 5.122725486755371, 맞은 개수 : 0\nEpoch : 0, batch 823\n(Train) Batch 823 Loss : 5.142366409301758, 맞은 개수 : 3\nEpoch : 0, batch 824\n(Train) Batch 824 Loss : 5.040765285491943, 맞은 개수 : 1\nEpoch : 0, batch 825\n(Train) Batch 825 Loss : 5.046748161315918, 맞은 개수 : 3\nEpoch : 0, batch 826\n(Train) Batch 826 Loss : 4.874614238739014, 맞은 개수 : 4\nEpoch : 0, batch 827\n(Train) Batch 827 Loss : 5.0628862380981445, 맞은 개수 : 3\nEpoch : 0, batch 828\n(Train) Batch 828 Loss : 5.009128093719482, 맞은 개수 : 3\nEpoch : 0, batch 829\n(Train) Batch 829 Loss : 5.055788040161133, 맞은 개수 : 4\nEpoch : 0, batch 830\n(Train) Batch 830 Loss : 4.936574935913086, 맞은 개수 : 4\nEpoch : 0, batch 831\n(Train) Batch 831 Loss : 4.858720302581787, 맞은 개수 : 4\nEpoch : 0, batch 832\n(Train) Batch 832 Loss : 4.9718708992004395, 맞은 개수 : 2\nEpoch : 0, batch 833\n(Train) Batch 833 Loss : 5.008012294769287, 맞은 개수 : 1\nEpoch : 0, batch 834\n(Train) Batch 834 Loss : 5.131991386413574, 맞은 개수 : 1\nEpoch : 0, batch 835\n(Train) Batch 835 Loss : 5.047250747680664, 맞은 개수 : 4\nEpoch : 0, batch 836\n(Train) Batch 836 Loss : 5.0601630210876465, 맞은 개수 : 2\nEpoch : 0, batch 837\n(Train) Batch 837 Loss : 4.994001865386963, 맞은 개수 : 2\nEpoch : 0, batch 838\n(Train) Batch 838 Loss : 5.034456729888916, 맞은 개수 : 2\nEpoch : 0, batch 839\n(Train) Batch 839 Loss : 4.95943546295166, 맞은 개수 : 1\nEpoch : 0, batch 840\n(Train) Batch 840 Loss : 5.089165687561035, 맞은 개수 : 2\nEpoch : 0, batch 841\n(Train) Batch 841 Loss : 4.944730758666992, 맞은 개수 : 4\nEpoch : 0, batch 842\n(Train) Batch 842 Loss : 5.089859962463379, 맞은 개수 : 4\nEpoch : 0, batch 843\n(Train) Batch 843 Loss : 5.028139114379883, 맞은 개수 : 0\nEpoch : 0, batch 844\n(Train) Batch 844 Loss : 4.95462703704834, 맞은 개수 : 1\nEpoch : 0, batch 845\n(Train) Batch 845 Loss : 4.939053058624268, 맞은 개수 : 1\nEpoch : 0, batch 846\n(Train) Batch 846 Loss : 5.05841064453125, 맞은 개수 : 3\nEpoch : 0, batch 847\n(Train) Batch 847 Loss : 4.959961891174316, 맞은 개수 : 3\nEpoch : 0, batch 848\n(Train) Batch 848 Loss : 4.882707595825195, 맞은 개수 : 3\nEpoch : 0, batch 849\n(Train) Batch 849 Loss : 4.997412204742432, 맞은 개수 : 4\nEpoch : 0, batch 850\n(Train) Batch 850 Loss : 4.998044490814209, 맞은 개수 : 3\nEpoch : 0, batch 851\n(Train) Batch 851 Loss : 5.064058780670166, 맞은 개수 : 3\nEpoch : 0, batch 852\n(Train) Batch 852 Loss : 4.990843772888184, 맞은 개수 : 6\nEpoch : 0, batch 853\n(Train) Batch 853 Loss : 5.0627241134643555, 맞은 개수 : 3\nEpoch : 0, batch 854\n(Train) Batch 854 Loss : 4.982196807861328, 맞은 개수 : 4\nEpoch : 0, batch 855\n(Train) Batch 855 Loss : 4.897100925445557, 맞은 개수 : 1\nEpoch : 0, batch 856\n(Train) Batch 856 Loss : 4.864068508148193, 맞은 개수 : 5\nEpoch : 0, batch 857\n(Train) Batch 857 Loss : 5.028078079223633, 맞은 개수 : 3\nEpoch : 0, batch 858\n(Train) Batch 858 Loss : 5.050622940063477, 맞은 개수 : 2\nEpoch : 0, batch 859\n(Train) Batch 859 Loss : 4.979085922241211, 맞은 개수 : 2\nEpoch : 0, batch 860\n(Train) Batch 860 Loss : 4.870096206665039, 맞은 개수 : 6\nEpoch : 0, batch 861\n(Train) Batch 861 Loss : 4.988411903381348, 맞은 개수 : 5\nEpoch : 0, batch 862\n(Train) Batch 862 Loss : 4.968929290771484, 맞은 개수 : 3\nEpoch : 0, batch 863\n(Train) Batch 863 Loss : 5.062673091888428, 맞은 개수 : 2\nEpoch : 0, batch 864\n(Train) Batch 864 Loss : 5.013427257537842, 맞은 개수 : 4\nEpoch : 0, batch 865\n(Train) Batch 865 Loss : 5.078405857086182, 맞은 개수 : 2\nEpoch : 0, batch 866\n(Train) Batch 866 Loss : 5.063515663146973, 맞은 개수 : 0\nEpoch : 0, batch 867\n(Train) Batch 867 Loss : 4.996541976928711, 맞은 개수 : 5\nEpoch : 0, batch 868\n(Train) Batch 868 Loss : 5.0712361335754395, 맞은 개수 : 1\nEpoch : 0, batch 869\n(Train) Batch 869 Loss : 4.978394031524658, 맞은 개수 : 8\nEpoch : 0, batch 870\n(Train) Batch 870 Loss : 5.0846028327941895, 맞은 개수 : 4\nEpoch : 0, batch 871\n(Train) Batch 871 Loss : 4.996719837188721, 맞은 개수 : 5\nEpoch : 0, batch 872\n(Train) Batch 872 Loss : 5.034754753112793, 맞은 개수 : 3\nEpoch : 0, batch 873\n(Train) Batch 873 Loss : 5.018557548522949, 맞은 개수 : 5\nEpoch : 0, batch 874\n(Train) Batch 874 Loss : 4.9721832275390625, 맞은 개수 : 7\nEpoch : 0, batch 875\n(Train) Batch 875 Loss : 5.066593647003174, 맞은 개수 : 3\nEpoch : 0, batch 876\n(Train) Batch 876 Loss : 5.010652542114258, 맞은 개수 : 4\nEpoch : 0, batch 877\n(Train) Batch 877 Loss : 4.983771800994873, 맞은 개수 : 4\nEpoch : 0, batch 878\n(Train) Batch 878 Loss : 4.942008972167969, 맞은 개수 : 7\nEpoch : 0, batch 879\n(Train) Batch 879 Loss : 4.984195232391357, 맞은 개수 : 4\nEpoch : 0, batch 880\n(Train) Batch 880 Loss : 4.961860179901123, 맞은 개수 : 4\nEpoch : 0, batch 881\n(Train) Batch 881 Loss : 5.087373733520508, 맞은 개수 : 1\nEpoch : 0, batch 882\n(Train) Batch 882 Loss : 5.072213649749756, 맞은 개수 : 0\nEpoch : 0, batch 883\n(Train) Batch 883 Loss : 4.833975791931152, 맞은 개수 : 2\nEpoch : 0, batch 884\n(Train) Batch 884 Loss : 5.166855335235596, 맞은 개수 : 4\nEpoch : 0, batch 885\n(Train) Batch 885 Loss : 4.999967575073242, 맞은 개수 : 2\nEpoch : 0, batch 886\n(Train) Batch 886 Loss : 5.054862022399902, 맞은 개수 : 3\nEpoch : 0, batch 887\n(Train) Batch 887 Loss : 5.11289644241333, 맞은 개수 : 1\nEpoch : 0, batch 888\n(Train) Batch 888 Loss : 4.906161785125732, 맞은 개수 : 1\nEpoch : 0, batch 889\n(Train) Batch 889 Loss : 4.997121334075928, 맞은 개수 : 5\nEpoch : 0, batch 890\n(Train) Batch 890 Loss : 5.085350513458252, 맞은 개수 : 1\nEpoch : 0, batch 891\n(Train) Batch 891 Loss : 5.060838222503662, 맞은 개수 : 2\nEpoch : 0, batch 892\n(Train) Batch 892 Loss : 5.014094352722168, 맞은 개수 : 3\nEpoch : 0, batch 893\n(Train) Batch 893 Loss : 4.989271640777588, 맞은 개수 : 6\nEpoch : 0, batch 894\n(Train) Batch 894 Loss : 5.063727378845215, 맞은 개수 : 2\nEpoch : 0, batch 895\n(Train) Batch 895 Loss : 5.063045978546143, 맞은 개수 : 0\nEpoch : 0, batch 896\n(Train) Batch 896 Loss : 5.119874477386475, 맞은 개수 : 4\nEpoch : 0, batch 897\n(Train) Batch 897 Loss : 4.9398698806762695, 맞은 개수 : 2\nEpoch : 0, batch 898\n(Train) Batch 898 Loss : 4.985837936401367, 맞은 개수 : 1\nEpoch : 0, batch 899\n(Train) Batch 899 Loss : 5.018032073974609, 맞은 개수 : 3\nEpoch : 0, batch 900\n(Train) Batch 900 Loss : 5.0653886795043945, 맞은 개수 : 1\nEpoch : 0, batch 901\n(Train) Batch 901 Loss : 4.910617828369141, 맞은 개수 : 5\nEpoch : 0, batch 902\n(Train) Batch 902 Loss : 4.978386402130127, 맞은 개수 : 3\nEpoch : 0, batch 903\n(Train) Batch 903 Loss : 5.0687079429626465, 맞은 개수 : 4\nEpoch : 0, batch 904\n(Train) Batch 904 Loss : 4.9537129402160645, 맞은 개수 : 2\nEpoch : 0, batch 905\n(Train) Batch 905 Loss : 4.9482903480529785, 맞은 개수 : 3\nEpoch : 0, batch 906\n(Train) Batch 906 Loss : 4.911672592163086, 맞은 개수 : 4\nEpoch : 0, batch 907\n(Train) Batch 907 Loss : 5.037461280822754, 맞은 개수 : 2\nEpoch : 0, batch 908\n(Train) Batch 908 Loss : 5.048425197601318, 맞은 개수 : 3\nEpoch : 0, batch 909\n(Train) Batch 909 Loss : 5.021365642547607, 맞은 개수 : 2\nEpoch : 0, batch 910\n(Train) Batch 910 Loss : 5.025561332702637, 맞은 개수 : 4\nEpoch : 0, batch 911\n(Train) Batch 911 Loss : 4.957777976989746, 맞은 개수 : 3\nEpoch : 0, batch 912\n(Train) Batch 912 Loss : 4.93647575378418, 맞은 개수 : 3\nEpoch : 0, batch 913\n(Train) Batch 913 Loss : 4.941343307495117, 맞은 개수 : 2\nEpoch : 0, batch 914\n(Train) Batch 914 Loss : 4.8611626625061035, 맞은 개수 : 5\nEpoch : 0, batch 915\n(Train) Batch 915 Loss : 4.9920549392700195, 맞은 개수 : 4\nEpoch : 0, batch 916\n(Train) Batch 916 Loss : 4.7720866203308105, 맞은 개수 : 3\nEpoch : 0, batch 917\n(Train) Batch 917 Loss : 4.997560501098633, 맞은 개수 : 3\nEpoch : 0, batch 918\n(Train) Batch 918 Loss : 5.089313983917236, 맞은 개수 : 4\nEpoch : 0, batch 919\n(Train) Batch 919 Loss : 4.930674076080322, 맞은 개수 : 3\nEpoch : 0, batch 920\n(Train) Batch 920 Loss : 5.0503830909729, 맞은 개수 : 1\nEpoch : 0, batch 921\n(Train) Batch 921 Loss : 5.013025283813477, 맞은 개수 : 4\nEpoch : 0, batch 922\n(Train) Batch 922 Loss : 5.0126142501831055, 맞은 개수 : 1\nEpoch : 0, batch 923\n(Train) Batch 923 Loss : 5.010087013244629, 맞은 개수 : 2\nEpoch : 0, batch 924\n(Train) Batch 924 Loss : 4.967422008514404, 맞은 개수 : 4\nEpoch : 0, batch 925\n(Train) Batch 925 Loss : 5.104611396789551, 맞은 개수 : 5\nEpoch : 0, batch 926\n(Train) Batch 926 Loss : 5.032960414886475, 맞은 개수 : 1\nEpoch : 0, batch 927\n(Train) Batch 927 Loss : 5.039957523345947, 맞은 개수 : 0\nEpoch : 0, batch 928\n(Train) Batch 928 Loss : 4.976638317108154, 맞은 개수 : 2\nEpoch : 0, batch 929\n(Train) Batch 929 Loss : 5.050272464752197, 맞은 개수 : 2\nEpoch : 0, batch 930\n(Train) Batch 930 Loss : 5.0753936767578125, 맞은 개수 : 4\nEpoch : 0, batch 931\n(Train) Batch 931 Loss : 4.955615997314453, 맞은 개수 : 1\nEpoch : 0, batch 932\n(Train) Batch 932 Loss : 5.042549133300781, 맞은 개수 : 2\nEpoch : 0, batch 933\n(Train) Batch 933 Loss : 5.048756122589111, 맞은 개수 : 1\nEpoch : 0, batch 934\n(Train) Batch 934 Loss : 5.012347221374512, 맞은 개수 : 1\nEpoch : 0, batch 935\n(Train) Batch 935 Loss : 5.0036187171936035, 맞은 개수 : 1\nEpoch : 0, batch 936\n(Train) Batch 936 Loss : 5.030818462371826, 맞은 개수 : 2\nEpoch : 0, batch 937\n(Train) Batch 937 Loss : 5.019242763519287, 맞은 개수 : 2\nEpoch : 0, batch 938\n(Train) Batch 938 Loss : 4.935873031616211, 맞은 개수 : 7\nEpoch : 0, batch 939\n(Train) Batch 939 Loss : 4.942859172821045, 맞은 개수 : 2\nEpoch : 0, batch 940\n(Train) Batch 940 Loss : 4.925128936767578, 맞은 개수 : 2\nEpoch : 0, batch 941\n(Train) Batch 941 Loss : 4.989316940307617, 맞은 개수 : 4\nEpoch : 0, batch 942\n(Train) Batch 942 Loss : 4.893932342529297, 맞은 개수 : 1\nEpoch : 0, batch 943\n(Train) Batch 943 Loss : 5.034304141998291, 맞은 개수 : 3\nEpoch : 0, batch 944\n(Train) Batch 944 Loss : 4.950748920440674, 맞은 개수 : 3\nEpoch : 0, batch 945\n(Train) Batch 945 Loss : 4.9249982833862305, 맞은 개수 : 3\nEpoch : 0, batch 946\n(Train) Batch 946 Loss : 4.96005916595459, 맞은 개수 : 5\nEpoch : 0, batch 947\n(Train) Batch 947 Loss : 4.980879306793213, 맞은 개수 : 1\nEpoch : 0, batch 948\n(Train) Batch 948 Loss : 4.9484171867370605, 맞은 개수 : 4\nEpoch : 0, batch 949\n(Train) Batch 949 Loss : 4.976953029632568, 맞은 개수 : 5\nEpoch : 0, batch 950\n(Train) Batch 950 Loss : 4.9414544105529785, 맞은 개수 : 2\nEpoch : 0, batch 951\n(Train) Batch 951 Loss : 4.946022033691406, 맞은 개수 : 5\nEpoch : 0, batch 952\n(Train) Batch 952 Loss : 4.887351989746094, 맞은 개수 : 7\nEpoch : 0, batch 953\n(Train) Batch 953 Loss : 5.075042247772217, 맞은 개수 : 3\nEpoch : 0, batch 954\n(Train) Batch 954 Loss : 4.998882293701172, 맞은 개수 : 0\nEpoch : 0, batch 955\n(Train) Batch 955 Loss : 4.97503662109375, 맞은 개수 : 4\nEpoch : 0, batch 956\n(Train) Batch 956 Loss : 5.139466762542725, 맞은 개수 : 4\nEpoch : 0, batch 957\n(Train) Batch 957 Loss : 4.868371486663818, 맞은 개수 : 6\nEpoch : 0, batch 958\n(Train) Batch 958 Loss : 4.8835296630859375, 맞은 개수 : 6\nEpoch : 0, batch 959\n(Train) Batch 959 Loss : 4.9965972900390625, 맞은 개수 : 4\nEpoch : 0, batch 960\n(Train) Batch 960 Loss : 5.055263996124268, 맞은 개수 : 2\nEpoch : 0, batch 961\n(Train) Batch 961 Loss : 4.948182582855225, 맞은 개수 : 4\nEpoch : 0, batch 962\n(Train) Batch 962 Loss : 4.912465572357178, 맞은 개수 : 4\nEpoch : 0, batch 963\n(Train) Batch 963 Loss : 5.055379390716553, 맞은 개수 : 2\nEpoch : 0, batch 964\n(Train) Batch 964 Loss : 5.002350807189941, 맞은 개수 : 2\nEpoch : 0, batch 965\n(Train) Batch 965 Loss : 4.981308460235596, 맞은 개수 : 3\nEpoch : 0, batch 966\n(Train) Batch 966 Loss : 5.054527759552002, 맞은 개수 : 2\nEpoch : 0, batch 967\n(Train) Batch 967 Loss : 4.949260711669922, 맞은 개수 : 3\nEpoch : 0, batch 968\n(Train) Batch 968 Loss : 4.9236602783203125, 맞은 개수 : 6\nEpoch : 0, batch 969\n(Train) Batch 969 Loss : 4.891200542449951, 맞은 개수 : 1\nEpoch : 0, batch 970\n(Train) Batch 970 Loss : 5.033117771148682, 맞은 개수 : 3\nEpoch : 0, batch 971\n(Train) Batch 971 Loss : 4.972772121429443, 맞은 개수 : 4\nEpoch : 0, batch 972\n(Train) Batch 972 Loss : 4.975060939788818, 맞은 개수 : 3\nEpoch : 0, batch 973\n(Train) Batch 973 Loss : 4.982376575469971, 맞은 개수 : 3\nEpoch : 0, batch 974\n(Train) Batch 974 Loss : 5.020580768585205, 맞은 개수 : 5\nEpoch : 0, batch 975\n(Train) Batch 975 Loss : 4.968312740325928, 맞은 개수 : 8\nEpoch : 0, batch 976\n(Train) Batch 976 Loss : 4.969586372375488, 맞은 개수 : 3\nEpoch : 0, batch 977\n(Train) Batch 977 Loss : 4.998631477355957, 맞은 개수 : 3\nEpoch : 0, batch 978\n(Train) Batch 978 Loss : 5.061235427856445, 맞은 개수 : 2\nEpoch : 0, batch 979\n(Train) Batch 979 Loss : 5.00495719909668, 맞은 개수 : 4\nEpoch : 0, batch 980\n(Train) Batch 980 Loss : 4.978033542633057, 맞은 개수 : 1\nEpoch : 0, batch 981\n(Train) Batch 981 Loss : 5.022899627685547, 맞은 개수 : 3\nEpoch : 0, batch 982\n(Train) Batch 982 Loss : 4.967092990875244, 맞은 개수 : 2\nEpoch : 0, batch 983\n(Train) Batch 983 Loss : 5.034454822540283, 맞은 개수 : 7\nEpoch : 0, batch 984\n(Train) Batch 984 Loss : 4.891849994659424, 맞은 개수 : 3\nEpoch : 0, batch 985\n(Train) Batch 985 Loss : 4.942686080932617, 맞은 개수 : 3\nEpoch : 0, batch 986\n(Train) Batch 986 Loss : 4.957866191864014, 맞은 개수 : 2\nEpoch : 0, batch 987\n(Train) Batch 987 Loss : 4.959563255310059, 맞은 개수 : 3\nEpoch : 0, batch 988\n(Train) Batch 988 Loss : 4.8856520652771, 맞은 개수 : 2\nEpoch : 0, batch 989\n(Train) Batch 989 Loss : 4.795248508453369, 맞은 개수 : 3\nEpoch : 0, batch 990\n(Train) Batch 990 Loss : 5.040251731872559, 맞은 개수 : 2\nEpoch : 0, batch 991\n(Train) Batch 991 Loss : 4.99778413772583, 맞은 개수 : 5\nEpoch : 0, batch 992\n(Train) Batch 992 Loss : 4.875551223754883, 맞은 개수 : 3\nEpoch : 0, batch 993\n(Train) Batch 993 Loss : 5.010756492614746, 맞은 개수 : 4\nEpoch : 0, batch 994\n(Train) Batch 994 Loss : 4.9686479568481445, 맞은 개수 : 1\nEpoch : 0, batch 995\n(Train) Batch 995 Loss : 4.895175457000732, 맞은 개수 : 7\nEpoch : 0, batch 996\n(Train) Batch 996 Loss : 4.993584632873535, 맞은 개수 : 1\nEpoch : 0, batch 997\n(Train) Batch 997 Loss : 5.109942436218262, 맞은 개수 : 2\nEpoch : 0, batch 998\n(Train) Batch 998 Loss : 4.9402875900268555, 맞은 개수 : 7\nEpoch : 0, batch 999\n(Train) Batch 999 Loss : 5.054152965545654, 맞은 개수 : 1\nEpoch : 0, batch 1000\n(Train) Batch 1000 Loss : 4.993593692779541, 맞은 개수 : 6\nEpoch : 0, batch 1001\n(Train) Batch 1001 Loss : 4.988557815551758, 맞은 개수 : 2\nEpoch : 0, batch 1002\n(Train) Batch 1002 Loss : 4.87027645111084, 맞은 개수 : 1\nEpoch : 0, batch 1003\n(Train) Batch 1003 Loss : 4.8353352546691895, 맞은 개수 : 5\nEpoch : 0, batch 1004\n(Train) Batch 1004 Loss : 4.898068904876709, 맞은 개수 : 4\nEpoch : 0, batch 1005\n(Train) Batch 1005 Loss : 4.943749904632568, 맞은 개수 : 4\nEpoch : 0, batch 1006\n(Train) Batch 1006 Loss : 4.918327331542969, 맞은 개수 : 1\nEpoch : 0, batch 1007\n(Train) Batch 1007 Loss : 4.9021830558776855, 맞은 개수 : 7\nEpoch : 0, batch 1008\n(Train) Batch 1008 Loss : 5.085334300994873, 맞은 개수 : 3\nEpoch : 0, batch 1009\n(Train) Batch 1009 Loss : 5.087441921234131, 맞은 개수 : 4\nEpoch : 0, batch 1010\n(Train) Batch 1010 Loss : 4.992289066314697, 맞은 개수 : 0\nEpoch : 0, batch 1011\n(Train) Batch 1011 Loss : 4.978529930114746, 맞은 개수 : 3\nEpoch : 0, batch 1012\n(Train) Batch 1012 Loss : 4.972496032714844, 맞은 개수 : 1\nEpoch : 0, batch 1013\n(Train) Batch 1013 Loss : 4.929067134857178, 맞은 개수 : 2\nEpoch : 0, batch 1014\n(Train) Batch 1014 Loss : 4.969886779785156, 맞은 개수 : 0\nEpoch : 0, batch 1015\n(Train) Batch 1015 Loss : 4.860677719116211, 맞은 개수 : 2\nEpoch : 0, batch 1016\n(Train) Batch 1016 Loss : 5.000919342041016, 맞은 개수 : 5\nEpoch : 0, batch 1017\n(Train) Batch 1017 Loss : 5.014664173126221, 맞은 개수 : 3\nEpoch : 0, batch 1018\n(Train) Batch 1018 Loss : 5.014403343200684, 맞은 개수 : 3\nEpoch : 0, batch 1019\n(Train) Batch 1019 Loss : 4.963906764984131, 맞은 개수 : 5\nEpoch : 0, batch 1020\n(Train) Batch 1020 Loss : 4.898834705352783, 맞은 개수 : 4\nEpoch : 0, batch 1021\n(Train) Batch 1021 Loss : 4.977836608886719, 맞은 개수 : 3\nEpoch : 0, batch 1022\n(Train) Batch 1022 Loss : 5.0363264083862305, 맞은 개수 : 5\nEpoch : 0, batch 1023\n(Train) Batch 1023 Loss : 4.930285453796387, 맞은 개수 : 3\nEpoch : 0, batch 1024\n(Train) Batch 1024 Loss : 4.868734836578369, 맞은 개수 : 5\nEpoch : 0, batch 1025\n(Train) Batch 1025 Loss : 4.965975284576416, 맞은 개수 : 3\nEpoch : 0, batch 1026\n(Train) Batch 1026 Loss : 4.91078519821167, 맞은 개수 : 4\nEpoch : 0, batch 1027\n(Train) Batch 1027 Loss : 5.008509159088135, 맞은 개수 : 2\nEpoch : 0, batch 1028\n(Train) Batch 1028 Loss : 4.975473880767822, 맞은 개수 : 3\nEpoch : 0, batch 1029\n(Train) Batch 1029 Loss : 4.921379566192627, 맞은 개수 : 3\nEpoch : 0, batch 1030\n(Train) Batch 1030 Loss : 4.943174839019775, 맞은 개수 : 7\nEpoch : 0, batch 1031\n(Train) Batch 1031 Loss : 4.982350826263428, 맞은 개수 : 5\nEpoch : 0, batch 1032\n(Train) Batch 1032 Loss : 5.007316589355469, 맞은 개수 : 3\nEpoch : 0, batch 1033\n(Train) Batch 1033 Loss : 4.859018325805664, 맞은 개수 : 4\nEpoch : 0, batch 1034\n(Train) Batch 1034 Loss : 4.959444522857666, 맞은 개수 : 7\nEpoch : 0, batch 1035\n(Train) Batch 1035 Loss : 4.90452241897583, 맞은 개수 : 0\nEpoch : 0, batch 1036\n(Train) Batch 1036 Loss : 5.141167640686035, 맞은 개수 : 3\nEpoch : 0, batch 1037\n(Train) Batch 1037 Loss : 5.126490592956543, 맞은 개수 : 0\nEpoch : 0, batch 1038\n(Train) Batch 1038 Loss : 4.931543827056885, 맞은 개수 : 5\nEpoch : 0, batch 1039\n(Train) Batch 1039 Loss : 4.931826591491699, 맞은 개수 : 3\nEpoch : 0, batch 1040\n(Train) Batch 1040 Loss : 4.95655632019043, 맞은 개수 : 3\nEpoch : 0, batch 1041\n(Train) Batch 1041 Loss : 4.909073829650879, 맞은 개수 : 4\nEpoch : 0, batch 1042\n(Train) Batch 1042 Loss : 4.961620807647705, 맞은 개수 : 1\nEpoch : 0, batch 1043\n(Train) Batch 1043 Loss : 4.976991653442383, 맞은 개수 : 1\nEpoch : 0, batch 1044\n(Train) Batch 1044 Loss : 5.014387607574463, 맞은 개수 : 3\nEpoch : 0, batch 1045\n(Train) Batch 1045 Loss : 4.9528656005859375, 맞은 개수 : 4\nEpoch : 0, batch 1046\n(Train) Batch 1046 Loss : 4.951576232910156, 맞은 개수 : 5\nEpoch : 0, batch 1047\n(Train) Batch 1047 Loss : 5.137771129608154, 맞은 개수 : 4\nEpoch : 0, batch 1048\n(Train) Batch 1048 Loss : 4.923192501068115, 맞은 개수 : 3\nEpoch : 0, batch 1049\n(Train) Batch 1049 Loss : 4.875385761260986, 맞은 개수 : 4\nEpoch : 0, batch 1050\n(Train) Batch 1050 Loss : 4.992441654205322, 맞은 개수 : 3\nEpoch : 0, batch 1051\n(Train) Batch 1051 Loss : 4.8759355545043945, 맞은 개수 : 2\nEpoch : 0, batch 1052\n(Train) Batch 1052 Loss : 4.931182384490967, 맞은 개수 : 6\nEpoch : 0, batch 1053\n(Train) Batch 1053 Loss : 4.877433776855469, 맞은 개수 : 6\nEpoch : 0, batch 1054\n(Train) Batch 1054 Loss : 4.844983100891113, 맞은 개수 : 3\nEpoch : 0, batch 1055\n(Train) Batch 1055 Loss : 4.799891471862793, 맞은 개수 : 8\nEpoch : 0, batch 1056\n(Train) Batch 1056 Loss : 4.853935241699219, 맞은 개수 : 9\nEpoch : 0, batch 1057\n(Train) Batch 1057 Loss : 4.973711013793945, 맞은 개수 : 3\nEpoch : 0, batch 1058\n(Train) Batch 1058 Loss : 4.898478984832764, 맞은 개수 : 2\nEpoch : 0, batch 1059\n(Train) Batch 1059 Loss : 5.021127223968506, 맞은 개수 : 3\nEpoch : 0, batch 1060\n(Train) Batch 1060 Loss : 4.848656177520752, 맞은 개수 : 4\nEpoch : 0, batch 1061\n(Train) Batch 1061 Loss : 4.865914344787598, 맞은 개수 : 6\nEpoch : 0, batch 1062\n(Train) Batch 1062 Loss : 4.908770561218262, 맞은 개수 : 2\nEpoch : 0, batch 1063\n(Train) Batch 1063 Loss : 5.000096321105957, 맞은 개수 : 4\nEpoch : 0, batch 1064\n(Train) Batch 1064 Loss : 4.9524922370910645, 맞은 개수 : 3\nEpoch : 0, batch 1065\n(Train) Batch 1065 Loss : 4.889653205871582, 맞은 개수 : 3\nEpoch : 0, batch 1066\n(Train) Batch 1066 Loss : 4.88541841506958, 맞은 개수 : 4\nEpoch : 0, batch 1067\n(Train) Batch 1067 Loss : 4.8634443283081055, 맞은 개수 : 5\nEpoch : 0, batch 1068\n(Train) Batch 1068 Loss : 4.928717136383057, 맞은 개수 : 2\nEpoch : 0, batch 1069\n(Train) Batch 1069 Loss : 4.8161468505859375, 맞은 개수 : 4\nEpoch : 0, batch 1070\n(Train) Batch 1070 Loss : 4.92908239364624, 맞은 개수 : 6\nEpoch : 0, batch 1071\n(Train) Batch 1071 Loss : 4.794762134552002, 맞은 개수 : 3\nEpoch : 0, batch 1072\n(Train) Batch 1072 Loss : 4.776320934295654, 맞은 개수 : 3\nEpoch : 0, batch 1073\n(Train) Batch 1073 Loss : 4.8211989402771, 맞은 개수 : 4\nEpoch : 0, batch 1074\n(Train) Batch 1074 Loss : 4.7996320724487305, 맞은 개수 : 6\nEpoch : 0, batch 1075\n(Train) Batch 1075 Loss : 4.9872727394104, 맞은 개수 : 4\nEpoch : 0, batch 1076\n(Train) Batch 1076 Loss : 5.067590236663818, 맞은 개수 : 2\nEpoch : 0, batch 1077\n(Train) Batch 1077 Loss : 5.010890007019043, 맞은 개수 : 0\nEpoch : 0, batch 1078\n(Train) Batch 1078 Loss : 4.883965015411377, 맞은 개수 : 8\nEpoch : 0, batch 1079\n(Train) Batch 1079 Loss : 4.948008060455322, 맞은 개수 : 4\nEpoch : 0, batch 1080\n(Train) Batch 1080 Loss : 4.827898025512695, 맞은 개수 : 3\nEpoch : 0, batch 1081\n(Train) Batch 1081 Loss : 4.8740692138671875, 맞은 개수 : 2\nEpoch : 0, batch 1082\n(Train) Batch 1082 Loss : 4.8651251792907715, 맞은 개수 : 2\nEpoch : 0, batch 1083\n(Train) Batch 1083 Loss : 5.103071212768555, 맞은 개수 : 2\nEpoch : 0, batch 1084\n(Train) Batch 1084 Loss : 4.900411128997803, 맞은 개수 : 2\nEpoch : 0, batch 1085\n(Train) Batch 1085 Loss : 4.912632465362549, 맞은 개수 : 3\nEpoch : 0, batch 1086\n(Train) Batch 1086 Loss : 5.005433082580566, 맞은 개수 : 2\nEpoch : 0, batch 1087\n(Train) Batch 1087 Loss : 4.877215385437012, 맞은 개수 : 9\nEpoch : 0, batch 1088\n(Train) Batch 1088 Loss : 4.8737053871154785, 맞은 개수 : 2\nEpoch : 0, batch 1089\n(Train) Batch 1089 Loss : 4.889665603637695, 맞은 개수 : 3\nEpoch : 0, batch 1090\n(Train) Batch 1090 Loss : 4.760704040527344, 맞은 개수 : 6\nEpoch : 0, batch 1091\n(Train) Batch 1091 Loss : 4.86221981048584, 맞은 개수 : 5\nEpoch : 0, batch 1092\n(Train) Batch 1092 Loss : 4.928625583648682, 맞은 개수 : 4\nEpoch : 0, batch 1093\n(Train) Batch 1093 Loss : 4.9910197257995605, 맞은 개수 : 5\nEpoch : 0, batch 1094\n(Train) Batch 1094 Loss : 5.064605712890625, 맞은 개수 : 3\nEpoch : 0, batch 1095\n(Train) Batch 1095 Loss : 4.9173431396484375, 맞은 개수 : 3\nEpoch : 0, batch 1096\n(Train) Batch 1096 Loss : 5.011984348297119, 맞은 개수 : 1\nEpoch : 0, batch 1097\n(Train) Batch 1097 Loss : 4.930966854095459, 맞은 개수 : 6\nEpoch : 0, batch 1098\n(Train) Batch 1098 Loss : 4.893895626068115, 맞은 개수 : 4\nEpoch : 0, batch 1099\n(Train) Batch 1099 Loss : 4.928869724273682, 맞은 개수 : 3\nEpoch : 0, batch 1100\n(Train) Batch 1100 Loss : 4.938936233520508, 맞은 개수 : 1\nEpoch : 0, batch 1101\n(Train) Batch 1101 Loss : 4.798024654388428, 맞은 개수 : 4\nEpoch : 0, batch 1102\n(Train) Batch 1102 Loss : 4.993969917297363, 맞은 개수 : 4\nEpoch : 0, batch 1103\n(Train) Batch 1103 Loss : 4.93380069732666, 맞은 개수 : 6\nEpoch : 0, batch 1104\n(Train) Batch 1104 Loss : 4.987980365753174, 맞은 개수 : 2\nEpoch : 0, batch 1105\n(Train) Batch 1105 Loss : 4.963308334350586, 맞은 개수 : 3\nEpoch : 0, batch 1106\n(Train) Batch 1106 Loss : 4.9915924072265625, 맞은 개수 : 2\nEpoch : 0, batch 1107\n(Train) Batch 1107 Loss : 4.8244781494140625, 맞은 개수 : 4\nEpoch : 0, batch 1108\n(Train) Batch 1108 Loss : 4.926740646362305, 맞은 개수 : 2\nEpoch : 0, batch 1109\n(Train) Batch 1109 Loss : 4.8663763999938965, 맞은 개수 : 4\nEpoch : 0, batch 1110\n(Train) Batch 1110 Loss : 4.953812122344971, 맞은 개수 : 2\nEpoch : 0, batch 1111\n(Train) Batch 1111 Loss : 4.995748996734619, 맞은 개수 : 2\nEpoch : 0, batch 1112\n(Train) Batch 1112 Loss : 4.973986625671387, 맞은 개수 : 4\nEpoch : 0, batch 1113\n(Train) Batch 1113 Loss : 4.7862091064453125, 맞은 개수 : 4\nEpoch : 0, batch 1114\n(Train) Batch 1114 Loss : 4.907382965087891, 맞은 개수 : 3\nEpoch : 0, batch 1115\n(Train) Batch 1115 Loss : 4.924670696258545, 맞은 개수 : 4\nEpoch : 0, batch 1116\n(Train) Batch 1116 Loss : 4.952431678771973, 맞은 개수 : 2\nEpoch : 0, batch 1117\n(Train) Batch 1117 Loss : 4.931551933288574, 맞은 개수 : 2\nEpoch : 0, batch 1118\n(Train) Batch 1118 Loss : 4.865945816040039, 맞은 개수 : 6\nEpoch : 0, batch 1119\n(Train) Batch 1119 Loss : 4.953742504119873, 맞은 개수 : 7\nEpoch : 0, batch 1120\n(Train) Batch 1120 Loss : 4.921837329864502, 맞은 개수 : 1\nEpoch : 0, batch 1121\n(Train) Batch 1121 Loss : 5.04351282119751, 맞은 개수 : 2\nEpoch : 0, batch 1122\n(Train) Batch 1122 Loss : 4.871674537658691, 맞은 개수 : 5\nEpoch : 0, batch 1123\n(Train) Batch 1123 Loss : 4.900397777557373, 맞은 개수 : 4\nEpoch : 0, batch 1124\n(Train) Batch 1124 Loss : 4.765171051025391, 맞은 개수 : 6\nEpoch : 0, batch 1125\n(Train) Batch 1125 Loss : 4.974676132202148, 맞은 개수 : 3\nEpoch : 0, batch 1126\n(Train) Batch 1126 Loss : 4.896040439605713, 맞은 개수 : 7\nEpoch : 0, batch 1127\n(Train) Batch 1127 Loss : 4.852694988250732, 맞은 개수 : 6\nEpoch : 0, batch 1128\n(Train) Batch 1128 Loss : 5.0544867515563965, 맞은 개수 : 2\nEpoch : 0, batch 1129\n(Train) Batch 1129 Loss : 4.792901515960693, 맞은 개수 : 3\nEpoch : 0, batch 1130\n(Train) Batch 1130 Loss : 4.982548236846924, 맞은 개수 : 2\nEpoch : 0, batch 1131\n(Train) Batch 1131 Loss : 4.951300621032715, 맞은 개수 : 4\nEpoch : 0, batch 1132\n(Train) Batch 1132 Loss : 5.049093246459961, 맞은 개수 : 4\nEpoch : 0, batch 1133\n(Train) Batch 1133 Loss : 4.862069606781006, 맞은 개수 : 3\nEpoch : 0, batch 1134\n(Train) Batch 1134 Loss : 4.9440484046936035, 맞은 개수 : 6\nEpoch : 0, batch 1135\n(Train) Batch 1135 Loss : 4.9120564460754395, 맞은 개수 : 3\nEpoch : 0, batch 1136\n(Train) Batch 1136 Loss : 4.862487316131592, 맞은 개수 : 5\nEpoch : 0, batch 1137\n(Train) Batch 1137 Loss : 4.711734294891357, 맞은 개수 : 6\nEpoch : 0, batch 1138\n(Train) Batch 1138 Loss : 4.797694683074951, 맞은 개수 : 4\nEpoch : 0, batch 1139\n(Train) Batch 1139 Loss : 4.84622859954834, 맞은 개수 : 6\nEpoch : 0, batch 1140\n(Train) Batch 1140 Loss : 4.909213542938232, 맞은 개수 : 4\nEpoch : 0, batch 1141\n(Train) Batch 1141 Loss : 4.991518497467041, 맞은 개수 : 2\nEpoch : 0, batch 1142\n(Train) Batch 1142 Loss : 4.935580730438232, 맞은 개수 : 4\nEpoch : 0, batch 1143\n(Train) Batch 1143 Loss : 4.862858772277832, 맞은 개수 : 5\nEpoch : 0, batch 1144\n(Train) Batch 1144 Loss : 4.865147590637207, 맞은 개수 : 7\nEpoch : 0, batch 1145\n(Train) Batch 1145 Loss : 4.873902797698975, 맞은 개수 : 1\nEpoch : 0, batch 1146\n(Train) Batch 1146 Loss : 4.851553916931152, 맞은 개수 : 7\nEpoch : 0, batch 1147\n(Train) Batch 1147 Loss : 4.866204261779785, 맞은 개수 : 2\nEpoch : 0, batch 1148\n(Train) Batch 1148 Loss : 4.976123332977295, 맞은 개수 : 5\nEpoch : 0, batch 1149\n(Train) Batch 1149 Loss : 4.980181694030762, 맞은 개수 : 3\nEpoch : 0, batch 1150\n(Train) Batch 1150 Loss : 5.0366034507751465, 맞은 개수 : 2\nEpoch : 0, batch 1151\n(Train) Batch 1151 Loss : 4.931472301483154, 맞은 개수 : 4\nEpoch : 0, batch 1152\n(Train) Batch 1152 Loss : 4.946502685546875, 맞은 개수 : 2\nEpoch : 0, batch 1153\n(Train) Batch 1153 Loss : 4.858121395111084, 맞은 개수 : 4\nEpoch : 0, batch 1154\n(Train) Batch 1154 Loss : 4.937358379364014, 맞은 개수 : 3\nEpoch : 0, batch 1155\n(Train) Batch 1155 Loss : 5.057107448577881, 맞은 개수 : 3\nEpoch : 0, batch 1156\n(Train) Batch 1156 Loss : 4.9057230949401855, 맞은 개수 : 1\nEpoch : 0, batch 1157\n(Train) Batch 1157 Loss : 4.86973237991333, 맞은 개수 : 5\nEpoch : 0, batch 1158\n(Train) Batch 1158 Loss : 4.922719955444336, 맞은 개수 : 4\nEpoch : 0, batch 1159\n(Train) Batch 1159 Loss : 4.85044527053833, 맞은 개수 : 5\nEpoch : 0, batch 1160\n(Train) Batch 1160 Loss : 4.649516582489014, 맞은 개수 : 6\nEpoch : 0, batch 1161\n(Train) Batch 1161 Loss : 4.9712371826171875, 맞은 개수 : 7\nEpoch : 0, batch 1162\n(Train) Batch 1162 Loss : 4.924729824066162, 맞은 개수 : 2\nEpoch : 0, batch 1163\n(Train) Batch 1163 Loss : 4.812784671783447, 맞은 개수 : 4\nEpoch : 0, batch 1164\n(Train) Batch 1164 Loss : 4.9068450927734375, 맞은 개수 : 3\nEpoch : 0, batch 1165\n(Train) Batch 1165 Loss : 4.874797821044922, 맞은 개수 : 4\nEpoch : 0, batch 1166\n(Train) Batch 1166 Loss : 4.721469402313232, 맞은 개수 : 7\nEpoch : 0, batch 1167\n(Train) Batch 1167 Loss : 4.811629772186279, 맞은 개수 : 5\nEpoch : 0, batch 1168\n(Train) Batch 1168 Loss : 4.877176761627197, 맞은 개수 : 9\nEpoch : 0, batch 1169\n(Train) Batch 1169 Loss : 5.012331008911133, 맞은 개수 : 2\nEpoch : 0, batch 1170\n(Train) Batch 1170 Loss : 4.954845905303955, 맞은 개수 : 3\nEpoch : 0, batch 1171\n(Train) Batch 1171 Loss : 4.935227394104004, 맞은 개수 : 6\nEpoch : 0, batch 1172\n(Train) Batch 1172 Loss : 4.787357807159424, 맞은 개수 : 4\nEpoch : 0, batch 1173\n(Train) Batch 1173 Loss : 4.801093101501465, 맞은 개수 : 5\nEpoch : 0, batch 1174\n(Train) Batch 1174 Loss : 4.827718257904053, 맞은 개수 : 3\nEpoch : 0, batch 1175\n(Train) Batch 1175 Loss : 4.842653274536133, 맞은 개수 : 8\nEpoch : 0, batch 1176\n(Train) Batch 1176 Loss : 5.013667583465576, 맞은 개수 : 1\nEpoch : 0, batch 1177\n(Train) Batch 1177 Loss : 4.878469467163086, 맞은 개수 : 5\nEpoch : 0, batch 1178\n(Train) Batch 1178 Loss : 4.771124362945557, 맞은 개수 : 5\nEpoch : 0, batch 1179\n(Train) Batch 1179 Loss : 4.869338512420654, 맞은 개수 : 3\nEpoch : 0, batch 1180\n(Train) Batch 1180 Loss : 4.94507360458374, 맞은 개수 : 0\nEpoch : 0, batch 1181\n(Train) Batch 1181 Loss : 4.899919509887695, 맞은 개수 : 11\nEpoch : 0, batch 1182\n(Train) Batch 1182 Loss : 4.856803894042969, 맞은 개수 : 4\nEpoch : 0, batch 1183\n(Train) Batch 1183 Loss : 4.954076290130615, 맞은 개수 : 13\nEpoch : 0, batch 1184\n(Train) Batch 1184 Loss : 4.771632671356201, 맞은 개수 : 7\nEpoch : 0, batch 1185\n(Train) Batch 1185 Loss : 4.735876083374023, 맞은 개수 : 11\nEpoch : 0, batch 1186\n(Train) Batch 1186 Loss : 4.991274356842041, 맞은 개수 : 5\nEpoch : 0, batch 1187\n(Train) Batch 1187 Loss : 5.028661251068115, 맞은 개수 : 2\nEpoch : 0, batch 1188\n(Train) Batch 1188 Loss : 4.903124809265137, 맞은 개수 : 3\nEpoch : 0, batch 1189\n(Train) Batch 1189 Loss : 4.812742710113525, 맞은 개수 : 9\nEpoch : 0, batch 1190\n(Train) Batch 1190 Loss : 4.854084014892578, 맞은 개수 : 4\nEpoch : 0, batch 1191\n(Train) Batch 1191 Loss : 4.933526515960693, 맞은 개수 : 5\nEpoch : 0, batch 1192\n(Train) Batch 1192 Loss : 4.803741455078125, 맞은 개수 : 8\nEpoch : 0, batch 1193\n(Train) Batch 1193 Loss : 4.883665561676025, 맞은 개수 : 5\nEpoch : 0, batch 1194\n(Train) Batch 1194 Loss : 4.7772216796875, 맞은 개수 : 7\nEpoch : 0, batch 1195\n(Train) Batch 1195 Loss : 5.072110176086426, 맞은 개수 : 6\nEpoch : 0, batch 1196\n(Train) Batch 1196 Loss : 5.002203941345215, 맞은 개수 : 6\nEpoch : 0, batch 1197\n(Train) Batch 1197 Loss : 4.846807956695557, 맞은 개수 : 2\nEpoch : 0, batch 1198\n(Train) Batch 1198 Loss : 4.917690753936768, 맞은 개수 : 5\nEpoch : 0, batch 1199\n(Train) Batch 1199 Loss : 4.860223293304443, 맞은 개수 : 6\nEpoch : 0, batch 1200\n(Train) Batch 1200 Loss : 4.887497425079346, 맞은 개수 : 2\nEpoch : 0, batch 1201\n(Train) Batch 1201 Loss : 4.915672779083252, 맞은 개수 : 4\nEpoch : 0, batch 1202\n(Train) Batch 1202 Loss : 4.859322547912598, 맞은 개수 : 5\nEpoch : 0, batch 1203\n(Train) Batch 1203 Loss : 4.920829772949219, 맞은 개수 : 3\nEpoch : 0, batch 1204\n(Train) Batch 1204 Loss : 4.875759124755859, 맞은 개수 : 8\nEpoch : 0, batch 1205\n(Train) Batch 1205 Loss : 4.880604267120361, 맞은 개수 : 7\nEpoch : 0, batch 1206\n(Train) Batch 1206 Loss : 4.9782490730285645, 맞은 개수 : 1\nEpoch : 0, batch 1207\n(Train) Batch 1207 Loss : 4.9884209632873535, 맞은 개수 : 5\nEpoch : 0, batch 1208\n(Train) Batch 1208 Loss : 5.029068946838379, 맞은 개수 : 6\nEpoch : 0, batch 1209\n(Train) Batch 1209 Loss : 4.940781116485596, 맞은 개수 : 3\nEpoch : 0, batch 1210\n(Train) Batch 1210 Loss : 4.849141597747803, 맞은 개수 : 9\nEpoch : 0, batch 1211\n(Train) Batch 1211 Loss : 4.902222633361816, 맞은 개수 : 4\nEpoch : 0, batch 1212\n(Train) Batch 1212 Loss : 4.95349645614624, 맞은 개수 : 6\nEpoch : 0, batch 1213\n(Train) Batch 1213 Loss : 4.923254489898682, 맞은 개수 : 5\nEpoch : 0, batch 1214\n(Train) Batch 1214 Loss : 4.920076370239258, 맞은 개수 : 4\nEpoch : 0, batch 1215\n(Train) Batch 1215 Loss : 4.834576606750488, 맞은 개수 : 4\nEpoch : 0, batch 1216\n(Train) Batch 1216 Loss : 4.8269829750061035, 맞은 개수 : 8\nEpoch : 0, batch 1217\n(Train) Batch 1217 Loss : 5.05085563659668, 맞은 개수 : 2\nEpoch : 0, batch 1218\n(Train) Batch 1218 Loss : 4.9056854248046875, 맞은 개수 : 3\nEpoch : 0, batch 1219\n(Train) Batch 1219 Loss : 4.917124271392822, 맞은 개수 : 0\nEpoch : 0, batch 1220\n(Train) Batch 1220 Loss : 4.784567832946777, 맞은 개수 : 4\nEpoch : 0, batch 1221\n(Train) Batch 1221 Loss : 4.871699333190918, 맞은 개수 : 4\nEpoch : 0, batch 1222\n(Train) Batch 1222 Loss : 4.844934940338135, 맞은 개수 : 5\nEpoch : 0, batch 1223\n(Train) Batch 1223 Loss : 4.896369457244873, 맞은 개수 : 3\nEpoch : 0, batch 1224\n(Train) Batch 1224 Loss : 4.918203353881836, 맞은 개수 : 2\nEpoch : 0, batch 1225\n(Train) Batch 1225 Loss : 4.779406547546387, 맞은 개수 : 2\nEpoch : 0, batch 1226\n(Train) Batch 1226 Loss : 4.714864730834961, 맞은 개수 : 7\nEpoch : 0, batch 1227\n(Train) Batch 1227 Loss : 4.755220413208008, 맞은 개수 : 6\nEpoch : 0, batch 1228\n(Train) Batch 1228 Loss : 4.866009712219238, 맞은 개수 : 8\nEpoch : 0, batch 1229\n(Train) Batch 1229 Loss : 4.9381303787231445, 맞은 개수 : 5\nEpoch : 0, batch 1230\n(Train) Batch 1230 Loss : 4.924129009246826, 맞은 개수 : 3\nEpoch : 0, batch 1231\n(Train) Batch 1231 Loss : 4.802941799163818, 맞은 개수 : 2\nEpoch : 0, batch 1232\n(Train) Batch 1232 Loss : 4.945315361022949, 맞은 개수 : 4\nEpoch : 0, batch 1233\n(Train) Batch 1233 Loss : 4.870114326477051, 맞은 개수 : 6\nEpoch : 0, batch 1234\n(Train) Batch 1234 Loss : 4.845371723175049, 맞은 개수 : 2\nEpoch : 0, batch 1235\n(Train) Batch 1235 Loss : 4.941413402557373, 맞은 개수 : 4\nEpoch : 0, batch 1236\n(Train) Batch 1236 Loss : 4.818348407745361, 맞은 개수 : 8\nEpoch : 0, batch 1237\n(Train) Batch 1237 Loss : 4.97501802444458, 맞은 개수 : 4\nEpoch : 0, batch 1238\n(Train) Batch 1238 Loss : 4.748711585998535, 맞은 개수 : 9\nEpoch : 0, batch 1239\n(Train) Batch 1239 Loss : 4.8655266761779785, 맞은 개수 : 8\nEpoch : 0, batch 1240\n(Train) Batch 1240 Loss : 4.905176639556885, 맞은 개수 : 3\nEpoch : 0, batch 1241\n(Train) Batch 1241 Loss : 4.934985637664795, 맞은 개수 : 6\nEpoch : 0, batch 1242\n(Train) Batch 1242 Loss : 4.828236103057861, 맞은 개수 : 4\nEpoch : 0, batch 1243\n(Train) Batch 1243 Loss : 4.811057090759277, 맞은 개수 : 3\nEpoch : 0, batch 1244\n(Train) Batch 1244 Loss : 4.843045234680176, 맞은 개수 : 1\nEpoch : 0, batch 1245\n(Train) Batch 1245 Loss : 4.833972454071045, 맞은 개수 : 2\nEpoch : 0, batch 1246\n(Train) Batch 1246 Loss : 4.836235523223877, 맞은 개수 : 4\nEpoch : 0, batch 1247\n(Train) Batch 1247 Loss : 4.996013641357422, 맞은 개수 : 1\nEpoch : 0, batch 1248\n(Train) Batch 1248 Loss : 4.817864418029785, 맞은 개수 : 4\nEpoch : 0, batch 1249\n(Train) Batch 1249 Loss : 4.860154151916504, 맞은 개수 : 5\nEpoch : 0, batch 1250\n(Train) Batch 1250 Loss : 4.976417064666748, 맞은 개수 : 2\nEpoch : 0, batch 1251\n(Train) Batch 1251 Loss : 4.897266864776611, 맞은 개수 : 3\nEpoch : 0, batch 1252\n(Train) Batch 1252 Loss : 4.810885906219482, 맞은 개수 : 9\nEpoch : 0, batch 1253\n(Train) Batch 1253 Loss : 4.857532501220703, 맞은 개수 : 2\nEpoch : 0, batch 1254\n(Train) Batch 1254 Loss : 4.775784015655518, 맞은 개수 : 4\nEpoch : 0, batch 1255\n(Train) Batch 1255 Loss : 4.925691604614258, 맞은 개수 : 3\nEpoch : 0, batch 1256\n(Train) Batch 1256 Loss : 4.943050384521484, 맞은 개수 : 3\nEpoch : 0, batch 1257\n(Train) Batch 1257 Loss : 4.907588958740234, 맞은 개수 : 7\nEpoch : 0, batch 1258\n(Train) Batch 1258 Loss : 4.729569435119629, 맞은 개수 : 2\nEpoch : 0, batch 1259\n(Train) Batch 1259 Loss : 4.735785484313965, 맞은 개수 : 11\nEpoch : 0, batch 1260\n(Train) Batch 1260 Loss : 5.036367416381836, 맞은 개수 : 6\nEpoch : 0, batch 1261\n(Train) Batch 1261 Loss : 4.79239559173584, 맞은 개수 : 4\nEpoch : 0, batch 1262\n(Train) Batch 1262 Loss : 4.963500022888184, 맞은 개수 : 2\nEpoch : 0, batch 1263\n(Train) Batch 1263 Loss : 4.862189292907715, 맞은 개수 : 6\nEpoch : 0, batch 1264\n(Train) Batch 1264 Loss : 4.870237350463867, 맞은 개수 : 3\nEpoch : 0, batch 1265\n(Train) Batch 1265 Loss : 4.866967678070068, 맞은 개수 : 6\nEpoch : 0, batch 1266\n(Train) Batch 1266 Loss : 4.93887996673584, 맞은 개수 : 5\nEpoch : 0, batch 1267\n(Train) Batch 1267 Loss : 4.818375110626221, 맞은 개수 : 6\nEpoch : 0, batch 1268\n(Train) Batch 1268 Loss : 4.895418643951416, 맞은 개수 : 3\nEpoch : 0, batch 1269\n(Train) Batch 1269 Loss : 4.748897075653076, 맞은 개수 : 7\nEpoch : 0, batch 1270\n(Train) Batch 1270 Loss : 5.111588001251221, 맞은 개수 : 3\nEpoch : 0, batch 1271\n(Train) Batch 1271 Loss : 4.797483444213867, 맞은 개수 : 4\nEpoch : 0, batch 1272\n(Train) Batch 1272 Loss : 4.615671634674072, 맞은 개수 : 8\nEpoch : 0, batch 1273\n(Train) Batch 1273 Loss : 4.8726725578308105, 맞은 개수 : 2\nEpoch : 0, batch 1274\n(Train) Batch 1274 Loss : 4.885016441345215, 맞은 개수 : 3\nEpoch : 0, batch 1275\n(Train) Batch 1275 Loss : 4.9320502281188965, 맞은 개수 : 6\nEpoch : 0, batch 1276\n(Train) Batch 1276 Loss : 4.816461086273193, 맞은 개수 : 5\nEpoch : 0, batch 1277\n(Train) Batch 1277 Loss : 4.814705848693848, 맞은 개수 : 5\nEpoch : 0, batch 1278\n(Train) Batch 1278 Loss : 4.937951564788818, 맞은 개수 : 2\nEpoch : 0, batch 1279\n(Train) Batch 1279 Loss : 4.644230365753174, 맞은 개수 : 4\nEpoch : 0, batch 1280\n(Train) Batch 1280 Loss : 4.828413009643555, 맞은 개수 : 5\nEpoch : 0, batch 1281\n(Train) Batch 1281 Loss : 4.786953926086426, 맞은 개수 : 8\nEpoch : 0, batch 1282\n(Train) Batch 1282 Loss : 4.788810729980469, 맞은 개수 : 12\nEpoch : 0, batch 1283\n(Train) Batch 1283 Loss : 4.8928961753845215, 맞은 개수 : 5\nEpoch : 0, batch 1284\n(Train) Batch 1284 Loss : 4.859633445739746, 맞은 개수 : 5\nEpoch : 0, batch 1285\n(Train) Batch 1285 Loss : 4.691983699798584, 맞은 개수 : 6\nEpoch : 0, batch 1286\n(Train) Batch 1286 Loss : 4.827515125274658, 맞은 개수 : 5\nEpoch : 0, batch 1287\n(Train) Batch 1287 Loss : 4.8681111335754395, 맞은 개수 : 6\nEpoch : 0, batch 1288\n(Train) Batch 1288 Loss : 4.766546726226807, 맞은 개수 : 8\nEpoch : 0, batch 1289\n(Train) Batch 1289 Loss : 4.817246913909912, 맞은 개수 : 4\nEpoch : 0, batch 1290\n(Train) Batch 1290 Loss : 5.030993461608887, 맞은 개수 : 4\nEpoch : 0, batch 1291\n(Train) Batch 1291 Loss : 4.840675354003906, 맞은 개수 : 6\nEpoch : 0, batch 1292\n(Train) Batch 1292 Loss : 4.877421855926514, 맞은 개수 : 5\nEpoch : 0, batch 1293\n(Train) Batch 1293 Loss : 4.770962238311768, 맞은 개수 : 6\nEpoch : 0, batch 1294\n(Train) Batch 1294 Loss : 4.713756084442139, 맞은 개수 : 4\nEpoch : 0, batch 1295\n(Train) Batch 1295 Loss : 4.973686218261719, 맞은 개수 : 2\nEpoch : 0, batch 1296\n(Train) Batch 1296 Loss : 4.88675594329834, 맞은 개수 : 5\nEpoch : 0, batch 1297\n(Train) Batch 1297 Loss : 4.784117221832275, 맞은 개수 : 4\nEpoch : 0, batch 1298\n(Train) Batch 1298 Loss : 4.785895824432373, 맞은 개수 : 3\nEpoch : 0, batch 1299\n(Train) Batch 1299 Loss : 4.847829341888428, 맞은 개수 : 2\nEpoch : 0, batch 1300\n(Train) Batch 1300 Loss : 4.707971096038818, 맞은 개수 : 6\nEpoch : 0, batch 1301\n(Train) Batch 1301 Loss : 4.946713924407959, 맞은 개수 : 4\nEpoch : 0, batch 1302\n(Train) Batch 1302 Loss : 4.845165252685547, 맞은 개수 : 11\nEpoch : 0, batch 1303\n(Train) Batch 1303 Loss : 4.812504291534424, 맞은 개수 : 3\nEpoch : 0, batch 1304\n(Train) Batch 1304 Loss : 4.697019577026367, 맞은 개수 : 7\nEpoch : 0, batch 1305\n(Train) Batch 1305 Loss : 4.854581832885742, 맞은 개수 : 5\nEpoch : 0, batch 1306\n(Train) Batch 1306 Loss : 4.977574825286865, 맞은 개수 : 3\nEpoch : 0, batch 1307\n(Train) Batch 1307 Loss : 4.985670566558838, 맞은 개수 : 0\nEpoch : 0, batch 1308\n(Train) Batch 1308 Loss : 4.797321319580078, 맞은 개수 : 6\nEpoch : 0, batch 1309\n(Train) Batch 1309 Loss : 4.63278865814209, 맞은 개수 : 7\nEpoch : 0, batch 1310\n(Train) Batch 1310 Loss : 4.820347309112549, 맞은 개수 : 5\nEpoch : 0, batch 1311\n(Train) Batch 1311 Loss : 5.017428398132324, 맞은 개수 : 4\nEpoch : 0, batch 1312\n(Train) Batch 1312 Loss : 4.681756973266602, 맞은 개수 : 10\nEpoch : 0, batch 1313\n(Train) Batch 1313 Loss : 4.589688301086426, 맞은 개수 : 6\nEpoch : 0, batch 1314\n(Train) Batch 1314 Loss : 4.836071968078613, 맞은 개수 : 6\nEpoch : 0, batch 1315\n(Train) Batch 1315 Loss : 4.85438346862793, 맞은 개수 : 8\nEpoch : 0, batch 1316\n(Train) Batch 1316 Loss : 4.870036602020264, 맞은 개수 : 6\nEpoch : 0, batch 1317\n(Train) Batch 1317 Loss : 4.790422439575195, 맞은 개수 : 4\nEpoch : 0, batch 1318\n(Train) Batch 1318 Loss : 4.795669078826904, 맞은 개수 : 9\nEpoch : 0, batch 1319\n(Train) Batch 1319 Loss : 4.922043800354004, 맞은 개수 : 2\nEpoch : 0, batch 1320\n(Train) Batch 1320 Loss : 4.873150825500488, 맞은 개수 : 5\nEpoch : 0, batch 1321\n(Train) Batch 1321 Loss : 4.879146099090576, 맞은 개수 : 6\nEpoch : 0, batch 1322\n(Train) Batch 1322 Loss : 4.7782368659973145, 맞은 개수 : 6\nEpoch : 0, batch 1323\n(Train) Batch 1323 Loss : 4.8384013175964355, 맞은 개수 : 4\nEpoch : 0, batch 1324\n(Train) Batch 1324 Loss : 4.7666473388671875, 맞은 개수 : 8\nEpoch : 0, batch 1325\n(Train) Batch 1325 Loss : 4.635772705078125, 맞은 개수 : 12\nEpoch : 0, batch 1326\n(Train) Batch 1326 Loss : 4.661132335662842, 맞은 개수 : 7\nEpoch : 0, batch 1327\n(Train) Batch 1327 Loss : 4.661229133605957, 맞은 개수 : 9\nEpoch : 0, batch 1328\n(Train) Batch 1328 Loss : 4.824615955352783, 맞은 개수 : 6\nEpoch : 0, batch 1329\n(Train) Batch 1329 Loss : 4.947545051574707, 맞은 개수 : 6\nEpoch : 0, batch 1330\n(Train) Batch 1330 Loss : 4.733171463012695, 맞은 개수 : 5\nEpoch : 0, batch 1331\n(Train) Batch 1331 Loss : 4.991293907165527, 맞은 개수 : 6\nEpoch : 0, batch 1332\n(Train) Batch 1332 Loss : 4.665768146514893, 맞은 개수 : 7\nEpoch : 0, batch 1333\n(Train) Batch 1333 Loss : 4.705544471740723, 맞은 개수 : 10\nEpoch : 0, batch 1334\n(Train) Batch 1334 Loss : 4.774397373199463, 맞은 개수 : 6\nEpoch : 0, batch 1335\n(Train) Batch 1335 Loss : 5.0170135498046875, 맞은 개수 : 5\nEpoch : 0, batch 1336\n(Train) Batch 1336 Loss : 4.736361503601074, 맞은 개수 : 6\nEpoch : 0, batch 1337\n(Train) Batch 1337 Loss : 4.946880340576172, 맞은 개수 : 7\nEpoch : 0, batch 1338\n(Train) Batch 1338 Loss : 4.739403247833252, 맞은 개수 : 7\nEpoch : 0, batch 1339\n(Train) Batch 1339 Loss : 4.842748641967773, 맞은 개수 : 2\nEpoch : 0, batch 1340\n(Train) Batch 1340 Loss : 4.854678630828857, 맞은 개수 : 2\nEpoch : 0, batch 1341\n(Train) Batch 1341 Loss : 4.861278057098389, 맞은 개수 : 7\nEpoch : 0, batch 1342\n(Train) Batch 1342 Loss : 4.791497230529785, 맞은 개수 : 4\nEpoch : 0, batch 1343\n(Train) Batch 1343 Loss : 4.868019104003906, 맞은 개수 : 3\nEpoch : 0, batch 1344\n(Train) Batch 1344 Loss : 4.790738582611084, 맞은 개수 : 6\nEpoch : 0, batch 1345\n(Train) Batch 1345 Loss : 4.89321231842041, 맞은 개수 : 3\nEpoch : 0, batch 1346\n(Train) Batch 1346 Loss : 4.592411041259766, 맞은 개수 : 9\nEpoch : 0, batch 1347\n(Train) Batch 1347 Loss : 4.806602954864502, 맞은 개수 : 13\nEpoch : 0, batch 1348\n(Train) Batch 1348 Loss : 4.831778049468994, 맞은 개수 : 5\nEpoch : 0, batch 1349\n(Train) Batch 1349 Loss : 4.818665027618408, 맞은 개수 : 9\nEpoch : 0, batch 1350\n(Train) Batch 1350 Loss : 4.780105113983154, 맞은 개수 : 5\nEpoch : 0, batch 1351\n(Train) Batch 1351 Loss : 4.91595458984375, 맞은 개수 : 6\nEpoch : 0, batch 1352\n(Train) Batch 1352 Loss : 4.747321128845215, 맞은 개수 : 4\nEpoch : 0, batch 1353\n(Train) Batch 1353 Loss : 4.7157368659973145, 맞은 개수 : 8\nEpoch : 0, batch 1354\n(Train) Batch 1354 Loss : 4.854523658752441, 맞은 개수 : 5\nEpoch : 0, batch 1355\n(Train) Batch 1355 Loss : 4.806301116943359, 맞은 개수 : 8\nEpoch : 0, batch 1356\n(Train) Batch 1356 Loss : 4.846883773803711, 맞은 개수 : 2\nEpoch : 0, batch 1357\n(Train) Batch 1357 Loss : 4.862553596496582, 맞은 개수 : 9\nEpoch : 0, batch 1358\n(Train) Batch 1358 Loss : 4.704910755157471, 맞은 개수 : 9\nEpoch : 0, batch 1359\n(Train) Batch 1359 Loss : 4.891355991363525, 맞은 개수 : 4\nEpoch : 0, batch 1360\n(Train) Batch 1360 Loss : 4.844851016998291, 맞은 개수 : 8\nEpoch : 0, batch 1361\n(Train) Batch 1361 Loss : 4.793909549713135, 맞은 개수 : 7\nEpoch : 0, batch 1362\n(Train) Batch 1362 Loss : 4.808333396911621, 맞은 개수 : 6\nEpoch : 0, batch 1363\n(Train) Batch 1363 Loss : 4.79287576675415, 맞은 개수 : 4\nEpoch : 0, batch 1364\n(Train) Batch 1364 Loss : 4.836704730987549, 맞은 개수 : 5\nEpoch : 0, batch 1365\n(Train) Batch 1365 Loss : 4.667664527893066, 맞은 개수 : 10\nEpoch : 0, batch 1366\n(Train) Batch 1366 Loss : 4.775096416473389, 맞은 개수 : 7\nEpoch : 0, batch 1367\n(Train) Batch 1367 Loss : 4.73137903213501, 맞은 개수 : 8\nEpoch : 0, batch 1368\n(Train) Batch 1368 Loss : 4.613057613372803, 맞은 개수 : 11\nEpoch : 0, batch 1369\n(Train) Batch 1369 Loss : 4.77661657333374, 맞은 개수 : 13\nEpoch : 0, batch 1370\n(Train) Batch 1370 Loss : 4.93673849105835, 맞은 개수 : 7\nEpoch : 0, batch 1371\n(Train) Batch 1371 Loss : 4.738605499267578, 맞은 개수 : 8\nEpoch : 0, batch 1372\n(Train) Batch 1372 Loss : 4.807338237762451, 맞은 개수 : 8\nEpoch : 0, batch 1373\n(Train) Batch 1373 Loss : 4.777783393859863, 맞은 개수 : 3\nEpoch : 0, batch 1374\n(Train) Batch 1374 Loss : 4.871095180511475, 맞은 개수 : 4\nEpoch : 0, batch 1375\n(Train) Batch 1375 Loss : 4.7428693771362305, 맞은 개수 : 8\nEpoch : 0, batch 1376\n(Train) Batch 1376 Loss : 4.744084358215332, 맞은 개수 : 5\nEpoch : 0, batch 1377\n(Train) Batch 1377 Loss : 4.663468360900879, 맞은 개수 : 8\nEpoch : 0, batch 1378\n(Train) Batch 1378 Loss : 4.791128635406494, 맞은 개수 : 6\nEpoch : 0, batch 1379\n(Train) Batch 1379 Loss : 4.889292240142822, 맞은 개수 : 4\nEpoch : 0, batch 1380\n(Train) Batch 1380 Loss : 4.648817539215088, 맞은 개수 : 11\nEpoch : 0, batch 1381\n(Train) Batch 1381 Loss : 4.707173824310303, 맞은 개수 : 5\nEpoch : 0, batch 1382\n(Train) Batch 1382 Loss : 4.7762322425842285, 맞은 개수 : 4\nEpoch : 0, batch 1383\n(Train) Batch 1383 Loss : 4.774559497833252, 맞은 개수 : 6\nEpoch : 0, batch 1384\n(Train) Batch 1384 Loss : 4.869712829589844, 맞은 개수 : 7\nEpoch : 0, batch 1385\n(Train) Batch 1385 Loss : 4.74000883102417, 맞은 개수 : 4\nEpoch : 0, batch 1386\n(Train) Batch 1386 Loss : 4.673650741577148, 맞은 개수 : 12\nEpoch : 0, batch 1387\n(Train) Batch 1387 Loss : 4.76875114440918, 맞은 개수 : 7\nEpoch : 0, batch 1388\n(Train) Batch 1388 Loss : 4.575660228729248, 맞은 개수 : 7\nEpoch : 0, batch 1389\n(Train) Batch 1389 Loss : 4.848622798919678, 맞은 개수 : 5\nEpoch : 0, batch 1390\n(Train) Batch 1390 Loss : 4.613898277282715, 맞은 개수 : 10\nEpoch : 0, batch 1391\n(Train) Batch 1391 Loss : 4.843135356903076, 맞은 개수 : 6\nEpoch : 0, batch 1392\n(Train) Batch 1392 Loss : 4.805928707122803, 맞은 개수 : 6\nEpoch : 0, batch 1393\n(Train) Batch 1393 Loss : 4.637911319732666, 맞은 개수 : 12\nEpoch : 0, batch 1394\n(Train) Batch 1394 Loss : 4.715143203735352, 맞은 개수 : 6\nEpoch : 0, batch 1395\n(Train) Batch 1395 Loss : 4.73441219329834, 맞은 개수 : 7\nEpoch : 0, batch 1396\n(Train) Batch 1396 Loss : 4.95165491104126, 맞은 개수 : 5\nEpoch : 0, batch 1397\n(Train) Batch 1397 Loss : 4.838230609893799, 맞은 개수 : 5\nEpoch : 0, batch 1398\n(Train) Batch 1398 Loss : 4.777774810791016, 맞은 개수 : 7\nEpoch : 0, batch 1399\n(Train) Batch 1399 Loss : 4.776305198669434, 맞은 개수 : 4\nEpoch : 0, batch 1400\n(Train) Batch 1400 Loss : 4.745235443115234, 맞은 개수 : 9\nEpoch : 0, batch 1401\n(Train) Batch 1401 Loss : 4.805240631103516, 맞은 개수 : 3\nEpoch : 0, batch 1402\n(Train) Batch 1402 Loss : 4.662799835205078, 맞은 개수 : 6\nEpoch : 0, batch 1403\n(Train) Batch 1403 Loss : 4.7145819664001465, 맞은 개수 : 6\nEpoch : 0, batch 1404\n(Train) Batch 1404 Loss : 4.551884174346924, 맞은 개수 : 6\nEpoch : 0, batch 1405\n(Train) Batch 1405 Loss : 4.843190670013428, 맞은 개수 : 8\nEpoch : 0, batch 1406\n(Train) Batch 1406 Loss : 4.707346439361572, 맞은 개수 : 4\nEpoch : 0, batch 1407\n(Train) Batch 1407 Loss : 4.776391983032227, 맞은 개수 : 7\nEpoch : 0, batch 1408\n(Train) Batch 1408 Loss : 4.566997051239014, 맞은 개수 : 6\nEpoch : 0, batch 1409\n(Train) Batch 1409 Loss : 4.720089435577393, 맞은 개수 : 9\nEpoch : 0, batch 1410\n(Train) Batch 1410 Loss : 4.58601713180542, 맞은 개수 : 9\nEpoch : 0, batch 1411\n(Train) Batch 1411 Loss : 4.746522903442383, 맞은 개수 : 5\nEpoch : 0, batch 1412\n(Train) Batch 1412 Loss : 4.73626708984375, 맞은 개수 : 6\nEpoch : 0, batch 1413\n(Train) Batch 1413 Loss : 4.73228645324707, 맞은 개수 : 6\nEpoch : 0, batch 1414\n(Train) Batch 1414 Loss : 4.65376091003418, 맞은 개수 : 7\nEpoch : 0, batch 1415\n(Train) Batch 1415 Loss : 4.625421524047852, 맞은 개수 : 10\nEpoch : 0, batch 1416\n(Train) Batch 1416 Loss : 4.686402797698975, 맞은 개수 : 5\nEpoch : 0, batch 1417\n(Train) Batch 1417 Loss : 4.91166353225708, 맞은 개수 : 7\nEpoch : 0, batch 1418\n(Train) Batch 1418 Loss : 4.658405303955078, 맞은 개수 : 11\nEpoch : 0, batch 1419\n(Train) Batch 1419 Loss : 4.7146124839782715, 맞은 개수 : 11\nEpoch : 0, batch 1420\n(Train) Batch 1420 Loss : 4.661270618438721, 맞은 개수 : 5\nEpoch : 0, batch 1421\n(Train) Batch 1421 Loss : 4.753420829772949, 맞은 개수 : 8\nEpoch : 0, batch 1422\n(Train) Batch 1422 Loss : 4.624728679656982, 맞은 개수 : 4\nEpoch : 0, batch 1423\n(Train) Batch 1423 Loss : 4.653442859649658, 맞은 개수 : 8\nEpoch : 0, batch 1424\n(Train) Batch 1424 Loss : 4.744135856628418, 맞은 개수 : 6\nEpoch : 0, batch 1425\n(Train) Batch 1425 Loss : 4.607093811035156, 맞은 개수 : 12\nEpoch : 0, batch 1426\n(Train) Batch 1426 Loss : 4.744073867797852, 맞은 개수 : 4\nEpoch : 0, batch 1427\n(Train) Batch 1427 Loss : 4.710644245147705, 맞은 개수 : 2\nEpoch : 0, batch 1428\n(Train) Batch 1428 Loss : 4.780989170074463, 맞은 개수 : 6\nEpoch : 0, batch 1429\n(Train) Batch 1429 Loss : 4.67894172668457, 맞은 개수 : 4\nEpoch : 0, batch 1430\n(Train) Batch 1430 Loss : 4.6801228523254395, 맞은 개수 : 4\nEpoch : 0, batch 1431\n(Train) Batch 1431 Loss : 4.838431358337402, 맞은 개수 : 7\nEpoch : 0, batch 1432\n(Train) Batch 1432 Loss : 4.942857265472412, 맞은 개수 : 1\nEpoch : 0, batch 1433\n(Train) Batch 1433 Loss : 4.761519908905029, 맞은 개수 : 3\nEpoch : 0, batch 1434\n(Train) Batch 1434 Loss : 4.749952793121338, 맞은 개수 : 5\nEpoch : 0, batch 1435\n(Train) Batch 1435 Loss : 4.740438461303711, 맞은 개수 : 5\nEpoch : 0, batch 1436\n(Train) Batch 1436 Loss : 4.6722002029418945, 맞은 개수 : 8\nEpoch : 0, batch 1437\n(Train) Batch 1437 Loss : 4.800940990447998, 맞은 개수 : 5\nEpoch : 0, batch 1438\n(Train) Batch 1438 Loss : 4.6661882400512695, 맞은 개수 : 7\nEpoch : 0, batch 1439\n(Train) Batch 1439 Loss : 4.828826904296875, 맞은 개수 : 2\nEpoch : 0, batch 1440\n(Train) Batch 1440 Loss : 4.601129055023193, 맞은 개수 : 10\nEpoch : 0, batch 1441\n(Train) Batch 1441 Loss : 4.853660583496094, 맞은 개수 : 11\nEpoch : 0, batch 1442\n(Train) Batch 1442 Loss : 4.6934075355529785, 맞은 개수 : 12\nEpoch : 0, batch 1443\n(Train) Batch 1443 Loss : 4.658187389373779, 맞은 개수 : 6\nEpoch : 0, batch 1444\n(Train) Batch 1444 Loss : 4.652318954467773, 맞은 개수 : 6\nEpoch : 0, batch 1445\n(Train) Batch 1445 Loss : 4.595021724700928, 맞은 개수 : 13\nEpoch : 0, batch 1446\n(Train) Batch 1446 Loss : 4.681366920471191, 맞은 개수 : 7\nEpoch : 0, batch 1447\n(Train) Batch 1447 Loss : 4.628608226776123, 맞은 개수 : 11\nEpoch : 0, batch 1448\n(Train) Batch 1448 Loss : 4.864552974700928, 맞은 개수 : 10\nEpoch : 0, batch 1449\n(Train) Batch 1449 Loss : 4.648342609405518, 맞은 개수 : 9\nEpoch : 0, batch 1450\n(Train) Batch 1450 Loss : 4.690108299255371, 맞은 개수 : 10\nEpoch : 0, batch 1451\n(Train) Batch 1451 Loss : 4.821976661682129, 맞은 개수 : 3\nEpoch : 0, batch 1452\n(Train) Batch 1452 Loss : 4.869457721710205, 맞은 개수 : 5\nEpoch : 0, batch 1453\n(Train) Batch 1453 Loss : 4.636270999908447, 맞은 개수 : 9\nEpoch : 0, batch 1454\n(Train) Batch 1454 Loss : 4.683342933654785, 맞은 개수 : 6\nEpoch : 0, batch 1455\n(Train) Batch 1455 Loss : 4.6375298500061035, 맞은 개수 : 8\nEpoch : 0, batch 1456\n(Train) Batch 1456 Loss : 4.935309886932373, 맞은 개수 : 0\nEpoch : 0, batch 1457\n(Train) Batch 1457 Loss : 4.589003562927246, 맞은 개수 : 5\nEpoch : 0, batch 1458\n(Train) Batch 1458 Loss : 4.515987396240234, 맞은 개수 : 4\nEpoch : 0, batch 1459\n(Train) Batch 1459 Loss : 4.983907222747803, 맞은 개수 : 7\nEpoch : 0, batch 1460\n(Train) Batch 1460 Loss : 4.538815975189209, 맞은 개수 : 9\nEpoch : 0, batch 1461\n(Train) Batch 1461 Loss : 4.626708507537842, 맞은 개수 : 11\nEpoch : 0, batch 1462\n(Train) Batch 1462 Loss : 4.765334606170654, 맞은 개수 : 8\nEpoch : 0, batch 1463\n(Train) Batch 1463 Loss : 4.708337783813477, 맞은 개수 : 5\nEpoch : 0, batch 1464\n(Train) Batch 1464 Loss : 4.867264747619629, 맞은 개수 : 5\nEpoch : 0, batch 1465\n(Train) Batch 1465 Loss : 4.796219825744629, 맞은 개수 : 9\nEpoch : 0, batch 1466\n(Train) Batch 1466 Loss : 4.5078630447387695, 맞은 개수 : 6\nEpoch : 0, batch 1467\n(Train) Batch 1467 Loss : 4.9034905433654785, 맞은 개수 : 9\nEpoch : 0, batch 1468\n(Train) Batch 1468 Loss : 4.626842498779297, 맞은 개수 : 9\nEpoch : 0, batch 1469\n(Train) Batch 1469 Loss : 4.706509113311768, 맞은 개수 : 7\nEpoch : 0, batch 1470\n(Train) Batch 1470 Loss : 4.710159778594971, 맞은 개수 : 6\nEpoch : 0, batch 1471\n(Train) Batch 1471 Loss : 4.773838043212891, 맞은 개수 : 6\nEpoch : 0, batch 1472\n(Train) Batch 1472 Loss : 4.760533332824707, 맞은 개수 : 4\nEpoch : 0, batch 1473\n(Train) Batch 1473 Loss : 4.568560600280762, 맞은 개수 : 7\nEpoch : 0, batch 1474\n(Train) Batch 1474 Loss : 4.674649715423584, 맞은 개수 : 7\nEpoch : 0, batch 1475\n(Train) Batch 1475 Loss : 4.738026142120361, 맞은 개수 : 8\nEpoch : 0, batch 1476\n(Train) Batch 1476 Loss : 4.798027992248535, 맞은 개수 : 5\nEpoch : 0, batch 1477\n(Train) Batch 1477 Loss : 4.756987571716309, 맞은 개수 : 5\nEpoch : 0, batch 1478\n(Train) Batch 1478 Loss : 4.773642063140869, 맞은 개수 : 4\nEpoch : 0, batch 1479\n(Train) Batch 1479 Loss : 4.692930221557617, 맞은 개수 : 6\nEpoch : 0, batch 1480\n(Train) Batch 1480 Loss : 4.774925708770752, 맞은 개수 : 7\nEpoch : 0, batch 1481\n(Train) Batch 1481 Loss : 4.621297359466553, 맞은 개수 : 5\nEpoch : 0, batch 1482\n(Train) Batch 1482 Loss : 4.693912982940674, 맞은 개수 : 6\nEpoch : 0, batch 1483\n(Train) Batch 1483 Loss : 4.879310131072998, 맞은 개수 : 4\nEpoch : 0, batch 1484\n(Train) Batch 1484 Loss : 4.610137939453125, 맞은 개수 : 9\nEpoch : 0, batch 1485\n(Train) Batch 1485 Loss : 4.567629337310791, 맞은 개수 : 4\nEpoch : 0, batch 1486\n(Train) Batch 1486 Loss : 4.6182050704956055, 맞은 개수 : 8\nEpoch : 0, batch 1487\n(Train) Batch 1487 Loss : 4.511805057525635, 맞은 개수 : 13\nEpoch : 0, batch 1488\n(Train) Batch 1488 Loss : 4.6511430740356445, 맞은 개수 : 11\nEpoch : 0, batch 1489\n(Train) Batch 1489 Loss : 4.687069416046143, 맞은 개수 : 7\nEpoch : 0, batch 1490\n(Train) Batch 1490 Loss : 4.72836971282959, 맞은 개수 : 8\nEpoch : 0, batch 1491\n(Train) Batch 1491 Loss : 4.5353569984436035, 맞은 개수 : 4\nEpoch : 0, batch 1492\n(Train) Batch 1492 Loss : 4.565059185028076, 맞은 개수 : 11\nEpoch : 0, batch 1493\n(Train) Batch 1493 Loss : 4.577873706817627, 맞은 개수 : 7\nEpoch : 0, batch 1494\n(Train) Batch 1494 Loss : 4.5819878578186035, 맞은 개수 : 8\nEpoch : 0, batch 1495\n(Train) Batch 1495 Loss : 4.583389759063721, 맞은 개수 : 3\nEpoch : 0, batch 1496\n(Train) Batch 1496 Loss : 4.897134780883789, 맞은 개수 : 6\nEpoch : 0, batch 1497\n(Train) Batch 1497 Loss : 4.708578109741211, 맞은 개수 : 9\nEpoch : 0, batch 1498\n(Train) Batch 1498 Loss : 4.542264938354492, 맞은 개수 : 13\nEpoch : 0, batch 1499\n(Train) Batch 1499 Loss : 4.680325984954834, 맞은 개수 : 11\nEpoch : 0, batch 1500\n(Train) Batch 1500 Loss : 4.774433135986328, 맞은 개수 : 10\nEpoch : 0, batch 1501\n(Train) Batch 1501 Loss : 4.80808162689209, 맞은 개수 : 8\nEpoch : 0, batch 1502\n(Train) Batch 1502 Loss : 4.600797653198242, 맞은 개수 : 13\nEpoch : 0, batch 1503\n(Train) Batch 1503 Loss : 4.490084171295166, 맞은 개수 : 15\nEpoch : 0, batch 1504\n(Train) Batch 1504 Loss : 4.724554538726807, 맞은 개수 : 5\nEpoch : 0, batch 1505\n(Train) Batch 1505 Loss : 4.679099082946777, 맞은 개수 : 6\nEpoch : 0, batch 1506\n(Train) Batch 1506 Loss : 4.552762508392334, 맞은 개수 : 5\nEpoch : 0, batch 1507\n(Train) Batch 1507 Loss : 4.540682792663574, 맞은 개수 : 11\nEpoch : 0, batch 1508\n(Train) Batch 1508 Loss : 4.662540435791016, 맞은 개수 : 5\nEpoch : 0, batch 1509\n(Train) Batch 1509 Loss : 4.622650146484375, 맞은 개수 : 7\nEpoch : 0, batch 1510\n(Train) Batch 1510 Loss : 4.663105487823486, 맞은 개수 : 9\nEpoch : 0, batch 1511\n(Train) Batch 1511 Loss : 4.556281566619873, 맞은 개수 : 5\nEpoch : 0, batch 1512\n(Train) Batch 1512 Loss : 4.774661540985107, 맞은 개수 : 8\nEpoch : 0, batch 1513\n(Train) Batch 1513 Loss : 4.7384724617004395, 맞은 개수 : 7\nEpoch : 0, batch 1514\n(Train) Batch 1514 Loss : 4.776629447937012, 맞은 개수 : 10\nEpoch : 0, batch 1515\n(Train) Batch 1515 Loss : 4.571683883666992, 맞은 개수 : 5\nEpoch : 0, batch 1516\n(Train) Batch 1516 Loss : 4.697457790374756, 맞은 개수 : 10\nEpoch : 0, batch 1517\n(Train) Batch 1517 Loss : 4.703777313232422, 맞은 개수 : 8\nEpoch : 0, batch 1518\n(Train) Batch 1518 Loss : 4.758201599121094, 맞은 개수 : 11\nEpoch : 0, batch 1519\n(Train) Batch 1519 Loss : 4.535541534423828, 맞은 개수 : 8\nEpoch : 0, batch 1520\n(Train) Batch 1520 Loss : 4.7185211181640625, 맞은 개수 : 9\nEpoch : 0, batch 1521\n(Train) Batch 1521 Loss : 4.630478382110596, 맞은 개수 : 9\nEpoch : 0, batch 1522\n(Train) Batch 1522 Loss : 4.577353000640869, 맞은 개수 : 6\nEpoch : 0, batch 1523\n(Train) Batch 1523 Loss : 4.552987575531006, 맞은 개수 : 10\nEpoch : 0, batch 1524\n(Train) Batch 1524 Loss : 4.774852275848389, 맞은 개수 : 6\nEpoch : 0, batch 1525\n(Train) Batch 1525 Loss : 4.658146381378174, 맞은 개수 : 8\nEpoch : 0, batch 1526\n(Train) Batch 1526 Loss : 4.613375663757324, 맞은 개수 : 6\nEpoch : 0, batch 1527\n(Train) Batch 1527 Loss : 4.7425079345703125, 맞은 개수 : 6\nEpoch : 0, batch 1528\n(Train) Batch 1528 Loss : 4.702774524688721, 맞은 개수 : 11\nEpoch : 0, batch 1529\n(Train) Batch 1529 Loss : 4.752157688140869, 맞은 개수 : 5\nEpoch : 0, batch 1530\n(Train) Batch 1530 Loss : 4.889330863952637, 맞은 개수 : 7\nEpoch : 0, batch 1531\n(Train) Batch 1531 Loss : 4.7958478927612305, 맞은 개수 : 5\nEpoch : 0, batch 1532\n(Train) Batch 1532 Loss : 4.523767948150635, 맞은 개수 : 10\nEpoch : 0, batch 1533\n(Train) Batch 1533 Loss : 4.725216388702393, 맞은 개수 : 7\nEpoch : 0, batch 1534\n(Train) Batch 1534 Loss : 4.680868148803711, 맞은 개수 : 5\nEpoch : 0, batch 1535\n(Train) Batch 1535 Loss : 4.601362228393555, 맞은 개수 : 9\nEpoch : 0, batch 1536\n(Train) Batch 1536 Loss : 4.653419017791748, 맞은 개수 : 5\nEpoch : 0, batch 1537\n(Train) Batch 1537 Loss : 4.7230048179626465, 맞은 개수 : 10\nEpoch : 0, batch 1538\n(Train) Batch 1538 Loss : 4.603575229644775, 맞은 개수 : 4\nEpoch : 0, batch 1539\n(Train) Batch 1539 Loss : 4.614596843719482, 맞은 개수 : 9\nEpoch : 0, batch 1540\n(Train) Batch 1540 Loss : 4.5232391357421875, 맞은 개수 : 6\nEpoch : 0, batch 1541\n(Train) Batch 1541 Loss : 4.568385124206543, 맞은 개수 : 9\nEpoch : 0, batch 1542\n(Train) Batch 1542 Loss : 4.687023639678955, 맞은 개수 : 6\nEpoch : 0, batch 1543\n(Train) Batch 1543 Loss : 4.6357316970825195, 맞은 개수 : 11\nEpoch : 0, batch 1544\n(Train) Batch 1544 Loss : 4.611301422119141, 맞은 개수 : 9\nEpoch : 0, batch 1545\n(Train) Batch 1545 Loss : 4.777605056762695, 맞은 개수 : 3\nEpoch : 0, batch 1546\n(Train) Batch 1546 Loss : 4.692143440246582, 맞은 개수 : 5\nEpoch : 0, batch 1547\n(Train) Batch 1547 Loss : 4.47592306137085, 맞은 개수 : 9\nEpoch : 0, batch 1548\n(Train) Batch 1548 Loss : 4.720655918121338, 맞은 개수 : 13\nEpoch : 0, batch 1549\n(Train) Batch 1549 Loss : 4.561309814453125, 맞은 개수 : 8\nEpoch : 0, batch 1550\n(Train) Batch 1550 Loss : 4.671509742736816, 맞은 개수 : 5\nEpoch : 0, batch 1551\n(Train) Batch 1551 Loss : 4.5692138671875, 맞은 개수 : 7\nEpoch : 0, batch 1552\n(Train) Batch 1552 Loss : 4.745554447174072, 맞은 개수 : 7\nEpoch : 0, batch 1553\n(Train) Batch 1553 Loss : 4.516510009765625, 맞은 개수 : 7\nEpoch : 0, batch 1554\n(Train) Batch 1554 Loss : 4.654479503631592, 맞은 개수 : 8\nEpoch : 0, batch 1555\n(Train) Batch 1555 Loss : 4.576936721801758, 맞은 개수 : 6\nEpoch : 0, batch 1556\n(Train) Batch 1556 Loss : 4.810029983520508, 맞은 개수 : 5\nEpoch : 0, batch 1557\n(Train) Batch 1557 Loss : 4.790529251098633, 맞은 개수 : 4\nEpoch : 0, batch 1558\n(Train) Batch 1558 Loss : 4.593464374542236, 맞은 개수 : 10\nEpoch : 0, batch 1559\n(Train) Batch 1559 Loss : 4.736374855041504, 맞은 개수 : 7\nEpoch : 0, batch 1560\n(Train) Batch 1560 Loss : 4.598611831665039, 맞은 개수 : 10\nEpoch : 0, batch 1561\n(Train) Batch 1561 Loss : 4.630438327789307, 맞은 개수 : 8\nEpoch : 0, batch 1562\n(Train) Batch 1562 Loss : 4.597813606262207, 맞은 개수 : 1\nepoch 0 Loss/Train :5.095089932244631 \nepoch 0 Accuracy/Train : 0.02396\nVAL : 예측라벨 : [ 26  69   0 188 145   0  13 161   0 192 188 189 198  52  13  39 188   0\n 188  13  11  69  39 192 158 177 188 143 188 192 188  13], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n(VAL) Batch 0 Loss : 4.37305212020874, accuracy: 0.125\nVAL : 예측라벨 : [180 192 192 143  31  74 192 192 192 192 192 192 188 192  74 192 187 199\n  57  93   1   6  37 146   6   6  37 167 117  54  37   1], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n(VAL) Batch 1 Loss : 4.216976642608643, accuracy: 0.0625\nVAL : 예측라벨 : [119  39   6  84  50  37  52  81 198 114  37  37  50   8  52   6 119  93\n  52   1   8  55  52  93  58   6 108  37   8   6   8  29], 정답 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n(VAL) Batch 2 Loss : 4.604251384735107, accuracy: 0.03125\nVAL : 예측라벨 : [ 37 119 185   1   6  39  50 199 170  39   8   8 160  52  69  57 114 110\n  37   6 166   8  57   8  48 170 198  39 192  39  52  52], 정답 [1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n(VAL) Batch 3 Loss : 4.795289516448975, accuracy: 0.03125\nVAL : 예측라벨 : [133  50  57 114 117 185   6 199  37  13   6 170  39  39 103 108  39 160\n  39   6  57 114  52  52  24  24  52 189 108  37   8  37], 정답 [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3]\n(VAL) Batch 4 Loss : 4.887068271636963, accuracy: 0.0\nVAL : 예측라벨 : [ 31 143  37 143  39   6  52   6  41  39 130 178  52  37   6 189   6  52\n  12  39   6   6  24  93  52  37   6  24   6  52  78  52], 정답 [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n(VAL) Batch 5 Loss : 4.782212257385254, accuracy: 0.0\nVAL : 예측라벨 : [  6  39 192  37 192  37  24  27  39  81  69  81 114   1  78  50 185  81\n  69  87   6 170   6  14  22 141   6  81 143  50 110   4], 정답 [3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n(VAL) Batch 6 Loss : 4.843558311462402, accuracy: 0.03125\nVAL : 예측라벨 : [ 37   6 167   4  13  48  48  81 122  52  50   4 101   1   8 166 117 107\n 107 187 185  12  56 114   6  37   6 103 189 108  24   6], 정답 [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5]\n(VAL) Batch 7 Loss : 4.763425350189209, accuracy: 0.0625\nVAL : 예측라벨 : [ 24  52  44  55  57  39  50  52   6 191  81  24 162  57  24  58 180  39\n   6 115  58  52  26   6 160  31 117  26  93 114 143 160], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n(VAL) Batch 8 Loss : 5.149746417999268, accuracy: 0.0\nVAL : 예측라벨 : [ 61  52 192   6 162  55   6 115 192  12  37   8 145   6 141  48 141   6\n   6   6   6   6   6  95 192   6  54   8  31  24  54   6], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\n(VAL) Batch 9 Loss : 4.262243747711182, accuracy: 0.28125\nVAL : 예측라벨 : [  6  57   6   6   9  57 160   6 192   6 117  24 103   6  55   6  27   8\n 192   6 160   6  57  27  50  50   6  27  55   8   8  52], 정답 [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7]\n(VAL) Batch 10 Loss : 3.930852174758911, accuracy: 0.3125\nVAL : 예측라벨 : [119  91   6  69 179  81  93   6   6   8  24  24  13   6  57   8  50  41\n   6  61 160 105  74 103  31 130  24   6 166   8 130  42], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n(VAL) Batch 11 Loss : 4.99851131439209, accuracy: 0.0\nVAL : 예측라벨 : [  8  93 192  57 117  31  39   8  24 103 164  31  13  41   6  91 117  37\n   8   8 141   6   8   8   8  69   8  50  95   8 108   8], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n(VAL) Batch 12 Loss : 4.3979339599609375, accuracy: 0.25\nVAL : 예측라벨 : [  8   8 160  37 117  37  37   8 158   8  55  13  50 122   8   8  93 117\n 122  37   8  50   8   8  37   6  56 117   8  24  37   8], 정답 [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n(VAL) Batch 13 Loss : 3.8337814807891846, accuracy: 0.34375\nVAL : 예측라벨 : [  8   6 170 196  57   6   6 119  37  55 117 128   8 117  24  87   6 143\n  29   6  57  55 143  52   5  24  57   8   8   6  24   6], 정답 [8 8 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\n(VAL) Batch 14 Loss : 4.664028167724609, accuracy: 0.03125\nVAL : 예측라벨 : [  8  24  12  66   8 163  12   6   1  52   6   6  24  24  58 105  93  24\n  52  60  37  55 119  81 143  42 189   8  41  81 163   1], 정답 [ 9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9 10 10 10 10\n 10 10 10 10 10 10 10 10]\n(VAL) Batch 15 Loss : 4.782379627227783, accuracy: 0.0\nVAL : 예측라벨 : [ 57 188  93  91   6  39  69  39 180  69  31   6  39  37   1 160 122 170\n  31  52   6  93 117  41  12  93  69   6  38 119  24 117], 정답 [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n 10 10 10 10 10 10 10 10]\n(VAL) Batch 16 Loss : 4.816859245300293, accuracy: 0.0\nVAL : 예측라벨 : [119  69 117   8  52 143   6  37  87 143  81  22 110 170  37  81  87  56\n 170 115   6  52 109   1  50   6 170 103  55  50  31   8], 정답 [10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n 11 11 11 11 11 11 11 11]\n(VAL) Batch 17 Loss : 4.740184307098389, accuracy: 0.0\nVAL : 예측라벨 : [117  57  78  60  56  39  58   8 117  39  58 117  81  37 122  41  95   4\n 108  41 110  71  69  39  12  55  24  56  31  12  12  52], 정답 [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n 12 12 12 12 12 12 12 12]\n(VAL) Batch 18 Loss : 4.556894302368164, accuracy: 0.09375\nVAL : 예측라벨 : [ 31  60   6  39  56  24  24 167 105   6  24  24  81  71   6  60   6  24\n  81  24  74  24  24  37   6  39 158  60  60 119   6   6], 정답 [12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n 12 12 12 12 12 12 12 12]\n(VAL) Batch 19 Loss : 4.857207775115967, accuracy: 0.0\nVAL : 예측라벨 : [ 24  48  27  81 105   6 138 119  31  81  13   6  13 158  23  13  13  13\n 101 196 192 160  13  69  13  60  23  13  13 101  13  13], 정답 [12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13 13 13 13 13\n 13 13 13 13 13 13 13 13]\n(VAL) Batch 20 Loss : 3.4500410556793213, accuracy: 0.34375\nVAL : 예측라벨 : [ 13  13  13  80  13  13  24  13 166   0  13 160  23  13  13  13  13  13\n  69  13 138  13  13  13  13  13  13  13  24  67  57   6], 정답 [13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13\n 13 13 13 13 14 14 14 14]\n(VAL) Batch 21 Loss : 2.7100911140441895, accuracy: 0.625\nVAL : 예측라벨 : [ 39  52 196 162  14 192 196 196   6  37  69  48 198  52 148  13  13 160\n  14 192  69  50  52  50 192  23  14  39 196   6  12 160], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n 14 14 14 14 14 14 14 14]\n(VAL) Batch 22 Loss : 4.3999128341674805, accuracy: 0.09375\nVAL : 예측라벨 : [145   6  93  52  23 196 196  52 103  23  52 192  13  39  39 177 192  69\n   8  37  39  81  58  38  31  48 117  39  55 199   6 133], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15\n 15 15 15 15 15 15 15 15]\n(VAL) Batch 23 Loss : 4.537618637084961, accuracy: 0.0\nVAL : 예측라벨 : [ 50  91 192  24 185  93   6 109  12 109 187  26   8  13  69  39  37  39\n 160  52   8  39  39   6  39   6  37  27  24   8  24 187], 정답 [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n 15 15 15 15 15 15 15 15]\n(VAL) Batch 24 Loss : 4.8324174880981445, accuracy: 0.0\nVAL : 예측라벨 : [  6  52 185   8   6 170 146 119  50  11   1 130  24  39   0 145 192 119\n 199  27  58 161  39 103  31  32  24   0 109 103  37  24], 정답 [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n 16 16 16 16 16 16 16 16]\n(VAL) Batch 25 Loss : 5.487782955169678, accuracy: 0.0\nVAL : 예측라벨 : [  1 114  58 192  52   6  27 192  52   6   6 170  40   6  55 199  52  13\n  52  58  57  87  47  31 177  93  24   8  55  78 158  24], 정답 [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 17 17 17 17 17 17\n 17 17 17 17 17 17 17 17]\n(VAL) Batch 26 Loss : 5.26153564453125, accuracy: 0.0\nVAL : 예측라벨 : [  6  24 196 108  31  93   6  31  24 192   6  24  57 177  27  24 110   6\n 192  37 192   8 117  24  44 189  39  39  13  58  69  52], 정답 [17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17\n 17 17 17 17 17 17 17 17]\n(VAL) Batch 27 Loss : 5.054065704345703, accuracy: 0.0\nVAL : 예측라벨 : [ 24  69 192  80  90  57 177 158  24 189  68  24  44 138 192  52  13 192\n  90 180 192 189  93 177  31  24 130  90 189 189 105 177], 정답 [17 17 17 17 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18\n 18 18 18 18 18 18 18 18]\n(VAL) Batch 28 Loss : 5.103025913238525, accuracy: 0.0\nVAL : 예측라벨 : [177  52  24 189  39  26  24 189 189  12  90 110 192 138 180 189  52 138\n  90 180 177  24  52  93 108   9   6 161  52   6  12  56], 정답 [18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 19 19\n 19 19 19 19 19 19 19 19]\n(VAL) Batch 29 Loss : 5.3033037185668945, accuracy: 0.0\nVAL : 예측라벨 : [ 12   6  93 196  39 190 108  37  57  93  93 143  55  39   9 119  37 114\n 143 110 143 119   1 145   8 110  60  89 110  24  66 196], 정답 [19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19\n 19 19 19 19 19 19 19 19]\n(VAL) Batch 30 Loss : 4.924496650695801, accuracy: 0.0\nVAL : 예측라벨 : [ 13  29   4 108 160 115 143 164 188 101  39  12 114 122 109  50   1  39\n  37  37  12  13  58  69   6  58  55  38   4 110  39   6], 정답 [19 19 19 19 19 19 19 19 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20\n 20 20 20 20 20 20 20 20]\n(VAL) Batch 31 Loss : 4.727226734161377, accuracy: 0.0\nVAL : 예측라벨 : [ 37  50  13 122  81  37  24  37   8   8  37 141 122 117  37  24   6   8\n  37 192  36  24  37  50 103   8  38  27 160  13  24 110], 정답 [20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20\n 20 20 21 21 21 21 21 21]\n(VAL) Batch 32 Loss : 4.880393028259277, accuracy: 0.0\nVAL : 예측라벨 : [ 69  12  24  71  13 160 170  69  37 158  71  27 160 160 109  80  69 148\n  55  24  27 109  24  24  58  24 177  37  24  81 166  27], 정답 [21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21\n 21 21 21 21 21 21 21 21]\n(VAL) Batch 33 Loss : 5.287156105041504, accuracy: 0.0\nVAL : 예측라벨 : [ 37  24  39 117  31 122 122  29  12  13   6  24  48 124  56  24 137 198\n  13  13  13 101  11   6  31  71  13 122 122  71  22  69], 정답 [21 21 21 21 21 21 21 21 21 21 21 21 22 22 22 22 22 22 22 22 22 22 22 22\n 22 22 22 22 22 22 22 22]\n(VAL) Batch 34 Loss : 4.626791954040527, accuracy: 0.03125\nVAL : 예측라벨 : [117  81  81 170 110 117 198 110 109   8 185   8  13 117 117  50 101  69\n 101  81   6  24  29  27  78 115  55 141  69  71  13  39], 정답 [22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22\n 22 22 22 22 22 22 23 23]\n(VAL) Batch 35 Loss : 4.558040618896484, accuracy: 0.0\nVAL : 예측라벨 : [ 13  14  23  23  13 198 198  13  69  13 137  23  23  23 130  23  13 196\n 198  71  14  13 148  23 198  23  23  13 141  22  13  69], 정답 [23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n 23 23 23 23 23 23 23 23]\n(VAL) Batch 36 Loss : 3.1188955307006836, accuracy: 0.28125\nVAL : 예측라벨 : [ 23  39  71  13  48  23  13 199 148 148  71  13  39  23  13  23  24  24\n  24  27  24  24   8  24 110 180  69 138 110  58  24  24], 정답 [23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 24 24 24 24 24 24 24 24\n 24 24 24 24 24 24 24 24]\n(VAL) Batch 37 Loss : 3.442819118499756, accuracy: 0.375\nVAL : 예측라벨 : [ 24 110  90  31  60  69  24  31  31 189  31  24  26  31  29  24  52  31\n  60  24  31  69  24   6  26  37  12 177  48  24  52  26], 정답 [24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24\n 24 24 24 24 24 24 24 24]\n(VAL) Batch 38 Loss : 4.20125150680542, accuracy: 0.21875\nVAL : 예측라벨 : [141  69  24 105 192  24  24 160  24  31  38  74 192  24  24  24  26 189\n 138 117  29 192  24  24 160  24  31  26  24   6  37  24], 정답 [24 24 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25\n 25 25 25 25 25 25 25 25]\n(VAL) Batch 39 Loss : 4.827285289764404, accuracy: 0.0\nVAL : 예측라벨 : [ 37  24  52  31  29  24  39 160  57 114  52  90  24  29  37  24 119 158\n 192  58 192 192  31  48  31  24  26 160  24 160 192  26], 정답 [25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 26 26 26 26\n 26 26 26 26 26 26 26 26]\n(VAL) Batch 40 Loss : 4.8074846267700195, accuracy: 0.0625\nVAL : 예측라벨 : [158  31  52  27 170 192  31  31  26  26   6 132  31  31  74 148   6  24\n  24 192  31 192  26  69 192  31 192 145  31  52  24  69], 정답 [26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26\n 26 26 26 26 26 26 26 26]\n(VAL) Batch 41 Loss : 4.190367221832275, accuracy: 0.09375\nVAL : 예측라벨 : [192  37  31  27 170  37  55  69 198  58  31  54  37  24  12 109  31   8\n  11  56 180 105   6  48 117  37   6  24  24 110 158  55], 정답 [26 26 26 26 26 26 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27\n 27 27 27 27 27 27 27 27]\n(VAL) Batch 42 Loss : 4.310516834259033, accuracy: 0.0\nVAL : 예측라벨 : [114 141 114 158  27  11  50 108 160 110  69 119 160  37   6 160 143  26\n 117  24 179  24 192 105 160  37 128 170  24  74 124  37], 정답 [27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27\n 28 28 28 28 28 28 28 28]\n(VAL) Batch 43 Loss : 4.536007881164551, accuracy: 0.03125\nVAL : 예측라벨 : [  8  27 138 192 160 143  31  37 138   8 143  37 114  24   1 114  37  24\n  24 185 143  37  24  27  24  24  24  54  31  57  52  29], 정답 [28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28\n 28 28 28 28 28 28 28 28]\n(VAL) Batch 44 Loss : 4.6885881423950195, accuracy: 0.0\nVAL : 예측라벨 : [ 37  31  31  37  37  39  26  26  56   8 109  60  58 117  37  37  24 154\n 114  69  39  27  24  24 110 110  24  58 117  31  24  27], 정답 [28 28 28 28 28 28 28 28 28 28 29 29 29 29 29 29 29 29 29 29 29 29 29 29\n 29 29 29 29 29 29 29 29]\n(VAL) Batch 45 Loss : 4.6563334465026855, accuracy: 0.0\nVAL : 예측라벨 : [ 39  31  37  56  27 138 110 138  24  27  39  24 110  27  48  56  26  13\n  69 170  37 164  50 170  39   6  31  39  96  26   8 160], 정답 [29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29\n 29 29 29 29 30 30 30 30]\n(VAL) Batch 46 Loss : 4.683154582977295, accuracy: 0.0\nVAL : 예측라벨 : [ 29 199  24  26  24  24  24 115 160 117  91  24  37   6   8  74   6  24\n   6  24 117 109 180  29  24  24 105  29 177  58  27  24], 정답 [30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30\n 30 30 30 30 30 30 30 30]\n(VAL) Batch 47 Loss : 4.880073547363281, accuracy: 0.0\nVAL : 예측라벨 : [ 55   6   6   1  52  24  29  54  53  37  56   8  29   8  24  69  24   8\n  31 160  24  69  31 160  41  24  31  71 124  24 117   6], 정답 [30 30 30 30 30 30 30 30 30 30 30 30 30 30 31 31 31 31 31 31 31 31 31 31\n 31 31 31 31 31 31 31 31]\n(VAL) Batch 48 Loss : 4.407881736755371, accuracy: 0.09375\nVAL : 예측라벨 : [ 29  31   6  31  24   6  31 137  69  55  31  55  31  24 130  31  24  24\n  69  69  81  31  69  31  69 124  31  24  24  31  31  31], 정답 [31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31\n 31 31 31 31 31 31 31 31]\n(VAL) Batch 49 Loss : 3.7935025691986084, accuracy: 0.375\nVAL : 예측라벨 : [109  26  24 122  27  29  90  58 160 192  13 141 158  26 110  41  24   8\n  37  41  27  50  57   8  31 110   6  24   8  31  24   6], 정답 [32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32\n 32 32 32 32 32 32 32 32]\n(VAL) Batch 50 Loss : 4.598175525665283, accuracy: 0.0\nVAL : 예측라벨 : [ 90  69  31  24 119   8  55 109  29 110 105  24   6 110   8   6  69 188\n  24   6  52  54 117  27  27  50 146 192   6  24   6  31], 정답 [32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 33 33 33 33 33 33\n 33 33 33 33 33 33 33 33]\n(VAL) Batch 51 Loss : 4.558887481689453, accuracy: 0.0\nVAL : 예측라벨 : [110  29  26  31   6  93  31 192  52   8 160  57  27  12  56   6  31  31\n 188  24  31  29   6  78   6  31 160 192  95  31  31 137], 정답 [33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33\n 33 33 33 33 33 33 33 33]\n(VAL) Batch 52 Loss : 4.513259410858154, accuracy: 0.0\nVAL : 예측라벨 : [ 31   6 119  26  27  37  31   6   6 192   6   6   6 195   6 103 192  11\n  78   6 187 192   6  52   6   6  37  52  37 192  48  27], 정답 [33 33 33 33 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34\n 34 34 34 34 34 34 34 34]\n(VAL) Batch 53 Loss : 4.682119369506836, accuracy: 0.0\nVAL : 예측라벨 : [ 31  37  37  24  24   6   6   6   6  31  37 192 160   8   6  29 192  12\n  54  52  31   6   6  26 117 108  39   6 103  24 117 196], 정답 [34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 35 35\n 35 35 35 35 35 35 35 35]\n(VAL) Batch 54 Loss : 4.805140972137451, accuracy: 0.0\nVAL : 예측라벨 : [  8  37  55  24  37   6 170 160  24   6  53  50 198  12  24 143 199 138\n  32  50 114   6   0  37  37 170   6  55  58 192  27 117], 정답 [35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35\n 35 35 35 35 35 35 35 35]\n(VAL) Batch 55 Loss : 4.661246299743652, accuracy: 0.0\nVAL : 예측라벨 : [185  58  58  26   6  37   6  24  37  37  41  37  39  69  37  37  37 186\n  24  24  58  37  37   6  37  39  31  37  39  37  37  37], 정답 [35 35 35 35 35 35 35 35 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n 36 36 36 36 36 36 36 36]\n(VAL) Batch 56 Loss : 3.983119487762451, accuracy: 0.0\nVAL : 예측라벨 : [189  37 188  90  27  39  37  71  37  39  37  39 108  52  37  55 177  39\n  39  27  69  39 117 187  39  26 117  93   8   6  37  24], 정답 [36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n 36 36 37 37 37 37 37 37]\n(VAL) Batch 57 Loss : 4.492997169494629, accuracy: 0.03125\nVAL : 예측라벨 : [ 69 188  41  37   8  37   8  37  37  39  37  37  37  37  37  71   8  37\n  39  37  69 160 122  11  37  37  71  24  24   8 177  37], 정답 [37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37\n 37 37 37 37 37 37 37 37]\n(VAL) Batch 58 Loss : 3.6582398414611816, accuracy: 0.4375\nVAL : 예측라벨 : [ 39 122  41  39 114  39  37   9   8 160  37 122  39 187 199  37 170  24\n  39  54  37 158 138 158 188 199  37 188  24 188  37  60], 정답 [37 37 37 37 37 37 37 37 37 37 37 37 38 38 38 38 38 38 38 38 38 38 38 38\n 38 38 38 38 38 38 38 38]\n(VAL) Batch 59 Loss : 3.8816275596618652, accuracy: 0.0625\nVAL : 예측라벨 : [ 31 192  69 109 189  24  39  38 158  37  37  37  37  39   6  52 188 138\n  24  39  24  90  47  24  37  45  52  44 187   8  37  37], 정답 [38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38\n 38 38 38 38 38 38 39 39]\n(VAL) Batch 60 Loss : 4.2199883460998535, accuracy: 0.03125\nVAL : 예측라벨 : [ 37 189  24  24  57  27  81  39  37  39  38 145 192  37  69  39  37 192\n  57  39  42  39 199  39  27 185  37  42 117 117  39  39], 정답 [39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39\n 39 39 39 39 39 39 39 39]\n(VAL) Batch 61 Loss : 3.9319050312042236, accuracy: 0.25\nVAL : 예측라벨 : [  9 160  39  37  24  52  37  37 198  37  39 198  39 189  69  50  39  42\n  31  39  39  24  24  37  39 114  22  39   8  24  39  29], 정답 [39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 40 40 40 40 40 40 40 40\n 40 40 40 40 40 40 40 40]\n(VAL) Batch 62 Loss : 3.951900005340576, accuracy: 0.09375\nVAL : 예측라벨 : [ 24  93  39 105  31 114  24  87  69   6  56  24 192  69  37  50   1  57\n  31  39 109 178   6  24 143  11  39  39  39  40 188  90], 정답 [40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n 40 40 40 40 40 40 40 40]\n(VAL) Batch 63 Loss : 4.504947185516357, accuracy: 0.03125\nVAL : 예측라벨 : [119 192 122  13   8  58   6  69   6  24 105  78   6  90  29 159  69  52\n  24 105  24  24  37   8 192  39  37  37  13  55  41  74], 정답 [40 40 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n 41 41 41 41 41 41 41 41]\n(VAL) Batch 64 Loss : 4.748383045196533, accuracy: 0.03125\nVAL : 예측라벨 : [160 187 146  55 117 192 143  37 160  29  39  31 192  31   8 109  93  54\n  37  29  39 158  39  24  37 117  52  39  39 130  58  69], 정답 [41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 42 42 42 42\n 42 42 42 42 42 42 42 42]\n(VAL) Batch 65 Loss : 4.701704502105713, accuracy: 0.0\nVAL : 예측라벨 : [  6 187  39  39 199 184  31  69 109 192  60 188 199  39 199  31   6  69\n  37  39 160  11  38  39 109  48 198 192 109   0  42  39], 정답 [42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42\n 42 42 42 42 42 42 42 42]\n(VAL) Batch 66 Loss : 4.331174373626709, accuracy: 0.03125\nVAL : 예측라벨 : [109 137  24  69 143  69  37  37  37  39  60  37  24  39  39   8  37  39\n   6  39  39  41  39 130  39  37  13  52  24 141  52  37], 정답 [42 42 42 42 42 42 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43\n 43 43 43 43 43 43 43 43]\n(VAL) Batch 67 Loss : 4.118344306945801, accuracy: 0.0\nVAL : 예측라벨 : [ 39  52  37  39  39   8  39  13  37  37  37  37 192  39 158 188  37  39\n 192  39  69 188  39   6  52  37 143 108  39  37  44  90], 정답 [43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43\n 44 44 44 44 44 44 44 44]\n(VAL) Batch 68 Loss : 4.03448486328125, accuracy: 0.03125\nVAL : 예측라벨 : [ 37 138 189 145  44 180  52  44  44 115 114  13  37 143  38 177  24  37\n  44  37  37 199  39  37  93  38  52  13  93  39 143  39], 정답 [44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44\n 44 44 44 44 44 44 44 44]\n(VAL) Batch 69 Loss : 4.420200347900391, accuracy: 0.125\nVAL : 예측라벨 : [ 44  44 115  90 135  93  13 145  44 170  24  39  39 192 192  39 192 189\n 192  52 187 192 199  38 192 188  39 192 187 192 192  39], 정답 [44 44 44 44 44 44 44 44 44 44 45 45 45 45 45 45 45 45 45 45 45 45 45 45\n 45 45 45 45 45 45 45 45]\n(VAL) Batch 70 Loss : 4.083578109741211, accuracy: 0.09375\nVAL : 예측라벨 : [ 24 192  39 192  39  39  39  38 178  37 199  39  39 185 192 192  39  39\n 114 187  38  39  38  37  37  38  39  45 199  13  39  13], 정답 [45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45\n 45 45 45 45 46 46 46 46]\n(VAL) Batch 71 Loss : 4.079615116119385, accuracy: 0.03125\nVAL : 예측라벨 : [ 31  26  52  31   8  81  84   6 190  31  95  31 196   6  91  27   6 108\n 196   6   6  24  31  14   1  69 188  14 146 141   6 114], 정답 [46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46\n 46 46 46 46 46 46 46 46]\n(VAL) Batch 72 Loss : 4.78412389755249, accuracy: 0.0\nVAL : 예측라벨 : [196 141  50   8 117 148   6  81  13  39   6 160 162 148  31  69 160 114\n 192  24  31 187 192   6  24 192  47   8 141  24  91 192], 정답 [46 46 46 46 46 46 46 46 46 46 46 46 46 46 47 47 47 47 47 47 47 47 47 47\n 47 47 47 47 47 47 47 47]\n(VAL) Batch 73 Loss : 4.595505237579346, accuracy: 0.03125\nVAL : 예측라벨 : [ 69  24  42  24  53  24 179  24 192 117  31 180  57  24 160 192  31  31\n   6  69  37  69 158   6   6  69 192   6  24 192 109  24], 정답 [47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47\n 47 47 47 47 47 47 47 47]\n(VAL) Batch 74 Loss : 4.598353385925293, accuracy: 0.0\nVAL : 예측라벨 : [  6 114   6 110  26 110   8 192  24  39 170  69  12   8  55  24  39  24\n 108   6  31  37  26  24  50  12   6   6  29  57 143  81], 정답 [48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48\n 48 48 48 48 48 48 48 48]\n(VAL) Batch 75 Loss : 4.568197727203369, accuracy: 0.0\nVAL : 예측라벨 : [192  37 177   6 110  31  55  48  31  12 133 160 105 105  12  56  66   6\n  31  27 170  24  12 170   1 110  12  37 101  24  52 122], 정답 [48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 49 49 49 49 49 49\n 49 49 49 49 49 49 49 49]\n(VAL) Batch 76 Loss : 4.9859089851379395, accuracy: 0.03125\nVAL : 예측라벨 : [ 12 170  55 119  31  37  56 166  24 170   6  12  27 128  91  24 117  95\n 130  55 110  31 108 105 198  66 122  52 174  50   6 148], 정답 [49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49\n 49 49 49 49 49 49 49 49]\n(VAL) Batch 77 Loss : 5.2476348876953125, accuracy: 0.0\nVAL : 예측라벨 : [ 24 170 105  57   8 110  52  50  39  27 108  31   6  24  39  52  24  24\n  37  31 160  37   6  50 114  37  29  24 160  50  55  27], 정답 [49 49 49 49 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n 50 50 50 50 50 50 50 50]\n(VAL) Batch 78 Loss : 4.470270156860352, accuracy: 0.09375\nVAL : 예측라벨 : [ 24  50  29 170  24  24  52  50 170  55  52  27 170  58 114  24  37  24\n 170  26  27  27  53 110  57 103  52 170  24 154 117  57], 정답 [50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 51 51\n 51 51 51 51 51 51 51 51]\n(VAL) Batch 79 Loss : 4.60454797744751, accuracy: 0.0625\nVAL : 예측라벨 : [ 27   6  29   6  52  11   6  52  50   6 137   6  31  24 117  50  27 163\n 198  57  24 132  12   6   6 148  31  12  37  48  57   6], 정답 [51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51\n 51 51 51 51 51 51 51 51]\n(VAL) Batch 80 Loss : 4.920237064361572, accuracy: 0.0\nVAL : 예측라벨 : [198   6 143 114  12  57   6  57  31 170   6 160  38  24 160   6 105   6\n  52 160  48   6  24  58   6  39  37   6  69   6  52  55], 정답 [51 51 51 51 51 51 51 51 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52\n 52 52 52 52 52 52 52 52]\n(VAL) Batch 81 Loss : 4.457653522491455, accuracy: 0.0625\nVAL : 예측라벨 : [ 53  39  39  57  50  39  24  39  31   8   6 143  39   6 198   6   8 137\n  52   6  39   8  50 192  50   6  12  57   6  24  39 170], 정답 [52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52\n 52 52 53 53 53 53 53 53]\n(VAL) Batch 82 Loss : 4.299494743347168, accuracy: 0.03125\nVAL : 예측라벨 : [ 41 170 170   6  96 160 170   6  71 170 180 180 170   6  57  37  41   6\n 143  69 170  24 143  90  57  31 170 189 192   6  24  56], 정답 [53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53\n 53 53 53 53 53 53 53 53]\n(VAL) Batch 83 Loss : 4.8685455322265625, accuracy: 0.0\nVAL : 예측라벨 : [105  31  26 114  12  27 198 177 105  58  57  26  52  56  24  24  12  52\n  31  31  24  91 199  26  37  27  31  12  12  24  26  52], 정답 [53 53 53 53 53 53 53 53 53 53 53 53 54 54 54 54 54 54 54 54 54 54 54 54\n 54 54 54 54 54 54 54 54]\n(VAL) Batch 84 Loss : 4.679981231689453, accuracy: 0.0\nVAL : 예측라벨 : [160 160  31  95 143   6  52  57 117   6 138  37  24 178 141  24  29  50\n  37   6  31 158   6  24  24  50  55  52  24  90  81 105], 정답 [54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54\n 54 54 54 54 54 54 55 55]\n(VAL) Batch 85 Loss : 4.676766395568848, accuracy: 0.0\nVAL : 예측라벨 : [ 27  24   6  55   1  55 110  69  58  27  24 119  29  37  48  29  24 110\n  55  37  27  37  81  24 141 103  55  37   8  24  12  24], 정답 [55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55\n 55 55 55 55 55 55 55 55]\n(VAL) Batch 86 Loss : 4.280120372772217, accuracy: 0.125\nVAL : 예측라벨 : [110 119  52  55 199  55 199   8  24 199 141  55  55 128  55 141   6  24\n 110  24  69 124  55   8  55   6  31   6   1   6  81   6], 정답 [55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 56 56 56 56 56 56 56 56\n 56 56 56 56 56 56 56 56]\n(VAL) Batch 87 Loss : 4.437618732452393, accuracy: 0.15625\nVAL : 예측라벨 : [  8   6  24  52 199  54  31   6  39   6 144   8  58  39  52  31   6  55\n  24  24  52  52   6  71  50  37   8   6  24  50  48  31], 정답 [56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56\n 56 56 56 56 56 56 56 56]\n(VAL) Batch 88 Loss : 4.428221702575684, accuracy: 0.0\nVAL : 예측라벨 : [  8 117   6   6  58  12  24 170 170  12  81 170  57   6  31  24  52  60\n  52 170  57  24  55 192  24  50  56  57  24 192 198 119], 정답 [56 56 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57\n 57 57 57 57 57 57 57 57]\n(VAL) Batch 89 Loss : 4.588903427124023, accuracy: 0.09375\nVAL : 예측라벨 : [ 53   4  50   6   6   7   6  57 170  52   6 119 145  58  56  55  26  27\n  71  24 128  24  52 124 114 158 117   6  50 108  39 103], 정답 [57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 58 58 58 58\n 58 58 58 58 58 58 58 58]\n(VAL) Batch 90 Loss : 4.7181267738342285, accuracy: 0.03125\nVAL : 예측라벨 : [ 37  12 141   8 143  12  24  24   8  58 170  60  27   8  58  24  37  37\n   6   8  27  52  24  12  58  58  24  24 160 143  24  52], 정답 [58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58\n 58 58 58 58 58 58 58 58]\n(VAL) Batch 91 Loss : 4.597782611846924, accuracy: 0.125\nVAL : 예측라벨 : [ 24 143  24   4  58  37 108  61 145 128 160 178 143 138  93  90  12 189\n 192 163 192 189  52  91 192 166 105 143   6  90 166  37], 정답 [58 58 58 58 58 58 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59\n 59 59 59 59 59 59 59 59]\n(VAL) Batch 92 Loss : 5.007131099700928, accuracy: 0.03125\nVAL : 예측라벨 : [  6  24 192 177  24  55 109 108  54 108  37 180  90 158 108 108  90   6\n  24   6 177  27  93  52  24 138  12 163 108 105  24 138], 정답 [59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59\n 60 60 60 60 60 60 60 60]\n(VAL) Batch 93 Loss : 5.054381370544434, accuracy: 0.0\nVAL : 예측라벨 : [109  60 138 143 143  24 180  12  13 109  60  68  24  24  24  24 110 115\n  24  24  69  60  55  60  93 138  38  60  24  55 109 109], 정답 [60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60\n 60 60 60 60 60 60 60 60]\n(VAL) Batch 94 Loss : 4.505169868469238, accuracy: 0.15625\nVAL : 예측라벨 : [ 48  90 106 110 138 108 160 105 138 138 103 167  93  68  58  57  52  93\n  39 189   1 189 103  57  93  60 103  93   6 108 189  57], 정답 [60 60 60 60 60 60 60 60 60 60 61 61 61 61 61 61 61 61 61 61 61 61 61 61\n 61 61 61 61 61 61 61 61]\n(VAL) Batch 95 Loss : 4.5433573722839355, accuracy: 0.0\nVAL : 예측라벨 : [  6 175  93 108  58  57  93   6  52 112 185  57  81  58   6  26  90 180\n 103   6 124 143  58  52  93  24  54   1  90  24  13 105], 정답 [61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61\n 61 61 61 61 62 62 62 62]\n(VAL) Batch 96 Loss : 4.917412757873535, accuracy: 0.0\nVAL : 예측라벨 : [163 108 189  12  24  86 192  24   0  90 189  69 128 138   6 141 160 158\n 110  24 108 143 189 189  24 189  24  90 189  24  24  90], 정답 [62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62\n 62 62 62 62 62 62 62 62]\n(VAL) Batch 97 Loss : 5.127272129058838, accuracy: 0.0\nVAL : 예측라벨 : [146  81 110  24  24  24 128  24 189  24  90 110 110 146 158 110  24 192\n 117  12  81  90   0 163 105  80 138 107  55  55 110  90], 정답 [62 62 62 62 62 62 62 62 62 62 62 62 62 62 63 63 63 63 63 63 63 63 63 63\n 63 63 63 63 63 63 63 63]\n(VAL) Batch 98 Loss : 5.024109363555908, accuracy: 0.0\nVAL : 예측라벨 : [ 81  90  24  86  90  24  24 138  24 108 138  12 128 110  50  27 110  24\n 110 160 128  50  24  55  87  27 180  55 133 158  24   6], 정답 [63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63\n 63 63 63 63 63 63 63 63]\n(VAL) Batch 99 Loss : 4.938862323760986, accuracy: 0.0\nVAL : 예측라벨 : [ 24  90  69 146  58  55 138 192  24  90  24  58 110  81  81  60  69 164\n   8 101 160 138   6   8 143   8  69   6 110  31  29  69], 정답 [64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n 64 64 64 64 64 64 64 64]\n(VAL) Batch 100 Loss : 5.163576602935791, accuracy: 0.0\nVAL : 예측라벨 : [ 39  69  93  69  55  87 114  95  27  93   6  24  24 189  52 119 119  31\n  12  93  60   6  93  24 108  50 143  31  55  27   6  81], 정답 [64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 65 65 65 65 65 65\n 65 65 65 65 65 65 65 65]\n(VAL) Batch 101 Loss : 5.1261491775512695, accuracy: 0.0\nVAL : 예측라벨 : [ 55  58  24  60 103  81  24 117 103 119 108  24 128 138 108 105 189 121\n  27 108  12 146   8 103 138 114 189  29  24 108 146 128], 정답 [65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65\n 65 65 65 65 65 65 65 65]\n(VAL) Batch 102 Loss : 4.956495761871338, accuracy: 0.0\nVAL : 예측라벨 : [ 90  24   1  24 170  81  50 158 135   6 130  13  91 198  95 166 146 170\n  57  48 170 146 170  78 143  52 170 170 170 195 170 170], 정답 [65 65 65 65 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66\n 66 66 66 66 66 66 66 66]\n(VAL) Batch 103 Loss : 4.64640474319458, accuracy: 0.0\nVAL : 예측라벨 : [170 121  44  29 170   1 191  68 164   1 108  66 160 196 170 198 170  39\n 114 170  37 198  13  13  81  31  57  27 192 198  37 189], 정답 [66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 67 67\n 67 67 67 67 67 67 67 67]\n(VAL) Batch 104 Loss : 4.566493511199951, accuracy: 0.03125\nVAL : 예측라벨 : [  8  31  24 192  69 193 180  31  57 109  78 138 158  31  78  26  13  31\n   6  81   6  81 160 143  12   6 110   6 110 196   6  12], 정답 [67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67\n 67 67 67 67 67 67 67 67]\n(VAL) Batch 105 Loss : 4.8397345542907715, accuracy: 0.0\nVAL : 예측라벨 : [ 31 146 177  31 177  13  69  24  90   8  93 143  90 105 138   1 103 158\n   1 185  24 145 138 138  90 107  68 185  68  90  31  90], 정답 [67 67 67 67 67 67 67 67 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68\n 68 68 68 68 68 68 68 68]\n(VAL) Batch 106 Loss : 4.682265758514404, accuracy: 0.0625\nVAL : 예측라벨 : [ 60  60  60  91 143 189  12  56  24 108  68  69  56 185  90  90   8  55\n 138 119  24 192 110 143 158 191  69  69  69  69  69 160], 정답 [68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68\n 68 68 69 69 69 69 69 69]\n(VAL) Batch 107 Loss : 4.44182825088501, accuracy: 0.1875\nVAL : 예측라벨 : [ 69  69  24  27 193 121 109  24  69  69  24  69  36  69 109 107 160  37\n  69 110  90  24 188  23  81  24 130  31  69  31  31 160], 정답 [69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69\n 69 69 69 69 69 69 69 69]\n(VAL) Batch 108 Loss : 4.158124923706055, accuracy: 0.25\nVAL : 예측라벨 : [ 69  69  69  24  52  31  69 193  71  24  69  31  81  95 166 108 162 133\n  55 103   8  81   6  81 107 133 166  55  78 108 115 166], 정답 [69 69 69 69 69 69 69 69 69 69 69 69 70 70 70 70 70 70 70 70 70 70 70 70\n 70 70 70 70 70 70 70 70]\n(VAL) Batch 109 Loss : 4.358726978302002, accuracy: 0.15625\nVAL : 예측라벨 : [163 121  81 145 166 115   6 154  81 164  81  50  95  80 166  81 170 121\n   8  93  81  81  81 108 110   6 170  12 103  95 198  71], 정답 [70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70\n 70 70 70 70 70 70 71 71]\n(VAL) Batch 110 Loss : 4.701897621154785, accuracy: 0.03125\nVAL : 예측라벨 : [117 170  69  31  71  71 193 170  71  95 198  13 198  13  71  22 124 124\n 170  71  13  95  71  71 196 170  71 198  95  69  71  95], 정답 [71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71\n 71 71 71 71 71 71 71 71]\n(VAL) Batch 111 Loss : 3.432910203933716, accuracy: 0.28125\nVAL : 예측라벨 : [198 122 198   6  71 198 198  71  71 121 170 198 117  48 124 101  13  13\n  26  31  69 160  74  23  74 192  90  37 192 177  74  55], 정답 [71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 72 72 72 72 72 72 72 72\n 72 72 72 72 72 72 72 72]\n(VAL) Batch 112 Loss : 4.252342224121094, accuracy: 0.09375\nVAL : 예측라벨 : [ 69  90 180  13  90 158 109  13   8  24 101 108 160  13 158 110  27  31\n   8  69 160 122 189 141  90  24 192   8 141 177  29  69], 정답 [72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72\n 72 72 72 72 72 72 72 72]\n(VAL) Batch 113 Loss : 4.74815034866333, accuracy: 0.0\nVAL : 예측라벨 : [199 189 135 192  57  13  86 109 192 110 199 199  68 148   8 138 138 179\n  27 170  13 146 199 187 192  24 143 138  39   6 185  58], 정답 [72 72 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73\n 73 73 73 73 73 73 73 73]\n(VAL) Batch 114 Loss : 5.08393669128418, accuracy: 0.0\nVAL : 예측라벨 : [158  52 128  24  13  52 110  24 119 192  81  90  24 189 143   4 152 167\n  24  24 138  74 158 124  24  29  24 158 144 160  24 143], 정답 [73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 74 74 74 74\n 74 74 74 74 74 74 74 74]\n(VAL) Batch 115 Loss : 4.841264247894287, accuracy: 0.03125\nVAL : 예측라벨 : [143 109  71  74 138 158  50 192  24  31  53  24  13 103 109  24 158  74\n 180  24  53 119 109  74 144  74 143 128 124  74  69 110], 정답 [74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74\n 74 74 74 74 74 74 74 74]\n(VAL) Batch 116 Loss : 4.461714744567871, accuracy: 0.15625\nVAL : 예측라벨 : [ 74 158 170  93 160  24  75 141 138  81 105  50  24   6  24 103  84 105\n 124 163  55  24  69 146  24  24 138  86 117  95 110  81], 정답 [74 74 74 74 74 74 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75\n 75 75 75 75 75 75 75 75]\n(VAL) Batch 117 Loss : 4.794223785400391, accuracy: 0.0625\nVAL : 예측라벨 : [130  39 141 103 146  58 171  81  24  39 128  55  24  24  26 158 166 146\n  55 117 170 146   1  24  52  91   6  55 114  71  55 110], 정답 [75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75\n 76 76 76 76 76 76 76 76]\n(VAL) Batch 118 Loss : 4.999104976654053, accuracy: 0.0\nVAL : 예측라벨 : [ 71 143  69  90   6 138  50  52  50 101  50 128  27  37  29  90  81  68\n  37   1  69  37  55  27 108 170  37 114  81  24  39 145], 정답 [76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76\n 76 76 76 76 76 76 76 76]\n(VAL) Batch 119 Loss : 5.208520889282227, accuracy: 0.0\nVAL : 예측라벨 : [170  37  56  24   6  13   6  81 108 154 192  24  81  24 128  60  26 180\n  39  24  58 141  24  31  38  24 110  24  58  12 108  90], 정답 [76 76 76 76 76 76 76 76 76 76 77 77 77 77 77 77 77 77 77 77 77 77 77 77\n 77 77 77 77 77 77 77 77]\n(VAL) Batch 120 Loss : 4.882067680358887, accuracy: 0.0\nVAL : 예측라벨 : [110  90 179  52 158  24 105  24   8  24 138  90 128  24   6  31  24  24\n 192  69  41  58 146  24  24 117  26  24  50  81   6  81], 정답 [77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77\n 77 77 77 77 78 78 78 78]\n(VAL) Batch 121 Loss : 4.658391952514648, accuracy: 0.0\nVAL : 예측라벨 : [ 81 166 198  31  89 108 110  56  37   6 192   8  55  69 170  78  69  81\n  81   6 133  81  81 198  78   6  52   6 166  78 119   6], 정답 [78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78\n 78 78 78 78 78 78 78 78]\n(VAL) Batch 122 Loss : 4.399647235870361, accuracy: 0.09375\nVAL : 예측라벨 : [ 78 166  78 109  53  78  78  55  78   6  29  78   6  81  52  69 130  42\n 199  37 105 110  24 138 117  58  37   8 110   8  27  24], 정답 [78 78 78 78 78 78 78 78 78 78 78 78 78 78 79 79 79 79 79 79 79 79 79 79\n 79 79 79 79 79 79 79 79]\n(VAL) Batch 123 Loss : 4.684513568878174, accuracy: 0.1875\nVAL : 예측라벨 : [143   8  55 177 189  24  31  69 110  87  60  26 158 110  23 115 119  31\n 158   6  37  69 106  24  27 177  69  24 122 192  24  41], 정답 [79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79\n 79 79 79 79 79 79 79 79]\n(VAL) Batch 124 Loss : 5.084885120391846, accuracy: 0.0\nVAL : 예측라벨 : [ 90  13 138  81 101  31  24 177  41 189 117  24 189  24  90 180 198  39\n 188  81 192  60 117   6 117 148  31 170 121  78  90 198], 정답 [80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80\n 80 80 80 80 80 80 80 80]\n(VAL) Batch 125 Loss : 4.985867977142334, accuracy: 0.0\nVAL : 예측라벨 : [ 81 160 189  69  39 138 148  80 192 160 192 160 133 160  26  50  41 138\n 133   6  81  81  81 110  81  81  81  81  81  81 166  55], 정답 [80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 81 81 81 81 81 81\n 81 81 81 81 81 81 81 81]\n(VAL) Batch 126 Loss : 4.221467971801758, accuracy: 0.3125\nVAL : 예측라벨 : [ 81 101  81 133 110  81  24  69 146  81  69  81   6 193  78  81  78 121\n 121 133  56  81  81 170  81  81  60 138  81 103  81  81], 정답 [81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81\n 81 81 81 81 81 81 81 81]\n(VAL) Batch 127 Loss : 3.7718918323516846, accuracy: 0.40625\nVAL : 예측라벨 : [110 133  81 119  90 192 189  90 143 189  93   6 189  93   9 190 189  24\n 119 189   9  24 103  24  93 103  24  24 192  93 138 189], 정답 [81 81 81 81 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82\n 82 82 82 82 82 82 82 82]\n(VAL) Batch 128 Loss : 5.105961799621582, accuracy: 0.03125\nVAL : 예측라벨 : [145 143  24 189  90 146  24 189  24 146  24   6 189  24  56 108 189 143\n   1 138 192 189  24  90  53  29 192  24 177 160 192 186], 정답 [82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 83 83\n 83 83 83 83 83 83 83 83]\n(VAL) Batch 129 Loss : 5.331108570098877, accuracy: 0.0\nVAL : 예측라벨 : [ 24  24  24  93  13  24 160  26 110 192 192  74  24  24  24  24 180 177\n 177  90  24 180  55  54 177  26  26  24 109 187 160  24], 정답 [83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83\n 83 83 83 83 83 83 83 83]\n(VAL) Batch 130 Loss : 4.726320266723633, accuracy: 0.0\nVAL : 예측라벨 : [ 26 113  26 160 177 180  68  52 119  81  75 110  71  52  50  81 198 154\n 137   8 170   4 170  27 128 166 170 187 117  81 170   8], 정답 [83 83 83 83 83 83 83 83 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84\n 84 84 84 84 84 84 84 84]\n(VAL) Batch 131 Loss : 4.7495341300964355, accuracy: 0.0\nVAL : 예측라벨 : [108  38 198  84  50 108 103  12  78  91 154   6 119  52  71  48 170 110\n  55  81 114 170 196  24  58 117 189  24 189  24   6  24], 정답 [84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84\n 84 84 85 85 85 85 85 85]\n(VAL) Batch 132 Loss : 5.215646266937256, accuracy: 0.03125\nVAL : 예측라벨 : [ 12  31 189 117  31 189  24 124 138  24 192  24 189 138   6 110 152  80\n 138  24 115  24 192  24 158  23  24  69  24 108  55 148], 정답 [85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85\n 85 85 85 85 85 85 85 85]\n(VAL) Batch 133 Loss : 5.3303046226501465, accuracy: 0.0\nVAL : 예측라벨 : [124  60  80  81  24  90  24  60 101  39 149   6  81 196  24 106 110  60\n 180  86  81  12 108   8 105 158 143 138  24 163  86 107], 정답 [85 85 85 85 85 85 85 85 85 85 85 85 86 86 86 86 86 86 86 86 86 86 86 86\n 86 86 86 86 86 86 86 86]\n(VAL) Batch 134 Loss : 4.958049297332764, accuracy: 0.0625\nVAL : 예측라벨 : [ 87  68  24  80  93   6 117  24   6 117  90  81   6  27  39 134  69   6\n  60 128  86  80 148 105  81  13 109 138 137  86  58 143], 정답 [86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86\n 86 86 86 86 86 86 87 87]\n(VAL) Batch 135 Loss : 4.8387370109558105, accuracy: 0.0625\nVAL : 예측라벨 : [133 110 108  24  78 103 143 110  78  27 189 166 166 117 185 117  92 146\n  26  81  81  55  96 117 170 166 121  69  97  31  97  78], 정답 [87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87\n 87 87 87 87 87 87 87 87]\n(VAL) Batch 136 Loss : 4.411336421966553, accuracy: 0.0\nVAL : 예측라벨 : [138  81  87 133  69  55   8 117 133 110  69  81 154 109 189  55  24 148\n  69   6 166  24  81  31  60 119  11 188  24 138  12  50], 정답 [87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 88 88 88 88 88 88 88 88\n 88 88 88 88 88 88 88 88]\n(VAL) Batch 137 Loss : 4.656735897064209, accuracy: 0.03125\nVAL : 예측라벨 : [ 50   8  31  31  55  24  27 108  27   1   8 191  24 117 189  24  60   8\n  24 108 103  90 135 108 108 170  24  93  69  22  74 117], 정답 [88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88\n 88 88 88 88 88 88 88 88]\n(VAL) Batch 138 Loss : 5.000371932983398, accuracy: 0.0\nVAL : 예측라벨 : [ 27 132  24 192  31  31 177 192 192 170  57 177   8 138  78  24   6  24\n 177 160  57 192   6 192  29 110   6  26 177 192  57 192], 정답 [88 88 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89\n 89 89 89 89 89 89 89 89]\n(VAL) Batch 139 Loss : 4.754986763000488, accuracy: 0.0\nVAL : 예측라벨 : [ 31  31 141 192  53   6   6 130  50 177 189 192  39  24 128  26  29  26\n   6 160  24  90  74 189 192  90 177  24 189  90  90 189], 정답 [89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 90 90 90 90\n 90 90 90 90 90 90 90 90]\n(VAL) Batch 140 Loss : 4.386987686157227, accuracy: 0.125\nVAL : 예측라벨 : [ 90  90  74 189  90  24 105  24  24  24  24 138  90  90 189 189  87  90\n  69  90 189  60  54  24  24 177  24  90  74 189  90 163], 정답 [90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90\n 90 90 90 90 90 90 90 90]\n(VAL) Batch 141 Loss : 3.778135299682617, accuracy: 0.28125\nVAL : 예측라벨 : [ 24  74 192  90 163 138   6   6  31  24   8  66 170  24  57   6   6   6\n  57  69   6  26  31 119  55 192  31 162  24  57  93   6], 정답 [90 90 90 90 90 90 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91\n 91 91 91 91 91 91 91 91]\n(VAL) Batch 142 Loss : 4.220043182373047, accuracy: 0.03125\nVAL : 예측라벨 : [105  29 170   6 170  31  31  24  57  12   6  24 105  81 110 192   6   6\n   6  57  50   6   6  31  69  60  37  81  24   6  60  81], 정답 [91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91\n 92 92 92 92 92 92 92 92]\n(VAL) Batch 143 Loss : 4.518113136291504, accuracy: 0.0\nVAL : 예측라벨 : [192 170 121  31 141 146  50 199 160 198 110  81 148   6  31 110  24   8\n 145  52  31 133  55  81  78  31 138  31  81  39 177 192], 정답 [92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92\n 92 92 92 92 92 92 92 92]\n(VAL) Batch 144 Loss : 5.319103717803955, accuracy: 0.0\nVAL : 예측라벨 : [ 87  71  24 166   6   1  55  24  81  81  24 189 108  52  68  93  26  86\n  93 119 108 145  52 166  93 145  50 143  52   1 189  35], 정답 [92 92 92 92 92 92 92 92 92 92 93 93 93 93 93 93 93 93 93 93 93 93 93 93\n 93 93 93 93 93 93 93 93]\n(VAL) Batch 145 Loss : 4.775440692901611, accuracy: 0.09375\nVAL : 예측라벨 : [ 60 138 143 189 143 185  39  93  93 135 180  24 191  93  93  24  26 192\n  52 143  52  93 189   1  52 143 189 191  81  81  27 198], 정답 [93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93\n 93 93 93 93 94 94 94 94]\n(VAL) Batch 146 Loss : 4.5684027671813965, accuracy: 0.15625\nVAL : 예측라벨 : [158  95 115 114 110 103 133 180 185 115 196 164 133 133 145 189   0 143\n 154 170 115  87  70 133 166  81  86   8  81 133  81 154], 정답 [94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94\n 94 94 94 94 94 94 94 94]\n(VAL) Batch 147 Loss : 4.438055038452148, accuracy: 0.0\nVAL : 예측라벨 : [105  87  80  81   6 103 166 108  87  87 133 133 115 115 166  22 109  81\n   9 122  55 158   8  12  71  13  13 198 124  71  81   8], 정답 [94 94 94 94 94 94 94 94 94 94 94 94 94 94 95 95 95 95 95 95 95 95 95 95\n 95 95 95 95 95 95 95 95]\n(VAL) Batch 148 Loss : 4.546040058135986, accuracy: 0.0\nVAL : 예측라벨 : [ 69 124 170 138 124 197 170 196 109 192 101 124 119 138  86 146 170  95\n 170 117 170  31 170 121  39  95  90  13  93  13  69 148], 정답 [95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95\n 95 95 95 95 95 95 95 95]\n(VAL) Batch 149 Loss : 4.663421630859375, accuracy: 0.0625\nVAL : 예측라벨 : [ 39  81   8  96  96 170 170 170 101  13 170  31  81  29 115 154  87  55\n 158 166 198 198 198  81 198  78 117 198  96  24 170   6], 정답 [96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96\n 96 96 96 96 96 96 96 96]\n(VAL) Batch 150 Loss : 4.251091480255127, accuracy: 0.09375\nVAL : 예측라벨 : [ 96 117 170 154  31  31 170  78  69 166 198 141  95 117 170 170 170  69\n  24  86  24  60 141  29  52 110 108 103 103  69  95  24], 정답 [96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 97 97 97 97 97 97\n 97 97 97 97 97 97 97 97]\n(VAL) Batch 151 Loss : 4.617824554443359, accuracy: 0.03125\nVAL : 예측라벨 : [ 84 114  31 166  81  81 117  27  69  58 103 103  31 110  50  58 177  93\n 177  27  26  27  55  39  29  39  87 170  55  31  81   8], 정답 [97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97\n 97 97 97 97 97 97 97 97]\n(VAL) Batch 152 Loss : 5.193736553192139, accuracy: 0.0\nVAL : 예측라벨 : [146   8  24 143  93 170 170  87   6  41  24 189 143 177 103  57  24  29\n  50  24 192 160  93 138 110  52 103  60  55 108 145 170], 정답 [97 97 97 97 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98\n 98 98 98 98 98 98 98 98]\n(VAL) Batch 153 Loss : 5.167954444885254, accuracy: 0.0\nVAL : 예측라벨 : [  8  55  60   8  58   6  60  26  93   8 103   8 189 143  12  61   6 141\n   6  29   6  53 130 141  81  24 171  90  92  31  37  24], 정답 [98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 99 99\n 99 99 99 99 99 99 99 99]\n(VAL) Batch 154 Loss : 4.984019756317139, accuracy: 0.0\nVAL : 예측라벨 : [ 55  58  31  12  90  68 105  13  69 163  24  52  31 180  90 158 108  81\n  55   6  24 109 198  93  78  24 103  13 141  60  24 160], 정답 [99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99\n 99 99 99 99 99 99 99 99]\n(VAL) Batch 155 Loss : 4.817842483520508, accuracy: 0.0\nVAL : 예측라벨 : [ 69  60 160 192  74  27 192  93 110 109  24  24  24  24   6  31 138  27\n 141 138  24  24 192  24  24 188  24  22 161 141  31  31], 정답 [ 99  99  99  99  99  99  99  99 100 100 100 100 100 100 100 100 100 100\n 100 100 100 100 100 100 100 100 100 100 100 100 100 100]\n(VAL) Batch 156 Loss : 4.7932448387146, accuracy: 0.0\nVAL : 예측라벨 : [ 55  69  60  69  24  24 138  55  24 138  27 117   8   8  27 119  29  69\n 141  24 122  60 188  48  24  24 198  13 166 198  13  58], 정답 [100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100\n 100 100 100 100 100 100 100 100 101 101 101 101 101 101]\n(VAL) Batch 157 Loss : 4.71614933013916, accuracy: 0.0\nVAL : 예측라벨 : [ 71 137  13  71  13 101  13  95  13 101  41 101 196 109  95 101 158 198\n  13  71 170 117  13 117 101 101 117  69 146 101  71  60], 정답 [101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n 101 101 101 101 101 101 101 101 101 101 101 101 101 101]\n(VAL) Batch 158 Loss : 3.507925510406494, accuracy: 0.21875\nVAL : 예측라벨 : [137 170 166 170  13  13  13  13  13  13 198 101 192 138 105 119 124 177\n 196  29  58 146 166  81  37  87 103  24 192  53   6 199], 정답 [101 101 101 101 101 101 101 101 101 101 101 101 102 102 102 102 102 102\n 102 102 102 102 102 102 102 102 102 102 102 102 102 102]\n(VAL) Batch 159 Loss : 4.584559917449951, accuracy: 0.03125\nVAL : 예측라벨 : [158  54  57  92  50  60  87  71  74 110  24 177  31  58  78   8   6   6\n 160 192  31 103 198  24   6  31 198 166  37 117  60 121], 정답 [102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102\n 102 102 102 102 102 102 102 102 102 102 102 102 103 103]\n(VAL) Batch 160 Loss : 5.017370700836182, accuracy: 0.0\nVAL : 예측라벨 : [ 69 103  57 166 103   6 103 199 103 103 170 114 185 103 108   4 103   6\n  58 103 166   6 196 170   6 143 198  61 154 170 103 133], 정답 [103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103\n 103 103 103 103 103 103 103 103 103 103 103 103 103 103]\n(VAL) Batch 161 Loss : 4.212522983551025, accuracy: 0.28125\nVAL : 예측라벨 : [ 78 145 137 166 103 170 103  58 121 170 103 158  37 133 162 103  69 192\n 143 117 192  31 177 192  29 188 145  58  24 141   6 192], 정답 [103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 104 104\n 104 104 104 104 104 104 104 104 104 104 104 104 104 104]\n(VAL) Batch 162 Loss : 4.79365873336792, accuracy: 0.125\nVAL : 예측라벨 : [ 48 192 192  39 192 166  24 117 192   9 192  54  52  24 192 141 189 192\n   6  24  24  31  11 103 192 174 192  69  26 192 138  31], 정답 [104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104\n 104 104 104 104 104 104 104 104 104 104 104 104 104 104]\n(VAL) Batch 163 Loss : 5.073546409606934, accuracy: 0.0\nVAL : 예측라벨 : [180 128 138  55  24   6  12  24  31 138  24  69  60  24  31 138 109  24\n 105  24 105  90  55  31  90 105  24  69  12  29  31  12], 정답 [104 104 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n 105 105 105 105 105 105 105 105 105 105 105 105 105 105]\n(VAL) Batch 164 Loss : 4.202897071838379, accuracy: 0.09375\nVAL : 예측라벨 : [  6  55  31  24  24 189  24 138   8  74 192  31 110 180  53  71   6  55\n  60  24  81  90  60  27 141  55 105  55  24  32  54  69], 정답 [105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n 105 105 106 106 106 106 106 106 106 106 106 106 106 106]\n(VAL) Batch 165 Loss : 4.485055923461914, accuracy: 0.0\nVAL : 예측라벨 : [ 60  29  50 128  24  24   6  58   8  24 105 119  31  57  24   6 189  24\n 105   8 110  24 109  60  27  24  24 105  81  58  55  50], 정답 [106 106 106 106 106 106 106 106 106 106 106 106 106 106 106 106 106 106\n 106 106 106 106 106 106 106 106 106 106 106 106 106 106]\n(VAL) Batch 166 Loss : 4.724116325378418, accuracy: 0.0\nVAL : 예측라벨 : [ 24  90  24  93  69 141  60  90  93  60  90 107 108 177  37 135 199  90\n  84  52 158 189 143  58  12 189  58 115 170 166  58 138], 정답 [106 106 106 106 106 106 107 107 107 107 107 107 107 107 107 107 107 107\n 107 107 107 107 107 107 107 107 107 107 107 107 107 107]\n(VAL) Batch 167 Loss : 4.932066440582275, accuracy: 0.03125\nVAL : 예측라벨 : [114 170 121 110   8 166 114 110  60  39  78   8  55   8  81 114 114  95\n  78  58 108 107  58  90  93 154 143  55 103 119   8 103], 정답 [107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107\n 107 107 107 107 107 107 108 108 108 108 108 108 108 108]\n(VAL) Batch 168 Loss : 4.64798641204834, accuracy: 0.03125\nVAL : 예측라벨 : [103  55 119  55  81 165 154  12 128 145 170 103  93  91 189  93  81 121\n  12 166 158 101 103 108 199  93 158 128 108 108 115 108], 정답 [108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108\n 108 108 108 108 108 108 108 108 108 108 108 108 108 108]\n(VAL) Batch 169 Loss : 4.5736212730407715, accuracy: 0.125\nVAL : 예측라벨 : [103 108 192 108  58 196 154 170  93 146 160  60   8  90  24  31  24 109\n 109 189  31  90  24  31 199 106 110  24  24 138  69  24], 정답 [108 108 108 108 108 108 108 108 108 108 109 109 109 109 109 109 109 109\n 109 109 109 109 109 109 109 109 109 109 109 109 109 109]\n(VAL) Batch 170 Loss : 4.135376930236816, accuracy: 0.125\nVAL : 예측라벨 : [113  24 189   8  31 192 192  24 138  24  24 110  90  93  37  74  69  24\n 143  24  81 108 138   8 109  31 138  90 160 110 110 166], 정답 [109 109 109 109 109 109 109 109 109 109 109 109 109 109 109 109 109 109\n 109 109 109 109 109 109 109 109 109 109 110 110 110 110]\n(VAL) Batch 171 Loss : 4.365425109863281, accuracy: 0.09375\nVAL : 예측라벨 : [160  29 110 138 110 137  24  24 138  29 163  58  24  80  69   8  39 110\n   8 110  90  81  60 138  24 110  12 160 108  50  24  31], 정답 [110 110 110 110 110 110 110 110 110 110 110 110 110 110 110 110 110 110\n 110 110 110 110 110 110 110 110 110 110 110 110 110 110]\n(VAL) Batch 172 Loss : 4.249098300933838, accuracy: 0.15625\nVAL : 예측라벨 : [ 55 138  69 117 141 160 110 117 110 193  24   8  24   8  13 119 108  60\n 119 158 108 108  60 119 108 170 143  12  81  60  81  93], 정답 [110 110 110 110 110 110 110 110 110 110 110 110 110 110 111 111 111 111\n 111 111 111 111 111 111 111 111 111 111 111 111 111 111]\n(VAL) Batch 173 Loss : 4.442086219787598, accuracy: 0.0625\nVAL : 예측라벨 : [138  31 128  60 108 119 108   6 108 108 108 108 143 108 167  55 146  60\n 134  37  60 108 167   8 143 108   8  55 158  12 115 108], 정답 [111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111\n 111 111 111 111 111 111 111 111 111 111 111 111 111 111]\n(VAL) Batch 174 Loss : 4.867717742919922, accuracy: 0.0\nVAL : 예측라벨 : [ 90   1  24 108  91  90 143 164  24  24  90 189  31  90  24  90 128 199\n  24  12  90  24  24 138 138  24 144  90 138  13  90 149], 정답 [112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112\n 112 112 112 112 112 112 112 112 112 112 112 112 112 112]\n(VAL) Batch 175 Loss : 4.893417835235596, accuracy: 0.0\nVAL : 예측라벨 : [ 24  60  60 192  24  24 110  55  60  38  90 180 105  58  12  90  60 138\n  58 192  69  31  31 188  74  69   8 192 113  90   8  24], 정답 [112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112\n 113 113 113 113 113 113 113 113 113 113 113 113 113 113]\n(VAL) Batch 176 Loss : 4.719596862792969, accuracy: 0.03125\nVAL : 예측라벨 : [ 69 192 192  24 109 130 192  69  27  24 144  52  69  37   8  24  24 192\n  69  29  24 110 192 187 192  37  42 189 192 192 160  24], 정답 [113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113\n 113 113 113 113 113 113 113 113 113 113 113 113 113 113]\n(VAL) Batch 177 Loss : 4.460659980773926, accuracy: 0.0\nVAL : 예측라벨 : [192  31 189  69 114  78  90 189  39 114  39 128 114 114 138  48  87 117\n  58  39 108  38  24 117 189  90  90 108 166 114 143  37], 정답 [113 113 113 113 114 114 114 114 114 114 114 114 114 114 114 114 114 114\n 114 114 114 114 114 114 114 114 114 114 114 114 114 114]\n(VAL) Batch 178 Loss : 4.495773792266846, accuracy: 0.15625\nVAL : 예측라벨 : [143  24  37 128 114 174 170 170  56   6 114 114 114 138  39  37  37 197\n  90 170  31  78 145 115   6 115 117  69 110 115 170  24], 정답 [114 114 114 114 114 114 114 114 114 114 114 114 114 114 114 114 114 114\n 114 114 114 114 115 115 115 115 115 115 115 115 115 115]\n(VAL) Batch 179 Loss : 4.682005882263184, accuracy: 0.21875\nVAL : 예측라벨 : [ 95 166 115  13 115 115 146 143  50  24 145 145 103 145  13  27 117 170\n  24 198 154 198   8  13 145  22 145 115  87  71 145 115], 정답 [115 115 115 115 115 115 115 115 115 115 115 115 115 115 115 115 115 115\n 115 115 115 115 115 115 115 115 115 115 115 115 115 115]\n(VAL) Batch 180 Loss : 4.395350933074951, accuracy: 0.15625\nVAL : 예측라벨 : [115  90 145 154 145 115  96  93  24  24 198 117  81 170 166 138 117  81\n  95 154  81 164 166 119   8  60 146 108  50 108 185  95], 정답 [115 115 115 115 115 115 115 115 116 116 116 116 116 116 116 116 116 116\n 116 116 116 116 116 116 116 116 116 116 116 116 116 116]\n(VAL) Batch 181 Loss : 4.638187885284424, accuracy: 0.0625\nVAL : 예측라벨 : [143  50  81  81 143   6 114  93 154   6 116 108 166  81 166  81  81  81\n 119 133 170 166  81  81  81 198 199 117  81  31  60 146], 정답 [116 116 116 116 116 116 116 116 116 116 116 116 116 116 116 116 116 116\n 116 116 116 116 116 116 116 116 117 117 117 117 117 117]\n(VAL) Batch 182 Loss : 4.364785671234131, accuracy: 0.0625\nVAL : 예측라벨 : [ 81  31  39 138 128  69  13  24  93 148 124 158 117  41  81 117 117 110\n 110 109 117   8 167  69  13 117  57 173   8 128  31  24], 정답 [117 117 117 117 117 117 117 117 117 117 117 117 117 117 117 117 117 117\n 117 117 117 117 117 117 117 117 117 117 117 117 117 117]\n(VAL) Batch 183 Loss : 4.253830432891846, accuracy: 0.15625\nVAL : 예측라벨 : [ 13 128   8 173  23  31  55 130 109 110 110 108  95 170 114 170 170 114\n  39 164 108   1  95   1 114 108  39  24   1 189   1  24], 정답 [117 117 117 117 117 117 117 117 117 117 117 117 118 118 118 118 118 118\n 118 118 118 118 118 118 118 118 118 118 118 118 118 118]\n(VAL) Batch 184 Loss : 4.486071586608887, accuracy: 0.0\nVAL : 예측라벨 : [ 93 108  24 164  93  37 143 108 117 143 143  60 124 143 114 198  37 170\n  57  50 117 170 165 170 119 110 170 188  60 119   6  24], 정답 [118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118\n 118 118 118 118 118 118 118 118 118 118 118 118 119 119]\n(VAL) Batch 185 Loss : 4.52719259262085, accuracy: 0.0\nVAL : 예측라벨 : [ 55   6 108  12  55 166 105 108   6 166   6 135  24  12   6  71  12  58\n 106 158  27  12  60  24  57   8  37   6  27 128  60   6], 정답 [119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119\n 119 119 119 119 119 119 119 119 119 119 119 119 119 119]\n(VAL) Batch 186 Loss : 4.560807228088379, accuracy: 0.0\nVAL : 예측라벨 : [143 138   6  57  93  55 165  57  27 119  90 166 128 109 146 103 117  24\n 128 143  24 138 185  90 122 109 143  69 109  24 138  24], 정답 [119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 120 120\n 120 120 120 120 120 120 120 120 120 120 120 120 120 120]\n(VAL) Batch 187 Loss : 4.579216003417969, accuracy: 0.03125\nVAL : 예측라벨 : [ 90  24 158 109  24 143  24 138  24 143  24  24  24 109 109 110  24 128\n  55  22  81  90  12  24  24   8  24 138 138 109  90  31], 정답 [120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120\n 120 120 120 120 120 120 120 120 120 120 120 120 120 120]\n(VAL) Batch 188 Loss : 4.675310134887695, accuracy: 0.0\nVAL : 예측라벨 : [160 189  81  81  81  84 180 164  81 115 117 198 121  81 146 146  12 133\n 103  95 177  80 103  60  71 103 103 119  81 189  57  67], 정답 [120 120 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n 121 121 121 121 121 121 121 121 121 121 121 121 121 121]\n(VAL) Batch 189 Loss : 4.411708354949951, accuracy: 0.03125\nVAL : 예측라벨 : [170 121 103 115 166  86 170 115  81 166 154 166  66  81  87  87  13  96\n  81 110  55 170 119 160 122  52  35 158  24  31   6  24], 정답 [121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n 121 121 122 122 122 122 122 122 122 122 122 122 122 122]\n(VAL) Batch 190 Loss : 4.181407928466797, accuracy: 0.0625\nVAL : 예측라벨 : [ 69 124  41 122 198 119  71  31   6  24  29 101 160  37 119 109 122 105\n  69 146 122  31  69 122   6   6  31  69  60  13  95  31], 정답 [122 122 122 122 122 122 122 122 122 122 122 122 122 122 122 122 122 122\n 122 122 122 122 122 122 122 122 122 122 122 122 122 122]\n(VAL) Batch 191 Loss : 4.386399745941162, accuracy: 0.125\nVAL : 예측라벨 : [ 69 109  52   6  50 101 180  24 110 158  27  24  60 117  26 138  12 143\n  24 117 110  90  24 108  24 143 138 180  24  93  55  81], 정답 [122 122 122 122 122 122 123 123 123 123 123 123 123 123 123 123 123 123\n 123 123 123 123 123 123 123 123 123 123 123 123 123 123]\n(VAL) Batch 192 Loss : 5.059376239776611, accuracy: 0.0\nVAL : 예측라벨 : [ 24   6  24 180  27 105  24 180  74  38  24  24  24 117 193  22 144  24\n  27  24  24  24  31 138  71 148 124  24 124  69  71 109], 정답 [123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123\n 123 123 123 123 123 123 124 124 124 124 124 124 124 124]\n(VAL) Batch 193 Loss : 4.6644439697265625, accuracy: 0.0625\nVAL : 예측라벨 : [ 24  24 109  71 198 137 170  13 124  71  71  24 170  71  24   8 124 124\n 158  81 154 124  24 146  71 198 109  71 170  13 103  60], 정답 [124 124 124 124 124 124 124 124 124 124 124 124 124 124 124 124 124 124\n 124 124 124 124 124 124 124 124 124 124 124 124 124 124]\n(VAL) Batch 194 Loss : 4.083433151245117, accuracy: 0.125\nVAL : 예측라벨 : [ 71 124  13 124  71 198 109 124  23 170  24  24  24 115  60   8  24  60\n 108 148 103   8 188  37 138 163  31  60 109 192  52  24], 정답 [124 124 124 124 124 124 124 124 124 124 125 125 125 125 125 125 125 125\n 125 125 125 125 125 125 125 125 125 125 125 125 125 125]\n(VAL) Batch 195 Loss : 4.548214912414551, accuracy: 0.09375\nVAL : 예측라벨 : [180 119  24   8  24  39  58  35  87  24  13 192  24  68 117  24   1 158\n  90  13   8  24  96  69  24  90  24 180  61 103 138   6], 정답 [125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125\n 125 125 125 125 125 125 125 125 125 125 126 126 126 126]\n(VAL) Batch 196 Loss : 5.1409382820129395, accuracy: 0.0\nVAL : 예측라벨 : [ 93  57   6  56 164 143  93   6 190   6   6  93 189 108 108  24   6  93\n   6   6 108  57  93  31 185  93 189 185 189  24 177  57], 정답 [126 126 126 126 126 126 126 126 126 126 126 126 126 126 126 126 126 126\n 126 126 126 126 126 126 126 126 126 126 126 126 126 126]\n(VAL) Batch 197 Loss : 5.295001983642578, accuracy: 0.0\nVAL : 예측라벨 : [ 57  24 119   6 180 143  93  57 108 143  52 119  24 189  55  80  48  57\n 170 108 138  81  55   8 110 138  78  55  50  80 110 189], 정답 [126 126 126 126 126 126 126 126 126 126 126 126 126 126 127 127 127 127\n 127 127 127 127 127 127 127 127 127 127 127 127 127 127]\n(VAL) Batch 198 Loss : 5.081165790557861, accuracy: 0.0\nVAL : 예측라벨 : [  8 143 128  39 109  13 145  22 109 128  55  31  81  69  81  60 133 110\n 121 117  29  13 110 117  71 110  24 138  24 110   8  78], 정답 [127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127\n 127 127 127 127 127 127 127 127 127 127 127 127 127 127]\n(VAL) Batch 199 Loss : 5.230678081512451, accuracy: 0.0\nVAL : 예측라벨 : [ 81 158 110  81  24  81 158  90 101 128 143  24 110  86 166 170 117 138\n 110  81 138  55   6 110  90 141 143  24 138  13  60 141], 정답 [128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n 128 128 128 128 128 128 128 128 128 128 128 128 128 128]\n(VAL) Batch 200 Loss : 4.755260467529297, accuracy: 0.03125\nVAL : 예측라벨 : [  8 109 138  60  12 143  24 107 108 128  55  55  37 180 138 141  24 143\n 114 178   1 110  39  12  57 166  12  78 103  24 133 103], 정답 [128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n 129 129 129 129 129 129 129 129 129 129 129 129 129 129]\n(VAL) Batch 201 Loss : 4.938320159912109, accuracy: 0.03125\nVAL : 예측라벨 : [192 110  95  81  60   4  71 178 187  24 160 117  36 170  69  95  93 192\n 138  69 198  48  60 166 119  24  93 196  52  60   1 185], 정답 [129 129 129 129 129 129 129 129 129 129 129 129 129 129 129 129 129 129\n 129 129 129 129 129 129 129 129 129 129 129 129 129 129]\n(VAL) Batch 202 Loss : 5.6399617195129395, accuracy: 0.0\nVAL : 예측라벨 : [ 71 114  48 143  69 158  31  26  90  26  13 143  69 138  95 186 192 188\n 199  81 117  90  37 110 130  26 192  24 130  31  69  69], 정답 [129 129 129 129 130 130 130 130 130 130 130 130 130 130 130 130 130 130\n 130 130 130 130 130 130 130 130 130 130 130 130 130 130]\n(VAL) Batch 203 Loss : 4.333065032958984, accuracy: 0.0625\nVAL : 예측라벨 : [ 26  71 141 170  58  24  52  24 177  81 192 177 160  24 130 192 189  52\n  31 103  26 160  58  31  24 137 160 143 160  24 138 177], 정답 [130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130\n 130 130 130 130 131 131 131 131 131 131 131 131 131 131]\n(VAL) Batch 204 Loss : 4.887951850891113, accuracy: 0.03125\nVAL : 예측라벨 : [  8  24  60  37  24 163  31  13 117  24  26  60  41 180  24  24  24  26\n  24  50  24   6  24  22  24 170  24 180  69 192  24  24], 정답 [131 131 131 131 131 131 131 131 131 131 131 131 131 131 131 131 131 131\n 131 131 131 131 131 131 131 131 131 131 131 131 131 131]\n(VAL) Batch 205 Loss : 5.0196852684021, accuracy: 0.0\nVAL : 예측라벨 : [ 60  31  31  69 109  58  52 188  93 198 170  24 196   6 160  84 124  55\n 138  74  31  95 124 110 124 154  13 124 124  90  69 162], 정답 [131 131 131 131 131 131 131 131 132 132 132 132 132 132 132 132 132 132\n 132 132 132 132 132 132 132 132 132 132 132 132 132 132]\n(VAL) Batch 206 Loss : 4.713531970977783, accuracy: 0.0\nVAL : 예측라벨 : [101 158  24 138  90 143 128  81  24   6  69   8  68 105  24 170 133  24\n  24 170 198 109  39  69 170  90 145  81 133 133 133 103], 정답 [132 132 132 132 132 132 132 132 132 132 132 132 132 132 132 132 132 132\n 132 132 132 132 132 132 132 132 133 133 133 133 133 133]\n(VAL) Batch 207 Loss : 4.771211624145508, accuracy: 0.09375\nVAL : 예측라벨 : [170 121  81  75 170 133 133  87 133 146 145 166 133 154  39 133 166  60\n  86 133 128  81 153 108  81  81 121 166 103 133 108 102], 정답 [133 133 133 133 133 133 133 133 133 133 133 133 133 133 133 133 133 133\n 133 133 133 133 133 133 133 133 133 133 133 133 133 133]\n(VAL) Batch 208 Loss : 3.930846929550171, accuracy: 0.21875\nVAL : 예측라벨 : [110  94 103 166 133 121 133 133 166 133  81 133 149 110 189  24 138  24\n 143  31  60  52 110  24  68  90  60 158 189  90  57  69], 정답 [133 133 133 133 133 133 133 133 133 133 133 133 134 134 134 134 134 134\n 134 134 134 134 134 134 134 134 134 134 134 134 134 134]\n(VAL) Batch 209 Loss : 4.165425777435303, accuracy: 0.15625\nVAL : 예측라벨 : [133  60  24 189  80  86 108 189  90  22 143 192 119   1 158  60   6  24\n  57 189  60 105 167  60  55  13 138   6  93 187 192  90], 정답 [134 134 134 134 134 134 134 134 134 134 134 134 134 134 134 134 134 134\n 134 134 134 134 134 134 134 134 134 134 134 134 135 135]\n(VAL) Batch 210 Loss : 4.967968940734863, accuracy: 0.0\nVAL : 예측라벨 : [ 69  57  50 192 192 158 124 199 171 189  39 192  56 110  93  24  24  93\n   8 177 192  24 192  50  68 192 192  39 189  60 114 143], 정답 [135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135\n 135 135 135 135 135 135 135 135 135 135 135 135 135 135]\n(VAL) Batch 211 Loss : 5.10219669342041, accuracy: 0.0\nVAL : 예측라벨 : [ 74  24 180 114  24 117 108 145  39 108 158  69 141  31  24  31  24 179\n  24  24  81  52  24 160  84 117  24  24  27  55   8  56], 정답 [135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 136 136\n 136 136 136 136 136 136 136 136 136 136 136 136 136 136]\n(VAL) Batch 212 Loss : 5.0730695724487305, accuracy: 0.0\nVAL : 예측라벨 : [ 31  24 103  24 105  92  24  81  69 119 117  24  31 193 105  24  31  41\n 117  24 110  31  31 110  31  24 180  24  24  24  24 177], 정답 [136 136 136 136 136 136 136 136 136 136 136 136 136 136 136 136 136 136\n 136 136 136 136 136 136 136 136 136 136 136 136 136 136]\n(VAL) Batch 213 Loss : 4.984697341918945, accuracy: 0.0\nVAL : 예측라벨 : [160  31 198 108   6  69 117   6 138  12 170  24 108 108  81  13 124 137\n  50   6  71 145   6  31  69 117  87  69  69 143  69  71], 정답 [136 136 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137\n 137 137 137 137 137 137 137 137 137 137 137 137 137 137]\n(VAL) Batch 214 Loss : 4.844036102294922, accuracy: 0.03125\nVAL : 예측라벨 : [108  41 124  29 154  63   6  13  13  56  22  27  52 121  69 198  13 117\n 198  69  74 110  74  24  24  24 138 158 109 180 110  27], 정답 [137 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137\n 137 137 138 138 138 138 138 138 138 138 138 138 138 138]\n(VAL) Batch 215 Loss : 4.480408668518066, accuracy: 0.03125\nVAL : 예측라벨 : [143  78  69  90  24 110  90  24 170  24 110  13  24 177 163  24  69  74\n  24  31   8 192  24  60 163  24  24 189 130  69 192  24], 정답 [138 138 138 138 138 138 138 138 138 138 138 138 138 138 138 138 138 138\n 138 138 138 138 138 138 138 138 138 138 138 138 138 138]\n(VAL) Batch 216 Loss : 4.41787052154541, accuracy: 0.0\nVAL : 예측라벨 : [ 55 158  13   6  24 158  52 165 141 148  69  95 128   8 117 110  84  95\n 138  31  90  81  13 198   6 117 108  55  24  58  58 124], 정답 [138 138 138 138 138 138 139 139 139 139 139 139 139 139 139 139 139 139\n 139 139 139 139 139 139 139 139 139 139 139 139 139 139]\n(VAL) Batch 217 Loss : 4.902936935424805, accuracy: 0.0\nVAL : 예측라벨 : [ 81  39 110 141  39  31  24  31 196 141  58 167  58   6  24  31 109  52\n   8 189 137 135  39  41  87  69  24 119 110 192 185  55], 정답 [139 139 139 139 139 139 139 139 139 139 139 139 139 139 139 139 139 139\n 139 139 139 139 139 139 140 140 140 140 140 140 140 140]\n(VAL) Batch 218 Loss : 5.262330055236816, accuracy: 0.0\nVAL : 예측라벨 : [ 69  29 110 108  58   6  24  69 110  95 141 110 192 138 138  13   6  31\n  48  12  24  13  31 138 177  90 103  26  39 137  81  90], 정답 [140 140 140 140 140 140 140 140 140 140 140 140 140 140 140 140 140 140\n 140 140 140 140 140 140 140 140 140 140 140 140 140 140]\n(VAL) Batch 219 Loss : 5.0337677001953125, accuracy: 0.0\nVAL : 예측라벨 : [117 108  56   6  69  31  29 175  72 160  55  31 141  81  24   4 160   8\n  69 143 108   8 110 117 105  37   8 108  68  60  55  81], 정답 [140 140 140 140 140 140 140 140 140 140 141 141 141 141 141 141 141 141\n 141 141 141 141 141 141 141 141 141 141 141 141 141 141]\n(VAL) Batch 220 Loss : 4.491804122924805, accuracy: 0.03125\nVAL : 예측라벨 : [101   8   4 128 110  69  81 109  69 124   8 138   6  80 110  52 138 110\n  27 117 110  29 108  31 110  24 117 103  24  86  66   8], 정답 [141 141 141 141 141 141 141 141 141 141 141 141 141 141 141 141 141 141\n 141 141 141 141 141 141 141 141 141 141 142 142 142 142]\n(VAL) Batch 221 Loss : 4.409125328063965, accuracy: 0.0\nVAL : 예측라벨 : [ 90  24 189  24   8 146 103  93 105 119 105  90 144   1  52  24  29 189\n  39 180 138 138  86 192 108  54   6 143  48  84 108  58], 정답 [142 142 142 142 142 142 142 142 142 142 142 142 142 142 142 142 142 142\n 142 142 142 142 142 142 142 142 142 142 142 142 142 142]\n(VAL) Batch 222 Loss : 5.098608016967773, accuracy: 0.0\nVAL : 예측라벨 : [ 93 110  57  58  80 115  86 108 180  24  24  27  69 179  52  39  37 143\n 138  37 160 107  39 138 143 138  24 143 143 143 143 143], 정답 [142 142 142 142 142 142 142 142 142 142 142 142 142 142 143 143 143 143\n 143 143 143 143 143 143 143 143 143 143 143 143 143 143]\n(VAL) Batch 223 Loss : 4.524401664733887, accuracy: 0.21875\nVAL : 예측라벨 : [143 164  50  24 143  12 141 114  60 114 143  55  39  39 183  27 128  24\n 148 143 170  37 107  60 198  60  37  60 114  24  90 109], 정답 [143 143 143 143 143 143 143 143 143 143 143 143 143 143 143 143 143 143\n 143 143 143 143 143 143 143 143 143 143 143 143 143 143]\n(VAL) Batch 224 Loss : 4.442108154296875, accuracy: 0.125\nVAL : 예측라벨 : [117  31  60  24  54  24  24  31 179  31  24  41  24  24  31  90  81 180\n 160  24  69  31 109 146  74  57 109  24  24   6 117  31], 정답 [144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144\n 144 144 144 144 144 144 144 144 144 144 144 144 144 144]\n(VAL) Batch 225 Loss : 4.5065107345581055, accuracy: 0.0\nVAL : 예측라벨 : [ 31 141  55 115  81  31  24 160 188 170  24 177   6 177 143  24 188  24\n 185 170 133 145 145 145 166 154 170 170 119 192 170 196], 정답 [144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144\n 145 145 145 145 145 145 145 145 145 145 145 145 145 145]\n(VAL) Batch 226 Loss : 4.371540546417236, accuracy: 0.09375\nVAL : 예측라벨 : [166 145 146 145   1  53 145 146 170 145 180 145 192 170 145 103 178 145\n 117 193 143  60 103 145  39 146 177 166 117 145 145 145], 정답 [145 145 145 145 145 145 145 145 145 145 145 145 145 145 145 145 145 145\n 145 145 145 145 145 145 145 145 145 145 145 145 145 145]\n(VAL) Batch 227 Loss : 4.199912071228027, accuracy: 0.34375\nVAL : 예측라벨 : [145 145 115 145  81 164 108 133 108 166 146 103  60  13 128 115 146 146\n 166  93 170 108 108 164  60  87 108 115  24 176 133  81], 정답 [145 145 145 145 146 146 146 146 146 146 146 146 146 146 146 146 146 146\n 146 146 146 146 146 146 146 146 146 146 146 146 146 146]\n(VAL) Batch 228 Loss : 3.9091286659240723, accuracy: 0.1875\nVAL : 예측라벨 : [170  81 114  12 166  60 189  69 146  96 108 146  93 170 133 108 115 138\n  29 108  95  95 138 143 109   6   6 141 114 141 160 143], 정답 [146 146 146 146 146 146 146 146 146 146 146 146 146 146 146 146 146 146\n 146 146 146 146 147 147 147 147 147 147 147 147 147 147]\n(VAL) Batch 229 Loss : 4.663611888885498, accuracy: 0.0625\nVAL : 예측라벨 : [ 90   6  52   6   4 160  24  81  81  12 164 166  81  24  12  24   6 141\n 166   6 160   1   6 166  69 141 192  81   8 121 160  81], 정답 [147 147 147 147 147 147 147 147 147 147 147 147 147 147 147 147 147 147\n 147 147 147 147 147 147 147 147 147 147 147 147 147 147]\n(VAL) Batch 230 Loss : 4.739528656005859, accuracy: 0.0\nVAL : 예측라벨 : [141  24  81 110 160  50 119  90  13 158 109 122 138  13  71  90 108 183\n 115  26  13  13 170 117 198  13  24 198  95  13 144  95], 정답 [147 147 147 147 147 147 147 147 148 148 148 148 148 148 148 148 148 148\n 148 148 148 148 148 148 148 148 148 148 148 148 148 148]\n(VAL) Batch 231 Loss : 4.601052761077881, accuracy: 0.0\nVAL : 예측라벨 : [ 13  13 101  13  13  81 105  55  50 148  13  13  13  13  24 148 199  24\n  22 101  69  69 148 196 117  13 110 110  24 135  60 110], 정답 [148 148 148 148 148 148 148 148 148 148 148 148 148 148 148 148 148 148\n 148 148 148 148 148 148 148 148 149 149 149 149 149 149]\n(VAL) Batch 232 Loss : 4.037899017333984, accuracy: 0.09375\nVAL : 예측라벨 : [ 90  24 138 138  81  24   8  31 115 188 138  86  31 189  13  80  39 144\n  13 166  24   5  86  52 110 189  24  52 149 138  52   8], 정답 [149 149 149 149 149 149 149 149 149 149 149 149 149 149 149 149 149 149\n 149 149 149 149 149 149 149 149 149 149 149 149 149 149]\n(VAL) Batch 233 Loss : 4.764091491699219, accuracy: 0.03125\nVAL : 예측라벨 : [ 90 108 114  60  80   6 103 141  24  24  50  39  78 192 143  55  90  24\n  26 189  27  24  24 105 110  24  24  90 189 192 105  60], 정답 [149 149 149 149 149 149 149 149 149 149 149 149 150 150 150 150 150 150\n 150 150 150 150 150 150 150 150 150 150 150 150 150 150]\n(VAL) Batch 234 Loss : 4.9703688621521, accuracy: 0.0\nVAL : 예측라벨 : [ 24  24 105 143  31   6 110  26 192  90 192 193  57   6  55  24 163  60\n  39  24  24 192  60  24 158  24 192 128  24  31 110  48], 정답 [150 150 150 150 150 150 150 150 150 150 150 150 150 150 150 150 150 150\n 150 150 150 150 150 150 150 150 150 150 150 150 151 151]\n(VAL) Batch 235 Loss : 5.053846836090088, accuracy: 0.0\nVAL : 예측라벨 : [ 24 189  55 133  69   8 117  69 108  24  24  31 110  55  24  31  29   6\n   8  56 122 170  69  96  87 183  31  81   8 108 110  69], 정답 [151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151\n 151 151 151 151 151 151 151 151 151 151 151 151 151 151]\n(VAL) Batch 236 Loss : 5.00949764251709, accuracy: 0.0\nVAL : 예측라벨 : [  8   8  24  69  95  31 117   6  81  69 192 124  55  87 198 163 121  50\n 170 170   1 154  42  40   6  52 152 133   8  37  50 119], 정답 [151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 152 152\n 152 152 152 152 152 152 152 152 152 152 152 152 152 152]\n(VAL) Batch 237 Loss : 4.703309059143066, accuracy: 0.03125\nVAL : 예측라벨 : [ 69 108 121  37  50 110   6 103  31 189  78  23  78 152 160  91  39 199\n  78  50  40   8  39   8  48   6  39  78  92  78 108  39], 정답 [152 152 152 152 152 152 152 152 152 152 152 152 152 152 152 152 152 152\n 152 152 152 152 152 152 152 152 152 152 152 152 152 152]\n(VAL) Batch 238 Loss : 4.677346706390381, accuracy: 0.03125\nVAL : 예측라벨 : [  6 119  90  86  95 128  87  78 189  39 133 145 116   6 115 115   4 103\n  90 143  81  87  86 189 145 178  87  81  90  81  93 189], 정답 [152 152 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153\n 153 153 153 153 153 153 153 153 153 153 153 153 153 153]\n(VAL) Batch 239 Loss : 4.8057169914245605, accuracy: 0.0\nVAL : 예측라벨 : [ 97 103 141 145 108 170  84 189  90  81 170  81 145 104  13 121 185  60\n  13  90 101 121  74  50 143  86  84 164 118 103 117 198], 정답 [153 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153\n 153 153 154 154 154 154 154 154 154 154 154 154 154 154]\n(VAL) Batch 240 Loss : 4.898131847381592, accuracy: 0.0\nVAL : 예측라벨 : [  1 108   6 170 198 154 166 154 198 166 170  60 146  13 198 166  71 166\n 192 121   6 154 198 196 170 108 198  81 154 166 108 154], 정답 [154 154 154 154 154 154 154 154 154 154 154 154 154 154 154 154 154 154\n 154 154 154 154 154 154 154 154 154 154 154 154 154 154]\n(VAL) Batch 241 Loss : 4.134603977203369, accuracy: 0.15625\nVAL : 예측라벨 : [ 95 192 198 170 117 154  24  55  24 117  24  24  13 121 110 117 143  13\n 141  24  24 124 110 141 110  55 141  69  37 177  69 144], 정답 [154 154 154 154 154 154 155 155 155 155 155 155 155 155 155 155 155 155\n 155 155 155 155 155 155 155 155 155 155 155 155 155 155]\n(VAL) Batch 242 Loss : 5.015463352203369, accuracy: 0.03125\nVAL : 예측라벨 : [ 55 117   4  60  71 117  69 158  31  87 177 149  90 196   6  58 110   6\n  71 110 160  55 117  69  90  24 110  74 109 141  24 117], 정답 [155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155\n 155 155 155 155 155 155 156 156 156 156 156 156 156 156]\n(VAL) Batch 243 Loss : 4.99340295791626, accuracy: 0.0\nVAL : 예측라벨 : [ 27  24  41  90  55  24 109  55  24 110  24  57  56  69  24 110   8  24\n  31  37   8 163  24  27 144 170 192   8  24  24  24  24], 정답 [156 156 156 156 156 156 156 156 156 156 156 156 156 156 156 156 156 156\n 156 156 156 156 156 156 156 156 156 156 156 156 156 156]\n(VAL) Batch 244 Loss : 5.1107401847839355, accuracy: 0.0\nVAL : 예측라벨 : [  8  55  41  95  12  24  24   6 143 110 170 108 166   8 173 198 170 108\n 146 154  69 109  87 152 170  78 196 170 170 146  95 198], 정답 [156 156 156 156 156 156 156 156 156 156 157 157 157 157 157 157 157 157\n 157 157 157 157 157 157 157 157 157 157 157 157 157 157]\n(VAL) Batch 245 Loss : 4.658163547515869, accuracy: 0.0\nVAL : 예측라벨 : [106 170  75 121 198 170 170  50 138 170   8 110 103  95 115 170 170  55\n  81 110 198  22 166  55 118  37  57 108 122 124 108  60], 정답 [157 157 157 157 157 157 157 157 157 157 157 157 157 157 157 157 157 157\n 157 157 157 157 157 157 157 157 157 157 158 158 158 158]\n(VAL) Batch 246 Loss : 4.803722858428955, accuracy: 0.0\nVAL : 예측라벨 : [138 193 115 110  95 158 189 158 138  54 110 110  95 158 199 196  24  66\n  24  58  37  44  74  60  31 110 180 119 179 192  24  96], 정답 [158 158 158 158 158 158 158 158 158 158 158 158 158 158 158 158 158 158\n 158 158 158 158 158 158 158 158 158 158 158 158 158 158]\n(VAL) Batch 247 Loss : 4.803909778594971, accuracy: 0.09375\nVAL : 예측라벨 : [ 24 117  12  81  13  24  41 148  24  71  13  13  69 108 109  69 160 192\n 177  69 189   6  90  31  24  52  81 177  69 110 160 141], 정답 [158 158 158 158 158 158 158 158 158 158 158 158 158 158 159 159 159 159\n 159 159 159 159 159 159 159 159 159 159 159 159 159 159]\n(VAL) Batch 248 Loss : 4.690473556518555, accuracy: 0.0\nVAL : 예측라벨 : [138  24 167  31  50  95  29  24  69  31 109  24  24 141  24  69   8  97\n  69  90  69  24  81  27 114  27   6 122  57  24 128  31], 정답 [159 159 159 159 159 159 159 159 159 159 159 159 159 159 159 159 159 159\n 159 159 159 159 159 159 159 159 159 159 159 159 159 159]\n(VAL) Batch 249 Loss : 4.980460166931152, accuracy: 0.0\nVAL : 예측라벨 : [  6 109  69 178   8   8   6 110  74  24  55  42  50 141  38  31  24  27\n  31  31 192  69 108 192 143 199 166  24 177  52  26 110], 정답 [160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160\n 160 160 160 160 160 160 160 160 160 160 160 160 160 160]\n(VAL) Batch 250 Loss : 4.4454779624938965, accuracy: 0.0\nVAL : 예측라벨 : [110  93 117 170  55  24  55  24 170 177  50  42  41 110   8 170  31  69\n  26 198  73  24  24  31 192 189  31  58 143  60  24  31], 정답 [160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160\n 161 161 161 161 161 161 161 161 161 161 161 161 161 161]\n(VAL) Batch 251 Loss : 4.643685817718506, accuracy: 0.0\nVAL : 예측라벨 : [189  74 160 143  24   8   6 192   6   6  27  24  31  24 189  24  31 124\n 158  31  55 160  31  31  55 160 192  31  24 192 192  26], 정답 [161 161 161 161 161 161 161 161 161 161 161 161 161 161 161 161 161 161\n 161 161 161 161 161 161 161 161 161 161 161 161 161 161]\n(VAL) Batch 252 Loss : 4.400186538696289, accuracy: 0.0\nVAL : 예측라벨 : [110 158  74  24 170 166   9  37 170 185 166  53 170   6 170   6  81 170\n 162   6 170  52 170 145   6   6 114 170   6   4 166  52], 정답 [161 161 161 161 162 162 162 162 162 162 162 162 162 162 162 162 162 162\n 162 162 162 162 162 162 162 162 162 162 162 162 162 162]\n(VAL) Batch 253 Loss : 4.7284393310546875, accuracy: 0.03125\nVAL : 예측라벨 : [ 84 146   1 170 170   6 198  78 167 143 170 170 170 103 170 146 166 170\n  50 170 185  52  71  24  31  24  24 115  44 105  24  24], 정답 [162 162 162 162 162 162 162 162 162 162 162 162 162 162 162 162 162 162\n 162 162 162 162 163 163 163 163 163 163 163 163 163 163]\n(VAL) Batch 254 Loss : 4.407191276550293, accuracy: 0.0\nVAL : 예측라벨 : [ 24 180  90 138  60  24  60  60  24  29 158  24  24 163  24  24 138  60\n  90 158 109  90  24  60 138  60 138 188 138  90 130 138], 정답 [163 163 163 163 163 163 163 163 163 163 163 163 163 163 163 163 163 163\n 163 163 163 163 163 163 163 163 163 163 163 163 163 163]\n(VAL) Batch 255 Loss : 4.060957431793213, accuracy: 0.03125\nVAL : 예측라벨 : [138  71  24 158  71  58  24  24  90 170 118 189 198  93 115 114 115  57\n 170 163  57  96 115  90 164  52 114  90 170 170 138  95], 정답 [163 163 163 163 163 163 163 163 164 164 164 164 164 164 164 164 164 164\n 164 164 164 164 164 164 164 164 164 164 164 164 164 164]\n(VAL) Batch 256 Loss : 4.657270431518555, accuracy: 0.03125\nVAL : 예측라벨 : [114 143 166 114 170 170   4 164 119  50  78  81 170  60 138 189  37 108\n 196 166 108 166  93 115 133 143 170  57  26  50 165   6], 정답 [164 164 164 164 164 164 164 164 164 164 164 164 164 164 164 164 164 164\n 164 164 164 164 164 164 164 164 165 165 165 165 165 165]\n(VAL) Batch 257 Loss : 4.66445255279541, accuracy: 0.0625\nVAL : 예측라벨 : [  6 170 170 165 143 128 165 198  75  57 146  81 170 124  57 192   6  57\n  57  84 165  81 195   6 128 124 165  86 166  26  52  24], 정답 [165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165\n 165 165 165 165 165 165 165 165 165 165 165 165 165 165]\n(VAL) Batch 258 Loss : 4.992000102996826, accuracy: 0.125\nVAL : 예측라벨 : [154  96   6  55 124 177  53 124   6 170  81  78 166 166 108  87 114 154\n  84  81 121 121 166 146 166 108 170 166 145 166 166 133], 정답 [165 165 165 165 165 165 165 165 165 165 165 165 166 166 166 166 166 166\n 166 166 166 166 166 166 166 166 166 166 166 166 166 166]\n(VAL) Batch 259 Loss : 3.973653793334961, accuracy: 0.21875\nVAL : 예측라벨 : [114 103 154 154  57 166   8 166 119 166  93 108 145 166 170 121 170  93\n  87 166 170 166 166 154 154 166 121 146  93  50  69  29], 정답 [166 166 166 166 166 166 166 166 166 166 166 166 166 166 166 166 166 166\n 166 166 166 166 166 166 166 166 166 166 166 166 167 167]\n(VAL) Batch 260 Loss : 3.8318843841552734, accuracy: 0.25\nVAL : 예측라벨 : [105 143 117 119  81 133   8 180 117 108 117  60 108 110 128 108  24  55\n 108  39  60 167 108  37  55 108  55 108  55  60 143 143], 정답 [167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 167\n 167 167 167 167 167 167 167 167 167 167 167 167 167 167]\n(VAL) Batch 261 Loss : 4.59470796585083, accuracy: 0.03125\nVAL : 예측라벨 : [108  39 117  27 110  95   6  24 108 138 108  24 146  81  24 143 186 108\n 128 163  13  60   8 170 143 103  24 105 103 166 108  31], 정답 [167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 168 168\n 168 168 168 168 168 168 168 168 168 168 168 168 168 168]\n(VAL) Batch 262 Loss : 5.024564743041992, accuracy: 0.0\nVAL : 예측라벨 : [130 108  81  95 108 110   6  39 189  31 198  24 143  38  80  81  80 119\n  90  13 158 192  74 143 170  31   4 122 109  58 158 115], 정답 [168 168 168 168 168 168 168 168 168 168 168 168 168 168 168 168 168 168\n 168 168 168 168 168 168 168 168 168 168 168 168 168 168]\n(VAL) Batch 263 Loss : 5.122860431671143, accuracy: 0.0\nVAL : 예측라벨 : [170  60  93   6 192  24  93  24  24  90  24  31  52  93 108  24 180  24\n 189  24  26 189 143  24  93  24 189 110  24 189  31 105], 정답 [168 168 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169\n 169 169 169 169 169 169 169 169 169 169 169 169 169 169]\n(VAL) Batch 264 Loss : 4.922849655151367, accuracy: 0.0\nVAL : 예측라벨 : [119 119 138  52  90  55 189  24 138  24 138  60  69  90  31  39  90  57\n  93  90  57 138 170 170 166 117 170 170 198   6 154 170], 정답 [169 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169\n 169 169 170 170 170 170 170 170 170 170 170 170 170 170]\n(VAL) Batch 265 Loss : 4.404758453369141, accuracy: 0.15625\nVAL : 예측라벨 : [170 148 108 108  78 170 114 166 170  71 166  39 166  31 170  39 170 160\n 170 121 121 146  66 109  50 170   8  50 170  50 170  50], 정답 [170 170 170 170 170 170 170 170 170 170 170 170 170 170 170 170 170 170\n 170 170 170 170 170 170 170 170 170 170 170 170 170 170]\n(VAL) Batch 266 Loss : 4.106904029846191, accuracy: 0.28125\nVAL : 예측라벨 : [ 50  24 166 170 170 103  31  29  39  90  24   1 192 166  90 143  39 110\n  24  81  39 166  24  90  24 128  24  41 143  45  93 109], 정답 [170 170 170 170 170 170 171 171 171 171 171 171 171 171 171 171 171 171\n 171 171 171 171 171 171 171 171 171 171 171 171 171 171]\n(VAL) Batch 267 Loss : 4.998032093048096, accuracy: 0.0625\nVAL : 예측라벨 : [121   8 167 128 170 158   6  50  12 185 109  37 178  24  84 115 180  24\n 160  13  81 143  26 185  90  23  24 110  31 117 119  52], 정답 [171 171 171 171 171 171 171 171 171 171 171 171 171 171 171 171 171 171\n 171 171 171 171 171 171 172 172 172 172 172 172 172 172]\n(VAL) Batch 268 Loss : 5.129139423370361, accuracy: 0.0\nVAL : 예측라벨 : [ 24  24  24  50  74  69  29 124 110  22  24 110 187  31  31  69 143 109\n  24  24 177   8 177  69 105  40  24  24  24  81  69 187], 정답 [172 172 172 172 172 172 172 172 172 172 172 172 172 172 172 172 172 172\n 172 172 172 172 172 172 172 172 172 172 172 172 172 172]\n(VAL) Batch 269 Loss : 4.76129150390625, accuracy: 0.0\nVAL : 예측라벨 : [188  22  69  81 121 109  31 130  29  69  27 105 117 173  24   8 101 109\n 124 196 109  75  69 121 170  24  60 198 101  69  71  71], 정답 [172 172 172 172 172 172 172 172 172 172 173 173 173 173 173 173 173 173\n 173 173 173 173 173 173 173 173 173 173 173 173 173 173]\n(VAL) Batch 270 Loss : 5.0521345138549805, accuracy: 0.03125\nVAL : 예측라벨 : [117 124  71 189  69 170  13 198 198  75 154 109 124   8 154  58  66 198\n  13 170 124 101  71 124  13 117 101 124  58 185  81 192], 정답 [173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173\n 173 173 173 173 173 173 173 173 173 173 174 174 174 174]\n(VAL) Batch 271 Loss : 4.826146125793457, accuracy: 0.0\nVAL : 예측라벨 : [192 178  69 160 143  52 114  56 177 192 114  58 192  24  81 190 145  39\n 178  50  69 177 145 119  52 192 192 192 145  24  91 104], 정답 [174 174 174 174 174 174 174 174 174 174 174 174 174 174 174 174 174 174\n 174 174 174 174 174 174 174 174 174 174 174 174 174 174]\n(VAL) Batch 272 Loss : 4.728959083557129, accuracy: 0.0\nVAL : 예측라벨 : [  6  24  93 104  81  69  29  90 192  52  31 192  39  97  69  69 192  29\n  58 109   6  69  69 188  24 193 192  31  93  12 109  24], 정답 [174 174 174 174 174 174 174 174 174 174 174 174 174 174 175 175 175 175\n 175 175 175 175 175 175 175 175 175 175 175 175 175 175]\n(VAL) Batch 273 Loss : 4.878110885620117, accuracy: 0.0\nVAL : 예측라벨 : [177 192 130  90 138 135 119 102  24  39 160  22  90  31 141  24  24 177\n 179  24  24  52   8 105  24  26 192 192 160  81 145  90], 정답 [175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175\n 175 175 175 175 175 175 175 175 175 175 175 175 175 175]\n(VAL) Batch 274 Loss : 4.829981803894043, accuracy: 0.0\nVAL : 예측라벨 : [ 93  93  93 103  93  90  93  90 128 114   1  90 167  93  93 128  24 190\n  24  93 162 138 143 108 164  37 135 146 108 189 176 143], 정답 [176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176\n 176 176 176 176 176 176 176 176 176 176 176 176 176 176]\n(VAL) Batch 275 Loss : 4.749758720397949, accuracy: 0.03125\nVAL : 예측라벨 : [143  60  60  93  58 108 143  90 108 145 108  90   6  13 138  93 128  26\n 158   0  90 192 192 177  41 177  31  24  39  69 192 192], 정답 [176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176\n 177 177 177 177 177 177 177 177 177 177 177 177 177 177]\n(VAL) Batch 276 Loss : 4.626439094543457, accuracy: 0.0625\nVAL : 예측라벨 : [160 177 177  74  50  52  69 171  26 180 160 192 186  24 177 177  27 170\n  31 199 192  26 189 192  24  31 178  24  69  37   6 177], 정답 [177 177 177 177 177 177 177 177 177 177 177 177 177 177 177 177 177 177\n 177 177 177 177 177 177 177 177 177 177 177 177 177 177]\n(VAL) Batch 277 Loss : 4.226873397827148, accuracy: 0.15625\nVAL : 예측라벨 : [160 192  26   6  37 160  39  37  39 192 192 188  39 192 187  52 187  34\n 192  39  39 192 187 178 192 143 192 192 177 188 192 143], 정답 [177 177 177 177 178 178 178 178 178 178 178 178 178 178 178 178 178 178\n 178 178 178 178 178 178 178 178 178 178 178 178 178 178]\n(VAL) Batch 278 Loss : 3.8934924602508545, accuracy: 0.03125\nVAL : 예측라벨 : [ 39 178 192 192  39 178 192 177 187  39  39 188  39  26 192  37 192 192\n 188  27 192 160 160 148  69  31 177  24 192 188  39 177], 정답 [178 178 178 178 178 178 178 178 178 178 178 178 178 178 178 178 178 178\n 178 178 178 178 179 179 179 179 179 179 179 179 179 179]\n(VAL) Batch 279 Loss : 4.101897239685059, accuracy: 0.0625\nVAL : 예측라벨 : [ 53   6  90 189  26 158  48 189  24  69  24 192  24 128 170 192   6 130\n  91  24  24 192  24  57  24 192 189 192 179 189  24  24], 정답 [179 179 179 179 179 179 179 179 179 179 179 179 179 179 179 179 179 179\n 179 179 179 179 179 179 179 179 179 179 179 179 179 179]\n(VAL) Batch 280 Loss : 4.245298385620117, accuracy: 0.03125\nVAL : 예측라벨 : [192  26  50   8  24 160 160  24 138 138 158  74  24 179  24  24  24  24\n  90 180  74  29 160  13  71 192  24 177 180  24  31  24], 정답 [179 179 179 179 179 179 179 179 180 180 180 180 180 180 180 180 180 180\n 180 180 180 180 180 180 180 180 180 180 180 180 180 180]\n(VAL) Batch 281 Loss : 4.235409736633301, accuracy: 0.0625\nVAL : 예측라벨 : [138  13  24  24 187  24 192 160  24  24  24  24  24 158  90  74  81  24\n 189 192 109 160  26  31  24  74  24  57 177  39 160  90], 정답 [180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180\n 180 180 180 180 180 180 180 180 181 181 181 181 181 181]\n(VAL) Batch 282 Loss : 4.5481133460998535, accuracy: 0.0\nVAL : 예측라벨 : [192 192 130  53  24 192 192  90 177 192  26 101 177  24 189  24  24  24\n 192  24  81 192   6 192  26  93  61  90  22 192 135 192], 정답 [181 181 181 181 181 181 181 181 181 181 181 181 181 181 181 181 181 181\n 181 181 181 181 181 181 181 181 181 181 181 181 181 181]\n(VAL) Batch 283 Loss : 4.985424041748047, accuracy: 0.0\nVAL : 예측라벨 : [ 57   8  24 192 192  69 178 198 177  26 192 192 192 174 177 192 192  24\n 143 160 192 192 192 177 192   6 177 160 192  27  91 177], 정답 [181 181 181 181 181 181 181 181 181 181 181 181 182 182 182 182 182 182\n 182 182 182 182 182 182 182 182 182 182 182 182 182 182]\n(VAL) Batch 284 Loss : 4.65972900390625, accuracy: 0.0\nVAL : 예측라벨 : [177 160  24 192  69  69 192 192 160  69 192 192  69 199 192  31 192 192\n 192 192   6 160  69 192  26 192 192  69  69 192   6 110], 정답 [182 182 182 182 182 182 182 182 182 182 182 182 182 182 182 182 182 182\n 182 182 182 182 182 182 182 182 182 182 182 182 183 183]\n(VAL) Batch 285 Loss : 4.4335618019104, accuracy: 0.0\nVAL : 예측라벨 : [ 31 192   6  39 177 192  69  69  24  95 187  39 192  69  24  24  58  24\n  48  39  31  31  69 178  39  31  39  69  69  24 114  23], 정답 [183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 183\n 183 183 183 183 183 183 183 183 183 183 183 183 183 183]\n(VAL) Batch 286 Loss : 4.589853763580322, accuracy: 0.0\nVAL : 예측라벨 : [187  69 188  31  39  39  24  24 192 192  39 192  26  39  39  27  39  36\n 192 199 187 199  39 199 199 192 189  39  39 199  74 188], 정답 [183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 184 184\n 184 184 184 184 184 184 184 184 184 184 184 184 184 184]\n(VAL) Batch 287 Loss : 4.2398681640625, accuracy: 0.0\nVAL : 예측라벨 : [189  39 192 187 148  39  38 189  39 192 192 192 188 187 199 143 177  69\n  39 192  90 192  39 192 177 177  39 199 189  39  39  39], 정답 [184 184 184 184 184 184 184 184 184 184 184 184 184 184 184 184 184 184\n 184 184 184 184 184 184 184 184 184 184 184 184 184 184]\n(VAL) Batch 288 Loss : 4.051609992980957, accuracy: 0.0\nVAL : 예측라벨 : [192 192 192  48  57 189  39 143  31 199   6  57   6 189  52  39  58 104\n  56  61  50   8  71  54 143  87 189  58   6 192  24   1], 정답 [184 184 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185\n 185 185 185 185 185 185 185 185 185 185 185 185 185 185]\n(VAL) Batch 289 Loss : 4.6824517250061035, accuracy: 0.0\nVAL : 예측라벨 : [114  39  52  39 189 188 177  69  52  39 177   6  93 108   4 160 103  52\n 158  57 192 138 192 188 199 192 187 192 145 192 192 160], 정답 [185 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185\n 185 185 186 186 186 186 186 186 186 186 186 186 186 186]\n(VAL) Batch 290 Loss : 4.3488664627075195, accuracy: 0.0\nVAL : 예측라벨 : [ 13 192 192 192 192  13 192  24  39 110 192 188 192 177 177 192 192 192\n 192 192 192   0   0 192 192 189 187 192   0  39 192 192], 정답 [186 186 186 186 186 186 186 186 186 186 186 186 186 186 186 186 186 186\n 186 186 186 186 186 186 186 186 186 186 186 186 186 186]\n(VAL) Batch 291 Loss : 3.43044376373291, accuracy: 0.0\nVAL : 예측라벨 : [110 192 192 192 192 192 192 177 192  39 192 177 187  37  13 110 192 192\n  39 187 187 187 188 192 188 192 192 192 187 104 188 192], 정답 [186 186 186 186 186 186 187 187 187 187 187 187 187 187 187 187 187 187\n 187 187 187 187 187 187 187 187 187 187 187 187 187 187]\n(VAL) Batch 292 Loss : 3.6097004413604736, accuracy: 0.15625\nVAL : 예측라벨 : [187   0  69  39 160 187  39  14 177 192 187 192 187 192 192 188 188   0\n 188  39 187 187 192 192 193  38 160  24 174 188 188 160], 정답 [187 187 187 187 187 187 187 187 187 187 187 187 187 187 187 187 187 187\n 187 187 187 187 187 187 188 188 188 188 188 188 188 188]\n(VAL) Batch 293 Loss : 3.3380579948425293, accuracy: 0.25\nVAL : 예측라벨 : [ 39 188 192 192 192  24 189 192 177 158 192 192  69 188  24 187 160 177\n 192  69  69 192 163 109  24 186 180 192 188 160  31 192], 정답 [188 188 188 188 188 188 188 188 188 188 188 188 188 188 188 188 188 188\n 188 188 188 188 188 188 188 188 188 188 188 188 188 188]\n(VAL) Batch 294 Loss : 3.783731460571289, accuracy: 0.09375\nVAL : 예측라벨 : [192 143 192 130  69  38  52 188  39 192  39 160  90 189 189 189 188 178\n  37 189 192 177 189  24 189  90  90  90  24 189  24 189], 정답 [188 188 188 188 188 188 188 188 188 188 189 189 189 189 189 189 189 189\n 189 189 189 189 189 189 189 189 189 189 189 189 189 189]\n(VAL) Batch 295 Loss : 3.7207651138305664, accuracy: 0.28125\nVAL : 예측라벨 : [ 24 189 192  24 189  74 179  69 199   1  74 189  24 192 189 189 189 189\n 189  38  24 192 189 177  39  24  93 189 192  52  91 177], 정답 [189 189 189 189 189 189 189 189 189 189 189 189 189 189 189 189 189 189\n 189 189 189 189 189 189 189 189 189 189 190 190 190 190]\n(VAL) Batch 296 Loss : 3.9282243251800537, accuracy: 0.3125\nVAL : 예측라벨 : [177 192 189 160 160  57 177  37  24 177 192 177  26   6 192  24 177 177\n 177 192  39 177 115  31  26 174   6 192 180 192 190  24], 정답 [190 190 190 190 190 190 190 190 190 190 190 190 190 190 190 190 190 190\n 190 190 190 190 190 190 190 190 190 190 190 190 190 190]\n(VAL) Batch 297 Loss : 4.728248596191406, accuracy: 0.03125\nVAL : 예측라벨 : [192 192 185  89 192 192 177 192 192   6 188 177  31  24  52 192 192 177\n 191 158  31 190 192 192  52 189 192 192  52 192 192 190], 정답 [190 190 190 190 190 190 190 190 190 190 190 190 190 190 191 191 191 191\n 191 191 191 191 191 191 191 191 191 191 191 191 191 191]\n(VAL) Batch 298 Loss : 4.567782878875732, accuracy: 0.03125\nVAL : 예측라벨 : [192 192  24 104 192 108 192 191 190 192  54  24 192 192  24 190 192  50\n 192   1  52 192 104   6 177 143 192  89 189 190 190 177], 정답 [191 191 191 191 191 191 191 191 191 191 191 191 191 191 191 191 191 191\n 191 191 191 191 191 191 191 191 191 191 191 191 191 191]\n(VAL) Batch 299 Loss : 4.753515720367432, accuracy: 0.03125\nVAL : 예측라벨 : [192 192 160  41 192 192 192 192 192 192 192 192 192 192 188 192  26 192\n 192 192 192 192 192  26 192 192  26 192 192 192 160 192], 정답 [192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192\n 192 192 192 192 192 192 192 192 192 192 192 192 192 192]\n(VAL) Batch 300 Loss : 2.717885732650757, accuracy: 0.78125\nVAL : 예측라벨 : [192 192 160 192 192 192  31  31 192 192 192 192 199  24 192 192 192   0\n  31  31  24 193  50 192 193  24 192 193  31  26 179  60], 정답 [192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192\n 193 193 193 193 193 193 193 193 193 193 193 193 193 193]\n(VAL) Batch 301 Loss : 3.3746676445007324, accuracy: 0.46875\nVAL : 예측라벨 : [ 31  69 160 193 192  24 192 192 160 193 180 160  48  69  24 110  31  31\n  31 192 192 192   0  81 192  26 108 177 193  69 177 160], 정답 [193 193 193 193 193 193 193 193 193 193 193 193 193 193 193 193 193 193\n 193 193 193 193 193 193 193 193 193 193 193 193 193 193]\n(VAL) Batch 302 Loss : 4.222588062286377, accuracy: 0.09375\nVAL : 예측라벨 : [ 58 180  31 180  13 146   6  95 170 102  69 117 121  95  71 198 154 198\n  13 198 198  50 170 198 101 192 132 101 198  24  95  31], 정답 [193 193 193 193 194 194 194 194 194 194 194 194 194 194 194 194 194 194\n 194 194 194 194 194 194 194 194 194 194 194 194 194 194]\n(VAL) Batch 303 Loss : 4.370561599731445, accuracy: 0.0\nVAL : 예측라벨 : [198 198  41 158 117 198 170  89 117 170 101  23  31  37 154  50 170 154\n 162  69 101 101  55  67  55 101 190  53 198   6  84 197], 정답 [194 194 194 194 194 194 194 194 194 194 194 194 194 194 194 194 194 194\n 194 194 194 194 195 195 195 195 195 195 195 195 195 195]\n(VAL) Batch 304 Loss : 4.59660530090332, accuracy: 0.0\nVAL : 예측라벨 : [177  24  69 158  93 166 198 198  52 170  69 165 170 154  31  12 170   6\n 198 170  81 198 170 198 198 170  55 198 170  12 196 192], 정답 [195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195\n 195 195 195 195 195 195 195 195 195 195 195 195 195 195]\n(VAL) Batch 305 Loss : 4.790582180023193, accuracy: 0.0\nVAL : 예측라벨 : [ 80 170 166 170 170 170 117 195  13 199 196  57 133  13  80  89  52  14\n 114  86 196  13  23  52  13 170  13 143  23  52 196  58], 정답 [195 195 195 195 195 195 195 195 196 196 196 196 196 196 196 196 196 196\n 196 196 196 196 196 196 196 196 196 196 196 196 196 196]\n(VAL) Batch 306 Loss : 4.033371448516846, accuracy: 0.125\nVAL : 예측라벨 : [146  52  12 196 178 196 196 196 148 196  14  60  11 196 170 162 146  13\n  14  14  14  13 108  93 148  23 170  86 198  39  13 198], 정답 [196 196 196 196 196 196 196 196 196 196 196 196 196 196 196 196 196 196\n 196 196 196 196 196 196 196 196 197 197 197 197 197 197]\n(VAL) Batch 307 Loss : 3.760774612426758, accuracy: 0.1875\nVAL : 예측라벨 : [196  13 170 108 170 198 170 115  23  23 198 170  23 132 170 162 170  37\n 170 170 198 198 166 170 198  50 170  95 108  50  95 114], 정답 [197 197 197 197 197 197 197 197 197 197 197 197 197 197 197 197 197 197\n 197 197 197 197 197 197 197 197 197 197 197 197 197 197]\n(VAL) Batch 308 Loss : 3.9610021114349365, accuracy: 0.0\nVAL : 예측라벨 : [119 124  39  29  71  50  23 154 117  81  31  95 114 170 198 124 117 197\n 196 199 198 199 196 198 198 170 160 198 196 198  71 154], 정답 [197 197 197 197 197 197 197 197 197 197 197 197 198 198 198 198 198 198\n 198 198 198 198 198 198 198 198 198 198 198 198 198 198]\n(VAL) Batch 309 Loss : 4.184602737426758, accuracy: 0.1875\nVAL : 예측라벨 : [ 31 198 198 198  13  71 198 198 198 170  26 154  69 198 198  84  78 198\n 115 198  23  55 198 170  66 198 148 198  23 198  50 177], 정답 [198 198 198 198 198 198 198 198 198 198 198 198 198 198 198 198 198 198\n 198 198 198 198 198 198 198 198 198 198 198 198 199 199]\n(VAL) Batch 310 Loss : 3.5041112899780273, accuracy: 0.4375\nVAL : 예측라벨 : [ 39  37 199   6  39  39  31  93  39  13 114   6  40  37  39  37  12  80\n  39 187 177  37  31 189  39  39  52  37 199  39 170  37], 정답 [199 199 199 199 199 199 199 199 199 199 199 199 199 199 199 199 199 199\n 199 199 199 199 199 199 199 199 199 199 199 199 199 199]\n(VAL) Batch 311 Loss : 4.040349006652832, accuracy: 0.0625\nVAL : 예측라벨 : [  6 187 115 199 143  37  52  39  26 109  37  74  39  12 199  39], 정답 [199 199 199 199 199 199 199 199 199 199 199 199 199 199 199 199]\n(VAL) Batch 312 Loss : 4.192059516906738, accuracy: 0.125\nepoch 0 Loss/Validate :4.5723817820747055 \nepoch 0 Accuracy/Validate : 0.0651\nEpoch : 1, batch 0\n(Train) Batch 0 Loss : 4.6487812995910645, 맞은 개수 : 3\nEpoch : 1, batch 1\n(Train) Batch 1 Loss : 4.624250411987305, 맞은 개수 : 10\nEpoch : 1, batch 2\n(Train) Batch 2 Loss : 4.510473728179932, 맞은 개수 : 1\nEpoch : 1, batch 3\n(Train) Batch 3 Loss : 4.680388450622559, 맞은 개수 : 6\nEpoch : 1, batch 4\n(Train) Batch 4 Loss : 4.621325969696045, 맞은 개수 : 8\nEpoch : 1, batch 5\n(Train) Batch 5 Loss : 4.7073163986206055, 맞은 개수 : 6\nEpoch : 1, batch 6\n(Train) Batch 6 Loss : 4.616400241851807, 맞은 개수 : 9\nEpoch : 1, batch 7\n(Train) Batch 7 Loss : 4.6257195472717285, 맞은 개수 : 10\nEpoch : 1, batch 8\n(Train) Batch 8 Loss : 4.695032596588135, 맞은 개수 : 5\nEpoch : 1, batch 9\n(Train) Batch 9 Loss : 4.601741790771484, 맞은 개수 : 9\nEpoch : 1, batch 10\n(Train) Batch 10 Loss : 4.646946430206299, 맞은 개수 : 10\nEpoch : 1, batch 11\n(Train) Batch 11 Loss : 4.631866931915283, 맞은 개수 : 9\nEpoch : 1, batch 12\n(Train) Batch 12 Loss : 4.560178279876709, 맞은 개수 : 11\nEpoch : 1, batch 13\n(Train) Batch 13 Loss : 4.774662971496582, 맞은 개수 : 9\nEpoch : 1, batch 14\n(Train) Batch 14 Loss : 4.604107856750488, 맞은 개수 : 8\nEpoch : 1, batch 15\n(Train) Batch 15 Loss : 4.70684814453125, 맞은 개수 : 7\nEpoch : 1, batch 16\n(Train) Batch 16 Loss : 4.671806335449219, 맞은 개수 : 7\nEpoch : 1, batch 17\n(Train) Batch 17 Loss : 4.692319869995117, 맞은 개수 : 6\nEpoch : 1, batch 18\n(Train) Batch 18 Loss : 4.671185493469238, 맞은 개수 : 8\nEpoch : 1, batch 19\n(Train) Batch 19 Loss : 4.475919723510742, 맞은 개수 : 4\nEpoch : 1, batch 20\n(Train) Batch 20 Loss : 4.644318580627441, 맞은 개수 : 4\nEpoch : 1, batch 21\n(Train) Batch 21 Loss : 4.431398868560791, 맞은 개수 : 9\nEpoch : 1, batch 22\n(Train) Batch 22 Loss : 4.561068058013916, 맞은 개수 : 8\nEpoch : 1, batch 23\n(Train) Batch 23 Loss : 4.7633819580078125, 맞은 개수 : 6\nEpoch : 1, batch 24\n(Train) Batch 24 Loss : 4.636386871337891, 맞은 개수 : 6\nEpoch : 1, batch 25\n(Train) Batch 25 Loss : 4.671214580535889, 맞은 개수 : 13\nEpoch : 1, batch 26\n(Train) Batch 26 Loss : 4.513723373413086, 맞은 개수 : 12\nEpoch : 1, batch 27\n(Train) Batch 27 Loss : 4.562493324279785, 맞은 개수 : 9\nEpoch : 1, batch 28\n(Train) Batch 28 Loss : 4.539872646331787, 맞은 개수 : 13\nEpoch : 1, batch 29\n(Train) Batch 29 Loss : 4.59567403793335, 맞은 개수 : 8\nEpoch : 1, batch 30\n(Train) Batch 30 Loss : 4.8583245277404785, 맞은 개수 : 8\nEpoch : 1, batch 31\n(Train) Batch 31 Loss : 4.629296779632568, 맞은 개수 : 6\nEpoch : 1, batch 32\n(Train) Batch 32 Loss : 4.5744829177856445, 맞은 개수 : 13\nEpoch : 1, batch 33\n(Train) Batch 33 Loss : 4.5784406661987305, 맞은 개수 : 9\nEpoch : 1, batch 34\n(Train) Batch 34 Loss : 4.559403419494629, 맞은 개수 : 7\nEpoch : 1, batch 35\n(Train) Batch 35 Loss : 4.696599006652832, 맞은 개수 : 7\nEpoch : 1, batch 36\n(Train) Batch 36 Loss : 4.478797435760498, 맞은 개수 : 8\nEpoch : 1, batch 37\n(Train) Batch 37 Loss : 4.793281078338623, 맞은 개수 : 6\nEpoch : 1, batch 38\n(Train) Batch 38 Loss : 4.479261875152588, 맞은 개수 : 8\nEpoch : 1, batch 39\n(Train) Batch 39 Loss : 4.619779586791992, 맞은 개수 : 5\nEpoch : 1, batch 40\n(Train) Batch 40 Loss : 4.56557035446167, 맞은 개수 : 10\nEpoch : 1, batch 41\n(Train) Batch 41 Loss : 4.793524265289307, 맞은 개수 : 6\nEpoch : 1, batch 42\n(Train) Batch 42 Loss : 4.48967170715332, 맞은 개수 : 9\nEpoch : 1, batch 43\n(Train) Batch 43 Loss : 4.5233049392700195, 맞은 개수 : 8\nEpoch : 1, batch 44\n(Train) Batch 44 Loss : 4.616415023803711, 맞은 개수 : 6\nEpoch : 1, batch 45\n(Train) Batch 45 Loss : 4.601109027862549, 맞은 개수 : 11\nEpoch : 1, batch 46\n(Train) Batch 46 Loss : 4.70587158203125, 맞은 개수 : 6\nEpoch : 1, batch 47\n(Train) Batch 47 Loss : 4.506044387817383, 맞은 개수 : 9\nEpoch : 1, batch 48\n(Train) Batch 48 Loss : 4.6614990234375, 맞은 개수 : 5\nEpoch : 1, batch 49\n(Train) Batch 49 Loss : 4.512164115905762, 맞은 개수 : 11\nEpoch : 1, batch 50\n(Train) Batch 50 Loss : 4.444525241851807, 맞은 개수 : 11\nEpoch : 1, batch 51\n(Train) Batch 51 Loss : 4.4775710105896, 맞은 개수 : 12\nEpoch : 1, batch 52\n(Train) Batch 52 Loss : 4.603933811187744, 맞은 개수 : 6\nEpoch : 1, batch 53\n(Train) Batch 53 Loss : 4.412142753601074, 맞은 개수 : 12\nEpoch : 1, batch 54\n(Train) Batch 54 Loss : 4.55493688583374, 맞은 개수 : 10\nEpoch : 1, batch 55\n(Train) Batch 55 Loss : 4.443729877471924, 맞은 개수 : 9\nEpoch : 1, batch 56\n(Train) Batch 56 Loss : 4.71801233291626, 맞은 개수 : 7\nEpoch : 1, batch 57\n(Train) Batch 57 Loss : 4.640041351318359, 맞은 개수 : 8\nEpoch : 1, batch 58\n(Train) Batch 58 Loss : 4.753332614898682, 맞은 개수 : 3\nEpoch : 1, batch 59\n(Train) Batch 59 Loss : 4.765293121337891, 맞은 개수 : 6\nEpoch : 1, batch 60\n(Train) Batch 60 Loss : 4.315831184387207, 맞은 개수 : 13\nEpoch : 1, batch 61\n(Train) Batch 61 Loss : 4.472515106201172, 맞은 개수 : 8\nEpoch : 1, batch 62\n(Train) Batch 62 Loss : 4.602235794067383, 맞은 개수 : 10\nEpoch : 1, batch 63\n(Train) Batch 63 Loss : 4.4928812980651855, 맞은 개수 : 12\nEpoch : 1, batch 64\n(Train) Batch 64 Loss : 4.717319011688232, 맞은 개수 : 3\nEpoch : 1, batch 65\n(Train) Batch 65 Loss : 4.4783034324646, 맞은 개수 : 12\nEpoch : 1, batch 66\n(Train) Batch 66 Loss : 4.4037370681762695, 맞은 개수 : 9\nEpoch : 1, batch 67\n(Train) Batch 67 Loss : 4.650239944458008, 맞은 개수 : 14\nEpoch : 1, batch 68\n(Train) Batch 68 Loss : 4.6027655601501465, 맞은 개수 : 4\nEpoch : 1, batch 69\n(Train) Batch 69 Loss : 4.52585506439209, 맞은 개수 : 4\nEpoch : 1, batch 70\n(Train) Batch 70 Loss : 4.449148654937744, 맞은 개수 : 8\nEpoch : 1, batch 71\n(Train) Batch 71 Loss : 4.543296813964844, 맞은 개수 : 9\nEpoch : 1, batch 72\n(Train) Batch 72 Loss : 4.609416961669922, 맞은 개수 : 10\nEpoch : 1, batch 73\n(Train) Batch 73 Loss : 4.616425037384033, 맞은 개수 : 9\nEpoch : 1, batch 74\n(Train) Batch 74 Loss : 4.6182475090026855, 맞은 개수 : 6\nEpoch : 1, batch 75\n(Train) Batch 75 Loss : 4.641360282897949, 맞은 개수 : 9\nEpoch : 1, batch 76\n(Train) Batch 76 Loss : 4.441648006439209, 맞은 개수 : 15\nEpoch : 1, batch 77\n(Train) Batch 77 Loss : 4.534512519836426, 맞은 개수 : 8\nEpoch : 1, batch 78\n(Train) Batch 78 Loss : 4.595564842224121, 맞은 개수 : 6\nEpoch : 1, batch 79\n(Train) Batch 79 Loss : 4.423922061920166, 맞은 개수 : 4\nEpoch : 1, batch 80\n(Train) Batch 80 Loss : 4.682920932769775, 맞은 개수 : 10\nEpoch : 1, batch 81\n(Train) Batch 81 Loss : 4.788388729095459, 맞은 개수 : 9\nEpoch : 1, batch 82\n(Train) Batch 82 Loss : 4.611081123352051, 맞은 개수 : 7\nEpoch : 1, batch 83\n(Train) Batch 83 Loss : 4.648664474487305, 맞은 개수 : 12\nEpoch : 1, batch 84\n(Train) Batch 84 Loss : 4.536221027374268, 맞은 개수 : 8\nEpoch : 1, batch 85\n(Train) Batch 85 Loss : 4.516265869140625, 맞은 개수 : 8\nEpoch : 1, batch 86\n(Train) Batch 86 Loss : 4.469579696655273, 맞은 개수 : 6\nEpoch : 1, batch 87\n(Train) Batch 87 Loss : 4.657878398895264, 맞은 개수 : 6\nEpoch : 1, batch 88\n(Train) Batch 88 Loss : 4.624610424041748, 맞은 개수 : 9\nEpoch : 1, batch 89\n(Train) Batch 89 Loss : 4.46173095703125, 맞은 개수 : 8\nEpoch : 1, batch 90\n(Train) Batch 90 Loss : 4.510593414306641, 맞은 개수 : 8\nEpoch : 1, batch 91\n(Train) Batch 91 Loss : 4.528807640075684, 맞은 개수 : 9\nEpoch : 1, batch 92\n(Train) Batch 92 Loss : 4.69610071182251, 맞은 개수 : 7\nEpoch : 1, batch 93\n(Train) Batch 93 Loss : 4.655025482177734, 맞은 개수 : 10\nEpoch : 1, batch 94\n(Train) Batch 94 Loss : 4.675252437591553, 맞은 개수 : 7\nEpoch : 1, batch 95\n(Train) Batch 95 Loss : 4.581568717956543, 맞은 개수 : 5\nEpoch : 1, batch 96\n(Train) Batch 96 Loss : 4.626100540161133, 맞은 개수 : 9\nEpoch : 1, batch 97\n(Train) Batch 97 Loss : 4.419193744659424, 맞은 개수 : 11\nEpoch : 1, batch 98\n(Train) Batch 98 Loss : 4.273855686187744, 맞은 개수 : 11\nEpoch : 1, batch 99\n(Train) Batch 99 Loss : 4.324686527252197, 맞은 개수 : 20\nEpoch : 1, batch 100\n(Train) Batch 100 Loss : 4.4986186027526855, 맞은 개수 : 6\nEpoch : 1, batch 101\n(Train) Batch 101 Loss : 4.5567426681518555, 맞은 개수 : 5\nEpoch : 1, batch 102\n(Train) Batch 102 Loss : 4.552922248840332, 맞은 개수 : 10\nEpoch : 1, batch 103\n(Train) Batch 103 Loss : 4.467449188232422, 맞은 개수 : 9\nEpoch : 1, batch 104\n(Train) Batch 104 Loss : 4.657275199890137, 맞은 개수 : 5\nEpoch : 1, batch 105\n(Train) Batch 105 Loss : 4.440479755401611, 맞은 개수 : 10\nEpoch : 1, batch 106\n(Train) Batch 106 Loss : 4.41671085357666, 맞은 개수 : 11\nEpoch : 1, batch 107\n(Train) Batch 107 Loss : 4.475038528442383, 맞은 개수 : 8\nEpoch : 1, batch 108\n(Train) Batch 108 Loss : 4.567761421203613, 맞은 개수 : 14\nEpoch : 1, batch 109\n(Train) Batch 109 Loss : 4.649515628814697, 맞은 개수 : 3\nEpoch : 1, batch 110\n(Train) Batch 110 Loss : 4.491899013519287, 맞은 개수 : 8\nEpoch : 1, batch 111\n(Train) Batch 111 Loss : 4.571893215179443, 맞은 개수 : 11\nEpoch : 1, batch 112\n(Train) Batch 112 Loss : 4.384097099304199, 맞은 개수 : 12\nEpoch : 1, batch 113\n(Train) Batch 113 Loss : 4.424828052520752, 맞은 개수 : 10\nEpoch : 1, batch 114\n(Train) Batch 114 Loss : 4.716273307800293, 맞은 개수 : 8\nEpoch : 1, batch 115\n(Train) Batch 115 Loss : 4.670564651489258, 맞은 개수 : 8\nEpoch : 1, batch 116\n(Train) Batch 116 Loss : 4.531219959259033, 맞은 개수 : 7\nEpoch : 1, batch 117\n(Train) Batch 117 Loss : 4.419628143310547, 맞은 개수 : 10\nEpoch : 1, batch 118\n(Train) Batch 118 Loss : 4.586391448974609, 맞은 개수 : 11\nEpoch : 1, batch 119\n(Train) Batch 119 Loss : 4.504581451416016, 맞은 개수 : 7\nEpoch : 1, batch 120\n(Train) Batch 120 Loss : 4.4078521728515625, 맞은 개수 : 5\nEpoch : 1, batch 121\n(Train) Batch 121 Loss : 4.51241397857666, 맞은 개수 : 13\nEpoch : 1, batch 122\n(Train) Batch 122 Loss : 4.603214263916016, 맞은 개수 : 8\nEpoch : 1, batch 123\n(Train) Batch 123 Loss : 4.476740837097168, 맞은 개수 : 11\nEpoch : 1, batch 124\n(Train) Batch 124 Loss : 4.477595329284668, 맞은 개수 : 10\nEpoch : 1, batch 125\n(Train) Batch 125 Loss : 4.62670373916626, 맞은 개수 : 12\nEpoch : 1, batch 126\n(Train) Batch 126 Loss : 4.402918338775635, 맞은 개수 : 11\nEpoch : 1, batch 127\n(Train) Batch 127 Loss : 4.498377323150635, 맞은 개수 : 8\nEpoch : 1, batch 128\n(Train) Batch 128 Loss : 4.744758129119873, 맞은 개수 : 13\nEpoch : 1, batch 129\n(Train) Batch 129 Loss : 4.595788955688477, 맞은 개수 : 8\nEpoch : 1, batch 130\n(Train) Batch 130 Loss : 4.6199140548706055, 맞은 개수 : 9\nEpoch : 1, batch 131\n(Train) Batch 131 Loss : 4.347417831420898, 맞은 개수 : 17\nEpoch : 1, batch 132\n(Train) Batch 132 Loss : 4.561505317687988, 맞은 개수 : 6\nEpoch : 1, batch 133\n(Train) Batch 133 Loss : 4.452179431915283, 맞은 개수 : 9\nEpoch : 1, batch 134\n(Train) Batch 134 Loss : 4.37794303894043, 맞은 개수 : 18\nEpoch : 1, batch 135\n(Train) Batch 135 Loss : 4.372910499572754, 맞은 개수 : 9\nEpoch : 1, batch 136\n(Train) Batch 136 Loss : 4.543917655944824, 맞은 개수 : 9\nEpoch : 1, batch 137\n(Train) Batch 137 Loss : 4.440880298614502, 맞은 개수 : 4\nEpoch : 1, batch 138\n(Train) Batch 138 Loss : 4.533618450164795, 맞은 개수 : 12\nEpoch : 1, batch 139\n(Train) Batch 139 Loss : 4.637908458709717, 맞은 개수 : 7\nEpoch : 1, batch 140\n(Train) Batch 140 Loss : 4.347987651824951, 맞은 개수 : 11\nEpoch : 1, batch 141\n(Train) Batch 141 Loss : 4.6806745529174805, 맞은 개수 : 7\nEpoch : 1, batch 142\n(Train) Batch 142 Loss : 4.603560447692871, 맞은 개수 : 5\nEpoch : 1, batch 143\n(Train) Batch 143 Loss : 4.60761833190918, 맞은 개수 : 7\nEpoch : 1, batch 144\n(Train) Batch 144 Loss : 4.61647891998291, 맞은 개수 : 7\nEpoch : 1, batch 145\n(Train) Batch 145 Loss : 4.360016345977783, 맞은 개수 : 16\nEpoch : 1, batch 146\n(Train) Batch 146 Loss : 4.4153242111206055, 맞은 개수 : 10\nEpoch : 1, batch 147\n(Train) Batch 147 Loss : 4.536672115325928, 맞은 개수 : 9\nEpoch : 1, batch 148\n(Train) Batch 148 Loss : 4.487403392791748, 맞은 개수 : 9\nEpoch : 1, batch 149\n(Train) Batch 149 Loss : 4.429606914520264, 맞은 개수 : 9\nEpoch : 1, batch 150\n(Train) Batch 150 Loss : 4.453622341156006, 맞은 개수 : 10\nEpoch : 1, batch 151\n(Train) Batch 151 Loss : 4.50095796585083, 맞은 개수 : 11\nEpoch : 1, batch 152\n(Train) Batch 152 Loss : 4.5442609786987305, 맞은 개수 : 8\nEpoch : 1, batch 153\n(Train) Batch 153 Loss : 4.452347755432129, 맞은 개수 : 12\nEpoch : 1, batch 154\n(Train) Batch 154 Loss : 4.552631855010986, 맞은 개수 : 11\nEpoch : 1, batch 155\n(Train) Batch 155 Loss : 4.6115193367004395, 맞은 개수 : 8\nEpoch : 1, batch 156\n(Train) Batch 156 Loss : 4.271300315856934, 맞은 개수 : 16\nEpoch : 1, batch 157\n(Train) Batch 157 Loss : 4.594778537750244, 맞은 개수 : 10\nEpoch : 1, batch 158\n(Train) Batch 158 Loss : 4.389721870422363, 맞은 개수 : 10\nEpoch : 1, batch 159\n(Train) Batch 159 Loss : 4.470536708831787, 맞은 개수 : 9\nEpoch : 1, batch 160\n(Train) Batch 160 Loss : 4.5648980140686035, 맞은 개수 : 5\nEpoch : 1, batch 161\n(Train) Batch 161 Loss : 4.359689712524414, 맞은 개수 : 14\nEpoch : 1, batch 162\n(Train) Batch 162 Loss : 4.4127678871154785, 맞은 개수 : 10\nEpoch : 1, batch 163\n(Train) Batch 163 Loss : 4.2700629234313965, 맞은 개수 : 13\nEpoch : 1, batch 164\n(Train) Batch 164 Loss : 4.555283546447754, 맞은 개수 : 9\nEpoch : 1, batch 165\n(Train) Batch 165 Loss : 4.440977573394775, 맞은 개수 : 11\nEpoch : 1, batch 166\n(Train) Batch 166 Loss : 4.452549457550049, 맞은 개수 : 11\nEpoch : 1, batch 167\n(Train) Batch 167 Loss : 4.4473676681518555, 맞은 개수 : 9\nEpoch : 1, batch 168\n(Train) Batch 168 Loss : 4.5697407722473145, 맞은 개수 : 10\nEpoch : 1, batch 169\n(Train) Batch 169 Loss : 4.4414963722229, 맞은 개수 : 7\nEpoch : 1, batch 170\n(Train) Batch 170 Loss : 4.460623264312744, 맞은 개수 : 12\nEpoch : 1, batch 171\n(Train) Batch 171 Loss : 4.602546215057373, 맞은 개수 : 10\nEpoch : 1, batch 172\n(Train) Batch 172 Loss : 4.433898448944092, 맞은 개수 : 13\nEpoch : 1, batch 173\n(Train) Batch 173 Loss : 4.486652851104736, 맞은 개수 : 9\nEpoch : 1, batch 174\n(Train) Batch 174 Loss : 4.477972984313965, 맞은 개수 : 8\nEpoch : 1, batch 175\n(Train) Batch 175 Loss : 4.504587173461914, 맞은 개수 : 14\nEpoch : 1, batch 176\n(Train) Batch 176 Loss : 4.3736042976379395, 맞은 개수 : 12\nEpoch : 1, batch 177\n(Train) Batch 177 Loss : 4.553923606872559, 맞은 개수 : 11\nEpoch : 1, batch 178\n(Train) Batch 178 Loss : 4.576418399810791, 맞은 개수 : 7\nEpoch : 1, batch 179\n(Train) Batch 179 Loss : 4.29347038269043, 맞은 개수 : 17\nEpoch : 1, batch 180\n(Train) Batch 180 Loss : 4.547341823577881, 맞은 개수 : 13\nEpoch : 1, batch 181\n(Train) Batch 181 Loss : 4.559422492980957, 맞은 개수 : 10\nEpoch : 1, batch 182\n(Train) Batch 182 Loss : 4.525412559509277, 맞은 개수 : 10\nEpoch : 1, batch 183\n(Train) Batch 183 Loss : 4.517836570739746, 맞은 개수 : 11\nEpoch : 1, batch 184\n(Train) Batch 184 Loss : 4.385989665985107, 맞은 개수 : 10\nEpoch : 1, batch 185\n(Train) Batch 185 Loss : 4.368431091308594, 맞은 개수 : 12\nEpoch : 1, batch 186\n(Train) Batch 186 Loss : 4.546039581298828, 맞은 개수 : 10\nEpoch : 1, batch 187\n(Train) Batch 187 Loss : 4.450444221496582, 맞은 개수 : 6\nEpoch : 1, batch 188\n(Train) Batch 188 Loss : 4.443729877471924, 맞은 개수 : 5\nEpoch : 1, batch 189\n(Train) Batch 189 Loss : 4.655210971832275, 맞은 개수 : 7\nEpoch : 1, batch 190\n(Train) Batch 190 Loss : 4.487964153289795, 맞은 개수 : 7\nEpoch : 1, batch 191\n(Train) Batch 191 Loss : 4.483654499053955, 맞은 개수 : 8\nEpoch : 1, batch 192\n(Train) Batch 192 Loss : 4.553023338317871, 맞은 개수 : 13\nEpoch : 1, batch 193\n(Train) Batch 193 Loss : 4.603404521942139, 맞은 개수 : 11\nEpoch : 1, batch 194\n(Train) Batch 194 Loss : 4.509671688079834, 맞은 개수 : 12\nEpoch : 1, batch 195\n(Train) Batch 195 Loss : 4.483078479766846, 맞은 개수 : 10\nEpoch : 1, batch 196\n(Train) Batch 196 Loss : 4.4716796875, 맞은 개수 : 7\nEpoch : 1, batch 197\n(Train) Batch 197 Loss : 4.544637680053711, 맞은 개수 : 8\nEpoch : 1, batch 198\n(Train) Batch 198 Loss : 4.443210124969482, 맞은 개수 : 11\nEpoch : 1, batch 199\n(Train) Batch 199 Loss : 4.495236873626709, 맞은 개수 : 12\nEpoch : 1, batch 200\n(Train) Batch 200 Loss : 4.492921829223633, 맞은 개수 : 8\nEpoch : 1, batch 201\n(Train) Batch 201 Loss : 4.595148086547852, 맞은 개수 : 9\nEpoch : 1, batch 202\n(Train) Batch 202 Loss : 4.454076766967773, 맞은 개수 : 8\nEpoch : 1, batch 203\n(Train) Batch 203 Loss : 4.354446887969971, 맞은 개수 : 12\nEpoch : 1, batch 204\n(Train) Batch 204 Loss : 4.488839626312256, 맞은 개수 : 11\nEpoch : 1, batch 205\n(Train) Batch 205 Loss : 4.360799312591553, 맞은 개수 : 15\nEpoch : 1, batch 206\n(Train) Batch 206 Loss : 4.344234943389893, 맞은 개수 : 9\nEpoch : 1, batch 207\n(Train) Batch 207 Loss : 4.367046356201172, 맞은 개수 : 11\nEpoch : 1, batch 208\n(Train) Batch 208 Loss : 4.53591251373291, 맞은 개수 : 10\nEpoch : 1, batch 209\n(Train) Batch 209 Loss : 4.437144756317139, 맞은 개수 : 6\nEpoch : 1, batch 210\n(Train) Batch 210 Loss : 4.433517932891846, 맞은 개수 : 8\nEpoch : 1, batch 211\n(Train) Batch 211 Loss : 4.589476585388184, 맞은 개수 : 10\nEpoch : 1, batch 212\n(Train) Batch 212 Loss : 4.50022554397583, 맞은 개수 : 11\nEpoch : 1, batch 213\n(Train) Batch 213 Loss : 4.2757768630981445, 맞은 개수 : 10\nEpoch : 1, batch 214\n(Train) Batch 214 Loss : 4.625362396240234, 맞은 개수 : 8\nEpoch : 1, batch 215\n(Train) Batch 215 Loss : 4.532722473144531, 맞은 개수 : 7\nEpoch : 1, batch 216\n(Train) Batch 216 Loss : 4.35736083984375, 맞은 개수 : 9\nEpoch : 1, batch 217\n(Train) Batch 217 Loss : 4.398082733154297, 맞은 개수 : 10\nEpoch : 1, batch 218\n(Train) Batch 218 Loss : 4.414875030517578, 맞은 개수 : 13\nEpoch : 1, batch 219\n(Train) Batch 219 Loss : 4.30014705657959, 맞은 개수 : 14\nEpoch : 1, batch 220\n(Train) Batch 220 Loss : 4.434840202331543, 맞은 개수 : 5\nEpoch : 1, batch 221\n(Train) Batch 221 Loss : 4.356337547302246, 맞은 개수 : 7\nEpoch : 1, batch 222\n(Train) Batch 222 Loss : 4.306587219238281, 맞은 개수 : 10\nEpoch : 1, batch 223\n(Train) Batch 223 Loss : 4.5559916496276855, 맞은 개수 : 13\nEpoch : 1, batch 224\n(Train) Batch 224 Loss : 4.48995304107666, 맞은 개수 : 6\nEpoch : 1, batch 225\n(Train) Batch 225 Loss : 4.437015533447266, 맞은 개수 : 7\nEpoch : 1, batch 226\n(Train) Batch 226 Loss : 4.482988357543945, 맞은 개수 : 13\nEpoch : 1, batch 227\n(Train) Batch 227 Loss : 4.557885646820068, 맞은 개수 : 8\nEpoch : 1, batch 228\n(Train) Batch 228 Loss : 4.475159168243408, 맞은 개수 : 7\nEpoch : 1, batch 229\n(Train) Batch 229 Loss : 4.344196319580078, 맞은 개수 : 16\nEpoch : 1, batch 230\n(Train) Batch 230 Loss : 4.320650577545166, 맞은 개수 : 10\nEpoch : 1, batch 231\n(Train) Batch 231 Loss : 4.307265281677246, 맞은 개수 : 15\nEpoch : 1, batch 232\n(Train) Batch 232 Loss : 4.422572612762451, 맞은 개수 : 12\nEpoch : 1, batch 233\n(Train) Batch 233 Loss : 4.519811153411865, 맞은 개수 : 7\nEpoch : 1, batch 234\n(Train) Batch 234 Loss : 4.3570661544799805, 맞은 개수 : 12\nEpoch : 1, batch 235\n(Train) Batch 235 Loss : 4.397956371307373, 맞은 개수 : 10\nEpoch : 1, batch 236\n(Train) Batch 236 Loss : 4.3526740074157715, 맞은 개수 : 10\nEpoch : 1, batch 237\n(Train) Batch 237 Loss : 4.569425582885742, 맞은 개수 : 11\nEpoch : 1, batch 238\n(Train) Batch 238 Loss : 4.372066497802734, 맞은 개수 : 7\nEpoch : 1, batch 239\n(Train) Batch 239 Loss : 4.481457710266113, 맞은 개수 : 10\nEpoch : 1, batch 240\n(Train) Batch 240 Loss : 4.4007391929626465, 맞은 개수 : 6\nEpoch : 1, batch 241\n(Train) Batch 241 Loss : 4.317457675933838, 맞은 개수 : 12\nEpoch : 1, batch 242\n(Train) Batch 242 Loss : 4.600196361541748, 맞은 개수 : 5\nEpoch : 1, batch 243\n(Train) Batch 243 Loss : 4.111753940582275, 맞은 개수 : 16\nEpoch : 1, batch 244\n(Train) Batch 244 Loss : 4.408873081207275, 맞은 개수 : 8\nEpoch : 1, batch 245\n(Train) Batch 245 Loss : 4.497731685638428, 맞은 개수 : 8\nEpoch : 1, batch 246\n(Train) Batch 246 Loss : 4.42066764831543, 맞은 개수 : 13\nEpoch : 1, batch 247\n(Train) Batch 247 Loss : 4.477870941162109, 맞은 개수 : 15\nEpoch : 1, batch 248\n(Train) Batch 248 Loss : 4.4178876876831055, 맞은 개수 : 8\nEpoch : 1, batch 249\n(Train) Batch 249 Loss : 4.360299587249756, 맞은 개수 : 8\nEpoch : 1, batch 250\n(Train) Batch 250 Loss : 4.381613731384277, 맞은 개수 : 10\nEpoch : 1, batch 251\n(Train) Batch 251 Loss : 4.448657989501953, 맞은 개수 : 11\nEpoch : 1, batch 252\n(Train) Batch 252 Loss : 4.491077423095703, 맞은 개수 : 7\nEpoch : 1, batch 253\n(Train) Batch 253 Loss : 4.404743671417236, 맞은 개수 : 8\nEpoch : 1, batch 254\n(Train) Batch 254 Loss : 4.316653728485107, 맞은 개수 : 14\nEpoch : 1, batch 255\n(Train) Batch 255 Loss : 4.286440372467041, 맞은 개수 : 16\nEpoch : 1, batch 256\n(Train) Batch 256 Loss : 4.527126789093018, 맞은 개수 : 10\nEpoch : 1, batch 257\n(Train) Batch 257 Loss : 4.505580425262451, 맞은 개수 : 9\nEpoch : 1, batch 258\n(Train) Batch 258 Loss : 4.362119197845459, 맞은 개수 : 9\nEpoch : 1, batch 259\n(Train) Batch 259 Loss : 4.472227096557617, 맞은 개수 : 8\nEpoch : 1, batch 260\n(Train) Batch 260 Loss : 4.216444969177246, 맞은 개수 : 11\nEpoch : 1, batch 261\n(Train) Batch 261 Loss : 4.475613117218018, 맞은 개수 : 9\nEpoch : 1, batch 262\n(Train) Batch 262 Loss : 4.394379138946533, 맞은 개수 : 12\nEpoch : 1, batch 263\n(Train) Batch 263 Loss : 4.436953067779541, 맞은 개수 : 6\nEpoch : 1, batch 264\n(Train) Batch 264 Loss : 4.286686897277832, 맞은 개수 : 11\nEpoch : 1, batch 265\n(Train) Batch 265 Loss : 4.49247407913208, 맞은 개수 : 10\nEpoch : 1, batch 266\n(Train) Batch 266 Loss : 4.474823474884033, 맞은 개수 : 8\nEpoch : 1, batch 267\n(Train) Batch 267 Loss : 4.445215225219727, 맞은 개수 : 10\nEpoch : 1, batch 268\n(Train) Batch 268 Loss : 4.4303998947143555, 맞은 개수 : 11\nEpoch : 1, batch 269\n(Train) Batch 269 Loss : 4.461825847625732, 맞은 개수 : 8\nEpoch : 1, batch 270\n(Train) Batch 270 Loss : 4.434131622314453, 맞은 개수 : 6\nEpoch : 1, batch 271\n(Train) Batch 271 Loss : 4.308616638183594, 맞은 개수 : 14\nEpoch : 1, batch 272\n(Train) Batch 272 Loss : 4.490494251251221, 맞은 개수 : 10\nEpoch : 1, batch 273\n(Train) Batch 273 Loss : 4.224937438964844, 맞은 개수 : 15\nEpoch : 1, batch 274\n(Train) Batch 274 Loss : 4.406970500946045, 맞은 개수 : 14\nEpoch : 1, batch 275\n(Train) Batch 275 Loss : 4.475498199462891, 맞은 개수 : 7\nEpoch : 1, batch 276\n(Train) Batch 276 Loss : 4.490392684936523, 맞은 개수 : 9\nEpoch : 1, batch 277\n(Train) Batch 277 Loss : 4.513619899749756, 맞은 개수 : 8\nEpoch : 1, batch 278\n(Train) Batch 278 Loss : 4.222044944763184, 맞은 개수 : 17\nEpoch : 1, batch 279\n(Train) Batch 279 Loss : 4.411944389343262, 맞은 개수 : 9\nEpoch : 1, batch 280\n(Train) Batch 280 Loss : 4.466689109802246, 맞은 개수 : 9\nEpoch : 1, batch 281\n(Train) Batch 281 Loss : 4.3896098136901855, 맞은 개수 : 9\nEpoch : 1, batch 282\n(Train) Batch 282 Loss : 4.426457405090332, 맞은 개수 : 13\nEpoch : 1, batch 283\n(Train) Batch 283 Loss : 4.400296211242676, 맞은 개수 : 11\nEpoch : 1, batch 284\n(Train) Batch 284 Loss : 4.341197490692139, 맞은 개수 : 11\nEpoch : 1, batch 285\n(Train) Batch 285 Loss : 4.276020050048828, 맞은 개수 : 12\nEpoch : 1, batch 286\n(Train) Batch 286 Loss : 4.497958660125732, 맞은 개수 : 13\nEpoch : 1, batch 287\n(Train) Batch 287 Loss : 4.579643249511719, 맞은 개수 : 11\nEpoch : 1, batch 288\n(Train) Batch 288 Loss : 4.286261558532715, 맞은 개수 : 14\nEpoch : 1, batch 289\n(Train) Batch 289 Loss : 4.299078464508057, 맞은 개수 : 15\nEpoch : 1, batch 290\n(Train) Batch 290 Loss : 4.452427864074707, 맞은 개수 : 10\nEpoch : 1, batch 291\n(Train) Batch 291 Loss : 4.218059062957764, 맞은 개수 : 18\nEpoch : 1, batch 292\n(Train) Batch 292 Loss : 4.329196453094482, 맞은 개수 : 14\nEpoch : 1, batch 293\n(Train) Batch 293 Loss : 4.347217559814453, 맞은 개수 : 17\nEpoch : 1, batch 294\n(Train) Batch 294 Loss : 4.468608856201172, 맞은 개수 : 9\nEpoch : 1, batch 295\n(Train) Batch 295 Loss : 4.629452228546143, 맞은 개수 : 10\nEpoch : 1, batch 296\n(Train) Batch 296 Loss : 4.3213348388671875, 맞은 개수 : 15\nEpoch : 1, batch 297\n(Train) Batch 297 Loss : 4.547257423400879, 맞은 개수 : 5\nEpoch : 1, batch 298\n(Train) Batch 298 Loss : 4.3901777267456055, 맞은 개수 : 9\nEpoch : 1, batch 299\n(Train) Batch 299 Loss : 4.591169357299805, 맞은 개수 : 5\nEpoch : 1, batch 300\n(Train) Batch 300 Loss : 4.439395904541016, 맞은 개수 : 11\nEpoch : 1, batch 301\n(Train) Batch 301 Loss : 4.490406513214111, 맞은 개수 : 10\nEpoch : 1, batch 302\n(Train) Batch 302 Loss : 4.483536243438721, 맞은 개수 : 14\nEpoch : 1, batch 303\n(Train) Batch 303 Loss : 4.303455352783203, 맞은 개수 : 12\nEpoch : 1, batch 304\n(Train) Batch 304 Loss : 4.35291862487793, 맞은 개수 : 10\nEpoch : 1, batch 305\n(Train) Batch 305 Loss : 4.424587726593018, 맞은 개수 : 12\nEpoch : 1, batch 306\n(Train) Batch 306 Loss : 4.360738277435303, 맞은 개수 : 17\nEpoch : 1, batch 307\n(Train) Batch 307 Loss : 4.4871602058410645, 맞은 개수 : 10\nEpoch : 1, batch 308\n(Train) Batch 308 Loss : 4.338685035705566, 맞은 개수 : 12\nEpoch : 1, batch 309\n(Train) Batch 309 Loss : 4.3272528648376465, 맞은 개수 : 10\nEpoch : 1, batch 310\n(Train) Batch 310 Loss : 4.348614692687988, 맞은 개수 : 11\nEpoch : 1, batch 311\n(Train) Batch 311 Loss : 4.511653423309326, 맞은 개수 : 8\nEpoch : 1, batch 312\n(Train) Batch 312 Loss : 4.4137139320373535, 맞은 개수 : 11\nEpoch : 1, batch 313\n(Train) Batch 313 Loss : 4.584090709686279, 맞은 개수 : 7\nEpoch : 1, batch 314\n(Train) Batch 314 Loss : 4.419106483459473, 맞은 개수 : 12\nEpoch : 1, batch 315\n(Train) Batch 315 Loss : 4.505282402038574, 맞은 개수 : 7\nEpoch : 1, batch 316\n(Train) Batch 316 Loss : 4.217808723449707, 맞은 개수 : 15\nEpoch : 1, batch 317\n(Train) Batch 317 Loss : 4.500133991241455, 맞은 개수 : 13\nEpoch : 1, batch 318\n(Train) Batch 318 Loss : 4.479151725769043, 맞은 개수 : 11\nEpoch : 1, batch 319\n(Train) Batch 319 Loss : 4.411420822143555, 맞은 개수 : 7\nEpoch : 1, batch 320\n(Train) Batch 320 Loss : 4.312432765960693, 맞은 개수 : 7\nEpoch : 1, batch 321\n(Train) Batch 321 Loss : 4.384578704833984, 맞은 개수 : 6\nEpoch : 1, batch 322\n(Train) Batch 322 Loss : 4.482275009155273, 맞은 개수 : 8\nEpoch : 1, batch 323\n(Train) Batch 323 Loss : 4.507946491241455, 맞은 개수 : 9\nEpoch : 1, batch 324\n(Train) Batch 324 Loss : 4.537929058074951, 맞은 개수 : 7\nEpoch : 1, batch 325\n(Train) Batch 325 Loss : 4.272923469543457, 맞은 개수 : 10\nEpoch : 1, batch 326\n(Train) Batch 326 Loss : 4.277898788452148, 맞은 개수 : 11\nEpoch : 1, batch 327\n(Train) Batch 327 Loss : 4.275486469268799, 맞은 개수 : 13\nEpoch : 1, batch 328\n(Train) Batch 328 Loss : 4.264481067657471, 맞은 개수 : 14\nEpoch : 1, batch 329\n(Train) Batch 329 Loss : 4.214528560638428, 맞은 개수 : 14\nEpoch : 1, batch 330\n(Train) Batch 330 Loss : 4.440369606018066, 맞은 개수 : 9\nEpoch : 1, batch 331\n(Train) Batch 331 Loss : 4.254384994506836, 맞은 개수 : 18\nEpoch : 1, batch 332\n(Train) Batch 332 Loss : 4.422275543212891, 맞은 개수 : 6\nEpoch : 1, batch 333\n(Train) Batch 333 Loss : 4.499237537384033, 맞은 개수 : 7\nEpoch : 1, batch 334\n(Train) Batch 334 Loss : 4.453978538513184, 맞은 개수 : 10\nEpoch : 1, batch 335\n(Train) Batch 335 Loss : 4.517036437988281, 맞은 개수 : 7\nEpoch : 1, batch 336\n(Train) Batch 336 Loss : 4.413137912750244, 맞은 개수 : 10\nEpoch : 1, batch 337\n(Train) Batch 337 Loss : 4.387532711029053, 맞은 개수 : 14\nEpoch : 1, batch 338\n(Train) Batch 338 Loss : 4.373851299285889, 맞은 개수 : 16\nEpoch : 1, batch 339\n(Train) Batch 339 Loss : 4.332447528839111, 맞은 개수 : 10\nEpoch : 1, batch 340\n(Train) Batch 340 Loss : 4.329898834228516, 맞은 개수 : 11\nEpoch : 1, batch 341\n(Train) Batch 341 Loss : 4.422133445739746, 맞은 개수 : 9\nEpoch : 1, batch 342\n(Train) Batch 342 Loss : 4.219834804534912, 맞은 개수 : 15\nEpoch : 1, batch 343\n(Train) Batch 343 Loss : 4.424216270446777, 맞은 개수 : 10\nEpoch : 1, batch 344\n(Train) Batch 344 Loss : 4.280879020690918, 맞은 개수 : 8\nEpoch : 1, batch 345\n(Train) Batch 345 Loss : 4.14443302154541, 맞은 개수 : 15\nEpoch : 1, batch 346\n(Train) Batch 346 Loss : 4.175534248352051, 맞은 개수 : 12\nEpoch : 1, batch 347\n(Train) Batch 347 Loss : 4.346441745758057, 맞은 개수 : 14\nEpoch : 1, batch 348\n(Train) Batch 348 Loss : 4.600396633148193, 맞은 개수 : 7\nEpoch : 1, batch 349\n(Train) Batch 349 Loss : 4.419697284698486, 맞은 개수 : 15\nEpoch : 1, batch 350\n(Train) Batch 350 Loss : 4.570143699645996, 맞은 개수 : 7\nEpoch : 1, batch 351\n(Train) Batch 351 Loss : 4.1860127449035645, 맞은 개수 : 13\nEpoch : 1, batch 352\n(Train) Batch 352 Loss : 4.161458969116211, 맞은 개수 : 14\nEpoch : 1, batch 353\n(Train) Batch 353 Loss : 4.4305901527404785, 맞은 개수 : 16\nEpoch : 1, batch 354\n(Train) Batch 354 Loss : 4.331954479217529, 맞은 개수 : 12\nEpoch : 1, batch 355\n(Train) Batch 355 Loss : 4.291622638702393, 맞은 개수 : 17\nEpoch : 1, batch 356\n(Train) Batch 356 Loss : 4.38142728805542, 맞은 개수 : 15\nEpoch : 1, batch 357\n(Train) Batch 357 Loss : 4.5372395515441895, 맞은 개수 : 6\nEpoch : 1, batch 358\n(Train) Batch 358 Loss : 4.420434951782227, 맞은 개수 : 7\nEpoch : 1, batch 359\n(Train) Batch 359 Loss : 4.308973789215088, 맞은 개수 : 13\nEpoch : 1, batch 360\n(Train) Batch 360 Loss : 4.346456527709961, 맞은 개수 : 10\nEpoch : 1, batch 361\n(Train) Batch 361 Loss : 4.121305465698242, 맞은 개수 : 13\nEpoch : 1, batch 362\n(Train) Batch 362 Loss : 4.278088092803955, 맞은 개수 : 13\nEpoch : 1, batch 363\n(Train) Batch 363 Loss : 4.369118690490723, 맞은 개수 : 10\nEpoch : 1, batch 364\n(Train) Batch 364 Loss : 4.492061138153076, 맞은 개수 : 9\nEpoch : 1, batch 365\n(Train) Batch 365 Loss : 4.177452564239502, 맞은 개수 : 16\nEpoch : 1, batch 366\n(Train) Batch 366 Loss : 4.462489604949951, 맞은 개수 : 10\nEpoch : 1, batch 367\n(Train) Batch 367 Loss : 4.354968070983887, 맞은 개수 : 13\nEpoch : 1, batch 368\n(Train) Batch 368 Loss : 4.284823894500732, 맞은 개수 : 12\nEpoch : 1, batch 369\n(Train) Batch 369 Loss : 4.4348835945129395, 맞은 개수 : 11\nEpoch : 1, batch 370\n(Train) Batch 370 Loss : 4.329544544219971, 맞은 개수 : 14\nEpoch : 1, batch 371\n(Train) Batch 371 Loss : 4.498174667358398, 맞은 개수 : 11\nEpoch : 1, batch 372\n(Train) Batch 372 Loss : 4.488705158233643, 맞은 개수 : 11\nEpoch : 1, batch 373\n(Train) Batch 373 Loss : 4.407313346862793, 맞은 개수 : 13\nEpoch : 1, batch 374\n(Train) Batch 374 Loss : 4.37343168258667, 맞은 개수 : 15\nEpoch : 1, batch 375\n(Train) Batch 375 Loss : 4.502649784088135, 맞은 개수 : 11\nEpoch : 1, batch 376\n(Train) Batch 376 Loss : 4.3811187744140625, 맞은 개수 : 8\nEpoch : 1, batch 377\n(Train) Batch 377 Loss : 3.974210739135742, 맞은 개수 : 19\nEpoch : 1, batch 378\n(Train) Batch 378 Loss : 4.409178733825684, 맞은 개수 : 9\nEpoch : 1, batch 379\n(Train) Batch 379 Loss : 4.297698497772217, 맞은 개수 : 14\nEpoch : 1, batch 380\n(Train) Batch 380 Loss : 4.4406633377075195, 맞은 개수 : 12\nEpoch : 1, batch 381\n(Train) Batch 381 Loss : 4.3558454513549805, 맞은 개수 : 12\nEpoch : 1, batch 382\n(Train) Batch 382 Loss : 4.25731086730957, 맞은 개수 : 18\nEpoch : 1, batch 383\n(Train) Batch 383 Loss : 4.416075706481934, 맞은 개수 : 7\nEpoch : 1, batch 384\n(Train) Batch 384 Loss : 4.395735740661621, 맞은 개수 : 9\nEpoch : 1, batch 385\n(Train) Batch 385 Loss : 4.332998275756836, 맞은 개수 : 12\nEpoch : 1, batch 386\n(Train) Batch 386 Loss : 4.328517436981201, 맞은 개수 : 12\nEpoch : 1, batch 387\n(Train) Batch 387 Loss : 4.316882610321045, 맞은 개수 : 6\nEpoch : 1, batch 388\n(Train) Batch 388 Loss : 4.4135847091674805, 맞은 개수 : 9\nEpoch : 1, batch 389\n(Train) Batch 389 Loss : 4.317301273345947, 맞은 개수 : 14\nEpoch : 1, batch 390\n(Train) Batch 390 Loss : 4.179178714752197, 맞은 개수 : 19\nEpoch : 1, batch 391\n(Train) Batch 391 Loss : 4.341955184936523, 맞은 개수 : 12\nEpoch : 1, batch 392\n(Train) Batch 392 Loss : 4.479216575622559, 맞은 개수 : 13\nEpoch : 1, batch 393\n(Train) Batch 393 Loss : 4.239006996154785, 맞은 개수 : 20\nEpoch : 1, batch 394\n(Train) Batch 394 Loss : 4.655948162078857, 맞은 개수 : 8\nEpoch : 1, batch 395\n(Train) Batch 395 Loss : 4.303894519805908, 맞은 개수 : 7\nEpoch : 1, batch 396\n(Train) Batch 396 Loss : 4.186761856079102, 맞은 개수 : 16\nEpoch : 1, batch 397\n(Train) Batch 397 Loss : 4.091723442077637, 맞은 개수 : 19\nEpoch : 1, batch 398\n(Train) Batch 398 Loss : 4.273441314697266, 맞은 개수 : 9\nEpoch : 1, batch 399\n(Train) Batch 399 Loss : 4.233892917633057, 맞은 개수 : 18\nEpoch : 1, batch 400\n(Train) Batch 400 Loss : 4.226498126983643, 맞은 개수 : 14\nEpoch : 1, batch 401\n(Train) Batch 401 Loss : 4.401076793670654, 맞은 개수 : 16\nEpoch : 1, batch 402\n(Train) Batch 402 Loss : 4.298981189727783, 맞은 개수 : 11\nEpoch : 1, batch 403\n(Train) Batch 403 Loss : 4.251899719238281, 맞은 개수 : 7\nEpoch : 1, batch 404\n(Train) Batch 404 Loss : 4.411290168762207, 맞은 개수 : 12\nEpoch : 1, batch 405\n(Train) Batch 405 Loss : 4.530987739562988, 맞은 개수 : 11\nEpoch : 1, batch 406\n(Train) Batch 406 Loss : 4.37045431137085, 맞은 개수 : 12\nEpoch : 1, batch 407\n(Train) Batch 407 Loss : 4.545722007751465, 맞은 개수 : 10\nEpoch : 1, batch 408\n(Train) Batch 408 Loss : 4.500020503997803, 맞은 개수 : 11\nEpoch : 1, batch 409\n(Train) Batch 409 Loss : 4.55048942565918, 맞은 개수 : 11\nEpoch : 1, batch 410\n(Train) Batch 410 Loss : 4.270393371582031, 맞은 개수 : 14\nEpoch : 1, batch 411\n(Train) Batch 411 Loss : 4.470760345458984, 맞은 개수 : 14\nEpoch : 1, batch 412\n(Train) Batch 412 Loss : 4.157742977142334, 맞은 개수 : 16\nEpoch : 1, batch 413\n(Train) Batch 413 Loss : 4.324591159820557, 맞은 개수 : 16\nEpoch : 1, batch 414\n(Train) Batch 414 Loss : 4.197793483734131, 맞은 개수 : 12\nEpoch : 1, batch 415\n(Train) Batch 415 Loss : 4.543370723724365, 맞은 개수 : 9\nEpoch : 1, batch 416\n(Train) Batch 416 Loss : 4.469200134277344, 맞은 개수 : 7\nEpoch : 1, batch 417\n(Train) Batch 417 Loss : 4.285710334777832, 맞은 개수 : 11\nEpoch : 1, batch 418\n(Train) Batch 418 Loss : 4.503061771392822, 맞은 개수 : 13\nEpoch : 1, batch 419\n(Train) Batch 419 Loss : 4.472436428070068, 맞은 개수 : 9\nEpoch : 1, batch 420\n(Train) Batch 420 Loss : 4.3951873779296875, 맞은 개수 : 11\nEpoch : 1, batch 421\n(Train) Batch 421 Loss : 4.346640110015869, 맞은 개수 : 10\nEpoch : 1, batch 422\n(Train) Batch 422 Loss : 4.331190586090088, 맞은 개수 : 14\nEpoch : 1, batch 423\n(Train) Batch 423 Loss : 4.418247699737549, 맞은 개수 : 11\nEpoch : 1, batch 424\n(Train) Batch 424 Loss : 4.2779316902160645, 맞은 개수 : 22\nEpoch : 1, batch 425\n(Train) Batch 425 Loss : 4.2427449226379395, 맞은 개수 : 15\nEpoch : 1, batch 426\n(Train) Batch 426 Loss : 4.531705856323242, 맞은 개수 : 9\nEpoch : 1, batch 427\n(Train) Batch 427 Loss : 4.353066444396973, 맞은 개수 : 16\nEpoch : 1, batch 428\n(Train) Batch 428 Loss : 4.470091342926025, 맞은 개수 : 12\nEpoch : 1, batch 429\n(Train) Batch 429 Loss : 4.309944152832031, 맞은 개수 : 16\nEpoch : 1, batch 430\n(Train) Batch 430 Loss : 4.3455047607421875, 맞은 개수 : 10\nEpoch : 1, batch 431\n(Train) Batch 431 Loss : 4.379581928253174, 맞은 개수 : 11\nEpoch : 1, batch 432\n(Train) Batch 432 Loss : 4.429100036621094, 맞은 개수 : 13\nEpoch : 1, batch 433\n(Train) Batch 433 Loss : 4.498064041137695, 맞은 개수 : 7\nEpoch : 1, batch 434\n(Train) Batch 434 Loss : 4.144539833068848, 맞은 개수 : 15\nEpoch : 1, batch 435\n(Train) Batch 435 Loss : 4.261295795440674, 맞은 개수 : 13\nEpoch : 1, batch 436\n(Train) Batch 436 Loss : 4.335395336151123, 맞은 개수 : 13\nEpoch : 1, batch 437\n(Train) Batch 437 Loss : 4.145646095275879, 맞은 개수 : 16\nEpoch : 1, batch 438\n(Train) Batch 438 Loss : 4.199160099029541, 맞은 개수 : 16\nEpoch : 1, batch 439\n(Train) Batch 439 Loss : 4.295467376708984, 맞은 개수 : 14\nEpoch : 1, batch 440\n(Train) Batch 440 Loss : 4.130193710327148, 맞은 개수 : 13\nEpoch : 1, batch 441\n(Train) Batch 441 Loss : 4.184993267059326, 맞은 개수 : 17\nEpoch : 1, batch 442\n(Train) Batch 442 Loss : 4.388779640197754, 맞은 개수 : 5\nEpoch : 1, batch 443\n(Train) Batch 443 Loss : 4.277528762817383, 맞은 개수 : 12\nEpoch : 1, batch 444\n(Train) Batch 444 Loss : 4.376688480377197, 맞은 개수 : 13\nEpoch : 1, batch 445\n(Train) Batch 445 Loss : 4.2688679695129395, 맞은 개수 : 13\nEpoch : 1, batch 446\n(Train) Batch 446 Loss : 4.2589497566223145, 맞은 개수 : 14\nEpoch : 1, batch 447\n(Train) Batch 447 Loss : 4.17917537689209, 맞은 개수 : 16\nEpoch : 1, batch 448\n(Train) Batch 448 Loss : 4.2423176765441895, 맞은 개수 : 14\nEpoch : 1, batch 449\n(Train) Batch 449 Loss : 4.345514297485352, 맞은 개수 : 16\nEpoch : 1, batch 450\n(Train) Batch 450 Loss : 4.125268459320068, 맞은 개수 : 16\nEpoch : 1, batch 451\n(Train) Batch 451 Loss : 4.334469318389893, 맞은 개수 : 11\nEpoch : 1, batch 452\n(Train) Batch 452 Loss : 4.203882217407227, 맞은 개수 : 11\nEpoch : 1, batch 453\n(Train) Batch 453 Loss : 4.113002777099609, 맞은 개수 : 15\nEpoch : 1, batch 454\n(Train) Batch 454 Loss : 4.330834865570068, 맞은 개수 : 12\nEpoch : 1, batch 455\n(Train) Batch 455 Loss : 4.417601108551025, 맞은 개수 : 10\nEpoch : 1, batch 456\n(Train) Batch 456 Loss : 4.175105094909668, 맞은 개수 : 12\nEpoch : 1, batch 457\n(Train) Batch 457 Loss : 4.235340595245361, 맞은 개수 : 14\nEpoch : 1, batch 458\n(Train) Batch 458 Loss : 4.434450626373291, 맞은 개수 : 8\nEpoch : 1, batch 459\n(Train) Batch 459 Loss : 4.369351387023926, 맞은 개수 : 8\nEpoch : 1, batch 460\n(Train) Batch 460 Loss : 4.226723670959473, 맞은 개수 : 13\nEpoch : 1, batch 461\n(Train) Batch 461 Loss : 4.231136322021484, 맞은 개수 : 10\nEpoch : 1, batch 462\n(Train) Batch 462 Loss : 4.195860385894775, 맞은 개수 : 14\nEpoch : 1, batch 463\n(Train) Batch 463 Loss : 4.2606916427612305, 맞은 개수 : 9\nEpoch : 1, batch 464\n(Train) Batch 464 Loss : 4.407851696014404, 맞은 개수 : 13\nEpoch : 1, batch 465\n(Train) Batch 465 Loss : 4.229635715484619, 맞은 개수 : 13\nEpoch : 1, batch 466\n(Train) Batch 466 Loss : 4.2435736656188965, 맞은 개수 : 10\nEpoch : 1, batch 467\n(Train) Batch 467 Loss : 4.213545799255371, 맞은 개수 : 17\nEpoch : 1, batch 468\n(Train) Batch 468 Loss : 4.139044284820557, 맞은 개수 : 19\nEpoch : 1, batch 469\n(Train) Batch 469 Loss : 3.963200092315674, 맞은 개수 : 23\nEpoch : 1, batch 470\n(Train) Batch 470 Loss : 4.369277000427246, 맞은 개수 : 8\nEpoch : 1, batch 471\n(Train) Batch 471 Loss : 4.322751998901367, 맞은 개수 : 11\nEpoch : 1, batch 472\n(Train) Batch 472 Loss : 4.4992899894714355, 맞은 개수 : 6\nEpoch : 1, batch 473\n(Train) Batch 473 Loss : 4.400405406951904, 맞은 개수 : 12\nEpoch : 1, batch 474\n(Train) Batch 474 Loss : 4.337303638458252, 맞은 개수 : 16\nEpoch : 1, batch 475\n(Train) Batch 475 Loss : 4.443632125854492, 맞은 개수 : 11\nEpoch : 1, batch 476\n(Train) Batch 476 Loss : 4.348686695098877, 맞은 개수 : 9\nEpoch : 1, batch 477\n(Train) Batch 477 Loss : 4.332098960876465, 맞은 개수 : 12\nEpoch : 1, batch 478\n(Train) Batch 478 Loss : 4.352717399597168, 맞은 개수 : 14\nEpoch : 1, batch 479\n(Train) Batch 479 Loss : 4.378131866455078, 맞은 개수 : 15\nEpoch : 1, batch 480\n(Train) Batch 480 Loss : 4.518581867218018, 맞은 개수 : 7\nEpoch : 1, batch 481\n(Train) Batch 481 Loss : 4.136387348175049, 맞은 개수 : 16\nEpoch : 1, batch 482\n(Train) Batch 482 Loss : 4.202359199523926, 맞은 개수 : 17\nEpoch : 1, batch 483\n(Train) Batch 483 Loss : 4.364089012145996, 맞은 개수 : 12\nEpoch : 1, batch 484\n(Train) Batch 484 Loss : 4.376181602478027, 맞은 개수 : 7\nEpoch : 1, batch 485\n(Train) Batch 485 Loss : 4.395984649658203, 맞은 개수 : 11\nEpoch : 1, batch 486\n(Train) Batch 486 Loss : 4.182525157928467, 맞은 개수 : 18\nEpoch : 1, batch 487\n(Train) Batch 487 Loss : 4.340043544769287, 맞은 개수 : 10\nEpoch : 1, batch 488\n(Train) Batch 488 Loss : 4.40802001953125, 맞은 개수 : 10\nEpoch : 1, batch 489\n(Train) Batch 489 Loss : 4.223537921905518, 맞은 개수 : 11\nEpoch : 1, batch 490\n(Train) Batch 490 Loss : 4.168557167053223, 맞은 개수 : 13\nEpoch : 1, batch 491\n(Train) Batch 491 Loss : 4.250314235687256, 맞은 개수 : 15\nEpoch : 1, batch 492\n(Train) Batch 492 Loss : 4.323762893676758, 맞은 개수 : 18\nEpoch : 1, batch 493\n(Train) Batch 493 Loss : 4.346417427062988, 맞은 개수 : 11\nEpoch : 1, batch 494\n(Train) Batch 494 Loss : 4.343480110168457, 맞은 개수 : 12\nEpoch : 1, batch 495\n(Train) Batch 495 Loss : 4.2693891525268555, 맞은 개수 : 16\nEpoch : 1, batch 496\n(Train) Batch 496 Loss : 4.302890300750732, 맞은 개수 : 16\nEpoch : 1, batch 497\n(Train) Batch 497 Loss : 4.3986406326293945, 맞은 개수 : 10\nEpoch : 1, batch 498\n(Train) Batch 498 Loss : 4.35249137878418, 맞은 개수 : 14\nEpoch : 1, batch 499\n(Train) Batch 499 Loss : 4.1780571937561035, 맞은 개수 : 12\nEpoch : 1, batch 500\n(Train) Batch 500 Loss : 4.386559009552002, 맞은 개수 : 12\nEpoch : 1, batch 501\n(Train) Batch 501 Loss : 4.23284912109375, 맞은 개수 : 11\nEpoch : 1, batch 502\n(Train) Batch 502 Loss : 4.17263650894165, 맞은 개수 : 20\nEpoch : 1, batch 503\n(Train) Batch 503 Loss : 4.150580883026123, 맞은 개수 : 10\nEpoch : 1, batch 504\n(Train) Batch 504 Loss : 4.038327693939209, 맞은 개수 : 16\nEpoch : 1, batch 505\n(Train) Batch 505 Loss : 4.310440540313721, 맞은 개수 : 16\nEpoch : 1, batch 506\n(Train) Batch 506 Loss : 4.128421783447266, 맞은 개수 : 17\nEpoch : 1, batch 507\n(Train) Batch 507 Loss : 4.373746395111084, 맞은 개수 : 16\nEpoch : 1, batch 508\n(Train) Batch 508 Loss : 4.245584011077881, 맞은 개수 : 21\nEpoch : 1, batch 509\n(Train) Batch 509 Loss : 4.20253324508667, 맞은 개수 : 11\nEpoch : 1, batch 510\n(Train) Batch 510 Loss : 4.308930397033691, 맞은 개수 : 11\nEpoch : 1, batch 511\n(Train) Batch 511 Loss : 4.304600238800049, 맞은 개수 : 14\nEpoch : 1, batch 512\n(Train) Batch 512 Loss : 4.150285243988037, 맞은 개수 : 13\nEpoch : 1, batch 513\n(Train) Batch 513 Loss : 4.3509321212768555, 맞은 개수 : 13\nEpoch : 1, batch 514\n(Train) Batch 514 Loss : 4.416178226470947, 맞은 개수 : 11\nEpoch : 1, batch 515\n(Train) Batch 515 Loss : 4.434813499450684, 맞은 개수 : 12\nEpoch : 1, batch 516\n(Train) Batch 516 Loss : 4.3006911277771, 맞은 개수 : 17\nEpoch : 1, batch 517\n(Train) Batch 517 Loss : 4.215596675872803, 맞은 개수 : 17\nEpoch : 1, batch 518\n(Train) Batch 518 Loss : 4.169618606567383, 맞은 개수 : 12\nEpoch : 1, batch 519\n(Train) Batch 519 Loss : 4.106656551361084, 맞은 개수 : 16\nEpoch : 1, batch 520\n(Train) Batch 520 Loss : 4.193689346313477, 맞은 개수 : 15\nEpoch : 1, batch 521\n(Train) Batch 521 Loss : 4.464126110076904, 맞은 개수 : 11\nEpoch : 1, batch 522\n(Train) Batch 522 Loss : 4.456634998321533, 맞은 개수 : 11\nEpoch : 1, batch 523\n(Train) Batch 523 Loss : 4.191620349884033, 맞은 개수 : 13\nEpoch : 1, batch 524\n(Train) Batch 524 Loss : 4.4687418937683105, 맞은 개수 : 13\nEpoch : 1, batch 525\n(Train) Batch 525 Loss : 4.244663238525391, 맞은 개수 : 15\nEpoch : 1, batch 526\n(Train) Batch 526 Loss : 4.26794958114624, 맞은 개수 : 15\nEpoch : 1, batch 527\n(Train) Batch 527 Loss : 4.130127429962158, 맞은 개수 : 15\nEpoch : 1, batch 528\n(Train) Batch 528 Loss : 4.282279014587402, 맞은 개수 : 11\nEpoch : 1, batch 529\n(Train) Batch 529 Loss : 4.638846397399902, 맞은 개수 : 8\nEpoch : 1, batch 530\n(Train) Batch 530 Loss : 4.215028762817383, 맞은 개수 : 15\nEpoch : 1, batch 531\n(Train) Batch 531 Loss : 4.24418306350708, 맞은 개수 : 16\nEpoch : 1, batch 532\n(Train) Batch 532 Loss : 4.045159339904785, 맞은 개수 : 15\nEpoch : 1, batch 533\n(Train) Batch 533 Loss : 4.306890487670898, 맞은 개수 : 9\nEpoch : 1, batch 534\n(Train) Batch 534 Loss : 4.157375335693359, 맞은 개수 : 13\nEpoch : 1, batch 535\n(Train) Batch 535 Loss : 4.0153703689575195, 맞은 개수 : 16\nEpoch : 1, batch 536\n(Train) Batch 536 Loss : 4.347683429718018, 맞은 개수 : 13\nEpoch : 1, batch 537\n(Train) Batch 537 Loss : 4.118958473205566, 맞은 개수 : 15\nEpoch : 1, batch 538\n(Train) Batch 538 Loss : 4.335235595703125, 맞은 개수 : 8\nEpoch : 1, batch 539\n(Train) Batch 539 Loss : 4.375036239624023, 맞은 개수 : 15\nEpoch : 1, batch 540\n(Train) Batch 540 Loss : 4.167716979980469, 맞은 개수 : 16\nEpoch : 1, batch 541\n(Train) Batch 541 Loss : 4.201095104217529, 맞은 개수 : 11\nEpoch : 1, batch 542\n(Train) Batch 542 Loss : 4.267427444458008, 맞은 개수 : 17\nEpoch : 1, batch 543\n(Train) Batch 543 Loss : 4.199605941772461, 맞은 개수 : 18\nEpoch : 1, batch 544\n(Train) Batch 544 Loss : 4.327365398406982, 맞은 개수 : 13\nEpoch : 1, batch 545\n(Train) Batch 545 Loss : 4.393747806549072, 맞은 개수 : 12\nEpoch : 1, batch 546\n(Train) Batch 546 Loss : 4.235937595367432, 맞은 개수 : 16\nEpoch : 1, batch 547\n(Train) Batch 547 Loss : 4.211054801940918, 맞은 개수 : 18\nEpoch : 1, batch 548\n(Train) Batch 548 Loss : 4.308022499084473, 맞은 개수 : 11\nEpoch : 1, batch 549\n(Train) Batch 549 Loss : 4.198078632354736, 맞은 개수 : 14\nEpoch : 1, batch 550\n(Train) Batch 550 Loss : 4.204214096069336, 맞은 개수 : 14\nEpoch : 1, batch 551\n(Train) Batch 551 Loss : 4.177989482879639, 맞은 개수 : 17\nEpoch : 1, batch 552\n(Train) Batch 552 Loss : 4.435117721557617, 맞은 개수 : 14\nEpoch : 1, batch 553\n(Train) Batch 553 Loss : 4.232096195220947, 맞은 개수 : 16\nEpoch : 1, batch 554\n(Train) Batch 554 Loss : 4.534824848175049, 맞은 개수 : 9\nEpoch : 1, batch 555\n(Train) Batch 555 Loss : 4.365970134735107, 맞은 개수 : 14\nEpoch : 1, batch 556\n(Train) Batch 556 Loss : 4.165049076080322, 맞은 개수 : 16\nEpoch : 1, batch 557\n(Train) Batch 557 Loss : 4.0886406898498535, 맞은 개수 : 16\nEpoch : 1, batch 558\n(Train) Batch 558 Loss : 4.199267387390137, 맞은 개수 : 17\nEpoch : 1, batch 559\n(Train) Batch 559 Loss : 4.446732044219971, 맞은 개수 : 11\nEpoch : 1, batch 560\n(Train) Batch 560 Loss : 4.261753082275391, 맞은 개수 : 14\nEpoch : 1, batch 561\n(Train) Batch 561 Loss : 4.346256732940674, 맞은 개수 : 13\nEpoch : 1, batch 562\n(Train) Batch 562 Loss : 4.375726222991943, 맞은 개수 : 20\nEpoch : 1, batch 563\n(Train) Batch 563 Loss : 4.548745632171631, 맞은 개수 : 8\nEpoch : 1, batch 564\n(Train) Batch 564 Loss : 4.205343723297119, 맞은 개수 : 18\nEpoch : 1, batch 565\n(Train) Batch 565 Loss : 3.996840715408325, 맞은 개수 : 16\nEpoch : 1, batch 566\n(Train) Batch 566 Loss : 4.1568284034729, 맞은 개수 : 17\nEpoch : 1, batch 567\n(Train) Batch 567 Loss : 4.211618900299072, 맞은 개수 : 11\nEpoch : 1, batch 568\n(Train) Batch 568 Loss : 4.086433410644531, 맞은 개수 : 23\nEpoch : 1, batch 569\n(Train) Batch 569 Loss : 4.418975830078125, 맞은 개수 : 8\nEpoch : 1, batch 570\n(Train) Batch 570 Loss : 4.257361888885498, 맞은 개수 : 11\nEpoch : 1, batch 571\n(Train) Batch 571 Loss : 4.279933452606201, 맞은 개수 : 14\nEpoch : 1, batch 572\n(Train) Batch 572 Loss : 4.1950154304504395, 맞은 개수 : 14\nEpoch : 1, batch 573\n(Train) Batch 573 Loss : 4.22550630569458, 맞은 개수 : 14\nEpoch : 1, batch 574\n(Train) Batch 574 Loss : 4.225069522857666, 맞은 개수 : 12\nEpoch : 1, batch 575\n(Train) Batch 575 Loss : 4.223036766052246, 맞은 개수 : 11\nEpoch : 1, batch 576\n(Train) Batch 576 Loss : 4.120362758636475, 맞은 개수 : 18\nEpoch : 1, batch 577\n(Train) Batch 577 Loss : 4.117526531219482, 맞은 개수 : 18\nEpoch : 1, batch 578\n(Train) Batch 578 Loss : 4.554066181182861, 맞은 개수 : 11\nEpoch : 1, batch 579\n(Train) Batch 579 Loss : 4.296501636505127, 맞은 개수 : 14\nEpoch : 1, batch 580\n(Train) Batch 580 Loss : 4.306344509124756, 맞은 개수 : 14\nEpoch : 1, batch 581\n(Train) Batch 581 Loss : 4.367803573608398, 맞은 개수 : 17\nEpoch : 1, batch 582\n(Train) Batch 582 Loss : 4.336038589477539, 맞은 개수 : 11\nEpoch : 1, batch 583\n(Train) Batch 583 Loss : 4.185019493103027, 맞은 개수 : 15\nEpoch : 1, batch 584\n(Train) Batch 584 Loss : 4.191349983215332, 맞은 개수 : 16\nEpoch : 1, batch 585\n(Train) Batch 585 Loss : 4.138333320617676, 맞은 개수 : 15\nEpoch : 1, batch 586\n(Train) Batch 586 Loss : 4.333066463470459, 맞은 개수 : 11\nEpoch : 1, batch 587\n(Train) Batch 587 Loss : 4.239064693450928, 맞은 개수 : 9\nEpoch : 1, batch 588\n(Train) Batch 588 Loss : 4.251058578491211, 맞은 개수 : 19\nEpoch : 1, batch 589\n(Train) Batch 589 Loss : 4.304985523223877, 맞은 개수 : 14\nEpoch : 1, batch 590\n(Train) Batch 590 Loss : 4.2182745933532715, 맞은 개수 : 12\nEpoch : 1, batch 591\n(Train) Batch 591 Loss : 4.195858001708984, 맞은 개수 : 16\nEpoch : 1, batch 592\n(Train) Batch 592 Loss : 4.0242156982421875, 맞은 개수 : 15\nEpoch : 1, batch 593\n(Train) Batch 593 Loss : 4.221553325653076, 맞은 개수 : 20\nEpoch : 1, batch 594\n(Train) Batch 594 Loss : 4.168332576751709, 맞은 개수 : 13\nEpoch : 1, batch 595\n(Train) Batch 595 Loss : 4.265994071960449, 맞은 개수 : 9\nEpoch : 1, batch 596\n(Train) Batch 596 Loss : 4.348770618438721, 맞은 개수 : 15\nEpoch : 1, batch 597\n(Train) Batch 597 Loss : 4.075443744659424, 맞은 개수 : 17\nEpoch : 1, batch 598\n(Train) Batch 598 Loss : 4.2008867263793945, 맞은 개수 : 11\nEpoch : 1, batch 599\n(Train) Batch 599 Loss : 4.306647777557373, 맞은 개수 : 12\nEpoch : 1, batch 600\n(Train) Batch 600 Loss : 4.4388580322265625, 맞은 개수 : 11\nEpoch : 1, batch 601\n(Train) Batch 601 Loss : 4.304982662200928, 맞은 개수 : 14\nEpoch : 1, batch 602\n(Train) Batch 602 Loss : 4.393968105316162, 맞은 개수 : 8\nEpoch : 1, batch 603\n(Train) Batch 603 Loss : 4.44252872467041, 맞은 개수 : 4\nEpoch : 1, batch 604\n(Train) Batch 604 Loss : 4.3386688232421875, 맞은 개수 : 10\nEpoch : 1, batch 605\n(Train) Batch 605 Loss : 4.07801628112793, 맞은 개수 : 14\nEpoch : 1, batch 606\n(Train) Batch 606 Loss : 4.291242599487305, 맞은 개수 : 13\nEpoch : 1, batch 607\n(Train) Batch 607 Loss : 4.320718288421631, 맞은 개수 : 14\nEpoch : 1, batch 608\n(Train) Batch 608 Loss : 4.240462303161621, 맞은 개수 : 17\nEpoch : 1, batch 609\n(Train) Batch 609 Loss : 4.018742561340332, 맞은 개수 : 14\nEpoch : 1, batch 610\n(Train) Batch 610 Loss : 4.438311576843262, 맞은 개수 : 7\nEpoch : 1, batch 611\n(Train) Batch 611 Loss : 4.263964653015137, 맞은 개수 : 9\nEpoch : 1, batch 612\n(Train) Batch 612 Loss : 4.317187786102295, 맞은 개수 : 12\nEpoch : 1, batch 613\n(Train) Batch 613 Loss : 4.347858428955078, 맞은 개수 : 8\nEpoch : 1, batch 614\n(Train) Batch 614 Loss : 4.174140453338623, 맞은 개수 : 13\nEpoch : 1, batch 615\n(Train) Batch 615 Loss : 4.266598224639893, 맞은 개수 : 12\nEpoch : 1, batch 616\n(Train) Batch 616 Loss : 4.120969295501709, 맞은 개수 : 18\nEpoch : 1, batch 617\n(Train) Batch 617 Loss : 4.25715446472168, 맞은 개수 : 14\nEpoch : 1, batch 618\n(Train) Batch 618 Loss : 4.175185680389404, 맞은 개수 : 14\nEpoch : 1, batch 619\n(Train) Batch 619 Loss : 4.216800212860107, 맞은 개수 : 14\nEpoch : 1, batch 620\n(Train) Batch 620 Loss : 3.9918088912963867, 맞은 개수 : 21\nEpoch : 1, batch 621\n(Train) Batch 621 Loss : 4.219696044921875, 맞은 개수 : 8\nEpoch : 1, batch 622\n(Train) Batch 622 Loss : 3.9974377155303955, 맞은 개수 : 17\nEpoch : 1, batch 623\n(Train) Batch 623 Loss : 4.3952531814575195, 맞은 개수 : 10\nEpoch : 1, batch 624\n(Train) Batch 624 Loss : 4.346087455749512, 맞은 개수 : 14\nEpoch : 1, batch 625\n(Train) Batch 625 Loss : 4.270837783813477, 맞은 개수 : 14\nEpoch : 1, batch 626\n(Train) Batch 626 Loss : 4.334577560424805, 맞은 개수 : 9\nEpoch : 1, batch 627\n(Train) Batch 627 Loss : 4.256292819976807, 맞은 개수 : 15\nEpoch : 1, batch 628\n(Train) Batch 628 Loss : 4.233389377593994, 맞은 개수 : 10\nEpoch : 1, batch 629\n(Train) Batch 629 Loss : 4.1960577964782715, 맞은 개수 : 18\nEpoch : 1, batch 630\n(Train) Batch 630 Loss : 4.155584812164307, 맞은 개수 : 13\nEpoch : 1, batch 631\n(Train) Batch 631 Loss : 4.137730121612549, 맞은 개수 : 17\nEpoch : 1, batch 632\n(Train) Batch 632 Loss : 4.30988073348999, 맞은 개수 : 12\nEpoch : 1, batch 633\n(Train) Batch 633 Loss : 4.285299301147461, 맞은 개수 : 14\nEpoch : 1, batch 634\n(Train) Batch 634 Loss : 4.411397457122803, 맞은 개수 : 13\nEpoch : 1, batch 635\n(Train) Batch 635 Loss : 3.9802145957946777, 맞은 개수 : 19\nEpoch : 1, batch 636\n(Train) Batch 636 Loss : 4.224644660949707, 맞은 개수 : 12\nEpoch : 1, batch 637\n(Train) Batch 637 Loss : 4.368740081787109, 맞은 개수 : 16\nEpoch : 1, batch 638\n(Train) Batch 638 Loss : 4.118885517120361, 맞은 개수 : 15\nEpoch : 1, batch 639\n(Train) Batch 639 Loss : 4.290266036987305, 맞은 개수 : 12\nEpoch : 1, batch 640\n(Train) Batch 640 Loss : 4.272775650024414, 맞은 개수 : 12\nEpoch : 1, batch 641\n(Train) Batch 641 Loss : 4.193767070770264, 맞은 개수 : 13\nEpoch : 1, batch 642\n(Train) Batch 642 Loss : 4.251850605010986, 맞은 개수 : 14\nEpoch : 1, batch 643\n(Train) Batch 643 Loss : 4.105836391448975, 맞은 개수 : 19\nEpoch : 1, batch 644\n(Train) Batch 644 Loss : 4.337116718292236, 맞은 개수 : 12\nEpoch : 1, batch 645\n(Train) Batch 645 Loss : 4.154313087463379, 맞은 개수 : 16\nEpoch : 1, batch 646\n(Train) Batch 646 Loss : 4.127784729003906, 맞은 개수 : 21\nEpoch : 1, batch 647\n(Train) Batch 647 Loss : 4.280391693115234, 맞은 개수 : 13\nEpoch : 1, batch 648\n(Train) Batch 648 Loss : 4.400440216064453, 맞은 개수 : 9\nEpoch : 1, batch 649\n(Train) Batch 649 Loss : 4.24193000793457, 맞은 개수 : 17\nEpoch : 1, batch 650\n(Train) Batch 650 Loss : 4.179074764251709, 맞은 개수 : 16\nEpoch : 1, batch 651\n(Train) Batch 651 Loss : 4.0588178634643555, 맞은 개수 : 20\nEpoch : 1, batch 652\n(Train) Batch 652 Loss : 4.4121527671813965, 맞은 개수 : 12\nEpoch : 1, batch 653\n(Train) Batch 653 Loss : 4.406287670135498, 맞은 개수 : 8\nEpoch : 1, batch 654\n(Train) Batch 654 Loss : 4.079382419586182, 맞은 개수 : 20\nEpoch : 1, batch 655\n(Train) Batch 655 Loss : 4.233243942260742, 맞은 개수 : 14\nEpoch : 1, batch 656\n(Train) Batch 656 Loss : 4.127623081207275, 맞은 개수 : 20\nEpoch : 1, batch 657\n(Train) Batch 657 Loss : 4.201323986053467, 맞은 개수 : 10\nEpoch : 1, batch 658\n(Train) Batch 658 Loss : 4.197878360748291, 맞은 개수 : 11\nEpoch : 1, batch 659\n(Train) Batch 659 Loss : 4.455838680267334, 맞은 개수 : 13\nEpoch : 1, batch 660\n(Train) Batch 660 Loss : 4.30544376373291, 맞은 개수 : 9\nEpoch : 1, batch 661\n(Train) Batch 661 Loss : 4.2140021324157715, 맞은 개수 : 14\nEpoch : 1, batch 662\n(Train) Batch 662 Loss : 4.27959680557251, 맞은 개수 : 16\nEpoch : 1, batch 663\n(Train) Batch 663 Loss : 4.341315269470215, 맞은 개수 : 11\nEpoch : 1, batch 664\n(Train) Batch 664 Loss : 4.181279182434082, 맞은 개수 : 15\nEpoch : 1, batch 665\n(Train) Batch 665 Loss : 4.2769012451171875, 맞은 개수 : 15\nEpoch : 1, batch 666\n(Train) Batch 666 Loss : 4.272092342376709, 맞은 개수 : 13\nEpoch : 1, batch 667\n(Train) Batch 667 Loss : 4.072110652923584, 맞은 개수 : 19\nEpoch : 1, batch 668\n(Train) Batch 668 Loss : 4.386553764343262, 맞은 개수 : 15\nEpoch : 1, batch 669\n(Train) Batch 669 Loss : 4.204662322998047, 맞은 개수 : 10\nEpoch : 1, batch 670\n(Train) Batch 670 Loss : 4.331669807434082, 맞은 개수 : 16\nEpoch : 1, batch 671\n(Train) Batch 671 Loss : 4.002004623413086, 맞은 개수 : 11\nEpoch : 1, batch 672\n(Train) Batch 672 Loss : 4.230751991271973, 맞은 개수 : 12\nEpoch : 1, batch 673\n(Train) Batch 673 Loss : 4.3592071533203125, 맞은 개수 : 17\nEpoch : 1, batch 674\n(Train) Batch 674 Loss : 4.2459893226623535, 맞은 개수 : 14\nEpoch : 1, batch 675\n(Train) Batch 675 Loss : 4.121060371398926, 맞은 개수 : 15\nEpoch : 1, batch 676\n(Train) Batch 676 Loss : 4.335634708404541, 맞은 개수 : 11\nEpoch : 1, batch 677\n(Train) Batch 677 Loss : 3.8036909103393555, 맞은 개수 : 21\nEpoch : 1, batch 678\n(Train) Batch 678 Loss : 4.104214191436768, 맞은 개수 : 17\nEpoch : 1, batch 679\n(Train) Batch 679 Loss : 4.424009323120117, 맞은 개수 : 9\nEpoch : 1, batch 680\n(Train) Batch 680 Loss : 4.4211273193359375, 맞은 개수 : 10\nEpoch : 1, batch 681\n(Train) Batch 681 Loss : 4.186471462249756, 맞은 개수 : 16\nEpoch : 1, batch 682\n(Train) Batch 682 Loss : 4.2342023849487305, 맞은 개수 : 12\nEpoch : 1, batch 683\n(Train) Batch 683 Loss : 4.246645450592041, 맞은 개수 : 12\nEpoch : 1, batch 684\n(Train) Batch 684 Loss : 4.0957255363464355, 맞은 개수 : 20\nEpoch : 1, batch 685\n(Train) Batch 685 Loss : 4.124649524688721, 맞은 개수 : 16\nEpoch : 1, batch 686\n(Train) Batch 686 Loss : 4.2739129066467285, 맞은 개수 : 11\nEpoch : 1, batch 687\n(Train) Batch 687 Loss : 3.9015562534332275, 맞은 개수 : 14\nEpoch : 1, batch 688\n(Train) Batch 688 Loss : 4.237074375152588, 맞은 개수 : 13\nEpoch : 1, batch 689\n(Train) Batch 689 Loss : 3.9658496379852295, 맞은 개수 : 15\nEpoch : 1, batch 690\n(Train) Batch 690 Loss : 4.1367692947387695, 맞은 개수 : 13\nEpoch : 1, batch 691\n(Train) Batch 691 Loss : 4.003360271453857, 맞은 개수 : 15\nEpoch : 1, batch 692\n(Train) Batch 692 Loss : 4.059765338897705, 맞은 개수 : 18\nEpoch : 1, batch 693\n(Train) Batch 693 Loss : 3.9374098777770996, 맞은 개수 : 20\nEpoch : 1, batch 694\n(Train) Batch 694 Loss : 4.522064685821533, 맞은 개수 : 11\nEpoch : 1, batch 695\n(Train) Batch 695 Loss : 4.404393196105957, 맞은 개수 : 12\nEpoch : 1, batch 696\n(Train) Batch 696 Loss : 4.316119194030762, 맞은 개수 : 17\nEpoch : 1, batch 697\n(Train) Batch 697 Loss : 4.1542816162109375, 맞은 개수 : 15\nEpoch : 1, batch 698\n(Train) Batch 698 Loss : 3.8862192630767822, 맞은 개수 : 21\nEpoch : 1, batch 699\n(Train) Batch 699 Loss : 4.365091800689697, 맞은 개수 : 14\nEpoch : 1, batch 700\n(Train) Batch 700 Loss : 4.104954719543457, 맞은 개수 : 19\nEpoch : 1, batch 701\n(Train) Batch 701 Loss : 4.168302059173584, 맞은 개수 : 16\nEpoch : 1, batch 702\n(Train) Batch 702 Loss : 4.042304039001465, 맞은 개수 : 17\nEpoch : 1, batch 703\n(Train) Batch 703 Loss : 4.122366905212402, 맞은 개수 : 18\nEpoch : 1, batch 704\n(Train) Batch 704 Loss : 4.22658634185791, 맞은 개수 : 11\nEpoch : 1, batch 705\n(Train) Batch 705 Loss : 4.236210823059082, 맞은 개수 : 9\nEpoch : 1, batch 706\n(Train) Batch 706 Loss : 4.227978706359863, 맞은 개수 : 14\nEpoch : 1, batch 707\n(Train) Batch 707 Loss : 4.126816272735596, 맞은 개수 : 15\nEpoch : 1, batch 708\n(Train) Batch 708 Loss : 4.478376388549805, 맞은 개수 : 4\nEpoch : 1, batch 709\n(Train) Batch 709 Loss : 4.207812309265137, 맞은 개수 : 13\nEpoch : 1, batch 710\n(Train) Batch 710 Loss : 4.19946813583374, 맞은 개수 : 14\nEpoch : 1, batch 711\n(Train) Batch 711 Loss : 3.8905186653137207, 맞은 개수 : 16\nEpoch : 1, batch 712\n(Train) Batch 712 Loss : 3.7026021480560303, 맞은 개수 : 22\nEpoch : 1, batch 713\n(Train) Batch 713 Loss : 4.171189785003662, 맞은 개수 : 10\nEpoch : 1, batch 714\n(Train) Batch 714 Loss : 4.495068073272705, 맞은 개수 : 11\nEpoch : 1, batch 715\n(Train) Batch 715 Loss : 4.254671096801758, 맞은 개수 : 15\nEpoch : 1, batch 716\n(Train) Batch 716 Loss : 4.240350723266602, 맞은 개수 : 16\nEpoch : 1, batch 717\n(Train) Batch 717 Loss : 4.251523017883301, 맞은 개수 : 11\nEpoch : 1, batch 718\n(Train) Batch 718 Loss : 4.215677738189697, 맞은 개수 : 12\nEpoch : 1, batch 719\n(Train) Batch 719 Loss : 4.067689418792725, 맞은 개수 : 17\nEpoch : 1, batch 720\n(Train) Batch 720 Loss : 4.046907424926758, 맞은 개수 : 15\nEpoch : 1, batch 721\n(Train) Batch 721 Loss : 4.229121208190918, 맞은 개수 : 12\nEpoch : 1, batch 722\n(Train) Batch 722 Loss : 4.059330940246582, 맞은 개수 : 16\nEpoch : 1, batch 723\n(Train) Batch 723 Loss : 4.060145854949951, 맞은 개수 : 16\nEpoch : 1, batch 724\n(Train) Batch 724 Loss : 4.456972599029541, 맞은 개수 : 10\nEpoch : 1, batch 725\n(Train) Batch 725 Loss : 4.172614574432373, 맞은 개수 : 15\nEpoch : 1, batch 726\n(Train) Batch 726 Loss : 4.238065719604492, 맞은 개수 : 17\nEpoch : 1, batch 727\n(Train) Batch 727 Loss : 4.194268226623535, 맞은 개수 : 9\nEpoch : 1, batch 728\n(Train) Batch 728 Loss : 4.400439739227295, 맞은 개수 : 10\nEpoch : 1, batch 729\n(Train) Batch 729 Loss : 4.053757190704346, 맞은 개수 : 16\nEpoch : 1, batch 730\n(Train) Batch 730 Loss : 4.287469387054443, 맞은 개수 : 15\nEpoch : 1, batch 731\n(Train) Batch 731 Loss : 4.226870536804199, 맞은 개수 : 16\nEpoch : 1, batch 732\n(Train) Batch 732 Loss : 4.104858875274658, 맞은 개수 : 12\nEpoch : 1, batch 733\n(Train) Batch 733 Loss : 4.075974464416504, 맞은 개수 : 22\nEpoch : 1, batch 734\n(Train) Batch 734 Loss : 4.272407531738281, 맞은 개수 : 21\nEpoch : 1, batch 735\n(Train) Batch 735 Loss : 4.208085536956787, 맞은 개수 : 17\nEpoch : 1, batch 736\n(Train) Batch 736 Loss : 4.117254734039307, 맞은 개수 : 17\nEpoch : 1, batch 737\n(Train) Batch 737 Loss : 4.306552886962891, 맞은 개수 : 11\nEpoch : 1, batch 738\n(Train) Batch 738 Loss : 4.029655456542969, 맞은 개수 : 21\nEpoch : 1, batch 739\n(Train) Batch 739 Loss : 3.986523389816284, 맞은 개수 : 20\nEpoch : 1, batch 740\n(Train) Batch 740 Loss : 4.010538101196289, 맞은 개수 : 13\nEpoch : 1, batch 741\n(Train) Batch 741 Loss : 4.0814104080200195, 맞은 개수 : 14\nEpoch : 1, batch 742\n(Train) Batch 742 Loss : 4.152431964874268, 맞은 개수 : 16\nEpoch : 1, batch 743\n(Train) Batch 743 Loss : 4.255220890045166, 맞은 개수 : 16\nEpoch : 1, batch 744\n(Train) Batch 744 Loss : 4.112833499908447, 맞은 개수 : 16\nEpoch : 1, batch 745\n(Train) Batch 745 Loss : 4.140677452087402, 맞은 개수 : 10\nEpoch : 1, batch 746\n(Train) Batch 746 Loss : 4.249732971191406, 맞은 개수 : 10\nEpoch : 1, batch 747\n(Train) Batch 747 Loss : 4.116728782653809, 맞은 개수 : 17\nEpoch : 1, batch 748\n(Train) Batch 748 Loss : 3.9962399005889893, 맞은 개수 : 16\nEpoch : 1, batch 749\n(Train) Batch 749 Loss : 4.209322452545166, 맞은 개수 : 16\nEpoch : 1, batch 750\n(Train) Batch 750 Loss : 4.025693893432617, 맞은 개수 : 18\nEpoch : 1, batch 751\n(Train) Batch 751 Loss : 4.205445766448975, 맞은 개수 : 18\nEpoch : 1, batch 752\n(Train) Batch 752 Loss : 4.11040735244751, 맞은 개수 : 15\nEpoch : 1, batch 753\n(Train) Batch 753 Loss : 4.285402297973633, 맞은 개수 : 12\nEpoch : 1, batch 754\n(Train) Batch 754 Loss : 4.574637413024902, 맞은 개수 : 11\nEpoch : 1, batch 755\n(Train) Batch 755 Loss : 4.135756015777588, 맞은 개수 : 12\nEpoch : 1, batch 756\n(Train) Batch 756 Loss : 4.156672954559326, 맞은 개수 : 14\nEpoch : 1, batch 757\n(Train) Batch 757 Loss : 4.043158054351807, 맞은 개수 : 14\nEpoch : 1, batch 758\n(Train) Batch 758 Loss : 4.082642078399658, 맞은 개수 : 22\nEpoch : 1, batch 759\n(Train) Batch 759 Loss : 4.381809711456299, 맞은 개수 : 14\nEpoch : 1, batch 760\n(Train) Batch 760 Loss : 4.046420574188232, 맞은 개수 : 20\nEpoch : 1, batch 761\n(Train) Batch 761 Loss : 4.313955307006836, 맞은 개수 : 19\nEpoch : 1, batch 762\n(Train) Batch 762 Loss : 4.11238956451416, 맞은 개수 : 19\nEpoch : 1, batch 763\n(Train) Batch 763 Loss : 4.1762471199035645, 맞은 개수 : 13\nEpoch : 1, batch 764\n(Train) Batch 764 Loss : 4.412215232849121, 맞은 개수 : 7\nEpoch : 1, batch 765\n(Train) Batch 765 Loss : 3.8631391525268555, 맞은 개수 : 13\nEpoch : 1, batch 766\n(Train) Batch 766 Loss : 4.180881977081299, 맞은 개수 : 14\nEpoch : 1, batch 767\n(Train) Batch 767 Loss : 4.140353679656982, 맞은 개수 : 20\nEpoch : 1, batch 768\n(Train) Batch 768 Loss : 4.059110641479492, 맞은 개수 : 18\nEpoch : 1, batch 769\n(Train) Batch 769 Loss : 4.100286483764648, 맞은 개수 : 17\nEpoch : 1, batch 770\n(Train) Batch 770 Loss : 4.381070613861084, 맞은 개수 : 15\nEpoch : 1, batch 771\n(Train) Batch 771 Loss : 4.079782009124756, 맞은 개수 : 17\nEpoch : 1, batch 772\n(Train) Batch 772 Loss : 3.9653046131134033, 맞은 개수 : 18\nEpoch : 1, batch 773\n(Train) Batch 773 Loss : 4.185616493225098, 맞은 개수 : 15\nEpoch : 1, batch 774\n(Train) Batch 774 Loss : 4.031400680541992, 맞은 개수 : 21\nEpoch : 1, batch 775\n(Train) Batch 775 Loss : 4.270502090454102, 맞은 개수 : 14\nEpoch : 1, batch 776\n(Train) Batch 776 Loss : 4.058002471923828, 맞은 개수 : 17\nEpoch : 1, batch 777\n(Train) Batch 777 Loss : 4.188292980194092, 맞은 개수 : 9\nEpoch : 1, batch 778\n(Train) Batch 778 Loss : 4.2307538986206055, 맞은 개수 : 10\nEpoch : 1, batch 779\n(Train) Batch 779 Loss : 4.299002647399902, 맞은 개수 : 16\nEpoch : 1, batch 780\n(Train) Batch 780 Loss : 4.3755645751953125, 맞은 개수 : 9\nEpoch : 1, batch 781\n(Train) Batch 781 Loss : 4.158334255218506, 맞은 개수 : 14\nEpoch : 1, batch 782\n(Train) Batch 782 Loss : 4.129848957061768, 맞은 개수 : 13\nEpoch : 1, batch 783\n(Train) Batch 783 Loss : 4.319173336029053, 맞은 개수 : 10\nEpoch : 1, batch 784\n(Train) Batch 784 Loss : 4.444852828979492, 맞은 개수 : 13\nEpoch : 1, batch 785\n(Train) Batch 785 Loss : 4.079015254974365, 맞은 개수 : 24\nEpoch : 1, batch 786\n(Train) Batch 786 Loss : 4.213467121124268, 맞은 개수 : 16\nEpoch : 1, batch 787\n(Train) Batch 787 Loss : 4.073822498321533, 맞은 개수 : 12\nEpoch : 1, batch 788\n(Train) Batch 788 Loss : 4.25994348526001, 맞은 개수 : 17\nEpoch : 1, batch 789\n(Train) Batch 789 Loss : 3.9934730529785156, 맞은 개수 : 16\nEpoch : 1, batch 790\n(Train) Batch 790 Loss : 4.224414825439453, 맞은 개수 : 14\nEpoch : 1, batch 791\n(Train) Batch 791 Loss : 4.170560359954834, 맞은 개수 : 10\nEpoch : 1, batch 792\n(Train) Batch 792 Loss : 4.015800952911377, 맞은 개수 : 16\nEpoch : 1, batch 793\n(Train) Batch 793 Loss : 4.112820148468018, 맞은 개수 : 18\nEpoch : 1, batch 794\n(Train) Batch 794 Loss : 4.23240327835083, 맞은 개수 : 10\nEpoch : 1, batch 795\n(Train) Batch 795 Loss : 4.1540656089782715, 맞은 개수 : 18\nEpoch : 1, batch 796\n(Train) Batch 796 Loss : 4.062768936157227, 맞은 개수 : 18\nEpoch : 1, batch 797\n(Train) Batch 797 Loss : 3.9778268337249756, 맞은 개수 : 23\nEpoch : 1, batch 798\n(Train) Batch 798 Loss : 4.367023468017578, 맞은 개수 : 10\nEpoch : 1, batch 799\n(Train) Batch 799 Loss : 4.170040607452393, 맞은 개수 : 14\nEpoch : 1, batch 800\n(Train) Batch 800 Loss : 4.11734676361084, 맞은 개수 : 17\nEpoch : 1, batch 801\n(Train) Batch 801 Loss : 4.349276542663574, 맞은 개수 : 17\nEpoch : 1, batch 802\n(Train) Batch 802 Loss : 4.248196601867676, 맞은 개수 : 13\nEpoch : 1, batch 803\n(Train) Batch 803 Loss : 4.310178279876709, 맞은 개수 : 18\nEpoch : 1, batch 804\n(Train) Batch 804 Loss : 4.213166236877441, 맞은 개수 : 20\nEpoch : 1, batch 805\n(Train) Batch 805 Loss : 4.017981052398682, 맞은 개수 : 20\nEpoch : 1, batch 806\n(Train) Batch 806 Loss : 4.189336776733398, 맞은 개수 : 15\nEpoch : 1, batch 807\n(Train) Batch 807 Loss : 4.0438032150268555, 맞은 개수 : 19\nEpoch : 1, batch 808\n(Train) Batch 808 Loss : 4.3726630210876465, 맞은 개수 : 10\nEpoch : 1, batch 809\n(Train) Batch 809 Loss : 4.275160312652588, 맞은 개수 : 10\nEpoch : 1, batch 810\n(Train) Batch 810 Loss : 4.184548377990723, 맞은 개수 : 10\nEpoch : 1, batch 811\n(Train) Batch 811 Loss : 4.0419697761535645, 맞은 개수 : 26\nEpoch : 1, batch 812\n(Train) Batch 812 Loss : 3.9723072052001953, 맞은 개수 : 17\nEpoch : 1, batch 813\n(Train) Batch 813 Loss : 4.061559200286865, 맞은 개수 : 18\nEpoch : 1, batch 814\n(Train) Batch 814 Loss : 4.271761894226074, 맞은 개수 : 13\nEpoch : 1, batch 815\n(Train) Batch 815 Loss : 4.2200212478637695, 맞은 개수 : 12\nEpoch : 1, batch 816\n(Train) Batch 816 Loss : 4.236298561096191, 맞은 개수 : 15\nEpoch : 1, batch 817\n(Train) Batch 817 Loss : 4.173993110656738, 맞은 개수 : 13\nEpoch : 1, batch 818\n(Train) Batch 818 Loss : 4.29099702835083, 맞은 개수 : 12\nEpoch : 1, batch 819\n(Train) Batch 819 Loss : 4.032778739929199, 맞은 개수 : 20\nEpoch : 1, batch 820\n(Train) Batch 820 Loss : 3.9582769870758057, 맞은 개수 : 16\nEpoch : 1, batch 821\n(Train) Batch 821 Loss : 4.090911388397217, 맞은 개수 : 14\nEpoch : 1, batch 822\n(Train) Batch 822 Loss : 4.046802043914795, 맞은 개수 : 18\nEpoch : 1, batch 823\n(Train) Batch 823 Loss : 4.032424449920654, 맞은 개수 : 20\nEpoch : 1, batch 824\n(Train) Batch 824 Loss : 3.9619827270507812, 맞은 개수 : 18\nEpoch : 1, batch 825\n(Train) Batch 825 Loss : 3.8174517154693604, 맞은 개수 : 20\nEpoch : 1, batch 826\n(Train) Batch 826 Loss : 3.9612843990325928, 맞은 개수 : 12\nEpoch : 1, batch 827\n(Train) Batch 827 Loss : 4.1275553703308105, 맞은 개수 : 15\nEpoch : 1, batch 828\n(Train) Batch 828 Loss : 4.184995174407959, 맞은 개수 : 16\nEpoch : 1, batch 829\n(Train) Batch 829 Loss : 4.263583183288574, 맞은 개수 : 12\nEpoch : 1, batch 830\n(Train) Batch 830 Loss : 4.132180213928223, 맞은 개수 : 20\nEpoch : 1, batch 831\n(Train) Batch 831 Loss : 4.194892883300781, 맞은 개수 : 14\nEpoch : 1, batch 832\n(Train) Batch 832 Loss : 4.051636695861816, 맞은 개수 : 17\nEpoch : 1, batch 833\n(Train) Batch 833 Loss : 4.083185195922852, 맞은 개수 : 21\nEpoch : 1, batch 834\n(Train) Batch 834 Loss : 4.155821800231934, 맞은 개수 : 16\nEpoch : 1, batch 835\n(Train) Batch 835 Loss : 4.361446857452393, 맞은 개수 : 12\nEpoch : 1, batch 836\n(Train) Batch 836 Loss : 3.9645345211029053, 맞은 개수 : 20\nEpoch : 1, batch 837\n(Train) Batch 837 Loss : 4.208293437957764, 맞은 개수 : 17\nEpoch : 1, batch 838\n(Train) Batch 838 Loss : 4.246950149536133, 맞은 개수 : 11\nEpoch : 1, batch 839\n(Train) Batch 839 Loss : 3.9805800914764404, 맞은 개수 : 24\nEpoch : 1, batch 840\n(Train) Batch 840 Loss : 4.0226521492004395, 맞은 개수 : 21\nEpoch : 1, batch 841\n(Train) Batch 841 Loss : 4.194557189941406, 맞은 개수 : 9\nEpoch : 1, batch 842\n(Train) Batch 842 Loss : 4.217310905456543, 맞은 개수 : 15\nEpoch : 1, batch 843\n(Train) Batch 843 Loss : 4.162322521209717, 맞은 개수 : 18\nEpoch : 1, batch 844\n(Train) Batch 844 Loss : 4.212508678436279, 맞은 개수 : 18\nEpoch : 1, batch 845\n(Train) Batch 845 Loss : 4.092071533203125, 맞은 개수 : 15\nEpoch : 1, batch 846\n(Train) Batch 846 Loss : 4.227842807769775, 맞은 개수 : 15\nEpoch : 1, batch 847\n(Train) Batch 847 Loss : 4.067930221557617, 맞은 개수 : 17\nEpoch : 1, batch 848\n(Train) Batch 848 Loss : 4.075432777404785, 맞은 개수 : 14\nEpoch : 1, batch 849\n(Train) Batch 849 Loss : 4.033441066741943, 맞은 개수 : 13\nEpoch : 1, batch 850\n(Train) Batch 850 Loss : 3.993107795715332, 맞은 개수 : 16\nEpoch : 1, batch 851\n(Train) Batch 851 Loss : 4.1193695068359375, 맞은 개수 : 14\nEpoch : 1, batch 852\n(Train) Batch 852 Loss : 4.246840000152588, 맞은 개수 : 11\nEpoch : 1, batch 853\n(Train) Batch 853 Loss : 4.062586784362793, 맞은 개수 : 13\nEpoch : 1, batch 854\n(Train) Batch 854 Loss : 3.937314510345459, 맞은 개수 : 18\nEpoch : 1, batch 855\n(Train) Batch 855 Loss : 4.172426700592041, 맞은 개수 : 15\nEpoch : 1, batch 856\n(Train) Batch 856 Loss : 4.076574802398682, 맞은 개수 : 21\nEpoch : 1, batch 857\n(Train) Batch 857 Loss : 4.202075481414795, 맞은 개수 : 8\nEpoch : 1, batch 858\n(Train) Batch 858 Loss : 4.1066412925720215, 맞은 개수 : 16\nEpoch : 1, batch 859\n(Train) Batch 859 Loss : 4.054783821105957, 맞은 개수 : 19\nEpoch : 1, batch 860\n(Train) Batch 860 Loss : 4.007154941558838, 맞은 개수 : 22\nEpoch : 1, batch 861\n(Train) Batch 861 Loss : 4.06319522857666, 맞은 개수 : 15\nEpoch : 1, batch 862\n(Train) Batch 862 Loss : 4.155957221984863, 맞은 개수 : 14\nEpoch : 1, batch 863\n(Train) Batch 863 Loss : 3.9094796180725098, 맞은 개수 : 18\nEpoch : 1, batch 864\n(Train) Batch 864 Loss : 4.205315589904785, 맞은 개수 : 15\nEpoch : 1, batch 865\n(Train) Batch 865 Loss : 3.82646107673645, 맞은 개수 : 19\nEpoch : 1, batch 866\n(Train) Batch 866 Loss : 4.177548408508301, 맞은 개수 : 18\nEpoch : 1, batch 867\n(Train) Batch 867 Loss : 4.187956809997559, 맞은 개수 : 14\nEpoch : 1, batch 868\n(Train) Batch 868 Loss : 3.9569406509399414, 맞은 개수 : 17\nEpoch : 1, batch 869\n(Train) Batch 869 Loss : 4.120030879974365, 맞은 개수 : 12\nEpoch : 1, batch 870\n(Train) Batch 870 Loss : 3.9442224502563477, 맞은 개수 : 18\nEpoch : 1, batch 871\n(Train) Batch 871 Loss : 4.383816719055176, 맞은 개수 : 21\nEpoch : 1, batch 872\n(Train) Batch 872 Loss : 4.11796236038208, 맞은 개수 : 17\nEpoch : 1, batch 873\n(Train) Batch 873 Loss : 4.4565110206604, 맞은 개수 : 13\nEpoch : 1, batch 874\n(Train) Batch 874 Loss : 4.146082401275635, 맞은 개수 : 14\nEpoch : 1, batch 875\n(Train) Batch 875 Loss : 3.9633517265319824, 맞은 개수 : 20\nEpoch : 1, batch 876\n(Train) Batch 876 Loss : 4.064102649688721, 맞은 개수 : 20\nEpoch : 1, batch 877\n(Train) Batch 877 Loss : 4.188717365264893, 맞은 개수 : 14\nEpoch : 1, batch 878\n(Train) Batch 878 Loss : 4.187719821929932, 맞은 개수 : 10\nEpoch : 1, batch 879\n(Train) Batch 879 Loss : 4.259748458862305, 맞은 개수 : 11\nEpoch : 1, batch 880\n(Train) Batch 880 Loss : 4.08763313293457, 맞은 개수 : 15\nEpoch : 1, batch 881\n(Train) Batch 881 Loss : 3.9301021099090576, 맞은 개수 : 24\nEpoch : 1, batch 882\n(Train) Batch 882 Loss : 4.018362522125244, 맞은 개수 : 22\nEpoch : 1, batch 883\n(Train) Batch 883 Loss : 4.307858943939209, 맞은 개수 : 9\nEpoch : 1, batch 884\n(Train) Batch 884 Loss : 3.9828078746795654, 맞은 개수 : 16\nEpoch : 1, batch 885\n(Train) Batch 885 Loss : 4.1165266036987305, 맞은 개수 : 15\nEpoch : 1, batch 886\n(Train) Batch 886 Loss : 3.834686517715454, 맞은 개수 : 22\nEpoch : 1, batch 887\n(Train) Batch 887 Loss : 4.125242233276367, 맞은 개수 : 20\nEpoch : 1, batch 888\n(Train) Batch 888 Loss : 4.008259296417236, 맞은 개수 : 18\nEpoch : 1, batch 889\n(Train) Batch 889 Loss : 4.281857967376709, 맞은 개수 : 10\nEpoch : 1, batch 890\n(Train) Batch 890 Loss : 4.11100435256958, 맞은 개수 : 22\nEpoch : 1, batch 891\n(Train) Batch 891 Loss : 4.221836566925049, 맞은 개수 : 16\nEpoch : 1, batch 892\n(Train) Batch 892 Loss : 4.37429141998291, 맞은 개수 : 11\nEpoch : 1, batch 893\n(Train) Batch 893 Loss : 4.086113929748535, 맞은 개수 : 20\nEpoch : 1, batch 894\n(Train) Batch 894 Loss : 4.11765718460083, 맞은 개수 : 8\nEpoch : 1, batch 895\n(Train) Batch 895 Loss : 4.049694538116455, 맞은 개수 : 17\nEpoch : 1, batch 896\n(Train) Batch 896 Loss : 3.979602336883545, 맞은 개수 : 20\nEpoch : 1, batch 897\n(Train) Batch 897 Loss : 4.029664039611816, 맞은 개수 : 16\nEpoch : 1, batch 898\n(Train) Batch 898 Loss : 3.958423614501953, 맞은 개수 : 16\nEpoch : 1, batch 899\n(Train) Batch 899 Loss : 4.091254234313965, 맞은 개수 : 16\nEpoch : 1, batch 900\n(Train) Batch 900 Loss : 4.0858330726623535, 맞은 개수 : 14\nEpoch : 1, batch 901\n(Train) Batch 901 Loss : 4.42571496963501, 맞은 개수 : 15\nEpoch : 1, batch 902\n(Train) Batch 902 Loss : 4.105976581573486, 맞은 개수 : 13\nEpoch : 1, batch 903\n(Train) Batch 903 Loss : 3.960209846496582, 맞은 개수 : 18\nEpoch : 1, batch 904\n(Train) Batch 904 Loss : 3.9203357696533203, 맞은 개수 : 19\nEpoch : 1, batch 905\n(Train) Batch 905 Loss : 4.16572380065918, 맞은 개수 : 14\nEpoch : 1, batch 906\n(Train) Batch 906 Loss : 4.020053386688232, 맞은 개수 : 17\nEpoch : 1, batch 907\n(Train) Batch 907 Loss : 4.0894389152526855, 맞은 개수 : 20\nEpoch : 1, batch 908\n(Train) Batch 908 Loss : 3.938730239868164, 맞은 개수 : 20\nEpoch : 1, batch 909\n(Train) Batch 909 Loss : 3.904311180114746, 맞은 개수 : 21\nEpoch : 1, batch 910\n(Train) Batch 910 Loss : 4.18079948425293, 맞은 개수 : 18\nEpoch : 1, batch 911\n(Train) Batch 911 Loss : 3.9810805320739746, 맞은 개수 : 12\nEpoch : 1, batch 912\n(Train) Batch 912 Loss : 4.024204254150391, 맞은 개수 : 17\nEpoch : 1, batch 913\n(Train) Batch 913 Loss : 4.044394493103027, 맞은 개수 : 13\nEpoch : 1, batch 914\n(Train) Batch 914 Loss : 4.551878929138184, 맞은 개수 : 14\nEpoch : 1, batch 915\n(Train) Batch 915 Loss : 3.9637625217437744, 맞은 개수 : 14\nEpoch : 1, batch 916\n(Train) Batch 916 Loss : 3.9541125297546387, 맞은 개수 : 24\nEpoch : 1, batch 917\n(Train) Batch 917 Loss : 4.0605998039245605, 맞은 개수 : 13\nEpoch : 1, batch 918\n(Train) Batch 918 Loss : 4.070384979248047, 맞은 개수 : 16\nEpoch : 1, batch 919\n(Train) Batch 919 Loss : 4.199050426483154, 맞은 개수 : 15\nEpoch : 1, batch 920\n(Train) Batch 920 Loss : 4.115669250488281, 맞은 개수 : 16\nEpoch : 1, batch 921\n(Train) Batch 921 Loss : 4.077130317687988, 맞은 개수 : 12\nEpoch : 1, batch 922\n(Train) Batch 922 Loss : 4.158627033233643, 맞은 개수 : 16\nEpoch : 1, batch 923\n(Train) Batch 923 Loss : 3.7526285648345947, 맞은 개수 : 18\nEpoch : 1, batch 924\n(Train) Batch 924 Loss : 3.868335723876953, 맞은 개수 : 20\nEpoch : 1, batch 925\n(Train) Batch 925 Loss : 3.9781713485717773, 맞은 개수 : 20\nEpoch : 1, batch 926\n(Train) Batch 926 Loss : 3.9379940032958984, 맞은 개수 : 20\nEpoch : 1, batch 927\n(Train) Batch 927 Loss : 4.191901683807373, 맞은 개수 : 14\nEpoch : 1, batch 928\n(Train) Batch 928 Loss : 4.118306636810303, 맞은 개수 : 17\nEpoch : 1, batch 929\n(Train) Batch 929 Loss : 3.956716299057007, 맞은 개수 : 15\nEpoch : 1, batch 930\n(Train) Batch 930 Loss : 4.120785236358643, 맞은 개수 : 15\nEpoch : 1, batch 931\n(Train) Batch 931 Loss : 4.093433380126953, 맞은 개수 : 22\nEpoch : 1, batch 932\n(Train) Batch 932 Loss : 4.040951728820801, 맞은 개수 : 19\nEpoch : 1, batch 933\n(Train) Batch 933 Loss : 3.9769952297210693, 맞은 개수 : 18\nEpoch : 1, batch 934\n(Train) Batch 934 Loss : 4.2549309730529785, 맞은 개수 : 12\nEpoch : 1, batch 935\n(Train) Batch 935 Loss : 4.0808305740356445, 맞은 개수 : 17\nEpoch : 1, batch 936\n(Train) Batch 936 Loss : 3.9985671043395996, 맞은 개수 : 19\nEpoch : 1, batch 937\n(Train) Batch 937 Loss : 4.212732315063477, 맞은 개수 : 12\nEpoch : 1, batch 938\n(Train) Batch 938 Loss : 4.081374168395996, 맞은 개수 : 22\nEpoch : 1, batch 939\n(Train) Batch 939 Loss : 4.07287073135376, 맞은 개수 : 11\nEpoch : 1, batch 940\n(Train) Batch 940 Loss : 3.898083448410034, 맞은 개수 : 14\nEpoch : 1, batch 941\n(Train) Batch 941 Loss : 3.9034132957458496, 맞은 개수 : 21\nEpoch : 1, batch 942\n(Train) Batch 942 Loss : 4.09360408782959, 맞은 개수 : 9\nEpoch : 1, batch 943\n(Train) Batch 943 Loss : 3.9346113204956055, 맞은 개수 : 18\nEpoch : 1, batch 944\n(Train) Batch 944 Loss : 4.219057559967041, 맞은 개수 : 14\nEpoch : 1, batch 945\n(Train) Batch 945 Loss : 3.9895544052124023, 맞은 개수 : 16\nEpoch : 1, batch 946\n(Train) Batch 946 Loss : 3.9447667598724365, 맞은 개수 : 19\nEpoch : 1, batch 947\n(Train) Batch 947 Loss : 3.941493034362793, 맞은 개수 : 18\nEpoch : 1, batch 948\n(Train) Batch 948 Loss : 3.9205081462860107, 맞은 개수 : 19\nEpoch : 1, batch 949\n(Train) Batch 949 Loss : 4.189223766326904, 맞은 개수 : 12\nEpoch : 1, batch 950\n(Train) Batch 950 Loss : 4.257063388824463, 맞은 개수 : 13\nEpoch : 1, batch 951\n(Train) Batch 951 Loss : 3.930117130279541, 맞은 개수 : 23\nEpoch : 1, batch 952\n(Train) Batch 952 Loss : 4.071526050567627, 맞은 개수 : 18\nEpoch : 1, batch 953\n(Train) Batch 953 Loss : 4.116600036621094, 맞은 개수 : 17\nEpoch : 1, batch 954\n(Train) Batch 954 Loss : 4.090342998504639, 맞은 개수 : 12\nEpoch : 1, batch 955\n(Train) Batch 955 Loss : 4.070734024047852, 맞은 개수 : 17\nEpoch : 1, batch 956\n(Train) Batch 956 Loss : 3.8246123790740967, 맞은 개수 : 24\nEpoch : 1, batch 957\n(Train) Batch 957 Loss : 3.8539254665374756, 맞은 개수 : 20\nEpoch : 1, batch 958\n(Train) Batch 958 Loss : 4.070537090301514, 맞은 개수 : 13\nEpoch : 1, batch 959\n(Train) Batch 959 Loss : 3.796942949295044, 맞은 개수 : 20\nEpoch : 1, batch 960\n(Train) Batch 960 Loss : 4.171819686889648, 맞은 개수 : 13\nEpoch : 1, batch 961\n(Train) Batch 961 Loss : 4.032878398895264, 맞은 개수 : 21\nEpoch : 1, batch 962\n(Train) Batch 962 Loss : 4.02528190612793, 맞은 개수 : 20\nEpoch : 1, batch 963\n(Train) Batch 963 Loss : 4.374813079833984, 맞은 개수 : 10\nEpoch : 1, batch 964\n(Train) Batch 964 Loss : 4.154538154602051, 맞은 개수 : 17\nEpoch : 1, batch 965\n(Train) Batch 965 Loss : 4.159627914428711, 맞은 개수 : 20\nEpoch : 1, batch 966\n(Train) Batch 966 Loss : 4.1638312339782715, 맞은 개수 : 17\nEpoch : 1, batch 967\n(Train) Batch 967 Loss : 4.094632148742676, 맞은 개수 : 15\nEpoch : 1, batch 968\n(Train) Batch 968 Loss : 4.008090019226074, 맞은 개수 : 26\nEpoch : 1, batch 969\n(Train) Batch 969 Loss : 4.0706467628479, 맞은 개수 : 20\nEpoch : 1, batch 970\n(Train) Batch 970 Loss : 4.199514865875244, 맞은 개수 : 9\nEpoch : 1, batch 971\n(Train) Batch 971 Loss : 3.9719505310058594, 맞은 개수 : 20\nEpoch : 1, batch 972\n(Train) Batch 972 Loss : 4.206221103668213, 맞은 개수 : 18\nEpoch : 1, batch 973\n(Train) Batch 973 Loss : 3.7876152992248535, 맞은 개수 : 26\nEpoch : 1, batch 974\n(Train) Batch 974 Loss : 4.07311487197876, 맞은 개수 : 17\nEpoch : 1, batch 975\n(Train) Batch 975 Loss : 4.05292272567749, 맞은 개수 : 12\nEpoch : 1, batch 976\n(Train) Batch 976 Loss : 3.7976131439208984, 맞은 개수 : 24\nEpoch : 1, batch 977\n(Train) Batch 977 Loss : 3.9265897274017334, 맞은 개수 : 17\nEpoch : 1, batch 978\n(Train) Batch 978 Loss : 4.020269870758057, 맞은 개수 : 21\nEpoch : 1, batch 979\n(Train) Batch 979 Loss : 3.917194128036499, 맞은 개수 : 20\nEpoch : 1, batch 980\n(Train) Batch 980 Loss : 3.924375295639038, 맞은 개수 : 19\nEpoch : 1, batch 981\n(Train) Batch 981 Loss : 3.9916889667510986, 맞은 개수 : 16\nEpoch : 1, batch 982\n(Train) Batch 982 Loss : 4.186917781829834, 맞은 개수 : 14\nEpoch : 1, batch 983\n(Train) Batch 983 Loss : 3.9614500999450684, 맞은 개수 : 18\nEpoch : 1, batch 984\n(Train) Batch 984 Loss : 4.0106587409973145, 맞은 개수 : 15\nEpoch : 1, batch 985\n(Train) Batch 985 Loss : 4.164152145385742, 맞은 개수 : 10\nEpoch : 1, batch 986\n(Train) Batch 986 Loss : 4.070222854614258, 맞은 개수 : 21\nEpoch : 1, batch 987\n(Train) Batch 987 Loss : 4.085848331451416, 맞은 개수 : 12\nEpoch : 1, batch 988\n(Train) Batch 988 Loss : 4.01315450668335, 맞은 개수 : 19\nEpoch : 1, batch 989\n(Train) Batch 989 Loss : 4.1897759437561035, 맞은 개수 : 9\nEpoch : 1, batch 990\n(Train) Batch 990 Loss : 3.9819064140319824, 맞은 개수 : 20\nEpoch : 1, batch 991\n(Train) Batch 991 Loss : 3.762550115585327, 맞은 개수 : 21\nEpoch : 1, batch 992\n(Train) Batch 992 Loss : 4.0904927253723145, 맞은 개수 : 16\nEpoch : 1, batch 993\n(Train) Batch 993 Loss : 4.042738437652588, 맞은 개수 : 19\nEpoch : 1, batch 994\n(Train) Batch 994 Loss : 4.167557716369629, 맞은 개수 : 10\nEpoch : 1, batch 995\n(Train) Batch 995 Loss : 4.058754920959473, 맞은 개수 : 18\nEpoch : 1, batch 996\n(Train) Batch 996 Loss : 3.9984169006347656, 맞은 개수 : 19\nEpoch : 1, batch 997\n(Train) Batch 997 Loss : 4.270613670349121, 맞은 개수 : 11\nEpoch : 1, batch 998\n(Train) Batch 998 Loss : 4.083585739135742, 맞은 개수 : 17\nEpoch : 1, batch 999\n(Train) Batch 999 Loss : 4.051357269287109, 맞은 개수 : 21\nEpoch : 1, batch 1000\n(Train) Batch 1000 Loss : 3.964383363723755, 맞은 개수 : 19\nEpoch : 1, batch 1001\n(Train) Batch 1001 Loss : 4.342789649963379, 맞은 개수 : 11\nEpoch : 1, batch 1002\n(Train) Batch 1002 Loss : 4.0457763671875, 맞은 개수 : 11\nEpoch : 1, batch 1003\n(Train) Batch 1003 Loss : 4.0157599449157715, 맞은 개수 : 20\nEpoch : 1, batch 1004\n(Train) Batch 1004 Loss : 4.1957221031188965, 맞은 개수 : 14\nEpoch : 1, batch 1005\n(Train) Batch 1005 Loss : 3.950450897216797, 맞은 개수 : 20\nEpoch : 1, batch 1006\n(Train) Batch 1006 Loss : 4.1652021408081055, 맞은 개수 : 12\nEpoch : 1, batch 1007\n(Train) Batch 1007 Loss : 4.339205741882324, 맞은 개수 : 14\nEpoch : 1, batch 1008\n(Train) Batch 1008 Loss : 3.9445106983184814, 맞은 개수 : 19\nEpoch : 1, batch 1009\n(Train) Batch 1009 Loss : 4.067902088165283, 맞은 개수 : 19\nEpoch : 1, batch 1010\n(Train) Batch 1010 Loss : 4.0638651847839355, 맞은 개수 : 22\nEpoch : 1, batch 1011\n(Train) Batch 1011 Loss : 4.053213596343994, 맞은 개수 : 15\nEpoch : 1, batch 1012\n(Train) Batch 1012 Loss : 3.9101991653442383, 맞은 개수 : 19\nEpoch : 1, batch 1013\n(Train) Batch 1013 Loss : 3.9926910400390625, 맞은 개수 : 12\nEpoch : 1, batch 1014\n(Train) Batch 1014 Loss : 3.9981789588928223, 맞은 개수 : 21\nEpoch : 1, batch 1015\n(Train) Batch 1015 Loss : 3.8924567699432373, 맞은 개수 : 23\nEpoch : 1, batch 1016\n(Train) Batch 1016 Loss : 4.090947151184082, 맞은 개수 : 11\nEpoch : 1, batch 1017\n(Train) Batch 1017 Loss : 4.228987216949463, 맞은 개수 : 19\nEpoch : 1, batch 1018\n(Train) Batch 1018 Loss : 3.9975061416625977, 맞은 개수 : 17\nEpoch : 1, batch 1019\n(Train) Batch 1019 Loss : 4.004919528961182, 맞은 개수 : 19\nEpoch : 1, batch 1020\n(Train) Batch 1020 Loss : 4.147227764129639, 맞은 개수 : 15\nEpoch : 1, batch 1021\n(Train) Batch 1021 Loss : 4.211664199829102, 맞은 개수 : 10\nEpoch : 1, batch 1022\n(Train) Batch 1022 Loss : 4.082762241363525, 맞은 개수 : 15\nEpoch : 1, batch 1023\n(Train) Batch 1023 Loss : 4.05486536026001, 맞은 개수 : 20\nEpoch : 1, batch 1024\n(Train) Batch 1024 Loss : 3.891688346862793, 맞은 개수 : 20\nEpoch : 1, batch 1025\n(Train) Batch 1025 Loss : 4.0095319747924805, 맞은 개수 : 16\nEpoch : 1, batch 1026\n(Train) Batch 1026 Loss : 3.895883560180664, 맞은 개수 : 16\nEpoch : 1, batch 1027\n(Train) Batch 1027 Loss : 4.043934345245361, 맞은 개수 : 18\nEpoch : 1, batch 1028\n(Train) Batch 1028 Loss : 3.9447743892669678, 맞은 개수 : 19\nEpoch : 1, batch 1029\n(Train) Batch 1029 Loss : 4.273204326629639, 맞은 개수 : 13\nEpoch : 1, batch 1030\n(Train) Batch 1030 Loss : 3.951688528060913, 맞은 개수 : 14\nEpoch : 1, batch 1031\n(Train) Batch 1031 Loss : 3.832170009613037, 맞은 개수 : 25\nEpoch : 1, batch 1032\n(Train) Batch 1032 Loss : 3.9315688610076904, 맞은 개수 : 23\nEpoch : 1, batch 1033\n(Train) Batch 1033 Loss : 3.847139358520508, 맞은 개수 : 22\nEpoch : 1, batch 1034\n(Train) Batch 1034 Loss : 3.924772024154663, 맞은 개수 : 18\nEpoch : 1, batch 1035\n(Train) Batch 1035 Loss : 4.2074384689331055, 맞은 개수 : 12\nEpoch : 1, batch 1036\n(Train) Batch 1036 Loss : 3.975721836090088, 맞은 개수 : 21\nEpoch : 1, batch 1037\n(Train) Batch 1037 Loss : 4.282103538513184, 맞은 개수 : 19\nEpoch : 1, batch 1038\n(Train) Batch 1038 Loss : 4.109470367431641, 맞은 개수 : 12\nEpoch : 1, batch 1039\n(Train) Batch 1039 Loss : 3.967686176300049, 맞은 개수 : 19\nEpoch : 1, batch 1040\n(Train) Batch 1040 Loss : 3.861937999725342, 맞은 개수 : 22\nEpoch : 1, batch 1041\n(Train) Batch 1041 Loss : 3.9160289764404297, 맞은 개수 : 12\nEpoch : 1, batch 1042\n(Train) Batch 1042 Loss : 4.120575428009033, 맞은 개수 : 14\nEpoch : 1, batch 1043\n(Train) Batch 1043 Loss : 3.843601703643799, 맞은 개수 : 21\nEpoch : 1, batch 1044\n(Train) Batch 1044 Loss : 4.3244476318359375, 맞은 개수 : 15\nEpoch : 1, batch 1045\n(Train) Batch 1045 Loss : 4.19830322265625, 맞은 개수 : 13\nEpoch : 1, batch 1046\n(Train) Batch 1046 Loss : 4.096632480621338, 맞은 개수 : 10\nEpoch : 1, batch 1047\n(Train) Batch 1047 Loss : 4.03519344329834, 맞은 개수 : 21\nEpoch : 1, batch 1048\n(Train) Batch 1048 Loss : 4.24518346786499, 맞은 개수 : 13\nEpoch : 1, batch 1049\n(Train) Batch 1049 Loss : 3.9684057235717773, 맞은 개수 : 15\nEpoch : 1, batch 1050\n(Train) Batch 1050 Loss : 3.916961193084717, 맞은 개수 : 22\nEpoch : 1, batch 1051\n(Train) Batch 1051 Loss : 3.9357523918151855, 맞은 개수 : 20\nEpoch : 1, batch 1052\n(Train) Batch 1052 Loss : 4.134713172912598, 맞은 개수 : 17\nEpoch : 1, batch 1053\n(Train) Batch 1053 Loss : 3.8245062828063965, 맞은 개수 : 26\nEpoch : 1, batch 1054\n(Train) Batch 1054 Loss : 3.950685977935791, 맞은 개수 : 21\nEpoch : 1, batch 1055\n(Train) Batch 1055 Loss : 4.158212184906006, 맞은 개수 : 20\nEpoch : 1, batch 1056\n(Train) Batch 1056 Loss : 4.379266738891602, 맞은 개수 : 10\nEpoch : 1, batch 1057\n(Train) Batch 1057 Loss : 3.983172655105591, 맞은 개수 : 20\nEpoch : 1, batch 1058\n(Train) Batch 1058 Loss : 4.245314598083496, 맞은 개수 : 14\nEpoch : 1, batch 1059\n(Train) Batch 1059 Loss : 4.126250743865967, 맞은 개수 : 15\nEpoch : 1, batch 1060\n(Train) Batch 1060 Loss : 3.847256898880005, 맞은 개수 : 16\nEpoch : 1, batch 1061\n(Train) Batch 1061 Loss : 4.033431053161621, 맞은 개수 : 18\nEpoch : 1, batch 1062\n(Train) Batch 1062 Loss : 4.027235507965088, 맞은 개수 : 17\nEpoch : 1, batch 1063\n(Train) Batch 1063 Loss : 4.072417736053467, 맞은 개수 : 13\nEpoch : 1, batch 1064\n(Train) Batch 1064 Loss : 4.041083335876465, 맞은 개수 : 18\nEpoch : 1, batch 1065\n(Train) Batch 1065 Loss : 3.946234941482544, 맞은 개수 : 34\nEpoch : 1, batch 1066\n(Train) Batch 1066 Loss : 4.0960001945495605, 맞은 개수 : 21\nEpoch : 1, batch 1067\n(Train) Batch 1067 Loss : 4.116066932678223, 맞은 개수 : 25\nEpoch : 1, batch 1068\n(Train) Batch 1068 Loss : 3.969452142715454, 맞은 개수 : 13\nEpoch : 1, batch 1069\n(Train) Batch 1069 Loss : 4.212738037109375, 맞은 개수 : 14\nEpoch : 1, batch 1070\n(Train) Batch 1070 Loss : 3.6800527572631836, 맞은 개수 : 22\nEpoch : 1, batch 1071\n(Train) Batch 1071 Loss : 3.946258544921875, 맞은 개수 : 12\nEpoch : 1, batch 1072\n(Train) Batch 1072 Loss : 4.172944068908691, 맞은 개수 : 12\nEpoch : 1, batch 1073\n(Train) Batch 1073 Loss : 3.9415993690490723, 맞은 개수 : 17\nEpoch : 1, batch 1074\n(Train) Batch 1074 Loss : 3.9693410396575928, 맞은 개수 : 19\nEpoch : 1, batch 1075\n(Train) Batch 1075 Loss : 4.122467517852783, 맞은 개수 : 12\nEpoch : 1, batch 1076\n(Train) Batch 1076 Loss : 3.9886159896850586, 맞은 개수 : 22\nEpoch : 1, batch 1077\n(Train) Batch 1077 Loss : 4.25666618347168, 맞은 개수 : 11\nEpoch : 1, batch 1078\n(Train) Batch 1078 Loss : 4.239164352416992, 맞은 개수 : 15\nEpoch : 1, batch 1079\n(Train) Batch 1079 Loss : 3.9468696117401123, 맞은 개수 : 18\nEpoch : 1, batch 1080\n(Train) Batch 1080 Loss : 4.0207200050354, 맞은 개수 : 20\nEpoch : 1, batch 1081\n(Train) Batch 1081 Loss : 3.938978910446167, 맞은 개수 : 15\nEpoch : 1, batch 1082\n(Train) Batch 1082 Loss : 3.7727959156036377, 맞은 개수 : 21\nEpoch : 1, batch 1083\n(Train) Batch 1083 Loss : 4.063830852508545, 맞은 개수 : 17\nEpoch : 1, batch 1084\n(Train) Batch 1084 Loss : 4.122809886932373, 맞은 개수 : 13\nEpoch : 1, batch 1085\n(Train) Batch 1085 Loss : 4.0570831298828125, 맞은 개수 : 16\nEpoch : 1, batch 1086\n(Train) Batch 1086 Loss : 3.925036907196045, 맞은 개수 : 24\nEpoch : 1, batch 1087\n(Train) Batch 1087 Loss : 3.9919118881225586, 맞은 개수 : 24\nEpoch : 1, batch 1088\n(Train) Batch 1088 Loss : 3.7813496589660645, 맞은 개수 : 21\nEpoch : 1, batch 1089\n(Train) Batch 1089 Loss : 4.289104461669922, 맞은 개수 : 16\nEpoch : 1, batch 1090\n(Train) Batch 1090 Loss : 4.147036552429199, 맞은 개수 : 17\nEpoch : 1, batch 1091\n(Train) Batch 1091 Loss : 3.952625274658203, 맞은 개수 : 18\nEpoch : 1, batch 1092\n(Train) Batch 1092 Loss : 3.994922637939453, 맞은 개수 : 21\nEpoch : 1, batch 1093\n(Train) Batch 1093 Loss : 4.127340316772461, 맞은 개수 : 20\nEpoch : 1, batch 1094\n(Train) Batch 1094 Loss : 3.8164165019989014, 맞은 개수 : 23\nEpoch : 1, batch 1095\n(Train) Batch 1095 Loss : 3.9814088344573975, 맞은 개수 : 14\nEpoch : 1, batch 1096\n(Train) Batch 1096 Loss : 3.834376335144043, 맞은 개수 : 22\nEpoch : 1, batch 1097\n(Train) Batch 1097 Loss : 4.202003002166748, 맞은 개수 : 17\nEpoch : 1, batch 1098\n(Train) Batch 1098 Loss : 3.935058355331421, 맞은 개수 : 21\nEpoch : 1, batch 1099\n(Train) Batch 1099 Loss : 3.977293014526367, 맞은 개수 : 21\nEpoch : 1, batch 1100\n(Train) Batch 1100 Loss : 4.091246128082275, 맞은 개수 : 12\nEpoch : 1, batch 1101\n(Train) Batch 1101 Loss : 3.8876943588256836, 맞은 개수 : 22\nEpoch : 1, batch 1102\n(Train) Batch 1102 Loss : 3.8292980194091797, 맞은 개수 : 17\nEpoch : 1, batch 1103\n(Train) Batch 1103 Loss : 4.014059543609619, 맞은 개수 : 15\nEpoch : 1, batch 1104\n(Train) Batch 1104 Loss : 4.115340709686279, 맞은 개수 : 12\nEpoch : 1, batch 1105\n(Train) Batch 1105 Loss : 3.9926135540008545, 맞은 개수 : 18\nEpoch : 1, batch 1106\n(Train) Batch 1106 Loss : 4.030170917510986, 맞은 개수 : 14\nEpoch : 1, batch 1107\n(Train) Batch 1107 Loss : 3.7560062408447266, 맞은 개수 : 21\nEpoch : 1, batch 1108\n(Train) Batch 1108 Loss : 3.9284322261810303, 맞은 개수 : 16\nEpoch : 1, batch 1109\n(Train) Batch 1109 Loss : 4.028692245483398, 맞은 개수 : 13\nEpoch : 1, batch 1110\n(Train) Batch 1110 Loss : 3.9626054763793945, 맞은 개수 : 17\nEpoch : 1, batch 1111\n(Train) Batch 1111 Loss : 4.017681121826172, 맞은 개수 : 12\nEpoch : 1, batch 1112\n(Train) Batch 1112 Loss : 3.7493484020233154, 맞은 개수 : 17\nEpoch : 1, batch 1113\n(Train) Batch 1113 Loss : 3.9153528213500977, 맞은 개수 : 17\nEpoch : 1, batch 1114\n(Train) Batch 1114 Loss : 3.893355369567871, 맞은 개수 : 20\nEpoch : 1, batch 1115\n(Train) Batch 1115 Loss : 4.131520748138428, 맞은 개수 : 11\nEpoch : 1, batch 1116\n(Train) Batch 1116 Loss : 3.924278974533081, 맞은 개수 : 20\nEpoch : 1, batch 1117\n(Train) Batch 1117 Loss : 4.066076755523682, 맞은 개수 : 19\nEpoch : 1, batch 1118\n(Train) Batch 1118 Loss : 3.9773805141448975, 맞은 개수 : 14\nEpoch : 1, batch 1119\n(Train) Batch 1119 Loss : 4.270177364349365, 맞은 개수 : 10\nEpoch : 1, batch 1120\n(Train) Batch 1120 Loss : 4.151745319366455, 맞은 개수 : 14\nEpoch : 1, batch 1121\n(Train) Batch 1121 Loss : 4.034531116485596, 맞은 개수 : 11\nEpoch : 1, batch 1122\n(Train) Batch 1122 Loss : 4.041015625, 맞은 개수 : 15\nEpoch : 1, batch 1123\n(Train) Batch 1123 Loss : 3.85907244682312, 맞은 개수 : 26\nEpoch : 1, batch 1124\n(Train) Batch 1124 Loss : 4.026463985443115, 맞은 개수 : 18\nEpoch : 1, batch 1125\n(Train) Batch 1125 Loss : 3.7776997089385986, 맞은 개수 : 21\nEpoch : 1, batch 1126\n(Train) Batch 1126 Loss : 3.9660284519195557, 맞은 개수 : 18\nEpoch : 1, batch 1127\n(Train) Batch 1127 Loss : 3.731369733810425, 맞은 개수 : 24\nEpoch : 1, batch 1128\n(Train) Batch 1128 Loss : 3.5517518520355225, 맞은 개수 : 24\nEpoch : 1, batch 1129\n(Train) Batch 1129 Loss : 4.09411096572876, 맞은 개수 : 16\nEpoch : 1, batch 1130\n(Train) Batch 1130 Loss : 4.079553127288818, 맞은 개수 : 17\nEpoch : 1, batch 1131\n(Train) Batch 1131 Loss : 3.911695957183838, 맞은 개수 : 24\nEpoch : 1, batch 1132\n(Train) Batch 1132 Loss : 4.200427055358887, 맞은 개수 : 15\nEpoch : 1, batch 1133\n(Train) Batch 1133 Loss : 3.75689435005188, 맞은 개수 : 21\nEpoch : 1, batch 1134\n(Train) Batch 1134 Loss : 4.035703182220459, 맞은 개수 : 17\nEpoch : 1, batch 1135\n(Train) Batch 1135 Loss : 4.206977367401123, 맞은 개수 : 11\nEpoch : 1, batch 1136\n(Train) Batch 1136 Loss : 3.955139636993408, 맞은 개수 : 17\nEpoch : 1, batch 1137\n(Train) Batch 1137 Loss : 4.004302978515625, 맞은 개수 : 19\nEpoch : 1, batch 1138\n(Train) Batch 1138 Loss : 4.31679630279541, 맞은 개수 : 12\nEpoch : 1, batch 1139\n(Train) Batch 1139 Loss : 4.035750865936279, 맞은 개수 : 19\nEpoch : 1, batch 1140\n(Train) Batch 1140 Loss : 4.1755876541137695, 맞은 개수 : 15\nEpoch : 1, batch 1141\n(Train) Batch 1141 Loss : 4.231799125671387, 맞은 개수 : 11\nEpoch : 1, batch 1142\n(Train) Batch 1142 Loss : 3.807393789291382, 맞은 개수 : 21\nEpoch : 1, batch 1143\n(Train) Batch 1143 Loss : 3.9792356491088867, 맞은 개수 : 17\nEpoch : 1, batch 1144\n(Train) Batch 1144 Loss : 3.8958163261413574, 맞은 개수 : 21\nEpoch : 1, batch 1145\n(Train) Batch 1145 Loss : 4.057845115661621, 맞은 개수 : 16\nEpoch : 1, batch 1146\n(Train) Batch 1146 Loss : 4.1080193519592285, 맞은 개수 : 16\nEpoch : 1, batch 1147\n(Train) Batch 1147 Loss : 3.9519219398498535, 맞은 개수 : 21\nEpoch : 1, batch 1148\n(Train) Batch 1148 Loss : 4.146450042724609, 맞은 개수 : 20\nEpoch : 1, batch 1149\n(Train) Batch 1149 Loss : 3.9949207305908203, 맞은 개수 : 17\nEpoch : 1, batch 1150\n(Train) Batch 1150 Loss : 3.916038751602173, 맞은 개수 : 19\nEpoch : 1, batch 1151\n(Train) Batch 1151 Loss : 3.9878506660461426, 맞은 개수 : 14\nEpoch : 1, batch 1152\n(Train) Batch 1152 Loss : 4.071300506591797, 맞은 개수 : 19\nEpoch : 1, batch 1153\n(Train) Batch 1153 Loss : 4.029533863067627, 맞은 개수 : 17\nEpoch : 1, batch 1154\n(Train) Batch 1154 Loss : 3.961998224258423, 맞은 개수 : 18\nEpoch : 1, batch 1155\n(Train) Batch 1155 Loss : 3.972710609436035, 맞은 개수 : 20\nEpoch : 1, batch 1156\n(Train) Batch 1156 Loss : 3.8063721656799316, 맞은 개수 : 12\nEpoch : 1, batch 1157\n(Train) Batch 1157 Loss : 3.9770939350128174, 맞은 개수 : 14\nEpoch : 1, batch 1158\n(Train) Batch 1158 Loss : 4.053114414215088, 맞은 개수 : 22\nEpoch : 1, batch 1159\n(Train) Batch 1159 Loss : 3.924098253250122, 맞은 개수 : 20\nEpoch : 1, batch 1160\n(Train) Batch 1160 Loss : 4.017251014709473, 맞은 개수 : 20\nEpoch : 1, batch 1161\n(Train) Batch 1161 Loss : 3.8196558952331543, 맞은 개수 : 17\nEpoch : 1, batch 1162\n(Train) Batch 1162 Loss : 3.82324481010437, 맞은 개수 : 24\nEpoch : 1, batch 1163\n(Train) Batch 1163 Loss : 4.344463348388672, 맞은 개수 : 13\nEpoch : 1, batch 1164\n(Train) Batch 1164 Loss : 3.9195191860198975, 맞은 개수 : 16\nEpoch : 1, batch 1165\n(Train) Batch 1165 Loss : 3.950809955596924, 맞은 개수 : 18\nEpoch : 1, batch 1166\n(Train) Batch 1166 Loss : 4.102197170257568, 맞은 개수 : 11\nEpoch : 1, batch 1167\n(Train) Batch 1167 Loss : 3.7158257961273193, 맞은 개수 : 25\nEpoch : 1, batch 1168\n(Train) Batch 1168 Loss : 3.812244415283203, 맞은 개수 : 17\nEpoch : 1, batch 1169\n(Train) Batch 1169 Loss : 3.9637389183044434, 맞은 개수 : 20\nEpoch : 1, batch 1170\n(Train) Batch 1170 Loss : 4.0556864738464355, 맞은 개수 : 13\nEpoch : 1, batch 1171\n(Train) Batch 1171 Loss : 4.037988185882568, 맞은 개수 : 18\nEpoch : 1, batch 1172\n(Train) Batch 1172 Loss : 4.028817176818848, 맞은 개수 : 12\nEpoch : 1, batch 1173\n(Train) Batch 1173 Loss : 3.969481945037842, 맞은 개수 : 15\nEpoch : 1, batch 1174\n(Train) Batch 1174 Loss : 4.0592427253723145, 맞은 개수 : 11\nEpoch : 1, batch 1175\n(Train) Batch 1175 Loss : 3.881972074508667, 맞은 개수 : 20\nEpoch : 1, batch 1176\n(Train) Batch 1176 Loss : 3.8214073181152344, 맞은 개수 : 23\nEpoch : 1, batch 1177\n(Train) Batch 1177 Loss : 3.945277214050293, 맞은 개수 : 18\nEpoch : 1, batch 1178\n(Train) Batch 1178 Loss : 4.172397136688232, 맞은 개수 : 14\nEpoch : 1, batch 1179\n(Train) Batch 1179 Loss : 4.334453105926514, 맞은 개수 : 17\nEpoch : 1, batch 1180\n(Train) Batch 1180 Loss : 4.20471715927124, 맞은 개수 : 14\nEpoch : 1, batch 1181\n(Train) Batch 1181 Loss : 3.8114547729492188, 맞은 개수 : 22\nEpoch : 1, batch 1182\n(Train) Batch 1182 Loss : 4.04710578918457, 맞은 개수 : 20\nEpoch : 1, batch 1183\n(Train) Batch 1183 Loss : 3.96030592918396, 맞은 개수 : 13\nEpoch : 1, batch 1184\n(Train) Batch 1184 Loss : 4.032403945922852, 맞은 개수 : 18\nEpoch : 1, batch 1185\n(Train) Batch 1185 Loss : 3.9333958625793457, 맞은 개수 : 22\nEpoch : 1, batch 1186\n(Train) Batch 1186 Loss : 3.9645798206329346, 맞은 개수 : 19\nEpoch : 1, batch 1187\n(Train) Batch 1187 Loss : 3.7441999912261963, 맞은 개수 : 27\nEpoch : 1, batch 1188\n(Train) Batch 1188 Loss : 3.960817337036133, 맞은 개수 : 22\nEpoch : 1, batch 1189\n(Train) Batch 1189 Loss : 3.954143524169922, 맞은 개수 : 19\nEpoch : 1, batch 1190\n(Train) Batch 1190 Loss : 4.2196455001831055, 맞은 개수 : 16\nEpoch : 1, batch 1191\n(Train) Batch 1191 Loss : 3.882699489593506, 맞은 개수 : 23\nEpoch : 1, batch 1192\n(Train) Batch 1192 Loss : 3.9245693683624268, 맞은 개수 : 12\nEpoch : 1, batch 1193\n(Train) Batch 1193 Loss : 4.016667366027832, 맞은 개수 : 16\nEpoch : 1, batch 1194\n(Train) Batch 1194 Loss : 4.115572452545166, 맞은 개수 : 13\nEpoch : 1, batch 1195\n(Train) Batch 1195 Loss : 3.6210672855377197, 맞은 개수 : 24\nEpoch : 1, batch 1196\n(Train) Batch 1196 Loss : 4.110978603363037, 맞은 개수 : 14\nEpoch : 1, batch 1197\n(Train) Batch 1197 Loss : 4.004062175750732, 맞은 개수 : 20\nEpoch : 1, batch 1198\n(Train) Batch 1198 Loss : 3.822279214859009, 맞은 개수 : 23\nEpoch : 1, batch 1199\n(Train) Batch 1199 Loss : 3.9863595962524414, 맞은 개수 : 16\nEpoch : 1, batch 1200\n(Train) Batch 1200 Loss : 3.878757953643799, 맞은 개수 : 20\nEpoch : 1, batch 1201\n(Train) Batch 1201 Loss : 4.037444114685059, 맞은 개수 : 20\nEpoch : 1, batch 1202\n(Train) Batch 1202 Loss : 4.006402492523193, 맞은 개수 : 11\nEpoch : 1, batch 1203\n(Train) Batch 1203 Loss : 3.9313180446624756, 맞은 개수 : 13\nEpoch : 1, batch 1204\n(Train) Batch 1204 Loss : 3.873270273208618, 맞은 개수 : 20\nEpoch : 1, batch 1205\n(Train) Batch 1205 Loss : 4.156735420227051, 맞은 개수 : 15\nEpoch : 1, batch 1206\n(Train) Batch 1206 Loss : 3.8142294883728027, 맞은 개수 : 25\nEpoch : 1, batch 1207\n(Train) Batch 1207 Loss : 3.9884872436523438, 맞은 개수 : 16\nEpoch : 1, batch 1208\n(Train) Batch 1208 Loss : 4.0277533531188965, 맞은 개수 : 18\nEpoch : 1, batch 1209\n(Train) Batch 1209 Loss : 3.965959310531616, 맞은 개수 : 19\nEpoch : 1, batch 1210\n(Train) Batch 1210 Loss : 4.1262407302856445, 맞은 개수 : 15\nEpoch : 1, batch 1211\n(Train) Batch 1211 Loss : 3.9671828746795654, 맞은 개수 : 21\nEpoch : 1, batch 1212\n(Train) Batch 1212 Loss : 3.9730935096740723, 맞은 개수 : 24\nEpoch : 1, batch 1213\n(Train) Batch 1213 Loss : 3.734600782394409, 맞은 개수 : 28\nEpoch : 1, batch 1214\n(Train) Batch 1214 Loss : 4.172285079956055, 맞은 개수 : 14\nEpoch : 1, batch 1215\n(Train) Batch 1215 Loss : 4.040274143218994, 맞은 개수 : 18\nEpoch : 1, batch 1216\n(Train) Batch 1216 Loss : 3.996204376220703, 맞은 개수 : 18\nEpoch : 1, batch 1217\n(Train) Batch 1217 Loss : 3.842482805252075, 맞은 개수 : 23\nEpoch : 1, batch 1218\n(Train) Batch 1218 Loss : 4.079507350921631, 맞은 개수 : 11\nEpoch : 1, batch 1219\n(Train) Batch 1219 Loss : 3.9423701763153076, 맞은 개수 : 10\nEpoch : 1, batch 1220\n(Train) Batch 1220 Loss : 3.8297922611236572, 맞은 개수 : 23\nEpoch : 1, batch 1221\n(Train) Batch 1221 Loss : 4.0860185623168945, 맞은 개수 : 20\nEpoch : 1, batch 1222\n(Train) Batch 1222 Loss : 3.942244291305542, 맞은 개수 : 24\nEpoch : 1, batch 1223\n(Train) Batch 1223 Loss : 4.038750171661377, 맞은 개수 : 22\nEpoch : 1, batch 1224\n(Train) Batch 1224 Loss : 3.984805107116699, 맞은 개수 : 15\nEpoch : 1, batch 1225\n(Train) Batch 1225 Loss : 3.8667490482330322, 맞은 개수 : 21\nEpoch : 1, batch 1226\n(Train) Batch 1226 Loss : 3.9451444149017334, 맞은 개수 : 21\nEpoch : 1, batch 1227\n(Train) Batch 1227 Loss : 4.076064586639404, 맞은 개수 : 13\nEpoch : 1, batch 1228\n(Train) Batch 1228 Loss : 4.035360336303711, 맞은 개수 : 17\nEpoch : 1, batch 1229\n(Train) Batch 1229 Loss : 4.078836917877197, 맞은 개수 : 10\nEpoch : 1, batch 1230\n(Train) Batch 1230 Loss : 4.04673957824707, 맞은 개수 : 23\nEpoch : 1, batch 1231\n(Train) Batch 1231 Loss : 4.048978805541992, 맞은 개수 : 20\nEpoch : 1, batch 1232\n(Train) Batch 1232 Loss : 3.937971591949463, 맞은 개수 : 22\nEpoch : 1, batch 1233\n(Train) Batch 1233 Loss : 4.011788368225098, 맞은 개수 : 18\nEpoch : 1, batch 1234\n(Train) Batch 1234 Loss : 3.6935040950775146, 맞은 개수 : 24\nEpoch : 1, batch 1235\n(Train) Batch 1235 Loss : 3.990053176879883, 맞은 개수 : 17\nEpoch : 1, batch 1236\n(Train) Batch 1236 Loss : 4.089309215545654, 맞은 개수 : 10\nEpoch : 1, batch 1237\n(Train) Batch 1237 Loss : 4.017725944519043, 맞은 개수 : 17\nEpoch : 1, batch 1238\n(Train) Batch 1238 Loss : 3.859330892562866, 맞은 개수 : 23\nEpoch : 1, batch 1239\n(Train) Batch 1239 Loss : 3.951439380645752, 맞은 개수 : 15\nEpoch : 1, batch 1240\n(Train) Batch 1240 Loss : 3.702381134033203, 맞은 개수 : 26\nEpoch : 1, batch 1241\n(Train) Batch 1241 Loss : 4.1513447761535645, 맞은 개수 : 16\nEpoch : 1, batch 1242\n(Train) Batch 1242 Loss : 4.2039289474487305, 맞은 개수 : 13\nEpoch : 1, batch 1243\n(Train) Batch 1243 Loss : 3.855502128601074, 맞은 개수 : 20\nEpoch : 1, batch 1244\n(Train) Batch 1244 Loss : 3.9254214763641357, 맞은 개수 : 15\nEpoch : 1, batch 1245\n(Train) Batch 1245 Loss : 4.154666900634766, 맞은 개수 : 15\nEpoch : 1, batch 1246\n(Train) Batch 1246 Loss : 4.1429314613342285, 맞은 개수 : 20\nEpoch : 1, batch 1247\n(Train) Batch 1247 Loss : 3.8244996070861816, 맞은 개수 : 17\nEpoch : 1, batch 1248\n(Train) Batch 1248 Loss : 4.043061256408691, 맞은 개수 : 16\nEpoch : 1, batch 1249\n(Train) Batch 1249 Loss : 3.9241762161254883, 맞은 개수 : 21\nEpoch : 1, batch 1250\n(Train) Batch 1250 Loss : 3.8339219093322754, 맞은 개수 : 18\nEpoch : 1, batch 1251\n(Train) Batch 1251 Loss : 4.10277795791626, 맞은 개수 : 13\nEpoch : 1, batch 1252\n(Train) Batch 1252 Loss : 4.16041898727417, 맞은 개수 : 19\nEpoch : 1, batch 1253\n(Train) Batch 1253 Loss : 3.666250467300415, 맞은 개수 : 31\nEpoch : 1, batch 1254\n(Train) Batch 1254 Loss : 4.114419460296631, 맞은 개수 : 16\nEpoch : 1, batch 1255\n(Train) Batch 1255 Loss : 3.876326560974121, 맞은 개수 : 21\nEpoch : 1, batch 1256\n(Train) Batch 1256 Loss : 4.115076541900635, 맞은 개수 : 14\nEpoch : 1, batch 1257\n(Train) Batch 1257 Loss : 3.7798638343811035, 맞은 개수 : 18\nEpoch : 1, batch 1258\n(Train) Batch 1258 Loss : 3.9037721157073975, 맞은 개수 : 26\nEpoch : 1, batch 1259\n(Train) Batch 1259 Loss : 4.19078254699707, 맞은 개수 : 15\nEpoch : 1, batch 1260\n(Train) Batch 1260 Loss : 4.06231689453125, 맞은 개수 : 14\nEpoch : 1, batch 1261\n(Train) Batch 1261 Loss : 3.697374105453491, 맞은 개수 : 23\nEpoch : 1, batch 1262\n(Train) Batch 1262 Loss : 3.902635335922241, 맞은 개수 : 22\nEpoch : 1, batch 1263\n(Train) Batch 1263 Loss : 4.019538879394531, 맞은 개수 : 20\nEpoch : 1, batch 1264\n(Train) Batch 1264 Loss : 3.7085773944854736, 맞은 개수 : 22\nEpoch : 1, batch 1265\n(Train) Batch 1265 Loss : 3.9067840576171875, 맞은 개수 : 17\nEpoch : 1, batch 1266\n(Train) Batch 1266 Loss : 3.9852335453033447, 맞은 개수 : 17\nEpoch : 1, batch 1267\n(Train) Batch 1267 Loss : 3.9673919677734375, 맞은 개수 : 19\nEpoch : 1, batch 1268\n(Train) Batch 1268 Loss : 3.939687967300415, 맞은 개수 : 21\nEpoch : 1, batch 1269\n(Train) Batch 1269 Loss : 3.865139961242676, 맞은 개수 : 20\nEpoch : 1, batch 1270\n(Train) Batch 1270 Loss : 3.6684813499450684, 맞은 개수 : 31\nEpoch : 1, batch 1271\n(Train) Batch 1271 Loss : 4.246883869171143, 맞은 개수 : 15\nEpoch : 1, batch 1272\n(Train) Batch 1272 Loss : 3.619974136352539, 맞은 개수 : 18\nEpoch : 1, batch 1273\n(Train) Batch 1273 Loss : 3.924355983734131, 맞은 개수 : 17\nEpoch : 1, batch 1274\n(Train) Batch 1274 Loss : 3.791278839111328, 맞은 개수 : 19\nEpoch : 1, batch 1275\n(Train) Batch 1275 Loss : 3.8690025806427, 맞은 개수 : 22\nEpoch : 1, batch 1276\n(Train) Batch 1276 Loss : 4.099946022033691, 맞은 개수 : 12\nEpoch : 1, batch 1277\n(Train) Batch 1277 Loss : 3.71695876121521, 맞은 개수 : 21\nEpoch : 1, batch 1278\n(Train) Batch 1278 Loss : 3.940380334854126, 맞은 개수 : 14\nEpoch : 1, batch 1279\n(Train) Batch 1279 Loss : 3.889519691467285, 맞은 개수 : 20\nEpoch : 1, batch 1280\n(Train) Batch 1280 Loss : 3.9933371543884277, 맞은 개수 : 17\nEpoch : 1, batch 1281\n(Train) Batch 1281 Loss : 4.046835899353027, 맞은 개수 : 23\nEpoch : 1, batch 1282\n(Train) Batch 1282 Loss : 4.046238422393799, 맞은 개수 : 17\nEpoch : 1, batch 1283\n(Train) Batch 1283 Loss : 3.791483163833618, 맞은 개수 : 23\nEpoch : 1, batch 1284\n(Train) Batch 1284 Loss : 3.893749713897705, 맞은 개수 : 18\nEpoch : 1, batch 1285\n(Train) Batch 1285 Loss : 3.851327657699585, 맞은 개수 : 22\nEpoch : 1, batch 1286\n(Train) Batch 1286 Loss : 3.889019727706909, 맞은 개수 : 21\nEpoch : 1, batch 1287\n(Train) Batch 1287 Loss : 3.9225473403930664, 맞은 개수 : 17\nEpoch : 1, batch 1288\n(Train) Batch 1288 Loss : 3.996934413909912, 맞은 개수 : 18\nEpoch : 1, batch 1289\n(Train) Batch 1289 Loss : 3.6261353492736816, 맞은 개수 : 22\nEpoch : 1, batch 1290\n(Train) Batch 1290 Loss : 4.107741832733154, 맞은 개수 : 17\nEpoch : 1, batch 1291\n(Train) Batch 1291 Loss : 3.8115408420562744, 맞은 개수 : 16\nEpoch : 1, batch 1292\n(Train) Batch 1292 Loss : 4.031208038330078, 맞은 개수 : 25\nEpoch : 1, batch 1293\n(Train) Batch 1293 Loss : 3.9683079719543457, 맞은 개수 : 18\nEpoch : 1, batch 1294\n(Train) Batch 1294 Loss : 3.7062604427337646, 맞은 개수 : 26\nEpoch : 1, batch 1295\n(Train) Batch 1295 Loss : 3.9681503772735596, 맞은 개수 : 16\nEpoch : 1, batch 1296\n(Train) Batch 1296 Loss : 3.9789316654205322, 맞은 개수 : 23\nEpoch : 1, batch 1297\n(Train) Batch 1297 Loss : 3.822618007659912, 맞은 개수 : 20\nEpoch : 1, batch 1298\n(Train) Batch 1298 Loss : 3.8682987689971924, 맞은 개수 : 21\nEpoch : 1, batch 1299\n(Train) Batch 1299 Loss : 3.804640054702759, 맞은 개수 : 21\nEpoch : 1, batch 1300\n(Train) Batch 1300 Loss : 4.1260504722595215, 맞은 개수 : 18\nEpoch : 1, batch 1301\n(Train) Batch 1301 Loss : 3.9776315689086914, 맞은 개수 : 16\nEpoch : 1, batch 1302\n(Train) Batch 1302 Loss : 3.8961730003356934, 맞은 개수 : 15\nEpoch : 1, batch 1303\n(Train) Batch 1303 Loss : 3.9264090061187744, 맞은 개수 : 16\nEpoch : 1, batch 1304\n(Train) Batch 1304 Loss : 4.033585071563721, 맞은 개수 : 18\nEpoch : 1, batch 1305\n(Train) Batch 1305 Loss : 3.9820449352264404, 맞은 개수 : 14\nEpoch : 1, batch 1306\n(Train) Batch 1306 Loss : 4.049341201782227, 맞은 개수 : 18\nEpoch : 1, batch 1307\n(Train) Batch 1307 Loss : 3.8462367057800293, 맞은 개수 : 21\nEpoch : 1, batch 1308\n(Train) Batch 1308 Loss : 3.834289312362671, 맞은 개수 : 15\nEpoch : 1, batch 1309\n(Train) Batch 1309 Loss : 3.9475553035736084, 맞은 개수 : 17\nEpoch : 1, batch 1310\n(Train) Batch 1310 Loss : 4.313357830047607, 맞은 개수 : 13\nEpoch : 1, batch 1311\n(Train) Batch 1311 Loss : 3.9716477394104004, 맞은 개수 : 14\nEpoch : 1, batch 1312\n(Train) Batch 1312 Loss : 4.08343505859375, 맞은 개수 : 15\nEpoch : 1, batch 1313\n(Train) Batch 1313 Loss : 3.7039852142333984, 맞은 개수 : 28\nEpoch : 1, batch 1314\n(Train) Batch 1314 Loss : 3.9498233795166016, 맞은 개수 : 22\nEpoch : 1, batch 1315\n(Train) Batch 1315 Loss : 3.8055739402770996, 맞은 개수 : 27\nEpoch : 1, batch 1316\n(Train) Batch 1316 Loss : 3.9008500576019287, 맞은 개수 : 19\nEpoch : 1, batch 1317\n(Train) Batch 1317 Loss : 3.9099700450897217, 맞은 개수 : 22\nEpoch : 1, batch 1318\n(Train) Batch 1318 Loss : 3.8012492656707764, 맞은 개수 : 24\nEpoch : 1, batch 1319\n(Train) Batch 1319 Loss : 3.910691022872925, 맞은 개수 : 24\nEpoch : 1, batch 1320\n(Train) Batch 1320 Loss : 3.7656729221343994, 맞은 개수 : 24\nEpoch : 1, batch 1321\n(Train) Batch 1321 Loss : 3.795428514480591, 맞은 개수 : 20\nEpoch : 1, batch 1322\n(Train) Batch 1322 Loss : 4.089565277099609, 맞은 개수 : 13\nEpoch : 1, batch 1323\n(Train) Batch 1323 Loss : 3.9867851734161377, 맞은 개수 : 16\nEpoch : 1, batch 1324\n(Train) Batch 1324 Loss : 3.7062337398529053, 맞은 개수 : 15\nEpoch : 1, batch 1325\n(Train) Batch 1325 Loss : 4.046214580535889, 맞은 개수 : 24\nEpoch : 1, batch 1326\n(Train) Batch 1326 Loss : 3.814706325531006, 맞은 개수 : 18\nEpoch : 1, batch 1327\n(Train) Batch 1327 Loss : 3.7735908031463623, 맞은 개수 : 23\nEpoch : 1, batch 1328\n(Train) Batch 1328 Loss : 4.189859867095947, 맞은 개수 : 15\nEpoch : 1, batch 1329\n(Train) Batch 1329 Loss : 4.06026029586792, 맞은 개수 : 18\nEpoch : 1, batch 1330\n(Train) Batch 1330 Loss : 3.630969762802124, 맞은 개수 : 18\nEpoch : 1, batch 1331\n(Train) Batch 1331 Loss : 3.8709750175476074, 맞은 개수 : 19\nEpoch : 1, batch 1332\n(Train) Batch 1332 Loss : 4.0907301902771, 맞은 개수 : 15\nEpoch : 1, batch 1333\n(Train) Batch 1333 Loss : 3.9961836338043213, 맞은 개수 : 17\nEpoch : 1, batch 1334\n(Train) Batch 1334 Loss : 3.7914135456085205, 맞은 개수 : 18\nEpoch : 1, batch 1335\n(Train) Batch 1335 Loss : 3.957167625427246, 맞은 개수 : 14\nEpoch : 1, batch 1336\n(Train) Batch 1336 Loss : 4.2942399978637695, 맞은 개수 : 11\nEpoch : 1, batch 1337\n(Train) Batch 1337 Loss : 4.133617401123047, 맞은 개수 : 19\nEpoch : 1, batch 1338\n(Train) Batch 1338 Loss : 3.9749112129211426, 맞은 개수 : 23\nEpoch : 1, batch 1339\n(Train) Batch 1339 Loss : 3.9627315998077393, 맞은 개수 : 22\nEpoch : 1, batch 1340\n(Train) Batch 1340 Loss : 3.838258743286133, 맞은 개수 : 21\nEpoch : 1, batch 1341\n(Train) Batch 1341 Loss : 3.8999760150909424, 맞은 개수 : 17\nEpoch : 1, batch 1342\n(Train) Batch 1342 Loss : 3.996603012084961, 맞은 개수 : 19\nEpoch : 1, batch 1343\n(Train) Batch 1343 Loss : 4.110109329223633, 맞은 개수 : 19\nEpoch : 1, batch 1344\n(Train) Batch 1344 Loss : 3.975682258605957, 맞은 개수 : 20\nEpoch : 1, batch 1345\n(Train) Batch 1345 Loss : 3.8787448406219482, 맞은 개수 : 22\nEpoch : 1, batch 1346\n(Train) Batch 1346 Loss : 3.949599266052246, 맞은 개수 : 16\nEpoch : 1, batch 1347\n(Train) Batch 1347 Loss : 4.020771503448486, 맞은 개수 : 18\nEpoch : 1, batch 1348\n(Train) Batch 1348 Loss : 3.7609713077545166, 맞은 개수 : 21\nEpoch : 1, batch 1349\n(Train) Batch 1349 Loss : 3.878835916519165, 맞은 개수 : 20\nEpoch : 1, batch 1350\n(Train) Batch 1350 Loss : 3.7142341136932373, 맞은 개수 : 23\nEpoch : 1, batch 1351\n(Train) Batch 1351 Loss : 4.06605339050293, 맞은 개수 : 16\nEpoch : 1, batch 1352\n(Train) Batch 1352 Loss : 3.8395259380340576, 맞은 개수 : 22\nEpoch : 1, batch 1353\n(Train) Batch 1353 Loss : 3.989656925201416, 맞은 개수 : 18\nEpoch : 1, batch 1354\n(Train) Batch 1354 Loss : 4.190229415893555, 맞은 개수 : 14\nEpoch : 1, batch 1355\n(Train) Batch 1355 Loss : 3.8226640224456787, 맞은 개수 : 22\nEpoch : 1, batch 1356\n(Train) Batch 1356 Loss : 4.066836357116699, 맞은 개수 : 19\nEpoch : 1, batch 1357\n(Train) Batch 1357 Loss : 3.7458996772766113, 맞은 개수 : 21\nEpoch : 1, batch 1358\n(Train) Batch 1358 Loss : 4.1430158615112305, 맞은 개수 : 15\nEpoch : 1, batch 1359\n(Train) Batch 1359 Loss : 3.754488945007324, 맞은 개수 : 22\nEpoch : 1, batch 1360\n(Train) Batch 1360 Loss : 3.5498459339141846, 맞은 개수 : 26\nEpoch : 1, batch 1361\n(Train) Batch 1361 Loss : 3.904679775238037, 맞은 개수 : 17\nEpoch : 1, batch 1362\n(Train) Batch 1362 Loss : 4.045659065246582, 맞은 개수 : 15\nEpoch : 1, batch 1363\n(Train) Batch 1363 Loss : 3.8400633335113525, 맞은 개수 : 19\nEpoch : 1, batch 1364\n(Train) Batch 1364 Loss : 3.748403549194336, 맞은 개수 : 20\nEpoch : 1, batch 1365\n(Train) Batch 1365 Loss : 3.6720926761627197, 맞은 개수 : 25\nEpoch : 1, batch 1366\n(Train) Batch 1366 Loss : 3.8724851608276367, 맞은 개수 : 27\nEpoch : 1, batch 1367\n(Train) Batch 1367 Loss : 4.080870151519775, 맞은 개수 : 16\nEpoch : 1, batch 1368\n(Train) Batch 1368 Loss : 3.9779698848724365, 맞은 개수 : 22\nEpoch : 1, batch 1369\n(Train) Batch 1369 Loss : 4.100226402282715, 맞은 개수 : 18\nEpoch : 1, batch 1370\n(Train) Batch 1370 Loss : 4.041308403015137, 맞은 개수 : 20\nEpoch : 1, batch 1371\n(Train) Batch 1371 Loss : 3.965574026107788, 맞은 개수 : 19\nEpoch : 1, batch 1372\n(Train) Batch 1372 Loss : 3.868474006652832, 맞은 개수 : 22\nEpoch : 1, batch 1373\n(Train) Batch 1373 Loss : 3.9359560012817383, 맞은 개수 : 19\nEpoch : 1, batch 1374\n(Train) Batch 1374 Loss : 3.7536306381225586, 맞은 개수 : 18\nEpoch : 1, batch 1375\n(Train) Batch 1375 Loss : 3.7667086124420166, 맞은 개수 : 20\nEpoch : 1, batch 1376\n(Train) Batch 1376 Loss : 3.8685131072998047, 맞은 개수 : 21\nEpoch : 1, batch 1377\n(Train) Batch 1377 Loss : 3.6496734619140625, 맞은 개수 : 31\nEpoch : 1, batch 1378\n(Train) Batch 1378 Loss : 3.8754184246063232, 맞은 개수 : 15\nEpoch : 1, batch 1379\n(Train) Batch 1379 Loss : 4.013761520385742, 맞은 개수 : 15\nEpoch : 1, batch 1380\n(Train) Batch 1380 Loss : 3.881887674331665, 맞은 개수 : 16\nEpoch : 1, batch 1381\n(Train) Batch 1381 Loss : 4.00294303894043, 맞은 개수 : 23\nEpoch : 1, batch 1382\n(Train) Batch 1382 Loss : 4.039645671844482, 맞은 개수 : 19\nEpoch : 1, batch 1383\n(Train) Batch 1383 Loss : 4.001858234405518, 맞은 개수 : 24\nEpoch : 1, batch 1384\n(Train) Batch 1384 Loss : 3.599177122116089, 맞은 개수 : 31\nEpoch : 1, batch 1385\n(Train) Batch 1385 Loss : 3.956779956817627, 맞은 개수 : 14\nEpoch : 1, batch 1386\n(Train) Batch 1386 Loss : 3.863499879837036, 맞은 개수 : 26\nEpoch : 1, batch 1387\n(Train) Batch 1387 Loss : 3.664032459259033, 맞은 개수 : 23\nEpoch : 1, batch 1388\n(Train) Batch 1388 Loss : 4.350733757019043, 맞은 개수 : 12\nEpoch : 1, batch 1389\n(Train) Batch 1389 Loss : 3.833591938018799, 맞은 개수 : 18\nEpoch : 1, batch 1390\n(Train) Batch 1390 Loss : 3.9025542736053467, 맞은 개수 : 21\nEpoch : 1, batch 1391\n(Train) Batch 1391 Loss : 3.814479112625122, 맞은 개수 : 20\nEpoch : 1, batch 1392\n(Train) Batch 1392 Loss : 3.9895589351654053, 맞은 개수 : 20\nEpoch : 1, batch 1393\n(Train) Batch 1393 Loss : 3.9934847354888916, 맞은 개수 : 19\nEpoch : 1, batch 1394\n(Train) Batch 1394 Loss : 3.9513468742370605, 맞은 개수 : 14\nEpoch : 1, batch 1395\n(Train) Batch 1395 Loss : 4.1160993576049805, 맞은 개수 : 14\nEpoch : 1, batch 1396\n(Train) Batch 1396 Loss : 3.8717548847198486, 맞은 개수 : 22\nEpoch : 1, batch 1397\n(Train) Batch 1397 Loss : 3.818483829498291, 맞은 개수 : 20\nEpoch : 1, batch 1398\n(Train) Batch 1398 Loss : 4.002280235290527, 맞은 개수 : 21\nEpoch : 1, batch 1399\n(Train) Batch 1399 Loss : 3.8389732837677, 맞은 개수 : 23\nEpoch : 1, batch 1400\n(Train) Batch 1400 Loss : 3.9765682220458984, 맞은 개수 : 18\nEpoch : 1, batch 1401\n(Train) Batch 1401 Loss : 3.7155776023864746, 맞은 개수 : 26\nEpoch : 1, batch 1402\n(Train) Batch 1402 Loss : 3.922982692718506, 맞은 개수 : 23\nEpoch : 1, batch 1403\n(Train) Batch 1403 Loss : 3.748424530029297, 맞은 개수 : 17\nEpoch : 1, batch 1404\n(Train) Batch 1404 Loss : 4.160039901733398, 맞은 개수 : 17\nEpoch : 1, batch 1405\n(Train) Batch 1405 Loss : 3.881458044052124, 맞은 개수 : 28\nEpoch : 1, batch 1406\n(Train) Batch 1406 Loss : 3.798449993133545, 맞은 개수 : 14\nEpoch : 1, batch 1407\n(Train) Batch 1407 Loss : 3.7915751934051514, 맞은 개수 : 21\nEpoch : 1, batch 1408\n(Train) Batch 1408 Loss : 4.003702640533447, 맞은 개수 : 14\nEpoch : 1, batch 1409\n(Train) Batch 1409 Loss : 3.9219908714294434, 맞은 개수 : 13\nEpoch : 1, batch 1410\n(Train) Batch 1410 Loss : 4.0436015129089355, 맞은 개수 : 18\nEpoch : 1, batch 1411\n(Train) Batch 1411 Loss : 3.8524169921875, 맞은 개수 : 17\nEpoch : 1, batch 1412\n(Train) Batch 1412 Loss : 3.9261608123779297, 맞은 개수 : 25\nEpoch : 1, batch 1413\n(Train) Batch 1413 Loss : 4.031528949737549, 맞은 개수 : 13\nEpoch : 1, batch 1414\n(Train) Batch 1414 Loss : 4.020121097564697, 맞은 개수 : 16\nEpoch : 1, batch 1415\n(Train) Batch 1415 Loss : 4.042149066925049, 맞은 개수 : 25\nEpoch : 1, batch 1416\n(Train) Batch 1416 Loss : 4.010863780975342, 맞은 개수 : 20\nEpoch : 1, batch 1417\n(Train) Batch 1417 Loss : 3.820094347000122, 맞은 개수 : 22\nEpoch : 1, batch 1418\n(Train) Batch 1418 Loss : 4.144808292388916, 맞은 개수 : 12\nEpoch : 1, batch 1419\n(Train) Batch 1419 Loss : 3.7580678462982178, 맞은 개수 : 21\nEpoch : 1, batch 1420\n(Train) Batch 1420 Loss : 3.65464448928833, 맞은 개수 : 25\nEpoch : 1, batch 1421\n(Train) Batch 1421 Loss : 3.8989906311035156, 맞은 개수 : 23\nEpoch : 1, batch 1422\n(Train) Batch 1422 Loss : 3.919140338897705, 맞은 개수 : 17\nEpoch : 1, batch 1423\n(Train) Batch 1423 Loss : 3.9747190475463867, 맞은 개수 : 16\nEpoch : 1, batch 1424\n(Train) Batch 1424 Loss : 3.7392690181732178, 맞은 개수 : 28\nEpoch : 1, batch 1425\n(Train) Batch 1425 Loss : 3.774815320968628, 맞은 개수 : 30\nEpoch : 1, batch 1426\n(Train) Batch 1426 Loss : 3.7575252056121826, 맞은 개수 : 26\nEpoch : 1, batch 1427\n(Train) Batch 1427 Loss : 3.8345794677734375, 맞은 개수 : 23\nEpoch : 1, batch 1428\n(Train) Batch 1428 Loss : 3.972313404083252, 맞은 개수 : 19\nEpoch : 1, batch 1429\n(Train) Batch 1429 Loss : 4.044759273529053, 맞은 개수 : 16\nEpoch : 1, batch 1430\n(Train) Batch 1430 Loss : 4.017478942871094, 맞은 개수 : 15\nEpoch : 1, batch 1431\n(Train) Batch 1431 Loss : 3.919370651245117, 맞은 개수 : 17\nEpoch : 1, batch 1432\n(Train) Batch 1432 Loss : 3.979400157928467, 맞은 개수 : 21\nEpoch : 1, batch 1433\n(Train) Batch 1433 Loss : 3.887436866760254, 맞은 개수 : 23\nEpoch : 1, batch 1434\n(Train) Batch 1434 Loss : 3.760239362716675, 맞은 개수 : 23\nEpoch : 1, batch 1435\n(Train) Batch 1435 Loss : 3.7931838035583496, 맞은 개수 : 24\nEpoch : 1, batch 1436\n(Train) Batch 1436 Loss : 4.402065277099609, 맞은 개수 : 12\nEpoch : 1, batch 1437\n(Train) Batch 1437 Loss : 3.7010583877563477, 맞은 개수 : 18\nEpoch : 1, batch 1438\n(Train) Batch 1438 Loss : 3.791015148162842, 맞은 개수 : 22\nEpoch : 1, batch 1439\n(Train) Batch 1439 Loss : 3.914628744125366, 맞은 개수 : 19\nEpoch : 1, batch 1440\n(Train) Batch 1440 Loss : 3.9024109840393066, 맞은 개수 : 20\nEpoch : 1, batch 1441\n(Train) Batch 1441 Loss : 3.89829158782959, 맞은 개수 : 19\nEpoch : 1, batch 1442\n(Train) Batch 1442 Loss : 3.8732943534851074, 맞은 개수 : 15\nEpoch : 1, batch 1443\n(Train) Batch 1443 Loss : 3.6647186279296875, 맞은 개수 : 22\nEpoch : 1, batch 1444\n(Train) Batch 1444 Loss : 3.995626449584961, 맞은 개수 : 20\nEpoch : 1, batch 1445\n(Train) Batch 1445 Loss : 3.8556480407714844, 맞은 개수 : 21\nEpoch : 1, batch 1446\n(Train) Batch 1446 Loss : 4.1432366371154785, 맞은 개수 : 15\nEpoch : 1, batch 1447\n(Train) Batch 1447 Loss : 3.887024164199829, 맞은 개수 : 25\nEpoch : 1, batch 1448\n(Train) Batch 1448 Loss : 3.9007391929626465, 맞은 개수 : 21\nEpoch : 1, batch 1449\n(Train) Batch 1449 Loss : 3.764984130859375, 맞은 개수 : 22\nEpoch : 1, batch 1450\n(Train) Batch 1450 Loss : 3.914310932159424, 맞은 개수 : 23\nEpoch : 1, batch 1451\n(Train) Batch 1451 Loss : 3.8418476581573486, 맞은 개수 : 19\nEpoch : 1, batch 1452\n(Train) Batch 1452 Loss : 3.8939874172210693, 맞은 개수 : 20\nEpoch : 1, batch 1453\n(Train) Batch 1453 Loss : 3.7132134437561035, 맞은 개수 : 26\nEpoch : 1, batch 1454\n(Train) Batch 1454 Loss : 4.123185157775879, 맞은 개수 : 16\nEpoch : 1, batch 1455\n(Train) Batch 1455 Loss : 3.9187941551208496, 맞은 개수 : 19\nEpoch : 1, batch 1456\n(Train) Batch 1456 Loss : 3.758545160293579, 맞은 개수 : 21\nEpoch : 1, batch 1457\n(Train) Batch 1457 Loss : 3.6961653232574463, 맞은 개수 : 22\nEpoch : 1, batch 1458\n(Train) Batch 1458 Loss : 4.087968826293945, 맞은 개수 : 19\nEpoch : 1, batch 1459\n(Train) Batch 1459 Loss : 3.918639659881592, 맞은 개수 : 15\nEpoch : 1, batch 1460\n(Train) Batch 1460 Loss : 3.9855964183807373, 맞은 개수 : 19\nEpoch : 1, batch 1461\n(Train) Batch 1461 Loss : 3.636364459991455, 맞은 개수 : 28\nEpoch : 1, batch 1462\n(Train) Batch 1462 Loss : 3.8430469036102295, 맞은 개수 : 22\nEpoch : 1, batch 1463\n(Train) Batch 1463 Loss : 3.7404277324676514, 맞은 개수 : 20\nEpoch : 1, batch 1464\n(Train) Batch 1464 Loss : 3.998626947402954, 맞은 개수 : 13\nEpoch : 1, batch 1465\n(Train) Batch 1465 Loss : 3.9650535583496094, 맞은 개수 : 17\nEpoch : 1, batch 1466\n(Train) Batch 1466 Loss : 3.990267515182495, 맞은 개수 : 20\nEpoch : 1, batch 1467\n(Train) Batch 1467 Loss : 3.9965362548828125, 맞은 개수 : 20\nEpoch : 1, batch 1468\n(Train) Batch 1468 Loss : 3.727081537246704, 맞은 개수 : 18\nEpoch : 1, batch 1469\n(Train) Batch 1469 Loss : 3.6460134983062744, 맞은 개수 : 25\nEpoch : 1, batch 1470\n(Train) Batch 1470 Loss : 4.029392719268799, 맞은 개수 : 16\nEpoch : 1, batch 1471\n(Train) Batch 1471 Loss : 3.689986228942871, 맞은 개수 : 33\nEpoch : 1, batch 1472\n(Train) Batch 1472 Loss : 3.8111729621887207, 맞은 개수 : 28\nEpoch : 1, batch 1473\n(Train) Batch 1473 Loss : 3.580028533935547, 맞은 개수 : 28\nEpoch : 1, batch 1474\n(Train) Batch 1474 Loss : 4.009424686431885, 맞은 개수 : 14\nEpoch : 1, batch 1475\n(Train) Batch 1475 Loss : 3.8381335735321045, 맞은 개수 : 18\nEpoch : 1, batch 1476\n(Train) Batch 1476 Loss : 3.7916319370269775, 맞은 개수 : 20\nEpoch : 1, batch 1477\n(Train) Batch 1477 Loss : 3.8215394020080566, 맞은 개수 : 19\nEpoch : 1, batch 1478\n(Train) Batch 1478 Loss : 3.993727445602417, 맞은 개수 : 15\nEpoch : 1, batch 1479\n(Train) Batch 1479 Loss : 3.895233154296875, 맞은 개수 : 26\nEpoch : 1, batch 1480\n(Train) Batch 1480 Loss : 4.138291835784912, 맞은 개수 : 12\nEpoch : 1, batch 1481\n(Train) Batch 1481 Loss : 3.912405490875244, 맞은 개수 : 13\nEpoch : 1, batch 1482\n(Train) Batch 1482 Loss : 4.067877292633057, 맞은 개수 : 16\nEpoch : 1, batch 1483\n(Train) Batch 1483 Loss : 3.876566171646118, 맞은 개수 : 24\nEpoch : 1, batch 1484\n(Train) Batch 1484 Loss : 3.8079750537872314, 맞은 개수 : 21\nEpoch : 1, batch 1485\n(Train) Batch 1485 Loss : 4.023036479949951, 맞은 개수 : 12\nEpoch : 1, batch 1486\n(Train) Batch 1486 Loss : 3.7991628646850586, 맞은 개수 : 21\nEpoch : 1, batch 1487\n(Train) Batch 1487 Loss : 3.9158992767333984, 맞은 개수 : 19\nEpoch : 1, batch 1488\n(Train) Batch 1488 Loss : 3.837980270385742, 맞은 개수 : 15\nEpoch : 1, batch 1489\n(Train) Batch 1489 Loss : 3.9406073093414307, 맞은 개수 : 13\nEpoch : 1, batch 1490\n(Train) Batch 1490 Loss : 3.7552030086517334, 맞은 개수 : 29\nEpoch : 1, batch 1491\n(Train) Batch 1491 Loss : 3.900729179382324, 맞은 개수 : 21\nEpoch : 1, batch 1492\n(Train) Batch 1492 Loss : 3.7992300987243652, 맞은 개수 : 22\nEpoch : 1, batch 1493\n(Train) Batch 1493 Loss : 3.8434813022613525, 맞은 개수 : 20\nEpoch : 1, batch 1494\n(Train) Batch 1494 Loss : 3.656681776046753, 맞은 개수 : 24\nEpoch : 1, batch 1495\n(Train) Batch 1495 Loss : 4.111375331878662, 맞은 개수 : 14\nEpoch : 1, batch 1496\n(Train) Batch 1496 Loss : 3.5168111324310303, 맞은 개수 : 28\nEpoch : 1, batch 1497\n(Train) Batch 1497 Loss : 3.8786392211914062, 맞은 개수 : 24\nEpoch : 1, batch 1498\n(Train) Batch 1498 Loss : 3.802257776260376, 맞은 개수 : 21\nEpoch : 1, batch 1499\n(Train) Batch 1499 Loss : 3.688692569732666, 맞은 개수 : 28\nEpoch : 1, batch 1500\n(Train) Batch 1500 Loss : 3.5292983055114746, 맞은 개수 : 23\nEpoch : 1, batch 1501\n(Train) Batch 1501 Loss : 3.905949354171753, 맞은 개수 : 24\nEpoch : 1, batch 1502\n(Train) Batch 1502 Loss : 3.9966626167297363, 맞은 개수 : 17\nEpoch : 1, batch 1503\n(Train) Batch 1503 Loss : 3.98783540725708, 맞은 개수 : 17\nEpoch : 1, batch 1504\n(Train) Batch 1504 Loss : 3.887319564819336, 맞은 개수 : 20\nEpoch : 1, batch 1505\n(Train) Batch 1505 Loss : 3.751354932785034, 맞은 개수 : 21\nEpoch : 1, batch 1506\n(Train) Batch 1506 Loss : 3.5146422386169434, 맞은 개수 : 29\nEpoch : 1, batch 1507\n(Train) Batch 1507 Loss : 3.903998851776123, 맞은 개수 : 15\nEpoch : 1, batch 1508\n(Train) Batch 1508 Loss : 3.804352283477783, 맞은 개수 : 22\nEpoch : 1, batch 1509\n(Train) Batch 1509 Loss : 4.031100749969482, 맞은 개수 : 21\nEpoch : 1, batch 1510\n(Train) Batch 1510 Loss : 3.8567593097686768, 맞은 개수 : 20\nEpoch : 1, batch 1511\n(Train) Batch 1511 Loss : 3.7984251976013184, 맞은 개수 : 19\nEpoch : 1, batch 1512\n(Train) Batch 1512 Loss : 4.007061004638672, 맞은 개수 : 15\nEpoch : 1, batch 1513\n(Train) Batch 1513 Loss : 3.832631826400757, 맞은 개수 : 16\nEpoch : 1, batch 1514\n(Train) Batch 1514 Loss : 3.714266300201416, 맞은 개수 : 26\nEpoch : 1, batch 1515\n(Train) Batch 1515 Loss : 4.101594924926758, 맞은 개수 : 16\nEpoch : 1, batch 1516\n(Train) Batch 1516 Loss : 3.9453418254852295, 맞은 개수 : 19\nEpoch : 1, batch 1517\n(Train) Batch 1517 Loss : 3.622436046600342, 맞은 개수 : 25\nEpoch : 1, batch 1518\n(Train) Batch 1518 Loss : 3.7993319034576416, 맞은 개수 : 18\nEpoch : 1, batch 1519\n(Train) Batch 1519 Loss : 3.8132503032684326, 맞은 개수 : 20\nEpoch : 1, batch 1520\n(Train) Batch 1520 Loss : 3.5986480712890625, 맞은 개수 : 25\nEpoch : 1, batch 1521\n(Train) Batch 1521 Loss : 4.072956085205078, 맞은 개수 : 22\nEpoch : 1, batch 1522\n(Train) Batch 1522 Loss : 3.9022409915924072, 맞은 개수 : 24\nEpoch : 1, batch 1523\n(Train) Batch 1523 Loss : 3.725224733352661, 맞은 개수 : 24\nEpoch : 1, batch 1524\n(Train) Batch 1524 Loss : 3.9046730995178223, 맞은 개수 : 18\nEpoch : 1, batch 1525\n(Train) Batch 1525 Loss : 4.138886451721191, 맞은 개수 : 11\nEpoch : 1, batch 1526\n(Train) Batch 1526 Loss : 3.9345169067382812, 맞은 개수 : 19\nEpoch : 1, batch 1527\n(Train) Batch 1527 Loss : 3.7064526081085205, 맞은 개수 : 20\nEpoch : 1, batch 1528\n(Train) Batch 1528 Loss : 3.747070789337158, 맞은 개수 : 14\nEpoch : 1, batch 1529\n(Train) Batch 1529 Loss : 3.815018653869629, 맞은 개수 : 28\nEpoch : 1, batch 1530\n(Train) Batch 1530 Loss : 4.0329203605651855, 맞은 개수 : 19\nEpoch : 1, batch 1531\n(Train) Batch 1531 Loss : 3.620164632797241, 맞은 개수 : 26\nEpoch : 1, batch 1532\n(Train) Batch 1532 Loss : 3.637784719467163, 맞은 개수 : 25\nEpoch : 1, batch 1533\n(Train) Batch 1533 Loss : 3.8083131313323975, 맞은 개수 : 24\nEpoch : 1, batch 1534\n(Train) Batch 1534 Loss : 4.022536754608154, 맞은 개수 : 15\nEpoch : 1, batch 1535\n(Train) Batch 1535 Loss : 3.9866421222686768, 맞은 개수 : 19\nEpoch : 1, batch 1536\n(Train) Batch 1536 Loss : 3.580782651901245, 맞은 개수 : 26\nEpoch : 1, batch 1537\n(Train) Batch 1537 Loss : 3.9137704372406006, 맞은 개수 : 17\nEpoch : 1, batch 1538\n(Train) Batch 1538 Loss : 4.137341022491455, 맞은 개수 : 13\nEpoch : 1, batch 1539\n(Train) Batch 1539 Loss : 3.684854745864868, 맞은 개수 : 30\nEpoch : 1, batch 1540\n(Train) Batch 1540 Loss : 3.769514799118042, 맞은 개수 : 26\nEpoch : 1, batch 1541\n(Train) Batch 1541 Loss : 3.9119582176208496, 맞은 개수 : 19\nEpoch : 1, batch 1542\n(Train) Batch 1542 Loss : 4.085310459136963, 맞은 개수 : 14\nEpoch : 1, batch 1543\n(Train) Batch 1543 Loss : 3.582273006439209, 맞은 개수 : 25\nEpoch : 1, batch 1544\n(Train) Batch 1544 Loss : 3.846843957901001, 맞은 개수 : 18\nEpoch : 1, batch 1545\n(Train) Batch 1545 Loss : 3.7645509243011475, 맞은 개수 : 24\nEpoch : 1, batch 1546\n(Train) Batch 1546 Loss : 3.9315149784088135, 맞은 개수 : 18\nEpoch : 1, batch 1547\n(Train) Batch 1547 Loss : 3.7212042808532715, 맞은 개수 : 24\nEpoch : 1, batch 1548\n(Train) Batch 1548 Loss : 3.894801616668701, 맞은 개수 : 10\nEpoch : 1, batch 1549\n(Train) Batch 1549 Loss : 3.807210683822632, 맞은 개수 : 23\nEpoch : 1, batch 1550\n(Train) Batch 1550 Loss : 3.6578919887542725, 맞은 개수 : 26\nEpoch : 1, batch 1551\n(Train) Batch 1551 Loss : 3.758455991744995, 맞은 개수 : 21\nEpoch : 1, batch 1552\n(Train) Batch 1552 Loss : 3.778867483139038, 맞은 개수 : 21\nEpoch : 1, batch 1553\n(Train) Batch 1553 Loss : 3.7784311771392822, 맞은 개수 : 22\nEpoch : 1, batch 1554\n(Train) Batch 1554 Loss : 4.173864364624023, 맞은 개수 : 18\nEpoch : 1, batch 1555\n(Train) Batch 1555 Loss : 3.9494364261627197, 맞은 개수 : 22\nEpoch : 1, batch 1556\n(Train) Batch 1556 Loss : 3.5606844425201416, 맞은 개수 : 30\nEpoch : 1, batch 1557\n(Train) Batch 1557 Loss : 4.036920070648193, 맞은 개수 : 18\nEpoch : 1, batch 1558\n(Train) Batch 1558 Loss : 3.7297301292419434, 맞은 개수 : 23\nEpoch : 1, batch 1559\n(Train) Batch 1559 Loss : 3.8417418003082275, 맞은 개수 : 17\nEpoch : 1, batch 1560\n(Train) Batch 1560 Loss : 3.7486889362335205, 맞은 개수 : 27\nEpoch : 1, batch 1561\n(Train) Batch 1561 Loss : 3.8407013416290283, 맞은 개수 : 24\nEpoch : 1, batch 1562\n(Train) Batch 1562 Loss : 3.671135187149048, 맞은 개수 : 10\nepoch 1 Loss/Train :4.171926782593388 \nepoch 1 Accuracy/Train : 0.11725\nVAL : 예측라벨 : [193   0 193   0 174   0 148   0   0   0   0  18  47   0  13  39 188   0\n 187  13  20  36  39 192  83   0   0   0  42 186   0   0], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n(VAL) Batch 0 Loss : 2.4826724529266357, accuracy: 0.4375\nVAL : 예측라벨 : [  0   0   0  18 179 184   0   0   0 193   0   0  45   0  83 193 188 184\n   9  93  20   1   2   1   9   2   1   1 145  17  20 129], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n(VAL) Batch 1 Loss : 2.6321403980255127, accuracy: 0.40625\nVAL : 예측라벨 : [174   2   1  55   1 199   2  81 197   1   1  20 142  41   2   1  19 126\n   9   9  11 108  17 126  20   9   1   2   8   4   9  41], 정답 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n(VAL) Batch 2 Loss : 3.663973093032837, accuracy: 0.1875\nVAL : 예측라벨 : [  2 129   2   1   3   2   2   2 170 199  77   2 183 199  42   2   2 160\n  43  48 170  43   4  55  48   2 170 185   2  39   6   3], 정답 [1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n(VAL) Batch 3 Loss : 3.1730756759643555, accuracy: 0.28125\nVAL : 예측라벨 : [146   2   2  45  55   2   9 197 152 196  52 170   2   2  14 196   2  42\n  58  31 185   2 185  56 150  24   9 190   5 185   9   2], 정답 [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3]\n(VAL) Batch 4 Loss : 3.569714307785034, accuracy: 0.21875\nVAL : 예측라벨 : [ 42 114  43 118   2  56   2  89 117  56  47   2   2   2   6 169   6  14\n  30   2  91  34 190   9 185  39   2  24  14  34  55 192], 정답 [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n(VAL) Batch 5 Loss : 4.225742340087891, accuracy: 0.0\nVAL : 예측라벨 : [  9  39 191  56   0  39  58  31  52 116  42  92   2   4  14 152   2   4\n  42   4  48 162   6  23  22  69  48  81  17   2   2   4], 정답 [3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n(VAL) Batch 6 Loss : 3.7538094520568848, accuracy: 0.125\nVAL : 예측라벨 : [ 56 152   4   4  23   2  58  12  40   2 152   4 115   4  52 162  72  55\n   4  45   2 123   4  20   4   4  56 103 150  61  77   1], 정답 [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5]\n(VAL) Batch 7 Loss : 3.5807971954345703, accuracy: 0.25\nVAL : 예측라벨 : [136  93  44  55  12 183   5   9  56 191 116  77   2  56  27  11 105 185\n  58 118 120 174  25  56  52  26  31  26  61 152   2  36], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n(VAL) Batch 8 Loss : 4.5886077880859375, accuracy: 0.03125\nVAL : 예측라벨 : [ 12  91  52   4 196  55  41 164  89  12  76  30 104   9   8  78  31   6\n   2   6   6   6  57  14 190   4   6   6 183 109 105  56], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\n(VAL) Batch 9 Loss : 3.574815273284912, accuracy: 0.1875\nVAL : 예측라벨 : [ 30  19   6   6   9  57  83   6 192   6 117  30  11   6   6   8  47   6\n 192   9 182   6  57  57  41   2   6   6  55  56  40   9], 정답 [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7]\n(VAL) Batch 10 Loss : 3.509854316711426, accuracy: 0.3125\nVAL : 예측라벨 : [  9  91   9  41 193   9  14   9  56  52  77  91  13  34  14   8 174  41\n 193  61  17   9 117   9  31  41  27   9  84   8  72 183], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n(VAL) Batch 11 Loss : 4.649999141693115, accuracy: 0.0\nVAL : 예측라벨 : [  9 126 193   7   9   6 183   9  25  58 164  31  13 122  34  61 117  42\n  43  43  55   9   8   8   8 122   8   1 117  41   9   8], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n(VAL) Batch 12 Loss : 3.9534761905670166, accuracy: 0.1875\nVAL : 예측라벨 : [  8   8 141   8 117   1  55   9  21   8 128 101  57   8   8  43   9 117\n   8  43   8  11   8   8  27  56   8   8   8   9  43   8], 정답 [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n(VAL) Batch 13 Loss : 3.1085047721862793, accuracy: 0.4375\nVAL : 예측라벨 : [117   9  57  96  53   9   9  55  37  55 152   9   8 155 193   4  34  19\n   6  33   5   9  58  34   9  56   9  55  55   9   9  57], 정답 [8 8 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\n(VAL) Batch 14 Loss : 3.6720924377441406, accuracy: 0.25\nVAL : 예측라벨 : [ 55  24  56   9   8  60   9 152   9   9  91  26  56  24   9   9  12 190\n  98   9  36  19   9  87 118  39 169   8  39   8  77   1], 정답 [ 9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9 10 10 10 10\n 10 10 10 10 10 10 10 10]\n(VAL) Batch 15 Loss : 3.8305513858795166, accuracy: 0.21875\nVAL : 예측라벨 : [ 34 192   1 191  91  39  40  36 150  42  42  56 199  43 126 193  40   2\n  31  57  34  19 117  40  12  19  42   6  39   9 193 122], 정답 [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n 10 10 10 10 10 10 10 10]\n(VAL) Batch 16 Loss : 4.486303806304932, accuracy: 0.0\nVAL : 예측라벨 : [ 10   0 117  40   1  12  56  20 151  48 104  22  11  57  43 151 194  56\n  11  11   9  12  21  11  11   9  11   4   4  50  21  21], 정답 [10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n 11 11 11 11 11 11 11 11]\n(VAL) Batch 17 Loss : 3.5970394611358643, accuracy: 0.21875\nVAL : 예측라벨 : [ 11  48  48  85  56   2  12  11 151   2   4 117  12  37  30 117  11  55\n 164  27  11  71  69   0  12  55  12  55  31  12  12  12], 정답 [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n 12 12 12 12 12 12 12 12]\n(VAL) Batch 18 Loss : 3.3053371906280518, accuracy: 0.28125\nVAL : 예측라벨 : [ 12 196  34 199  12  29  21  55  12  55  12  26  12 124  12  60  55  54\n 129  21 105  12  12  56  55  42  55  85 128  12  12  55], 정답 [12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n 12 12 12 12 12 12 12 12]\n(VAL) Batch 19 Loss : 3.326547145843506, accuracy: 0.3125\nVAL : 예측라벨 : [ 54  47  47 157  12  34 105  12  26  22  13  91  13  60  23  13  13  13\n  23 196 187 183  13  23  13  17   0  13  23  23  13 196], 정답 [12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13 13 13 13 13\n 13 13 13 13 13 13 13 13]\n(VAL) Batch 20 Loss : 2.852900266647339, accuracy: 0.34375\nVAL : 예측라벨 : [ 13  13  13  13  13  23  83  13  92  23  13 182  23  13 148  13  13  13\n  69 148  13  13  13  23  13  13  13  13  12  14  14  14], 정답 [13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13\n 13 13 13 13 14 14 14 14]\n(VAL) Batch 21 Loss : 2.042011022567749, accuracy: 0.65625\nVAL : 예측라벨 : [  2 196 196  14 196  47 196 196  12 152 183  12  23   9 148 196 148 183\n 196 178 183 196   2   2 183  23  14 183 196  55  12 143], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n 14 14 14 14 14 14 14 14]\n(VAL) Batch 22 Loss : 3.322007417678833, accuracy: 0.0625\nVAL : 예측라벨 : [183   6 129 183  23 196 196   2 152  23  14 183 196  14 183 177 192  42\n 103  36 185   4 193  39 136  83  35  39  92  42   6  81], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15\n 15 15 15 15 15 15 15 15]\n(VAL) Batch 23 Loss : 3.622729539871216, accuracy: 0.0625\nVAL : 예측라벨 : [ 52  93 192 136   2  17  91 105  12  56  23  12   2  80 185 152  55  39\n  21   2  56   2  39  91 185   4  36  31 193  17 178  42], 정답 [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n 15 15 15 15 15 15 15 15]\n(VAL) Batch 24 Loss : 4.582955360412598, accuracy: 0.0\nVAL : 예측라벨 : [  4   5   2   8   4   2  19  61  37   4 176  72  24 184   2  45 192  12\n  24  32  17  36 185  61 192  30  41  58  83  16  20  47], 정답 [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n 16 16 16 16 16 16 16 16]\n(VAL) Batch 25 Loss : 4.183225631713867, accuracy: 0.03125\nVAL : 예측라벨 : [  4 152 185 192   2  99  12  58  34 193  15 197  56 185  55  45   4  13\n  26  38  17   1  47  31  47 118 193  17  55   4  17  58], 정답 [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 17 17 17 17 17 17\n 17 17 17 17 17 17 17 17]\n(VAL) Batch 26 Loss : 3.952141761779785, accuracy: 0.09375\nVAL : 예측라벨 : [ 58  24 196 196 182  68  34  21 190 187 183 190  82  19 174 150 138  34\n 190  17 178  11  21  17  18  18   2 185 168  58  79 185], 정답 [17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17\n 17 17 17 17 17 17 17 17]\n(VAL) Batch 27 Loss : 3.878904342651367, accuracy: 0.0625\nVAL : 예측라벨 : [ 26 136 190  17  82 190 177 190  60  18  82 123  44 136 190 181  47 184\n  36 180 186 189  82 104 136  18 189  18 189 189  55 177], 정답 [17 17 17 17 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18\n 18 18 18 18 18 18 18 18]\n(VAL) Batch 28 Loss : 3.6147940158843994, accuracy: 0.125\nVAL : 예측라벨 : [190  18   9 189   2 181 189  18  18  54  90  77   0  24 163  18 177  90\n 163 180 150 150  19  61  19   9  55 181  19   9  82  19], 정답 [18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 19 19\n 19 19 19 19 19 19 19 19]\n(VAL) Batch 29 Loss : 3.7946906089782715, accuracy: 0.25\nVAL : 예측라벨 : [  9  19  93 196   2 190  19  46  19  82  93  19  19 152   9  19  93 183\n  19  30  82  19   1 104  19   2  62 190  19  60 185 196], 정답 [19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19\n 19 19 19 19 19 19 19 19]\n(VAL) Batch 30 Loss : 3.433586597442627, accuracy: 0.3125\nVAL : 예측라벨 : [ 19  26   4  19   6  94  17  61 183  22 199  12 114   8 117  50   2   4\n  20  55  12  22  11  11  56  55  55  11 152 127   2   9], 정답 [19 19 19 19 19 19 19 19 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20\n 20 20 20 20 20 20 20 20]\n(VAL) Batch 31 Loss : 3.5923447608947754, accuracy: 0.09375\nVAL : 예측라벨 : [  2 143  22  11  55  43  77  20  52  55  20  55  40  21  20  55  11  55\n 199  34  36  21  27  50  11  11  38  58 193  23  27  21], 정답 [20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20\n 20 20 21 21 21 21 21 21]\n(VAL) Batch 32 Loss : 3.7698869705200195, accuracy: 0.125\nVAL : 예측라벨 : [ 42 105  77 198  23  21 170 141  43  21  22 106  21  21 105  21 110  23\n  55  21  55  21  26  21  21  55  21  21  21  21 154  21], 정답 [21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21\n 21 21 21 21 21 21 21 21]\n(VAL) Batch 33 Loss : 3.3393938541412354, accuracy: 0.4375\nVAL : 예측라벨 : [ 43 109  42  21  31 144 122  22  55  22  12  21  56  22  78  55  22 198\n  22 148  22  22  55  12  21  22  22 117 122  22  22  40], 정답 [21 21 21 21 21 21 21 21 21 21 21 21 22 22 22 22 22 22 22 22 22 22 22 22\n 22 22 22 22 22 22 22 22]\n(VAL) Batch 34 Loss : 3.148010730743408, accuracy: 0.34375\nVAL : 예측라벨 : [ 22  96  22 170  12  22  22  21  21  27  56  72 124  22  22   2 101  47\n  22  22   6  21  55  27  91  22  55  11 193  22  22  42], 정답 [22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22\n 22 22 22 22 22 22 23 23]\n(VAL) Batch 35 Loss : 3.0058157444000244, accuracy: 0.3125\nVAL : 예측라벨 : [ 23  23  23  23  23  53 195  23  23  23  31  23  23  23  22  23  47 196\n  50  23  23  23  23  23  23  23  23  23  55  22  22  24], 정답 [23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n 23 23 23 23 23 23 23 23]\n(VAL) Batch 36 Loss : 1.8055883646011353, accuracy: 0.65625\nVAL : 예측라벨 : [ 23  42  23  13  50  23  13  23  23  23 194  23  23  23  23  47  26  26\n  12  55  24  27  55  26  27  60  77  12  21  60  26  24], 정답 [23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 24 24 24 24 24 24 24 24\n 24 24 24 24 24 24 24 24]\n(VAL) Batch 37 Loss : 2.6034047603607178, accuracy: 0.375\nVAL : 예측라벨 : [ 12  92  54  26 105  24  27  31  31  90  24  24  24  31  48  24  28  26\n  60  55  31  69 105  12  25  20  55  38  21  27  56 181], 정답 [24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24\n 24 24 24 24 24 24 24 24]\n(VAL) Batch 38 Loss : 3.6926138401031494, accuracy: 0.15625\nVAL : 예측라벨 : [ 21  22  27 123  26  26 174  48  26  26  35  74  26 150  24  24  26 105\n 136  11  31 182  27  25  47  24  26  26  24  34  20  30], 정답 [24 24 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25\n 25 25 25 25 25 25 25 25]\n(VAL) Batch 39 Loss : 3.653123378753662, accuracy: 0.03125\nVAL : 예측라벨 : [199  25  58  26  27  54 199  53 104  58   9 163  24  27  28  24  65  53\n  26  58  26 185  26  26  26  26  26 181  58 182 199  26], 정답 [25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 26 26 26 26\n 26 26 26 26 26 26 26 26]\n(VAL) Batch 40 Loss : 3.5218827724456787, accuracy: 0.25\nVAL : 예측라벨 : [196  31  28  41 170  57  26  31  26  26  26  28  53  26  25  60  47 136\n  26  26 183 183  26  27 193  47  24  19  26  11  26  21], 정답 [26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26\n 26 26 26 26 26 26 26 26]\n(VAL) Batch 41 Loss : 3.276305913925171, accuracy: 0.3125\nVAL : 예측라벨 : [ 26  55  26  26  57  20  55  26  53  58  24  12  55  24  24  31 175  77\n  29  48 123 105  55  26  22  20  48  31  55  55  77  55], 정답 [26 26 26 26 26 26 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27\n 27 27 27 27 27 27 27 27]\n(VAL) Batch 42 Loss : 3.558602809906006, accuracy: 0.09375\nVAL : 예측라벨 : [114  11 114  63  55 199  56 152 179  55 183  55  24  28 105  54 114  26\n  21  26 161  27  26  55  26 199 127  57  56  26  71  28], 정답 [27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27\n 28 28 28 28 28 28 28 28]\n(VAL) Batch 43 Loss : 3.729694366455078, accuracy: 0.0625\nVAL : 예측라벨 : [ 28  28  12 191  20  55  27  20  60  56 143 185 143  27  55 152  43  21\n  54 114 150  28  33 147 180  28  24  48  56  14   7  56], 정답 [28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28\n 28 28 28 28 28 28 28 28]\n(VAL) Batch 44 Loss : 4.155547142028809, accuracy: 0.125\nVAL : 예측라벨 : [ 56  26  55  28  28  28  26  27  48  56  79  60  17  55  28  20  31  55\n 157  31   2  21 136 105  81  55  12  58  11  26  21  58], 정답 [28 28 28 28 28 28 28 28 28 28 29 29 29 29 29 29 29 29 29 29 29 29 29 29\n 29 29 29 29 29 29 29 29]\n(VAL) Batch 45 Loss : 3.78001070022583, accuracy: 0.09375\nVAL : 예측라벨 : [  2  26  43  55  34  55  31 105  33  55 183  12  21  77  56  57  26  22\n  31  53  56 118  57 143 183  17  34 114  55  24  41  27], 정답 [29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29\n 29 29 29 29 30 30 30 30]\n(VAL) Batch 46 Loss : 3.8315677642822266, accuracy: 0.0\nVAL : 예측라벨 : [ 27  20  55  25  55 106 105  60 193 117   9  26  55  61  55  74  30  31\n  56  25  27  31 150  26  11   9 105  31  30  12  27  77], 정답 [30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30\n 30 30 30 30 30 30 30 30]\n(VAL) Batch 47 Loss : 3.8126940727233887, accuracy: 0.0625\nVAL : 예측라벨 : [ 55  74  34   9  91 105 136  12  53  37  32  30  54   8 193  31  26   6\n  31  30 199  69  26  26  31  24  27  13  31  24  22   9], 정답 [30 30 30 30 30 30 30 30 30 30 30 30 30 30 31 31 31 31 31 31 31 31 31 31\n 31 31 31 31 31 31 31 31]\n(VAL) Batch 48 Loss : 3.616539239883423, accuracy: 0.15625\nVAL : 예측라벨 : [ 31  31  34  31  47  35  31  22  31  55  31  55  31  31  25  31  31  47\n  31  31  48  26  31  31  31 161  31  24 193  31  31  31], 정답 [31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31\n 31 31 31 31 31 31 31 31]\n(VAL) Batch 49 Loss : 2.759794235229492, accuracy: 0.59375\nVAL : 예측라벨 : [117  30  26  31  56  27  85  11 193 104 117  55  21  24  72  57  25   9\n  20 117  77  56  57 141 182  55  33  47  55  26  30  57], 정답 [32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32\n 32 32 32 32 32 32 32 32]\n(VAL) Batch 50 Loss : 4.3648834228515625, accuracy: 0.0\nVAL : 예측라벨 : [105  47 105  27  58 125  55 109  31 136 105  24  30  78  55  12 122 182\n  89  26  33  33  53  27  56  57 115  34  57  30   9  31], 정답 [32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 33 33 33 33 33 33\n 33 33 33 33 33 33 33 33]\n(VAL) Batch 51 Loss : 3.903733730316162, accuracy: 0.0625\nVAL : 예측라벨 : [127  56  53  50  26 105  26  26  56  56  21  34 152  12  32  12  11  26\n  26  54  56  17  56   4   9  31  26  26 196  31  34  47], 정답 [33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33\n 33 33 33 33 33 33 33 33]\n(VAL) Batch 52 Loss : 3.6255605220794678, accuracy: 0.0\nVAL : 예측라벨 : [ 31  57  58  53  56  34  31  57  57  26 183  33  34  53  33 152  57  11\n  11  33 183  53  34  54  34  34  44  34  26  34  55  55], 정답 [33 33 33 33 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34\n 34 34 34 34 34 34 34 34]\n(VAL) Batch 53 Loss : 3.3719005584716797, accuracy: 0.21875\nVAL : 예측라벨 : [ 26  52  56  47  26  57  35  34  34  31   2 182  26  55  91  55 192  12\n   9  34  26  34  54  58  50 133 185  57 157  54  35 196], 정답 [34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 35 35\n 35 35 35 35 35 35 35 35]\n(VAL) Batch 54 Loss : 3.4739997386932373, accuracy: 0.15625\nVAL : 예측라벨 : [ 55  34  11  55   2  35  15  11  58  34  35  50  53  81  26  58  23  33\n  35  35  50  34   2  50  34 170  55  50  61 191  55  31], 정답 [35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35\n 35 35 35 35 35 35 35 35]\n(VAL) Batch 55 Loss : 3.4497110843658447, accuracy: 0.125\nVAL : 예측라벨 : [ 58  58  58  57   9  26  56  57  43  36  36  36  39  36  36  36  43 184\n  83  38 163  39  36   9  42  36  26  43  39  36  36  36], 정답 [35 35 35 35 35 35 35 35 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n 36 36 36 36 36 36 36 36]\n(VAL) Batch 56 Loss : 2.531670093536377, accuracy: 0.34375\nVAL : 예측라벨 : [189  36 184  90  55  36  36  71  37  39  36  43 103  43  36  21 184  36\n 184 163  69  36  22 188  42  58 117   9   8   4  43  28], 정답 [36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n 36 36 37 37 37 37 37 37]\n(VAL) Batch 57 Loss : 3.341945171356201, accuracy: 0.21875\nVAL : 예측라벨 : [ 42 192  72  37   9  37 185  37  37  39  42  37  43  38  43 198   8  38\n  42  36  31  83   8  39  38  39 157 184  24   9  44  43], 정답 [37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37\n 37 37 37 37 37 37 37 37]\n(VAL) Batch 58 Loss : 3.3752262592315674, accuracy: 0.15625\nVAL : 예측라벨 : [ 39   8  15 185  37  42  43   9   9  15  37   8  45 184 184  38 188 181\n  36  50   2  60  60  53  38 184  38 188 179 178  37  60], 정답 [37 37 37 37 37 37 37 37 37 37 37 37 38 38 38 38 38 38 38 38 38 38 38 38\n 38 38 38 38 38 38 38 38]\n(VAL) Batch 59 Loss : 3.4536919593811035, accuracy: 0.15625\nVAL : 예측라벨 : [182 186  42  21 189 189  39  36 101  38  37  38  17  43  55  37  38  60\n  36  45  24 143  45 189  38  45  34  44  45  43  39  43], 정답 [38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38\n 38 38 38 38 38 38 39 39]\n(VAL) Batch 60 Loss : 3.631577730178833, accuracy: 0.15625\nVAL : 예측라벨 : [ 39  90  54  18   2  42  81  39  39  42  38   2 182  43  42  39  39 186\n   9  42  40  39  43  39   2   2   2  39  69  22  39  43], 정답 [39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39\n 39 39 39 39 39 39 39 39]\n(VAL) Batch 61 Loss : 3.0947835445404053, accuracy: 0.28125\nVAL : 예측라벨 : [117  42  39 199 105   2  39  39  42  39  39 198  42 105  39   8  39  42\n  41  45  39 105 189  42 199   4  22  42   2  24  39 152], 정답 [39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 40 40 40 40 40 40 40 40\n 40 40 40 40 40 40 40 40]\n(VAL) Batch 62 Loss : 3.1442043781280518, accuracy: 0.1875\nVAL : 예측라벨 : [ 24 126 184   9  31   2  31  99  31  24 170 109   0  69  43   2  55  12\n  26  39  79 184  56  26  73  39  39 184   2  42 183 105], 정답 [40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n 40 40 40 40 40 40 40 40]\n(VAL) Batch 63 Loss : 4.14263391494751, accuracy: 0.0\nVAL : 예측라벨 : [ 12 183  31  79   8  39  41  72 177  34   9 151  34 142  47  22  69   7\n  24 105  41 105  37   8 182  36  41  37  23   8  41  41], 정답 [40 40 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n 41 41 41 41 41 41 41 41]\n(VAL) Batch 64 Loss : 4.273782253265381, accuracy: 0.15625\nVAL : 예측라벨 : [ 26  43 146  55  22 192 188  44 160   9  39  89  24  31  42  79   9  54\n  39  21  42  42  42  24  43  20 185  43  45  47  50  42], 정답 [41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 42 42 42 42\n 42 42 42 42 42 42 42 42]\n(VAL) Batch 65 Loss : 4.4038896560668945, accuracy: 0.125\nVAL : 예측라벨 : [  2  42  42  45 183 184 105  79  55 185  21  42  42   2  42  24  14  42\n   2  42 193  42  39  42 109 193 198 186  11  42  42  39], 정답 [42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42\n 42 42 42 42 42 42 42 42]\n(VAL) Batch 66 Loss : 2.997735023498535, accuracy: 0.34375\nVAL : 예측라벨 : [ 22  42  21  42  42  42  39  37  43  39 105  39  21  43  36  43  43  39\n   6  36  42   6  39  43  39  43  36  24  24  21   2  39], 정답 [42 42 42 42 42 42 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43\n 43 43 43 43 43 43 43 43]\n(VAL) Batch 67 Loss : 3.0068912506103516, accuracy: 0.3125\nVAL : 예측라벨 : [ 39   1  43  43  39 152 184 148  43  36  43  43  45  36 163  42  43  42\n  45  39  42  43  39   6   1  36  44 152  39  43  44 142], 정답 [43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43\n 44 44 44 44 44 44 44 44]\n(VAL) Batch 68 Loss : 2.7431578636169434, accuracy: 0.28125\nVAL : 예측라벨 : [ 36  60 189  93  44 189 126  44  44  44  43 101  44  44  36 150  54  44\n  44   1  37  45  44  44  44  44   2 184  44  44   1 199], 정답 [44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44\n 44 44 44 44 44 44 44 44]\n(VAL) Batch 69 Loss : 2.653348207473755, accuracy: 0.4375\nVAL : 예측라벨 : [ 44  44 163 112  44  44 101  44  44 196 179  45  45   2  45 199  45 184\n  45 199 184  45  45  36  45 188  45 182 187  45  34  45], 정답 [44 44 44 44 44 44 44 44 44 44 45 45 45 45 45 45 45 45 45 45 45 45 45 45\n 45 45 45 45 45 45 45 45]\n(VAL) Batch 70 Loss : 2.281907081604004, accuracy: 0.53125\nVAL : 예측라벨 : [179   2  45 188  45  36  45  36  45 199  45  45  45 185 182 188  39  42\n  45  45  45  45  45  38  45 184 199  45 148 148 199  23], 정답 [45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45\n 45 45 45 45 46 46 46 46]\n(VAL) Batch 71 Loss : 2.3552393913269043, accuracy: 0.4375\nVAL : 예측라벨 : [183  58   2  26  50   4   6  56 104  31 196 183  14  56 190  27  25 152\n 196   9  91   6  34  14   4 185 183 148 167  55 152 196], 정답 [46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46\n 46 46 46 46 46 46 46 46]\n(VAL) Batch 72 Loss : 4.133401393890381, accuracy: 0.0\nVAL : 예측라벨 : [ 17   8  52  55 117  14   4   4  13 183   2 183  19 148  31  31  26  76\n 193  31  47 183 193  19  24 193  47  57  47  47  12 192], 정답 [46 46 46 46 46 46 46 46 46 46 46 46 46 46 47 47 47 47 47 47 47 47 47 47\n 47 47 47 47 47 47 47 47]\n(VAL) Batch 73 Loss : 3.538778781890869, accuracy: 0.125\nVAL : 예측라벨 : [ 47  41  47  31 195  30  18  77 190  27 193 193  61  32  21 193  31  31\n   9  26  55 192  47   9   6  47 181  12  24  34  83  26], 정답 [47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47\n 47 47 47 47 47 47 47 47]\n(VAL) Batch 74 Loss : 3.8422107696533203, accuracy: 0.125\nVAL : 예측라벨 : [ 55 143  56  55  26  55  42 181  26  11 170  52  12  56  55  54   2  54\n  60  57  47  52  48  33  50  55  14  50  48  57  12  55], 정답 [48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48\n 48 48 48 48 48 48 48 48]\n(VAL) Batch 75 Loss : 3.9281277656555176, accuracy: 0.0625\nVAL : 예측라벨 : [150  52 180  48  92  26  55  58  48  57   4  35 105  58  55  56 162  57\n  31  40  53  55  55 118   4  50  12  20  22  24  12 117], 정답 [48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 49 49 49 49 49 49\n 49 49 49 49 49 49 49 49]\n(VAL) Batch 76 Loss : 4.251183986663818, accuracy: 0.0625\nVAL : 예측라벨 : [ 49  50  55  55  27  50  55 108  54  53  57  12  28 111  54 105   9 121\n  31  55  55  31 111 105 194  66  21   2  44  50  54  55], 정답 [49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49\n 49 49 49 49 49 49 49 49]\n(VAL) Batch 77 Loss : 4.432949542999268, accuracy: 0.03125\nVAL : 예측라벨 : [ 29  48 150  50  55  55  57  50  50  33 196  55  55  55 143  58  55  35\n  35  35  55  50  34   9  52  50  50  54  52  57  56  21], 정답 [49 49 49 49 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n 50 50 50 50 50 50 50 50]\n(VAL) Batch 78 Loss : 3.1320748329162598, accuracy: 0.15625\nVAL : 예측라벨 : [ 54  50  57  50  54   9  57  55  50  55  57  55  50  55   4  54  57  26\n  57  26  21  55  53  31  50  35  56  57  31 170  85  53], 정답 [50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 51 51\n 51 51 51 51 51 51 51 51]\n(VAL) Batch 79 Loss : 3.3899130821228027, accuracy: 0.125\nVAL : 예측라벨 : [ 26  55  27  78  57  55   2  52  55  56 170  58  48  54  21  48  54  60\n 197  33 105  55  55  57  91  55  31  55  37  56   4  34], 정답 [51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51\n 51 51 51 51 51 51 51 51]\n(VAL) Batch 80 Loss : 4.123851299285889, accuracy: 0.0\nVAL : 예측라벨 : [ 96  56   4 152  12 157  33 150 136  52  56 193  11  24  26  52 120  34\n  52  52  48  26  27 185  52  43  28  56 113   4  57  57], 정답 [51 51 51 51 51 51 51 51 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52\n 52 52 52 52 52 52 52 52]\n(VAL) Batch 81 Loss : 3.8269155025482178, accuracy: 0.15625\nVAL : 예측라벨 : [ 53  76  27  57  12  52  77 183  52  52  52   9  52  48 198  48 125 157\n  52  52 199   7  57  34 143  28  12  57  57  24  57  75], 정답 [52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52\n 52 52 53 53 53 53 53 53]\n(VAL) Batch 82 Loss : 3.645883321762085, accuracy: 0.21875\nVAL : 예측라벨 : [ 53 170 170  53  57 136 143  56  71  57 111  57  21  56  57  52  53  34\n  12  69 195  26 143  60  53  26  57  53  53  34 163 106], 정답 [53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53\n 53 53 53 53 53 53 53 53]\n(VAL) Batch 83 Loss : 3.9775352478027344, accuracy: 0.1875\nVAL : 예측라벨 : [ 74  26  53 114  12  11 198  26 105  58  57  26  58  12  54  54  12  35\n  47  26  54  91  56  26  56  33  55  12  82  26  54  54], 정답 [53 53 53 53 53 53 53 53 53 53 53 53 54 54 54 54 54 54 54 54 54 54 54 54\n 54 54 54 54 54 54 54 54]\n(VAL) Batch 84 Loss : 3.2948720455169678, accuracy: 0.1875\nVAL : 예측라벨 : [ 26 183  26  55 147  56 152  34  55  58  24 199  24  45  55  60  55  55\n  20  56  81 101  55  24  31  50  55  57 136  54  55  55], 정답 [54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54\n 54 54 54 54 54 54 55 55]\n(VAL) Batch 85 Loss : 3.7493138313293457, accuracy: 0.09375\nVAL : 예측라벨 : [ 55  55  55  55 114  55 155  31  55  55  55  77  27  55  56  55  55  55\n  55  20  55  55  55  55  55 152  55  55  60  55  55 105], 정답 [55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55\n 55 55 55 55 55 55 55 55]\n(VAL) Batch 86 Loss : 2.366266965866089, accuracy: 0.6875\nVAL : 예측라벨 : [ 55 111  58  55  55 106 183  55  55   2 151  55 155  55  55  55 183  55\n  55  30  69  53  55  56  55  56  31  56  12  56  55  56], 정답 [55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 56 56 56 56 56 56 56 56\n 56 56 56 56 56 56 56 56]\n(VAL) Batch 87 Loss : 3.278223991394043, accuracy: 0.4375\nVAL : 예측라벨 : [ 55  12  40  56  23  54  31  34  39  56  26  55  12  20  34  26  26  55\n  24  12  56 183  56  71  56  34  56  55  34  56  56  31], 정답 [56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56\n 56 56 56 56 56 56 56 56]\n(VAL) Batch 88 Loss : 3.221068859100342, accuracy: 0.25\nVAL : 예측라벨 : [ 20 107  55  57  58  81   4  57 170  12  55  57  57  46  26  27  58  60\n  57 170  57  57 111   2 105  57  55  57  24  57  53  55], 정답 [56 56 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57\n 57 57 57 57 57 57 57 57]\n(VAL) Batch 89 Loss : 3.2892355918884277, accuracy: 0.3125\nVAL : 예측라벨 : [ 57   2  57  57  55 105  56  57  57   2  57 111  58  12  57  57  35  12\n  71  56 170  26  50  35  58  60  55  33  58  19  58  58], 정답 [57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 58 58 58 58\n 58 58 58 58 58 58 58 58]\n(VAL) Batch 90 Loss : 3.1199724674224854, accuracy: 0.375\nVAL : 예측라벨 : [ 55  55  55  58  58  58  58  24  58  58  50  55 138  27  55 136  52  58\n  12  55  58  58  58  58  58  58  47  58  47  58 181  58], 정답 [58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58\n 58 58 58 58 58 58 58 58]\n(VAL) Batch 91 Loss : 2.754837989807129, accuracy: 0.5\nVAL : 예측라벨 : [ 58  25  58  58  58  58 108  82 153 128  24  44 176  60 126  90  55  60\n  36 164 184  36  24 105  18 166 105  94 151  25 115 188], 정답 [58 58 58 58 58 58 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59\n 59 59 59 59 59 59 59 59]\n(VAL) Batch 92 Loss : 4.496536731719971, accuracy: 0.15625\nVAL : 예측라벨 : [150 150 178 190 123  44  36 176 142  78  37 184  61  96 111  78  79 147\n 189  57  18 193 126 126  21  55  60  60 167  21 105  68], 정답 [59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59\n 60 60 60 60 60 60 60 60]\n(VAL) Batch 93 Loss : 4.571942329406738, accuracy: 0.0625\nVAL : 예측라벨 : [ 21  60  60  55 143  60  83 105  60  77  60 169 105  55 105 150 138  60\n  77  60  72  60  55  60  60  60 141  60  24  60 109  60], 정답 [60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60\n 60 60 60 60 60 60 60 60]\n(VAL) Batch 94 Loss : 3.0653696060180664, accuracy: 0.4375\nVAL : 예측라벨 : [ 21  60  60 138  60 126  27  60  58  60  61 152 126  61 103  61 103  61\n 199 189  12  54  61  58 126  60  61 176 119 108 176  78], 정답 [60 60 60 60 60 60 60 60 60 60 61 61 61 61 61 61 61 61 61 61 61 61 61 61\n 61 61 61 61 61 61 61 61]\n(VAL) Batch 95 Loss : 3.221608877182007, accuracy: 0.34375\nVAL : 예측라벨 : [ 30  61 126 108  61  91 126   9  54  61  61  61   9  12 126 104  82  61\n 107  34 173 174  61  61 126  61  61  61  90  17  62  82], 정답 [61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61\n 61 61 61 61 62 62 62 62]\n(VAL) Batch 96 Loss : 3.2518746852874756, accuracy: 0.375\nVAL : 예측라벨 : [ 19 111  90  12 180 134 109  28  45  90  82 142 152  62  85 141 193  47\n 107  85 128 114 189  82  62 189  85 169 134 181  85  90], 정답 [62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62\n 62 62 62 62 62 62 62 62]\n(VAL) Batch 97 Loss : 4.3101654052734375, accuracy: 0.0625\nVAL : 예측라벨 : [146 152  55 105  28  85 169 179 191 169  90  55  63 146  63  21  63 104\n 155  60  63  90 187 163  77 138  60 107  60  55  28  90], 정답 [62 62 62 62 62 62 62 62 62 62 62 62 62 62 63 63 63 63 63 63 63 63 63 63\n 63 63 63 63 63 63 63 63]\n(VAL) Batch 98 Loss : 4.321245193481445, accuracy: 0.09375\nVAL : 예측라벨 : [ 55  85  85 127 134  21 193 128 150 129  60  55 155 106 155  77  47  21\n 127  24  63  37  36  87 116  21  60  55 133 165 193  53], 정답 [63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63\n 63 63 63 63 63 63 63 63]\n(VAL) Batch 99 Loss : 4.368721961975098, accuracy: 0.03125\nVAL : 예측라벨 : [ 24  61  69 154 106  55 128 182 193 105  55 110 127 108  78  55  72 108\n  28  22  21 105  34  99 136  55  31  64  32 193  96  92], 정답 [64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n 64 64 64 64 64 64 64 64]\n(VAL) Batch 100 Loss : 5.032466411590576, accuracy: 0.03125\nVAL : 예측라벨 : [ 42 109 126  42  55  87 114  95  55 176  48 109  21 150  61  65   9  24\n  55 126  60 181  82  54   4  12 164 136 155 117  57 127], 정답 [64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 65 65 65 65 65 65\n 65 65 65 65 65 65 65 65]\n(VAL) Batch 101 Loss : 4.918953895568848, accuracy: 0.0\nVAL : 예측라벨 : [167  60 150 134 103  81 150 117 151  73 119  31  60 136 107  24  18  84\n  97  65  81  60  98  65  24   2 189  29  28 129 111 127], 정답 [65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65\n 65 65 65 65 65 65 65 65]\n(VAL) Batch 102 Loss : 4.216207981109619, accuracy: 0.0625\nVAL : 예측라벨 : [163 105  93 105 170  49  98  53  73  26 115 196 143 115 116  84 108  66\n  12  24  71 118 162 152 162   4 170  96 197  66 170 170], 정답 [65 65 65 65 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66\n 66 66 66 66 66 66 66 66]\n(VAL) Batch 103 Loss : 4.186234951019287, accuracy: 0.0625\nVAL : 예측라벨 : [ 66 121  44  58   2   1  82  98 111   4 157 115 193 170 197 170 165  36\n 164 170   4 157 101 148  55 193 165  33  26 198  55  18], 정답 [66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 67 67\n 67 67 67 67 67 67 67 67]\n(VAL) Batch 104 Loss : 4.30521821975708, accuracy: 0.03125\nVAL : 예측라벨 : [141  31  24  26  72 110 150 193  57  31  78 141  77  67 167 190  23 109\n 136 152 193  55 143  82  81  67   6 141  86 133 147  12], 정답 [67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67\n 67 67 67 67 67 67 67 67]\n(VAL) Batch 105 Loss : 4.5525665283203125, accuracy: 0.0625\nVAL : 예측라벨 : [117 167  83  29 193 127  31  17 163   8  19  68  77   9  60 143 126  71\n   1 105  17 114  12  60 169 107  24  26  68  60  31  18], 정답 [67 67 67 67 67 67 67 67 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68\n 68 68 68 68 68 68 68 68]\n(VAL) Batch 106 Loss : 4.473134994506836, accuracy: 0.0625\nVAL : 예측라벨 : [127 106  60 126 143 189 169  24  24 111  68  69 105 171 112   9  75 127\n 143 105  74 193 143 143 163  93 193  40 136  72 136 179], 정답 [68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68\n 68 68 69 69 69 69 69 69]\n(VAL) Batch 107 Loss : 4.311646461486816, accuracy: 0.03125\nVAL : 예측라벨 : [ 69 193 120  28 182 116  36 123  72  83  83  69  42  69  24  11  21  20\n  11  77 105 144  11  23  81  74  41 140  69  69  69 179], 정답 [69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69\n 69 69 69 69 69 69 69 69]\n(VAL) Batch 108 Loss : 4.209991931915283, accuracy: 0.1875\nVAL : 예측라벨 : [ 69  69  71  18 181  31  42 193  71 105  23  47  81 194  70  87  70 133\n  65 103  84 133  81 116 107 133 116  55   2 133 197 116], 정답 [69 69 69 69 69 69 69 69 69 69 69 69 70 70 70 70 70 70 70 70 70 70 70 70\n 70 70 70 70 70 70 70 70]\n(VAL) Batch 109 Loss : 3.7489407062530518, accuracy: 0.125\nVAL : 예측라벨 : [ 94 116 133  94 166  94  91 116  81  94  70  84 154 133  94 116 102  94\n 122  82 116  92  81 133 117  89   2 150  94  81 197  71], 정답 [70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70\n 70 70 70 70 70 70 71 71]\n(VAL) Batch 110 Loss : 3.5682754516601562, accuracy: 0.0625\nVAL : 예측라벨 : [117 170  72  69  71  71 193  75 101 157  71  71  71  71  71 101 173  71\n 170 197 124  71 124 173  71  71  71 198 124  69  71  71], 정답 [71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71\n 71 71 71 71 71 71 71 71]\n(VAL) Batch 111 Loss : 2.6165103912353516, accuracy: 0.4375\nVAL : 예측라벨 : [157 101 197  96  71  71  71 124  22 197  66  71  11  47 124 194  72  72\n 190 183  72  72  90  23 180 184  90  20 183 184 123  77], 정답 [71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 72 72 72 72 72 72 72 72\n 72 72 72 72 72 72 72 72]\n(VAL) Batch 112 Loss : 3.6034398078918457, accuracy: 0.25\nVAL : 예측라벨 : [ 69  90  60  72  85  72  72  72 109  31  72  63 141  13  72 155  28  31\n   6 193 136 122  90 108 163 163 186 151 155  18  31  72], 정답 [72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72\n 72 72 72 72 72 72 72 72]\n(VAL) Batch 113 Loss : 3.6662750244140625, accuracy: 0.21875\nVAL : 예측라벨 : [ 72 184 111  83 117 101  86  28  83  27  42 135 125 169 109  60  73 180\n  73  98  60 128  21 184 178 120 169  77  43   6  76  99], 정답 [72 72 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73\n 73 73 73 73 73 73 73 73]\n(VAL) Batch 114 Loss : 4.29080057144165, accuracy: 0.09375\nVAL : 예측라벨 : [ 73 105  60 191  83 183  97 142 119  83 155 134  24  61  73 126   2  81\n 141  73  60  74 158 132  24  78 105  71  53  25 179  12], 정답 [73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 74 74 74 74\n 74 74 74 74 74 74 74 74]\n(VAL) Batch 115 Loss : 4.132147312164307, accuracy: 0.125\nVAL : 예측라벨 : [120  79  11  28  60  21 150  24  24  24  53  74 158  61 124  24 120  74\n 136  21 150  54  24  74 144 105  68 167  74  74 124  11], 정답 [74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74\n 74 74 74 74 74 74 74 74]\n(VAL) Batch 116 Loss : 3.9859061241149902, accuracy: 0.15625\nVAL : 예측라벨 : [ 74  74  21  12  26  60 170  77  21 174 105  28  25   9 193 124  11  27\n 173  60 170 163  31 165 193  24  60 127 156 194 156 147], 정답 [74 74 74 74 74 74 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75\n 75 75 75 75 75 75 75 75]\n(VAL) Batch 117 Loss : 4.183609962463379, accuracy: 0.0625\nVAL : 예측라벨 : [193  15 141 185  94 136  98  12  27 199  55  65  77  24  57  60  87 121\n  55  21  50 154 126  60  54   9 152  55 152  22  55 155], 정답 [75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75\n 76 76 76 76 76 76 76 76]\n(VAL) Batch 118 Loss : 4.825827121734619, accuracy: 0.0\nVAL : 예측라벨 : [173  12  98 189  34  60  20  58  56  22  20 155  27  11  24  68 121 111\n 199  55  31 114 105  55   4 173  42  28  81  77 199 104], 정답 [76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76\n 76 76 76 76 76 76 76 76]\n(VAL) Batch 119 Loss : 4.595451354980469, accuracy: 0.0\nVAL : 예측라벨 : [ 48  54  11 180  14 124  21  55  55 157 182  77  87  60  60  60  47  77\n  42  25  77 138  77  31  36  77  55 105  55  21 119 112], 정답 [76 76 76 76 76 76 76 76 76 76 77 77 77 77 77 77 77 77 77 77 77 77 77 77\n 77 77 77 77 77 77 77 77]\n(VAL) Batch 120 Loss : 3.923487901687622, accuracy: 0.15625\nVAL : 예측라벨 : [ 77 163 193  17  24 105  77  77  77 193 150  90  55  60  55 109  28  24\n 181  72  10 106 147 123  24  22  24  24  89  12  78  78], 정답 [77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77\n 77 77 77 77 78 78 78 78]\n(VAL) Batch 121 Loss : 3.8767476081848145, accuracy: 0.1875\nVAL : 예측라벨 : [ 78   4 170  31 165 119  55  69   4 152  14   6 151 183 157  78  78  87\n 155   6   4  78  78 110 152  14   2   6 152  78  78   4], 정답 [78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78\n 78 78 78 78 78 78 78 78]\n(VAL) Batch 122 Loss : 3.5387256145477295, accuracy: 0.21875\nVAL : 예측라벨 : [152 170  78  85  89  78  78 151  78  78  55  78  89  78   1  69 193  42\n  42   2  21  77 130 120 127 105  36 117  81   4 105 193], 정답 [78 78 78 78 78 78 78 78 78 78 78 78 78 78 79 79 79 79 79 79 79 79 79 79\n 79 79 79 79 79 79 79 79]\n(VAL) Batch 123 Loss : 3.9464309215545654, accuracy: 0.21875\nVAL : 예측라벨 : [126  36 105 189  18  24 182  31  24 141  55  91 117 136   0  82  12  31\n  36  56  39  69 155 109 193  18  71  27  31  73  60  42], 정답 [79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79\n 79 79 79 79 79 79 79 79]\n(VAL) Batch 124 Loss : 4.575176239013672, accuracy: 0.0\nVAL : 예측라벨 : [189 148 110 110  72  31  72  89 117  18  72  31 189 151 176  18  72  36\n 188 110 184  68  22  24  72  23 160 170 121  81 163 194], 정답 [80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80\n 80 80 80 80 80 80 80 80]\n(VAL) Batch 125 Loss : 4.838561534881592, accuracy: 0.0\nVAL : 예측라벨 : [ 81  28 189  72  52  82 169  72 192  24  89 193 157 183  83  55 193  60\n 133 142  81  81 116 151  87 133  81  81  81  81 116  55], 정답 [80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 81 81 81 81 81 81\n 81 81 81 81 81 81 81 81]\n(VAL) Batch 126 Loss : 3.817135810852051, accuracy: 0.1875\nVAL : 예측라벨 : [ 81  23  81   4  22  81 120  81 167  81  81  81  30 193  81  81  81   4\n  87 116 170  81  81 170  55  97 176  60  81 103  81  81], 정답 [81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81\n 81 81 81 81 81 81 81 81]\n(VAL) Batch 127 Loss : 3.0541892051696777, accuracy: 0.46875\nVAL : 예측라벨 : [ 81 116  81 119 189 182  82  82 147  61  82  34  82  82   9 191  82  24\n 108 190  98  18 115   9  82 103 109 127 169  82  82  82], 정답 [81 81 81 81 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82\n 82 82 82 82 82 82 82 82]\n(VAL) Batch 128 Loss : 3.2930452823638916, accuracy: 0.375\nVAL : 예측라벨 : [104 126 192  82 163  82  68  82  25  82 105  49  74  24   2  93 189  82\n   1 105 182  82  61 163 104  55 190 193 184  26 186 184], 정답 [82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 83 83\n 83 83 83 83 83 83 83 83]\n(VAL) Batch 129 Loss : 4.108688831329346, accuracy: 0.1875\nVAL : 예측라벨 : [136  24  24  93  13 161  21  24  21 193  83  83 123  24  24  24  83 191\n   0 163  83  83 193  54  83  18  25  83  83 184 178  24], 정답 [83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83\n 83 83 83 83 83 83 83 83]\n(VAL) Batch 130 Loss : 3.735626459121704, accuracy: 0.25\nVAL : 예측라벨 : [ 83  42 179  24   0  83  90 174  55  55  50  27 101   2 114  55  23 170\n  11 100  66   4  70  28  55 162  84 113 166  81  78  55], 정답 [83 83 83 83 83 83 83 83 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84\n 84 84 84 84 84 84 84 84]\n(VAL) Batch 131 Loss : 4.140150547027588, accuracy: 0.09375\nVAL : 예측라벨 : [  4  54 170  11  49 108 108  12  55  61 146   9 170  91 124  56 170  21\n  55   4 114  57 148  54  57  11 169 123   9  60 190  24], 정답 [84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84\n 84 84 85 85 85 85 85 85]\n(VAL) Batch 132 Loss : 4.523368835449219, accuracy: 0.0\nVAL : 예측라벨 : [ 85  31 189  19  24  85  12 105  85  90  34 105 169  60  14  55 196  85\n  85  85  75 169 186  27  72  72  85 172 105 111  55  23], 정답 [85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85\n 85 85 85 85 85 85 85 85]\n(VAL) Batch 133 Loss : 3.669442892074585, accuracy: 0.21875\nVAL : 예측라벨 : [ 27  60  60  85  54 189 120 155  85 199  60   9  87  23  24 138  63  60\n 163 110  86  12 155   8 155  60 140  90 105  58  86 152], 정답 [85 85 85 85 85 85 85 85 85 85 85 85 86 86 86 86 86 86 86 86 86 86 86 86\n 86 86 86 86 86 86 86 86]\n(VAL) Batch 134 Loss : 4.075539588928223, accuracy: 0.125\nVAL : 예측라벨 : [ 87  97 105  86 147 105 117 127  55 110  60 110 105 156  73  90  69 151\n  60 128 155 110  72 105 155 110  21 120  22 110  68 147], 정답 [86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86\n 86 86 86 86 86 86 87 87]\n(VAL) Batch 135 Loss : 3.9553186893463135, accuracy: 0.03125\nVAL : 예측라벨 : [ 87 122  87  77  87 121  65  87  87 141  90 166  87 117  97 117 155 133\n 193  87  87  55  89 117 170  78  87 104  97  31  97 174], 정답 [87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87\n 87 87 87 87 87 87 87 87]\n(VAL) Batch 136 Loss : 3.7678515911102295, accuracy: 0.28125\nVAL : 예측라벨 : [ 90 110  87  87  69 193 141  55 133 110  72  87  87  21 126 116  42 148\n 140  78 166 123 104 140  60 108 193 188  31  77  12 117], 정답 [87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 88 88 88 88 88 88 88 88\n 88 88 88 88 88 88 88 88]\n(VAL) Batch 137 Loss : 4.437117099761963, accuracy: 0.125\nVAL : 예측라벨 : [ 19   5  34  69 105 193 136 111  24   3   8  82 193  22  82  74  55   8\n  31 108 196  24 109  55   4 121  24 126  69  22  79 127], 정답 [88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88\n 88 88 88 88 88 88 88 88]\n(VAL) Batch 138 Loss : 4.999481678009033, accuracy: 0.0\nVAL : 예측라벨 : [193  11  72  89  42 193  89  89 181 110 181 181 193  65  78 123  61 193\n   0  24  56 192 190 104 151 110 104  24  41  89  78 190], 정답 [88 88 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89\n 89 89 89 89 89 89 89 89]\n(VAL) Batch 139 Loss : 4.280967712402344, accuracy: 0.125\nVAL : 예측라벨 : [193 136 193  89  89  89  30 193  23 190 190 190   2 190  97 193  67 150\n  92  25  24  60  18  90 186  90 189  90  90 169  90  82], 정답 [89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 90 90 90 90\n 90 90 90 90 90 90 90 90]\n(VAL) Batch 140 Loss : 3.504446506500244, accuracy: 0.25\nVAL : 예측라벨 : [ 90  90  90  90 112  85  60 138  68  60  31 134  90  90  90 189   6  90\n   6 163  90 128 169 120  72 184 169  82  90 189  90  60], 정답 [90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90\n 90 90 90 90 90 90 90 90]\n(VAL) Batch 141 Loss : 3.4672844409942627, accuracy: 0.34375\nVAL : 예측라벨 : [ 90  90 189  90  60 155  58  34  47  91   8  66  96  31  91  56  30  91\n  11  78  34 105  57  35  78  34  91 196  60  48 126  89], 정답 [90 90 90 90 90 90 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91\n 91 91 91 91 91 91 91 91]\n(VAL) Batch 142 Loss : 3.545570135116577, accuracy: 0.21875\nVAL : 예측라벨 : [ 12  56  96  34 165  34  91  35 190  60  24  91  48  14  22 183  91  24\n  58  12  28  12   6  31  31  60  87 147  24  55  77  97], 정답 [91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91\n 92 92 92 92 92 92 92 92]\n(VAL) Batch 143 Loss : 3.9761109352111816, accuracy: 0.09375\nVAL : 예측라벨 : [ 34  70   4  21 110 154 110 184  48  96  99 116 148 193 160 110  21   6\n  45   2  26 116  77  78  55  58  55  31  96  39  97  83], 정답 [92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92\n 92 92 92 92 92 92 92 92]\n(VAL) Batch 144 Loss : 5.037339687347412, accuracy: 0.0\nVAL : 예측라벨 : [116 104  27  97  55 103 127  30  81  55 169  82  93 126  93  93 181 196\n  93 150 146 115  93 196  82  93  17  93 126 176 191 150], 정답 [92 92 92 92 92 92 92 92 92 92 93 93 93 93 93 93 93 93 93 93 93 93 93 93\n 93 93 93 93 93 93 93 93]\n(VAL) Batch 145 Loss : 3.4977540969848633, accuracy: 0.21875\nVAL : 예측라벨 : [127  60 176 191  28  44 184  93  82 176 184  54  93  82  93  25  26 191\n 191 142   2  93  82  93  61  93 189  61 133 164  55 198], 정답 [93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93\n 93 93 93 93 94 94 94 94]\n(VAL) Batch 146 Loss : 3.7846035957336426, accuracy: 0.1875\nVAL : 예측라벨 : [121 133  94 133  81  87 116  60 115  94  94  94  94 133  94 153  70 156\n 154 115 115  87  94 133 116 116 133  99  81 133 116 146], 정답 [94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94\n 94 94 94 94 94 94 94 94]\n(VAL) Batch 147 Loss : 2.5136005878448486, accuracy: 0.21875\nVAL : 예측라벨 : [ 21  87  94 116  30  97 166  65  87  94  94 133  70 153 157 101 173 116\n 144   8 127  71 109  51  22 173 101 170  95  71   6 126], 정답 [94 94 94 94 94 94 94 94 94 94 94 94 94 94 95 95 95 95 95 95 95 95 95 95\n 95 95 95 95 95 95 95 95]\n(VAL) Batch 148 Loss : 3.9657540321350098, accuracy: 0.125\nVAL : 예측라벨 : [ 72  71 170 109  71  71 170  71  21 113 101  71  75  60  86 196 145 194\n  66  11 170 140 145 121  97  11 112  71 108 173  72 148], 정답 [95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95\n 95 95 95 95 95 95 95 95]\n(VAL) Batch 149 Loss : 4.404387474060059, accuracy: 0.0\nVAL : 예측라벨 : [ 26  78 108 197  96 170 197 170 197  22 197  31  96  56  53 157  87  55\n  22  87 198 197 170 108 198  96  55  66  96  27  84  57], 정답 [96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96\n 96 96 96 96 96 96 96 96]\n(VAL) Batch 150 Loss : 3.7730343341827393, accuracy: 0.125\nVAL : 예측라벨 : [ 55 167 197 197 177  31 198  55  69  96  96 152 139  41 104 170 170  23\n 193  97 156 134 116 136 185  11 107  97 146 147 121  24], 정답 [96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 97 97 97 97 97 97\n 97 97 97 97 97 97 97 97]\n(VAL) Batch 151 Loss : 4.129265785217285, accuracy: 0.125\nVAL : 예측라벨 : [121  89 193 116  97 110  97 143  72 113  97 147 136 155  87  65 193  65\n  89 147 181 110  55   2  89  11 117 121 147 144 133  21], 정답 [97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97\n 97 97 97 97 97 97 97 97]\n(VAL) Batch 152 Loss : 4.086492538452148, accuracy: 0.09375\nVAL : 예측라벨 : [146 110  24  65  82 118  97  87  98  42  77 190  98  98 128  98 117  21\n 193 117 192  26 106  60 174  98  97  60  55 176  75  98], 정답 [97 97 97 97 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98\n 98 98 98 98 98 98 98 98]\n(VAL) Batch 153 Loss : 4.8397369384765625, accuracy: 0.1875\nVAL : 예측라벨 : [ 84  97 150   7 150 150 142 144 111 151 103  55 126 142 147  61   9  77\n 104  89  49  98  47  87  55  55  18  24  87  99  39 123], 정답 [98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 99 99\n 99 99 99 99 99 99 99 99]\n(VAL) Batch 154 Loss : 4.375396251678467, accuracy: 0.0625\nVAL : 예측라벨 : [ 55  58  31  55  60  82 136 123 113 163  21  93   0  24 105 158 108  87\n 136  57  77 124 194  82  87 123 116 177  22  60 169 193], 정답 [99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99\n 99 99 99 99 99 99 99 99]\n(VAL) Batch 155 Loss : 4.539608478546143, accuracy: 0.0\nVAL : 예측라벨 : [ 69 126 193 186  24  55 145  82  60  79 105 105 123 136  55  31  60  50\n 155 193 136  24 193  54  55  24  60  22  24 141  31  26], 정답 [ 99  99  99  99  99  99  99  99 100 100 100 100 100 100 100 100 100 100\n 100 100 100 100 100 100 100 100 100 100 100 100 100 100]\n(VAL) Batch 156 Loss : 4.545617580413818, accuracy: 0.0\nVAL : 예측라벨 : [ 77 117 120 109  24 163  60  77 100  77  21  22 117  77  27  65  74  21\n 193  77  55  60 187 100 193  60 197 101 152  71 101  97], 정답 [100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100\n 100 100 100 100 100 100 100 100 101 101 101 101 101 101]\n(VAL) Batch 157 Loss : 4.078971862792969, accuracy: 0.125\nVAL : 예측라벨 : [124 101  71  71 101 101 115  71 101 101  42  22  71 117  71 101 115  22\n 101 173  57 117 101 101 101 101 101  72 111 101  71 146], 정답 [101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n 101 101 101 101 101 101 101 101 101 101 101 101 101 101]\n(VAL) Batch 158 Loss : 2.7634620666503906, accuracy: 0.40625\nVAL : 예측라벨 : [101 118 166 170 101 101 124 101 101 101 198 101 104  90   9  19 102 177\n  71 109  58 154 170 108  58  87 118  83 188  98 136 148], 정답 [101 101 101 101 101 101 101 101 101 101 101 101 102 102 102 102 102 102\n 102 102 102 102 102 102 102 102 102 102 102 102 102 102]\n(VAL) Batch 159 Loss : 3.9742839336395264, accuracy: 0.25\nVAL : 예측라벨 : [ 13  91  24 152  49  60  78  71  83  72  21   9  31 119  55  99  55  55\n  58 104  55  61 137 105 103  31 157 157 178  75 108 116], 정답 [102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102\n 102 102 102 102 102 102 102 102 102 102 102 102 103 103]\n(VAL) Batch 160 Loss : 4.690633296966553, accuracy: 0.0\nVAL : 예측라벨 : [ 23 103  61  87 103 103 108 103 103 103 170   2 103 108 108 103 103   1\n 103 103 115   5 196  96 142  12 157  61 154  97 103 103], 정답 [103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103\n 103 103 103 103 103 103 103 103 103 103 103 103 103 103]\n(VAL) Batch 161 Loss : 3.0799500942230225, accuracy: 0.40625\nVAL : 예측라벨 : [133 145  55 116 103 115 103 103 194 157 103  21   4 133 103  87  21 192\n 181 117 182  31 104 192 193 188 104 103 123 140 178 193], 정답 [103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 104 104\n 104 104 104 104 104 104 104 104 104 104 104 104 104 104]\n(VAL) Batch 162 Loss : 3.68097186088562, accuracy: 0.21875\nVAL : 예측라벨 : [ 48 104 104 104 104 151 123  69 187   9 104 104  24  21 191  21 184 186\n  34 136  12  31 104 147 192  14 192 193 174 104 136 192], 정답 [104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104\n 104 104 104 104 104 104 104 104 104 104 104 104 104 104]\n(VAL) Batch 163 Loss : 3.6531784534454346, accuracy: 0.25\nVAL : 예측라벨 : [163 133 105  55  27  91 105  58  31 105 105  31  55 150  69  60  31 105\n   9 123  55  90  55  56 169 105  24 144  12  21  24 105], 정답 [104 104 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n 105 105 105 105 105 105 105 105 105 105 105 105 105 105]\n(VAL) Batch 164 Loss : 3.4408209323883057, accuracy: 0.21875\nVAL : 예측라벨 : [ 30  55  54  24 105  82   9  60 105 105 105  31  55 105  53 124  55 155\n 127 105 138 150  60 105  55  55  77  55 120  63  61 122], 정답 [105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n 105 105 106 106 106 106 106 106 106 106 106 106 106 106]\n(VAL) Batch 165 Loss : 3.4819774627685547, accuracy: 0.1875\nVAL : 예측라벨 : [106 100  27  55  21  24  26 105 117  55  55  61  55  54  54  55 105 123\n 106   9 155 120 105 106 106  58  28 105  97  77  55  55], 정답 [106 106 106 106 106 106 106 106 106 106 106 106 106 106 106 106 106 106\n 106 106 106 106 106 106 106 106 106 106 106 106 106 106]\n(VAL) Batch 166 Loss : 3.945817470550537, accuracy: 0.125\nVAL : 예측라벨 : [163 105  60  65  69  55 127  90   9  65  90 133 167 188 107 107 107 111\n  11   2  60  94  82 107  56  82  55 107 170 166 107  21], 정답 [106 106 106 106 106 106 107 107 107 107 107 107 107 107 107 107 107 107\n 107 107 107 107 107 107 107 107 107 107 107 107 107 107]\n(VAL) Batch 167 Loss : 4.059963703155518, accuracy: 0.1875\nVAL : 예측라벨 : [107 162 107 107  43  70 107 107 117 107 116 117  55 116 121 145 107 107\n 116 107 108 107  36 153 105 170 114  55 108   9  52 107], 정답 [107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107\n 107 107 107 107 107 107 108 108 108 108 108 108 108 108]\n(VAL) Batch 168 Loss : 3.2081336975097656, accuracy: 0.34375\nVAL : 예측라벨 : [108 108 108  55  81 108 108 105 167 115 166   4  61 105  82 126 108 108\n  65 108  55 173 103 108  23 108  90  60  60 108 115 111], 정답 [108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108\n 108 108 108 108 108 108 108 108 108 108 108 108 108 108]\n(VAL) Batch 169 Loss : 3.2511422634124756, accuracy: 0.34375\nVAL : 예측라벨 : [108 108  89 108  58 133 154 103 108 108 193 109 105  77  77 140 105  79\n 117 181  31  83  24 105  73 106 110 109  24 120 130  60], 정답 [108 108 108 108 108 108 108 108 108 108 109 109 109 109 109 109 109 109\n 109 109 109 109 109 109 109 109 109 109 109 109 109 109]\n(VAL) Batch 170 Loss : 3.5510730743408203, accuracy: 0.21875\nVAL : 예측라벨 : [ 83 105 189  77 193 189 182 105  60  77  24 117 106  82 199 109  42  27\n  24  24 102 138  85  42  24 109  24 189  24  72  97 151], 정답 [109 109 109 109 109 109 109 109 109 109 109 109 109 109 109 109 109 109\n 109 109 109 109 109 109 109 109 109 109 110 110 110 110]\n(VAL) Batch 171 Loss : 4.209181308746338, accuracy: 0.0625\nVAL : 예측라벨 : [109  21 110  68 110  31  24  77 110  55  60  21  24 110 193  27  43  69\n 193 127 163  81 127 138  77 193 138  27 127   2 155  31], 정답 [110 110 110 110 110 110 110 110 110 110 110 110 110 110 110 110 110 110\n 110 110 110 110 110 110 110 110 110 110 110 110 110 110]\n(VAL) Batch 172 Loss : 4.096823215484619, accuracy: 0.125\nVAL : 예측라벨 : [151 193  31 117  55  99 110 110 155 187 163 193 105 117  74 129  93  60\n 111 148 111 111 111   1  65 170 111  12 167  68 128  82], 정답 [110 110 110 110 110 110 110 110 110 110 110 110 110 110 111 111 111 111\n 111 111 111 111 111 111 111 111 111 111 111 111 111 111]\n(VAL) Batch 173 Loss : 3.581254243850708, accuracy: 0.21875\nVAL : 예측라벨 : [ 60 140 111  60 108 111 111  56 108 167 167 111  12 111 167 167 111 111\n 155 114 111 111 111   9 176 111 114 167  71 119 111 128], 정답 [111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111\n 111 111 111 111 111 111 111 111 111 111 111 111 111 111]\n(VAL) Batch 174 Loss : 3.4191622734069824, accuracy: 0.375\nVAL : 예측라벨 : [ 90  58 120 111  82 189 143 118 120 136 105 189 105 134 105  90  60  45\n  24  12  90  90  77  60 163  21 124 112  90 101 105  60], 정답 [112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112\n 112 112 112 112 112 112 112 112 112 112 112 112 112 112]\n(VAL) Batch 175 Loss : 4.225273132324219, accuracy: 0.03125\nVAL : 예측라벨 : [105 176 176  26 169  77  21  55  60 189  90  60 105 106 105 163 134 134\n  98 186  47 193  81  42  83  40 126 113  40 149  77 189], 정답 [112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112\n 113 113 113 113 113 113 113 113 113 113 113 113 113 113]\n(VAL) Batch 176 Loss : 4.393163681030273, accuracy: 0.03125\nVAL : 예측라벨 : [144 193  26 113  31 123  89  83  77  24  41  24  69  38 193 126  24 192\n  99 113 105 120 193 187 192  93   8 189 178 190   6 189], 정답 [113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113\n 113 113 113 113 113 113 113 113 113 113 113 113 113 113]\n(VAL) Batch 177 Loss : 4.393849849700928, accuracy: 0.0625\nVAL : 예측라벨 : [104  31 189 109 118 162  60  82  39 118 114  55  17 114  90  21  55 139\n  55 114 111 184 180 139 189  60  94  93  70 152  94  20], 정답 [113 113 113 113 114 114 114 114 114 114 114 114 114 114 114 114 114 114\n 114 114 114 114 114 114 114 114 114 114 114 114 114 114]\n(VAL) Batch 178 Loss : 3.996561288833618, accuracy: 0.09375\nVAL : 예측라벨 : [ 55 107 114  65 114 107 164 114 152  55 143 114 118  60   2 114  58 170\n 163 111 152  55  94 115   6 115 115  69 115 115 115  24], 정답 [114 114 114 114 114 114 114 114 114 114 114 114 114 114 114 114 114 114\n 114 114 114 114 115 115 115 115 115 115 115 115 115 115]\n(VAL) Batch 179 Loss : 3.6448662281036377, accuracy: 0.34375\nVAL : 예측라벨 : [115 115 115  36 115 111 115 111  41 189 115 115 121 115 115 144 115 115\n 150 101 197  11 107  72 115  22 145 115 107 115 115 115], 정답 [115 115 115 115 115 115 115 115 115 115 115 115 115 115 115 115 115 115\n 115 115 115 115 115 115 115 115 115 115 115 115 115 115]\n(VAL) Batch 180 Loss : 2.531482696533203, accuracy: 0.5\nVAL : 예측라벨 : [115  18 115 115 145 115  96  82  55 123 116 110 116 197 166  60  23  81\n 154 116  81 170 166 115  77 120  75 133  60  65 104 194], 정답 [115 115 115 115 115 115 115 115 116 116 116 116 116 116 116 116 116 116\n 116 116 116 116 116 116 116 116 116 116 116 116 116 116]\n(VAL) Batch 181 Loss : 3.3411242961883545, accuracy: 0.21875\nVAL : 예측라벨 : [ 82 151  11 116  70  91 108 111 197  87 116 108 116  63 116  81 116 116\n 165 116 145 157 116  81 116 157  45 151 155  67 147 167], 정답 [116 116 116 116 116 116 116 116 116 116 116 116 116 116 116 116 116 116\n 116 116 116 116 116 116 116 116 117 117 117 117 117 117]\n(VAL) Batch 182 Loss : 3.247062921524048, accuracy: 0.28125\nVAL : 예측라벨 : [133 193 183  83 117 117  13 109 145  72 117 173 117 117  78 117 117  21\n 155 117 117  25   9 117  22  72  21 117 117 155 117 100], 정답 [117 117 117 117 117 117 117 117 117 117 117 117 117 117 117 117 117 117\n 117 117 117 117 117 117 117 117 117 117 117 117 117 117]\n(VAL) Batch 183 Loss : 3.2237682342529297, accuracy: 0.40625\nVAL : 예측라벨 : [173 127 117 117   0  85 117 130  20 141  11 155 173 170 118 170 170 118\n 118 118 152 118  71   1 118 118  43  55   1 164 118  21], 정답 [117 117 117 117 117 117 117 117 117 117 117 117 118 118 118 118 118 118\n 118 118 118 118 118 118 118 118 118 118 118 118 118 118]\n(VAL) Batch 184 Loss : 3.0153250694274902, accuracy: 0.34375\nVAL : 예측라벨 : [169  12 150 118 176  20 118 108 176 118 118  60 173 118 118  71  44 118\n 118 118 117 118 165 118 129 155 170 114  60  12  57 119], 정답 [118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118\n 118 118 118 118 118 118 118 118 118 118 118 118 119 119]\n(VAL) Batch 185 Loss : 2.9743170738220215, accuracy: 0.375\nVAL : 예측라벨 : [ 55  48  60  12 127  75  21 108  60  75 105  43  26  12  12  21  55  58\n 167  54  27 105  60  26  53 106  38  55  21 128  55 104], 정답 [119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119\n 119 119 119 119 119 119 119 119 119 119 119 119 119 119]\n(VAL) Batch 186 Loss : 4.213011741638184, accuracy: 0.0\nVAL : 예측라벨 : [118  60  12 129 126  81 170 105 120   9 163 157  60 124 173 139 127  24\n 105  68 105 120  58  60  79 120 143  83 120 156  60  85], 정답 [119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 120 120\n 120 120 120 120 120 120 120 120 120 120 120 120 120 120]\n(VAL) Batch 187 Loss : 3.935635566711426, accuracy: 0.09375\nVAL : 예측라벨 : [ 77  24  79 117  21 123  24  68  28 150  21 105 120 109  42 141  24 143\n  28  78  92  60  33  77 105  21 123  21  60  79 163  41], 정답 [120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120\n 120 120 120 120 120 120 120 120 120 120 120 120 120 120]\n(VAL) Batch 188 Loss : 4.092504501342773, accuracy: 0.03125\nVAL : 예측라벨 : [ 21 189 133  81 116 151 121  70 121  94 167 121 133 128 146 128  21 133\n 121 121 121 133  97 128 157 133 166 121 133  94  58 104], 정답 [120 120 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n 121 121 121 121 121 121 121 121 121 121 121 121 121 121]\n(VAL) Batch 189 Loss : 3.5023624897003174, accuracy: 0.21875\nVAL : 예측라벨 : [107 166 103 133 166 133 121  94  81 166 121 166  53 116 116 121  72  96\n 116 116  21   2 126  42 122  34  58 121 160  31  91  24], 정답 [121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n 121 121 122 122 122 122 122 122 122 122 122 122 122 122]\n(VAL) Batch 190 Loss : 3.974966049194336, accuracy: 0.125\nVAL : 예측라벨 : [ 69 124  53  40 137  12 101 113  27  25  21 115 113  42  12 109 122  30\n  69 108 122  34  72 122  55  91  31  31 105  22 106  69], 정답 [122 122 122 122 122 122 122 122 122 122 122 122 122 122 122 122 122 122\n 122 122 122 122 122 122 122 122 122 122 122 122 122 122]\n(VAL) Batch 191 Loss : 4.299442768096924, accuracy: 0.09375\nVAL : 예측라벨 : [193 105  19   9  55 101 105  77  21  36  21  24  60 117  26  60  12 143\n 123  47 117 106  26 110 110 143  60  73  60  82  30  63], 정답 [122 122 122 122 122 122 123 123 123 123 123 123 123 123 123 123 123 123\n 123 123 123 123 123 123 123 123 123 123 123 123 123 123]\n(VAL) Batch 192 Loss : 4.181491374969482, accuracy: 0.03125\nVAL : 예측라벨 : [156  26  24 163  31  60 105  60  24  38 105 193 123  55  24  72  27  24\n  26  12  55  24  24 138 124  23 124  24 124 109 124 109], 정답 [123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123\n 123 123 123 123 123 123 124 124 124 124 124 124 124 124]\n(VAL) Batch 193 Loss : 3.469944953918457, accuracy: 0.15625\nVAL : 예측라벨 : [ 79 125 109  71  66 124 170 124 105 124 101 109 170  71  24 124 124 124\n 124 170 157 124  77 103  71 198 124  71 124 124 157 105], 정답 [124 124 124 124 124 124 124 124 124 124 124 124 124 124 124 124 124 124\n 124 124 124 124 124 124 124 124 124 124 124 124 124 124]\n(VAL) Batch 194 Loss : 2.8251729011535645, accuracy: 0.34375\nVAL : 예측라벨 : [124 124 124 124  71  71  21  71  72  21  77  77 161 115  60  73 139  60\n 103 158 103 125  37   8  60 123 109  60  55  38 182 105], 정답 [124 124 124 124 124 124 124 124 124 124 125 125 125 125 125 125 125 125\n 125 125 125 125 125 125 125 125 125 125 125 125 125 125]\n(VAL) Batch 195 Loss : 3.7081143856048584, accuracy: 0.15625\nVAL : 예측라벨 : [163   4  54   8  77  39  58 105 116  83 196 187 150  61  95  58  97  60\n 105 148  60 123 116  79  77 163  24 105  61 167 120  55], 정답 [125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125\n 125 125 125 125 125 125 125 125 125 125 126 126 126 126]\n(VAL) Batch 196 Loss : 4.636107921600342, accuracy: 0.0\nVAL : 예측라벨 : [  9  61  12  12  61   1 126  12  61   9  56 126  73 108 108 105  57 105\n  34  30 108   7  61 193 185 105 134  57 169  24   5 126], 정답 [126 126 126 126 126 126 126 126 126 126 126 126 126 126 126 126 126 126\n 126 126 126 126 126 126 126 126 126 126 126 126 126 126]\n(VAL) Batch 197 Loss : 4.072361469268799, accuracy: 0.09375\nVAL : 예측라벨 : [ 89  60 126   6 136  82 126  61 126 126 126 165 105  82  55  21  69  61\n 147 117 117  21  78 139  55 127 147  89 110 197 110 189], 정답 [126 126 126 126 126 126 126 126 126 126 126 126 126 126 127 127 127 127\n 127 127 127 127 127 127 127 127 127 127 127 127 127 127]\n(VAL) Batch 198 Loss : 3.948122262954712, accuracy: 0.1875\nVAL : 예측라벨 : [ 55 143  55 184 109  21   2  22 127  87  55  31 143  40  87 105  87  21\n 121 193  55 194 160 140 198 155  24  63  21  85 109 151], 정답 [127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127\n 127 127 127 127 127 127 127 127 127 127 127 127 127 127]\n(VAL) Batch 199 Loss : 4.170989513397217, accuracy: 0.03125\nVAL : 예측라벨 : [ 63  60 155 116  24 111  63 163 196 128 165 105 155 127 114 157  60 128\n 155  55 128  81  55 127 163  55 111  77 105  72  60  77], 정답 [128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n 128 128 128 128 128 128 128 128 128 128 128 128 128 128]\n(VAL) Batch 200 Loss : 4.323274612426758, accuracy: 0.09375\nVAL : 예측라벨 : [127 109  60 155  60 128 143 139 167  60  55  55  43 146 105  60 105  76\n   2 177 152  12 184  55  66 152  12   4 152 105 116 152], 정답 [128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n 129 129 129 129 129 129 129 129 129 129 129 129 129 129]\n(VAL) Batch 201 Loss : 4.0010666847229, accuracy: 0.03125\nVAL : 예측라벨 : [178  21 127  81 109 152 198   2 178 152  26  21  47 170  22  22 105 183\n 134  27  71  21 105 108 129  55 176 157 152 119 118   2], 정답 [129 129 129 129 129 129 129 129 129 129 129 129 129 129 129 129 129 129\n 129 129 129 129 129 129 129 129 129 129 129 129 129 129]\n(VAL) Batch 202 Loss : 4.486298084259033, accuracy: 0.03125\nVAL : 예측라벨 : [146 157  57   4  69  90  31 179 163   0 123 143  24  90 139 187 193  83\n  23  81  72  18 117 151  21  24  42 163  36  31  69 136], 정답 [129 129 129 129 130 130 130 130 130 130 130 130 130 130 130 130 130 130\n 130 130 130 130 130 130 130 130 130 130 130 130 130 130]\n(VAL) Batch 203 Loss : 4.379342555999756, accuracy: 0.0\nVAL : 예측라벨 : [ 47 173  87 145  30 160 113  98 181  81 192 179 147  77  36   0 189 160\n  13 103 193 193  77 109  24  22  26  82  21  58 120 177], 정답 [130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130\n 130 130 130 130 131 131 131 131 131 131 131 131 131 131]\n(VAL) Batch 204 Loss : 4.653834342956543, accuracy: 0.0\nVAL : 예측라벨 : [ 55 123  60 152  24 163  24  90  22  77  26 134 122  83  77 123 193  83\n  83 183  60 127  58  22  54 196  77 150  69 193  24 109], 정답 [131 131 131 131 131 131 131 131 131 131 131 131 131 131 131 131 131 131\n 131 131 131 131 131 131 131 131 131 131 131 131 131 131]\n(VAL) Batch 205 Loss : 5.135335922241211, accuracy: 0.0\nVAL : 예측라벨 : [106 122  83 122  83 106  68  42  93  71 157  77 157  12 143  84 124  78\n 127  24  26  71 124 128 124 154 124  71  22 112  69 196], 정답 [131 131 131 131 131 131 131 131 132 132 132 132 132 132 132 132 132 132\n 132 132 132 132 132 132 132 132 132 132 132 132 132 132]\n(VAL) Batch 206 Loss : 4.696678161621094, accuracy: 0.0\nVAL : 예측라벨 : [ 22  71  24 109 169 111  60  86  12  26  71 120  17 138 124 170  94  12\n 105 170 124  79  42  24 129 163 133 133 116 133 133 107], 정답 [132 132 132 132 132 132 132 132 132 132 132 132 132 132 132 132 132 132\n 132 132 132 132 132 132 132 132 133 133 133 133 133 133]\n(VAL) Batch 207 Loss : 4.533517837524414, accuracy: 0.125\nVAL : 예측라벨 : [ 97  94 116  97 170 133 133 110 133 133 133 166 133 166 133 133 133 111\n 133 133  60 133 133 133 133 133 116 116 107 133 133 133], 정답 [133 133 133 133 133 133 133 133 133 133 133 133 133 133 133 133 133 133\n 133 133 133 133 133 133 133 133 133 133 133 133 133 133]\n(VAL) Batch 208 Loss : 2.0060250759124756, accuracy: 0.59375\nVAL : 예측라벨 : [133 133 133 116 133 133 133 133 133 116  81 133  47 134 190 105  60  60\n  54  31   9   7  85 189  18  25  85 158 134  60  61 160], 정답 [133 133 133 133 133 133 133 133 133 133 133 133 134 134 134 134 134 134\n 134 134 134 134 134 134 134 134 134 134 134 134 134 134]\n(VAL) Batch 209 Loss : 3.2497031688690186, accuracy: 0.34375\nVAL : 예측라벨 : [  4 134 105 189  72 134 129 134  90  85  76 189 106   1  72 134  56  85\n  14 134  58 105 152  60 155  13 134  93  61  45 183  90], 정답 [134 134 134 134 134 134 134 134 134 134 134 134 134 134 134 134 134 134\n 134 134 134 134 134 134 134 134 134 134 134 134 135 135]\n(VAL) Batch 210 Loss : 3.9669597148895264, accuracy: 0.1875\nVAL : 예측라벨 : [ 31  91 108 104 193  60 124  21  18 189 184 184 105  60 176  60  58 176\n   8  18 192  17 177 118  61 193 192 114  90  60  43  76], 정답 [135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135\n 135 135 135 135 135 135 135 135 135 135 135 135 135 135]\n(VAL) Batch 211 Loss : 5.409218788146973, accuracy: 0.0\nVAL : 예측라벨 : [ 73  54  18 114  58  21 133 164  39 108  73  85 155  24 117  83  12 193\n 193 136  81  65  24 136 107  86  24  24  27  55  30  32], 정답 [135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 136 136\n 136 136 136 136 136 136 136 136 136 136 136 136 136 136]\n(VAL) Batch 212 Loss : 4.6999125480651855, accuracy: 0.0625\nVAL : 예측라벨 : [ 31  54  87 193  21  81  31  11 193  97 151  77 193 193  12 105 136  24\n  22 105 136  26  31  55  24  12  74 136  24  83 161 136], 정답 [136 136 136 136 136 136 136 136 136 136 136 136 136 136 136 136 136 136\n 136 136 136 136 136 136 136 136 136 136 136 136 136 136]\n(VAL) Batch 213 Loss : 3.6947827339172363, accuracy: 0.125\nVAL : 예측라벨 : [182  31 157 146  55  69  21  55  60 105  81  21 133  12  81  21  21 101\n   4 105 124 115  91  55  79  22  87  31 183 118  71  71], 정답 [136 136 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137\n 137 137 137 137 137 137 137 137 137 137 137 137 137 137]\n(VAL) Batch 214 Loss : 4.700894832611084, accuracy: 0.0\nVAL : 예측라벨 : [128  41 124  21 170 147  12 124  23  21  87 193  24 157  21  69 124  22\n 198  72  72  55 109  60  24 105  60  90 109  60 110 106], 정답 [137 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137\n 137 137 138 138 138 138 138 138 138 138 138 138 138 138]\n(VAL) Batch 215 Loss : 4.095123767852783, accuracy: 0.0\nVAL : 예측라벨 : [143  55  69  60  77 110  60  24 170  77 138  23 105 184  60  60 193  72\n  21  21 155  30 117  17 163 180  60  90  21 193  83  77], 정답 [138 138 138 138 138 138 138 138 138 138 138 138 138 138 138 138 138 138\n 138 138 138 138 138 138 138 138 138 138 138 138 138 138]\n(VAL) Batch 216 Loss : 4.124205112457275, accuracy: 0.03125\nVAL : 예측라벨 : [ 55 117  63  55 123 148 185 196  72  23  69  71  12 117 110 156 139  71\n 110  69 163 138  22 157 151  43 146   9 106 106 107  71], 정답 [138 138 138 138 138 138 139 139 139 139 139 139 139 139 139 139 139 139\n 139 139 139 139 139 139 139 139 139 139 139 139 139 139]\n(VAL) Batch 217 Loss : 4.485316276550293, accuracy: 0.03125\nVAL : 예측라벨 : [121   4 155  21   1  31  24  69 194 106 107  89  58  32 105 122 188 126\n   9  90 137 163   4 117 155 140 193  60  97 192  89 127], 정답 [139 139 139 139 139 139 139 139 139 139 139 139 139 139 139 139 139 139\n 139 139 139 139 139 139 140 140 140 140 140 140 140 140]\n(VAL) Batch 218 Loss : 4.795530796051025, accuracy: 0.03125\nVAL : 예측라벨 : [ 72  24 143 151  73 193  60  69 140  11 110 155 190  60 136  47 136  31\n  98  55  77  13 140 105 104 190  89 104   2  22  78  60], 정답 [140 140 140 140 140 140 140 140 140 140 140 140 140 140 140 140 140 140\n 140 140 140 140 140 140 140 140 140 140 140 140 140 140]\n(VAL) Batch 219 Loss : 5.015477657318115, accuracy: 0.0625\nVAL : 예측라벨 : [117 108 105 193 151  21 193 181  72  24 106  11 193 116 110  84 193  27\n  71  65 133   8 110 117 155  77 107 155  61 105  92 110], 정답 [140 140 140 140 140 140 140 140 140 140 141 141 141 141 141 141 141 141\n 141 141 141 141 141 141 141 141 141 141 141 141 141 141]\n(VAL) Batch 220 Loss : 4.1898298263549805, accuracy: 0.0\nVAL : 예측라벨 : [ 72 117 107 110 120  40  78 155 160 144 117 117  48 148 110  58  60 193\n 193 117 193 109 133  47 155 127 117 103  27 127  66 117], 정답 [141 141 141 141 141 141 141 141 141 141 141 141 141 141 141 141 141 141\n 141 141 141 141 141 141 141 141 141 141 142 142 142 142]\n(VAL) Batch 221 Loss : 4.4007720947265625, accuracy: 0.0\nVAL : 예측라벨 : [106  77  82 105  77 173 129 126 105 126 169  90 117 107   2 142  31 142\n 107 123 121 127 103  24 146  61 155   9 120 119  55 163], 정답 [142 142 142 142 142 142 142 142 142 142 142 142 142 142 142 142 142 142\n 142 142 142 142 142 142 142 142 142 142 142 142 142 142]\n(VAL) Batch 222 Loss : 4.480893135070801, accuracy: 0.0625\nVAL : 예측라벨 : [126  55 142 105  90 115  62 152  18 144 123 117  69 182  28 143 143 143\n 163 143  45  24 143 143 143  60  29 143 143  28 143 143], 정답 [142 142 142 142 142 142 142 142 142 142 142 142 142 142 143 143 143 143\n 143 143 143 143 143 143 143 143 143 143 143 143 143 143]\n(VAL) Batch 223 Loss : 3.372309684753418, accuracy: 0.375\nVAL : 예측라벨 : [143 118 143 163 143 143  11 118  60 143 143  55  14  42  52 143 143  24\n 152 143 143 143 143 143 198 143 143  60 143  24 143  55], 정답 [143 143 143 143 143 143 143 143 143 143 143 143 143 143 143 143 143 143\n 143 143 143 143 143 143 143 143 143 143 143 143 143 143]\n(VAL) Batch 224 Loss : 3.3012773990631104, accuracy: 0.53125\nVAL : 예측라벨 : [  9  26 106  24 150 136  85  24  18 183  85 179  60  24  31  61 116 150\n 193  77  42 193  31  78 117 144 117  77  24  32 193  31], 정답 [144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144\n 144 144 144 144 144 144 144 144 144 144 144 144 144 144]\n(VAL) Batch 225 Loss : 4.464454650878906, accuracy: 0.03125\nVAL : 예측라벨 : [105  21  55  97  55  26 124 193 188 170  31 193  30 193 143  74 193  24\n  93 145 133  93 145 145 145 145 145 145 133   2 170  71], 정답 [144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144\n 145 145 145 145 145 145 145 145 145 145 145 145 145 145]\n(VAL) Batch 226 Loss : 3.58589243888855, accuracy: 0.21875\nVAL : 예측라벨 : [145 145 145 145 145 104 145 145 145 176 150  49 192 145 145 145 145 145\n  39 193 145  60 145 145 145 145 188 133 139 104 145 145], 정답 [145 145 145 145 145 145 145 145 145 145 145 145 145 145 145 145 145 145\n 145 145 145 145 145 145 145 145 145 145 145 145 145 145]\n(VAL) Batch 227 Loss : 2.422971725463867, accuracy: 0.625\nVAL : 예측라벨 : [181  82 115 176  92 108 108 133 108 133 146 145  60  86 111 146 146 146\n 146 115 146 133  65  93 111   4 146 115 105 146 133 116], 정답 [145 145 145 145 146 146 146 146 146 146 146 146 146 146 146 146 146 146\n 146 146 146 146 146 146 146 146 146 146 146 146 146 146]\n(VAL) Batch 228 Loss : 3.488525390625, accuracy: 0.25\nVAL : 예측라벨 : [ 66  70 114 105  84 127  93 151 146  96 128 167  61  84 108 111 121 163\n  54 129  55  11 120  21  73 147 126  55  97 141 120 126], 정답 [146 146 146 146 146 146 146 146 146 146 146 146 146 146 146 146 146 146\n 146 146 146 146 147 147 147 147 147 147 147 147 147 147]\n(VAL) Batch 229 Loss : 4.469023704528809, accuracy: 0.0625\nVAL : 예측라벨 : [ 82 147 126 147  97 193 147 147  81 120  68  97   4  24   9 151  89 141\n  97 126  98  97 105 151  69 151  89  97  97 157 188 136], 정답 [147 147 147 147 147 147 147 147 147 147 147 147 147 147 147 147 147 147\n 147 147 147 147 147 147 147 147 147 147 147 147 147 147]\n(VAL) Batch 230 Loss : 3.6679465770721436, accuracy: 0.125\nVAL : 예측라벨 : [193  90  87  86 193  55  55 153 148 181 148 101  28 148 158  60 194 183\n 164 181 148 148 196  60 194  23  31  23 115 148  54  60], 정답 [147 147 147 147 147 147 147 147 148 148 148 148 148 148 148 148 148 148\n 148 148 148 148 148 148 148 148 148 148 148 148 148 148]\n(VAL) Batch 231 Loss : 3.864346742630005, accuracy: 0.1875\nVAL : 예측라벨 : [148 148  22  19  23   4  21  55  58  23 148 148 148  23 193 148  23 136\n  72 197 141 110  23 196 148 148 110 134 109  73 150  21], 정답 [148 148 148 148 148 148 148 148 148 148 148 148 148 148 148 148 148 148\n 148 148 148 148 148 148 148 148 149 149 149 149 149 149]\n(VAL) Batch 232 Loss : 3.684945821762085, accuracy: 0.25\nVAL : 예측라벨 : [ 18 123  90 134 127  24  44  24  82 183 106  62  31 189  18 149 178  18\n 149 146 105  82  12 169 110  82 183  24  63  60   2 109], 정답 [149 149 149 149 149 149 149 149 149 149 149 149 149 149 149 149 149 149\n 149 149 149 149 149 149 149 149 149 149 149 149 149 149]\n(VAL) Batch 233 Loss : 4.515087127685547, accuracy: 0.0625\nVAL : 예측라벨 : [ 90  93   4  77  72 181 134 141 180  55 183 183  12  21  94  55  60  77\n  47 189 147  77 117  60 160  55  58  60 190 150 150  60], 정답 [149 149 149 149 149 149 149 149 149 149 149 149 150 150 150 150 150 150\n 150 150 150 150 150 150 150 150 150 150 150 150 150 150]\n(VAL) Batch 234 Loss : 4.267397880554199, accuracy: 0.0625\nVAL : 예측라벨 : [ 55 150 127  61 193  26  60  24 190  90 104  58  12  34  55 150  60  85\n  45  54  58  89  61  25 150  58 123 127 123  24 141  11], 정답 [150 150 150 150 150 150 150 150 150 150 150 150 150 150 150 150 150 150\n 150 150 150 150 150 150 150 150 150 150 150 150 151 151]\n(VAL) Batch 235 Loss : 4.683925151824951, accuracy: 0.09375\nVAL : 예측라벨 : [136 189  78 151  69 117 117  21 152 136 180 193  55 151  83 193  55 174\n 151 160   6 157  69 167 151  99  31  97  27 108 109 140], 정답 [151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151\n 151 151 151 151 151 151 151 151 151 151 151 151 151 151]\n(VAL) Batch 236 Loss : 4.116318702697754, accuracy: 0.125\nVAL : 예측라벨 : [117  40  24  21  69  47 117 182 151  79 188  57 117  78 198  18   4 152\n  48 196 152 152 152 152  56   2 152 152   6  43 152 155], 정답 [151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 152 152\n 152 152 152 152 152 152 152 152 152 152 152 152 152 152]\n(VAL) Batch 237 Loss : 3.5694568157196045, accuracy: 0.28125\nVAL : 예측라벨 : [152 152   4  43   8  85   9 152  31 189 152  23 152 152 183  82 152  23\n 152   2 152   9  39  43 183 152 152 152   4 152 128  14], 정답 [152 152 152 152 152 152 152 152 152 152 152 152 152 152 152 152 152 152\n 152 152 152 152 152 152 152 152 152 152 152 152 152 152]\n(VAL) Batch 238 Loss : 3.1886253356933594, accuracy: 0.40625\nVAL : 예측라벨 : [ 55 152 153 133 116  73 107  87  94 145 133  97 116  81 115  94 107 116\n  94  94  92 116 133 150 133 104  87  87 111  81 111  94], 정답 [152 152 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153\n 153 153 153 153 153 153 153 153 153 153 153 153 153 153]\n(VAL) Batch 239 Loss : 3.2776055335998535, accuracy: 0.0625\nVAL : 예측라벨 : [ 94  70  87 145 133 145 107  94  94 116 103 133 104 104 107 116 153 134\n  87  94 154  87 105  87 118 128 100   1 170 154  55 198], 정답 [153 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153\n 153 153 154 154 154 154 154 154 154 154 154 154 154 154]\n(VAL) Batch 240 Loss : 3.5329408645629883, accuracy: 0.09375\nVAL : 예측라벨 : [103 152  91 170 157 103 166 154 197 170 170  12 146 173 194 154 170 166\n 178 157  57 198 157 157 197  95  66 166 154 157 119 194], 정답 [154 154 154 154 154 154 154 154 154 154 154 154 154 154 154 154 154 154\n 154 154 154 154 154 154 154 154 154 154 154 154 154 154]\n(VAL) Batch 241 Loss : 3.872314929962158, accuracy: 0.09375\nVAL : 예측라벨 : [154  89 198  66  22 154  24  55  24 117 100 193  13  87  63 117 143  72\n 141  24 117  13 117 141 193 155 155  72  38 188  24  24], 정답 [154 154 154 154 154 154 155 155 155 155 155 155 155 155 155 155 155 155\n 155 155 155 155 155 155 155 155 155 155 155 155 155 155]\n(VAL) Batch 242 Loss : 4.087854385375977, accuracy: 0.125\nVAL : 예측라벨 : [155 117 155 141  22 117 193 117  31  99 190  85  73  73  48 107 155   6\n 124 155 169  55 117  69  90 123  21  74 109 100  24 193], 정답 [155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155\n 155 155 155 155 155 155 156 156 156 156 156 156 156 156]\n(VAL) Batch 243 Loss : 3.999799966812134, accuracy: 0.125\nVAL : 예측라벨 : [193 136 122  83   8  24  27   9  24 193  27  26  27  40  27 110   8  77\n  26  83  55  60  60  27 147  53 187   9  77  77  24  60], 정답 [156 156 156 156 156 156 156 156 156 156 156 156 156 156 156 156 156 156\n 156 156 156 156 156 156 156 156 156 156 156 156 156 156]\n(VAL) Batch 244 Loss : 4.3688063621521, accuracy: 0.0\nVAL : 예측라벨 : [144  92 144  22  12  24  27  48  12  27 170 152  84  99 173 157 170 129\n 154 157  71 124 116 196 170   4  75 102 195 157 173 170], 정답 [156 156 156 156 156 156 156 156 156 156 157 157 157 157 157 157 157 157\n 157 157 157 157 157 157 157 157 157 157 157 157 157 157]\n(VAL) Batch 245 Loss : 3.58764910697937, accuracy: 0.09375\nVAL : 예측라벨 : [ 60 157  60 103 198   4 170 118 127 170  99 109 157 124 115 170 170  77\n  87  55 197  22 154 157 165 152  12 167 122  74 108  60], 정답 [157 157 157 157 157 157 157 157 157 157 157 157 157 157 157 157 157 157\n 157 157 157 157 157 157 157 157 157 157 158 158 158 158]\n(VAL) Batch 246 Loss : 3.6756582260131836, accuracy: 0.09375\nVAL : 예측라벨 : [ 90 193 165  55  55  14 189 123 105  54  68 150  97  21 199 148  60 143\n 163  60 193 114  74 169  31 127 181 108 123  24 169  11], 정답 [158 158 158 158 158 158 158 158 158 158 158 158 158 158 158 158 158 158\n 158 158 158 158 158 158 158 158 158 158 158 158 158 158]\n(VAL) Batch 247 Loss : 4.554005146026611, accuracy: 0.0\nVAL : 예측라벨 : [123 144  12  55  13 120  42 148 123 158  74 148  71 176  83 122 193 193\n  24  31 189  12  60   6  24   2 127  18  72 182  69 141], 정답 [158 158 158 158 158 158 158 158 158 158 158 158 158 158 159 159 159 159\n 159 159 159 159 159 159 159 159 159 159 159 159 159 159]\n(VAL) Batch 248 Loss : 4.547624111175537, accuracy: 0.03125\nVAL : 예측라벨 : [ 77  77  55  91 116 173  33  77  69  31  79  24 123  55  25  40  55  97\n  26  60  40 180   4  57 196  26 174 122   9  24  55  31], 정답 [159 159 159 159 159 159 159 159 159 159 159 159 159 159 159 159 159 159\n 159 159 159 159 159 159 159 159 159 159 159 159 159 159]\n(VAL) Batch 249 Loss : 5.054035186767578, accuracy: 0.0\nVAL : 예측라벨 : [177  21  31  45   8  60  31  89 189 140 110 160  97  21 178  31  90 151\n  69 193 193  69 126  26  97   2 117 193  18  19 181 110], 정답 [160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160\n 160 160 160 160 160 160 160 160 160 160 160 160 160 160]\n(VAL) Batch 250 Loss : 4.064600467681885, accuracy: 0.03125\nVAL : 예측라벨 : [160   1 151 176  55  61  55 193 197 161 155 135 117  87  77   2 193  48\n  24  53  83  77 189  31  45 190 193  58 143 106  54  31], 정답 [160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160\n 161 161 161 161 161 161 161 161 161 161 161 161 161 161]\n(VAL) Batch 251 Loss : 4.508298873901367, accuracy: 0.03125\nVAL : 예측라벨 : [163  36  24  82  25 117 183 192  56  34  55 180  31  24  18  25  26  14\n 109  26  55 184  26  47  55  27  34  26  28 181  26 181], 정답 [161 161 161 161 161 161 161 161 161 161 161 161 161 161 161 161 161 161\n 161 161 161 161 161 161 161 161 161 161 161 161 161 161]\n(VAL) Batch 252 Loss : 4.451172828674316, accuracy: 0.0\nVAL : 예측라벨 : [ 27  60  18  24 170   4   9   2  53 193 170  53  50   6 170  91  81   2\n 162  56 170  14 103   2  91  34  58 170  56 162 170   2], 정답 [161 161 161 161 162 162 162 162 162 162 162 162 162 162 162 162 162 162\n 162 162 162 162 162 162 162 162 162 162 162 162 162 162]\n(VAL) Batch 253 Loss : 4.068601131439209, accuracy: 0.0625\nVAL : 예측라벨 : [170 165 152 162  70 162  53   4 152   1 162 170 162 162  96 152   4 165\n 185 170  12  56 101  55  47  18 163  97 163  77  77  60], 정답 [162 162 162 162 162 162 162 162 162 162 162 162 162 162 162 162 162 162\n 162 162 162 162 163 163 163 163 163 163 163 163 163 163]\n(VAL) Batch 254 Loss : 3.561985492706299, accuracy: 0.21875\nVAL : 예측라벨 : [ 60 163 163 163 108  90  60  60 163  27 163  77 163 180  58 163  17  60\n 169  13 109   9 120  60 150  60  21 188 163 163 107 120], 정답 [163 163 163 163 163 163 163 163 163 163 163 163 163 163 163 163 163 163\n 163 163 163 163 163 163 163 163 163 163 163 163 163 163]\n(VAL) Batch 255 Loss : 3.3521575927734375, accuracy: 0.28125\nVAL : 예측라벨 : [ 60  72  60  18  71 118 163  24  94  66 164 163  41  82  70 114  94  93\n  66 111  93 117 164 163 108  28 178 111 170  84 150  75], 정답 [163 163 163 163 163 163 163 163 164 164 164 164 164 164 164 164 164 164\n 164 164 164 164 164 164 164 164 164 164 164 164 164 164]\n(VAL) Batch 256 Loss : 3.8230199813842773, accuracy: 0.09375\nVAL : 예측라벨 : [ 73  12 164 114  66  66   4  82  55 107  55  92 114 127  60  18 118 107\n 196 166 111  84  60 164  84 118 170  57 165 127 170 165], 정답 [164 164 164 164 164 164 164 164 164 164 164 164 164 164 164 164 164 164\n 164 164 164 164 164 164 164 164 165 165 165 165 165 165]\n(VAL) Batch 257 Loss : 3.8840277194976807, accuracy: 0.125\nVAL : 예측라벨 : [ 78 170  84 165 165  63 165 166  75  61 165 165 165 165  57 183  33  57\n  91 170 165  81  89  57  63 165 165  86 154 169  56  30], 정답 [165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165\n 165 165 165 165 165 165 165 165 165 165 165 165 165 165]\n(VAL) Batch 258 Loss : 3.6143765449523926, accuracy: 0.3125\nVAL : 예측라벨 : [ 84 165  50 165 124  53  57 105  26  57  81  55 166 166 111 166 145 166\n 151  94 107 157 166 146 166 133 166 166 145 166 166 133], 정답 [165 165 165 165 165 165 165 165 165 165 165 165 166 166 166 166 166 166\n 166 166 166 166 166 166 166 166 166 166 166 166 166 166]\n(VAL) Batch 259 Loss : 2.859267234802246, accuracy: 0.375\nVAL : 예측라벨 : [ 19 108 166 154 145 154 111 166  97 133 166 108 145 157 108 166 166  94\n 107  87 170 166 166 166 166 115 133 146  82 118  31  21], 정답 [166 166 166 166 166 166 166 166 166 166 166 166 166 166 166 166 166 166\n 166 166 166 166 166 166 166 166 166 166 166 166 167 167]\n(VAL) Batch 260 Loss : 3.146223783493042, accuracy: 0.28125\nVAL : 예측라벨 : [105 143 117 129  81  87 100 150 155 167  22 128 155 151  55  65 109 127\n  60 152 105 167 155  55  55  60 114 167  55  60 105 143], 정답 [167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 167\n 167 167 167 167 167 167 167 167 167 167 167 167 167 167]\n(VAL) Batch 261 Loss : 3.950590133666992, accuracy: 0.09375\nVAL : 예측라벨 : [196 152  78  77 155  71   1  77 167 120 167 120 167  92  77 126   0 152\n 111  60 148  60 117 170 143 157  24 105 129 116 111  31], 정답 [167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 168 168\n 168 168 168 168 168 168 168 168 168 168 168 168 168 168]\n(VAL) Batch 262 Loss : 4.599977016448975, accuracy: 0.09375\nVAL : 예측라벨 : [ 24  82 116  87  60 127  49  42 189  21  11  24 143 143  60  55  13  60\n  90 194 123  42 189  60  21 136  11  21  21  55  72 111], 정답 [168 168 168 168 168 168 168 168 168 168 168 168 168 168 168 168 168 168\n 168 168 168 168 168 168 168 168 168 168 168 168 168 168]\n(VAL) Batch 263 Loss : 4.8971405029296875, accuracy: 0.0\nVAL : 예측라벨 : [ 94 176  93 169 182 105  61 169 105 169 105  85  42  61 117  21 109 105\n 189  21 193 189   1 169  61 105 105  17 169 169  31 105], 정답 [168 168 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169\n 169 169 169 169 169 169 169 169 169 169 169 169 169 169]\n(VAL) Batch 264 Loss : 3.7479324340820312, accuracy: 0.1875\nVAL : 예측라벨 : [  9 128 120 105 189 105  60  54 105 180  85  60  31 163  31  39 169  68\n  61 112 108  60 170 170 170  55 170  96 195  12 170 170], 정답 [169 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169\n 169 169 170 170 170 170 170 170 170 170 170 170 170 170]\n(VAL) Batch 265 Loss : 3.4837708473205566, accuracy: 0.21875\nVAL : 예측라벨 : [  2 148 167  58 152 162 118 162  50  21  84   2  55  31  50  49  96  11\n 170 157   4   4  53  24  57 170  55  57 170   2 170  78], 정답 [170 170 170 170 170 170 170 170 170 170 170 170 170 170 170 170 170 170\n 170 170 170 170 170 170 170 170 170 170 170 170 170 170]\n(VAL) Batch 266 Loss : 3.8922810554504395, accuracy: 0.125\nVAL : 예측라벨 : [ 57  12 164 170 170 103 117 147  38 112  77   1  26 164 163 147  45 127\n 179 106  43 133  24  60 109 111  79 117 143  45  68 120], 정답 [170 170 170 170 170 170 171 171 171 171 171 171 171 171 171 171 171 171\n 171 171 171 171 171 171 171 171 171 171 171 171 171 171]\n(VAL) Batch 267 Loss : 4.643066883087158, accuracy: 0.0625\nVAL : 예측라벨 : [121  68 167 164 118 158  24 164 105 171 120  38  45  60 139 164  60  77\n  21 128  81 143 150 119  60  23 109  21 136  72 106  42], 정답 [171 171 171 171 171 171 171 171 171 171 171 171 171 171 171 171 171 171\n 171 171 171 171 171 171 172 172 172 172 172 172 172 172]\n(VAL) Batch 268 Loss : 4.654754161834717, accuracy: 0.03125\nVAL : 예측라벨 : [ 77 136 179  63  83 193  31  32 138  22  21 155  42 136  31 188  60 117\n  21  21 104 109 117  42 105 183 136 109  77  55  69  45], 정답 [172 172 172 172 172 172 172 172 172 172 172 172 172 172 172 172 172 172\n 172 172 172 172 172 172 172 172 172 172 172 172 172 172]\n(VAL) Batch 269 Loss : 4.515489101409912, accuracy: 0.0\nVAL : 예측라벨 : [  0  72 172 110 157 179 150  72  48 117  21  55 117 101 105  99 101 173\n  71 196 109  75  11 157 170  12  57 117 101 155  71  71], 정답 [172 172 172 172 172 172 172 172 172 172 173 173 173 173 173 173 173 173\n 173 173 173 173 173 173 173 173 173 173 173 173 173 173]\n(VAL) Batch 270 Loss : 4.416549205780029, accuracy: 0.0625\nVAL : 예측라벨 : [ 95  71 101  36  79 197 101  66  71 119 157  79 173  73 157 103  71 198\n 173 170  40 173  71 173  13 117 173 173 103  11  87 192], 정답 [173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173\n 173 173 173 173 173 173 173 173 173 173 174 174 174 174]\n(VAL) Batch 271 Loss : 3.570925235748291, accuracy: 0.1875\nVAL : 예측라벨 : [178 174  31 104  48 183 178  55 177 190   2 174 104  24 104 191 104 178\n 178   2  83 181 104 181  11 104 191 192 104  24 104 104], 정답 [174 174 174 174 174 174 174 174 174 174 174 174 174 174 174 174 174 174\n 174 174 174 174 174 174 174 174 174 174 174 174 174 174]\n(VAL) Batch 272 Loss : 3.4556167125701904, accuracy: 0.0625\nVAL : 예측라벨 : [177  24  44 104  81  69  27 150 193 104 183 192  45  97 122 193 184  27\n  24  77  33 187  69 188  24 193 192 193  93  12 109  24], 정답 [174 174 174 174 174 174 174 174 174 174 174 174 174 174 175 175 175 175\n 175 175 175 175 175 175 175 175 175 175 175 175 175 175]\n(VAL) Batch 273 Loss : 4.297910690307617, accuracy: 0.0\nVAL : 예측라벨 : [100 183  41  90 150 163  12 104  24  42  89  22 176  24  21  68  77  18\n 193  24  60  97 100 105  24 136 191 193 193  97  94 156], 정답 [175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175\n 175 175 175 175 175 175 175 175 175 175 175 175 175 175]\n(VAL) Batch 274 Loss : 4.793783187866211, accuracy: 0.0\nVAL : 예측라벨 : [ 82 176  93 176 176  90 176  90 128 176 126 123   1  93  93 128 193 176\n  24 176  93  73 176 176  93  43  93  68  65  82 176 176], 정답 [176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176\n 176 176 176 176 176 176 176 176 176 176 176 176 176 176]\n(VAL) Batch 275 Loss : 3.06762957572937, accuracy: 0.34375\nVAL : 예측라벨 : [176  60  62 176  58 176 176  82 176  93  60  90  12 148 176  82 176  26\n  19 177 163 192 193 177  36 177  31 182 199 193 104 192], 정답 [176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176\n 177 177 177 177 177 177 177 177 177 177 177 177 177 177]\n(VAL) Batch 276 Loss : 3.6301000118255615, accuracy: 0.3125\nVAL : 예측라벨 : [193 190 181 117  98 185  23 181  26 177 177 182   0  90 177 193  27 192\n  24  39 193 181 190 104 177 193 178  24 178  37  54 104], 정답 [177 177 177 177 177 177 177 177 177 177 177 177 177 177 177 177 177 177\n 177 177 177 177 177 177 177 177 177 177 177 177 177 177]\n(VAL) Batch 277 Loss : 3.511476516723633, accuracy: 0.125\nVAL : 예측라벨 : [193 177 182  48   2  21 178  44 199 178 178 183 199 178 183 178 183 189\n 178  42 178 190 183 178 182 138 183 178  58 178 182  44], 정답 [177 177 177 177 178 178 178 178 178 178 178 178 178 178 178 178 178 178\n 178 178 178 178 178 178 178 178 178 178 178 178 178 178]\n(VAL) Batch 278 Loss : 2.782536029815674, accuracy: 0.34375\nVAL : 예측라벨 : [  2 178 192 182  39  45 178  25 183 183 184 183   2 183 178  39 182 178\n 178  11 178 178  24  42 193  53 100  24 181 182  23 104], 정답 [178 178 178 178 178 178 178 178 178 178 178 178 178 178 178 178 178 178\n 178 178 178 178 179 179 179 179 179 179 179 179 179 179]\n(VAL) Batch 279 Loss : 3.125354051589966, accuracy: 0.21875\nVAL : 예측라벨 : [ 47  26  18  18 181  71 193  18 193  31 181 182 189 127 104 182 193 193\n 160  54  24 192  24   5 136 193  18 193  24  18  24  24], 정답 [179 179 179 179 179 179 179 179 179 179 179 179 179 179 179 179 179 179\n 179 179 179 179 179 179 179 179 179 179 179 179 179 179]\n(VAL) Batch 280 Loss : 3.7876698970794678, accuracy: 0.0\nVAL : 예측라벨 : [193 193  65  30 193  42 136 105 123 138  74 149  60 180  24  24  58  24\n  90  77 105  27  26  72 158 193   8   0  72 163  31  58], 정답 [179 179 179 179 179 179 179 179 180 180 180 180 180 180 180 180 180 180\n 180 180 180 180 180 180 180 180 180 180 180 180 180 180]\n(VAL) Batch 281 Loss : 4.001646041870117, accuracy: 0.03125\nVAL : 예측라벨 : [ 60  71  24  60 187 150  83  36 189 123 105  60  24 180 163  74 114 150\n 189 182 124  26  26 193 169  36 150 185  24  36 188  82], 정답 [180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180\n 180 180 180 180 180 180 180 180 181 181 181 181 181 181]\n(VAL) Batch 282 Loss : 4.117528438568115, accuracy: 0.03125\nVAL : 예측라벨 : [183 191 193  47 105 181 182 180 181 181 181  22 177  12 190  55 180  26\n  14  33 151 189 181 190  24  82  61 127  22 181 193 190], 정답 [181 181 181 181 181 181 181 181 181 181 181 181 181 181 181 181 181 181\n 181 181 181 181 181 181 181 181 181 181 181 181 181 181]\n(VAL) Batch 283 Loss : 4.234416961669922, accuracy: 0.1875\nVAL : 예측라벨 : [190  30  24 190  24 182  45  21 177  25   0 192 192 104  18 182 181  26\n 177  26  26 188 188 177 182 179 177  27 182 178 191 190], 정답 [181 181 181 181 181 181 181 181 181 181 181 181 182 182 182 182 182 182\n 182 182 182 182 182 182 182 182 182 182 182 182 182 182]\n(VAL) Batch 284 Loss : 3.2038681507110596, accuracy: 0.09375\nVAL : 예측라벨 : [ 47  11 175 192  31 182 192 192 193 182 192   0 182  39 192 182  83 190\n 192 186  34  36  42 182  25 182 192 182 183 190 183  47], 정답 [182 182 182 182 182 182 182 182 182 182 182 182 182 182 182 182 182 182\n 182 182 182 182 182 182 182 182 182 182 182 182 183 183]\n(VAL) Batch 285 Loss : 2.9018661975860596, accuracy: 0.25\nVAL : 예측라벨 : [ 47 182 182 183 104 182 183 183  54 196 184  45 178 183  24 169 105  38\n 183  39  24 193 183  45 183 183 199 183 183  24   2  14], 정답 [183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 183\n 183 183 183 183 183 183 183 183 183 183 183 183 183 183]\n(VAL) Batch 286 Loss : 2.7339835166931152, accuracy: 0.3125\nVAL : 예측라벨 : [178 183 188  47 199 183 163  60 183 183  42  34  47   2 199 183  39 199\n 184 184 184 184 184 184  45   0 189 184 184 184 189 184], 정답 [183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 184 184\n 184 184 184 184 184 184 184 184 184 184 184 184 184 184]\n(VAL) Batch 287 Loss : 2.4637997150421143, accuracy: 0.46875\nVAL : 예측라벨 : [184 184   0 187  42 184 184 189 184 179 184   0 184  42 184 184 184  72\n 184 184 169 121 184   0 184 184 184  23 184 184 184 199], 정답 [184 184 184 184 184 184 184 184 184 184 184 184 184 184 184 184 184 184\n 184 184 184 184 184 184 184 184 184 184 184 184 184 184]\n(VAL) Batch 288 Loss : 2.2213966846466064, accuracy: 0.59375\nVAL : 예측라벨 : [184 187 190  58 157 189  42  50  48 184  34  58  52  44 185  42  56 183\n  57  61  56 141 158  58 176   4  18   5 192 190  77  55], 정답 [184 184 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185\n 185 185 185 185 185 185 185 185 185 185 185 185 185 185]\n(VAL) Batch 289 Loss : 3.7645201683044434, accuracy: 0.0625\nVAL : 예측라벨 : [114  39  35 199 189 185   0  27  58  58 178  58  44 185   4  26 164 185\n 193  58 186 138 186 187 187 192 187 104 176 193 186  21], 정답 [185 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185\n 185 185 186 186 186 186 186 186 186 186 186 186 186 186]\n(VAL) Batch 290 Loss : 3.3767893314361572, accuracy: 0.1875\nVAL : 예측라벨 : [ 13 186 186 186   0  47 192  24 199 140 186 188 186 193   0 177 184 190\n 192 192 186 187 187 192 186  18 184 188 193  45   0 193], 정답 [186 186 186 186 186 186 186 186 186 186 186 186 186 186 186 186 186 186\n 186 186 186 186 186 186 186 186 186 186 186 186 186 186]\n(VAL) Batch 291 Loss : 3.087341547012329, accuracy: 0.21875\nVAL : 예측라벨 : [ 31 186 186 186 190 186 193 178 192   2 184   0 187 187 187  21 192 104\n  42 188  42 187 187 187  42 186 187  45 187 188 187 181], 정답 [186 186 186 186 186 186 187 187 187 187 187 187 187 187 187 187 187 187\n 187 187 187 187 187 187 187 187 187 187 187 187 187 187]\n(VAL) Batch 292 Loss : 2.949223518371582, accuracy: 0.40625\nVAL : 예측라벨 : [187  45 187 184  21  45  45 184   0  45 187 187 188 184 192 186 187 187\n 188  43 187 188 182 192 188  45 193  24 177 188  83  21], 정답 [187 187 187 187 187 187 187 187 187 187 187 187 187 187 187 187 187 187\n 187 187 187 187 187 187 188 188 188 188 188 188 188 188]\n(VAL) Batch 293 Loss : 2.560358762741089, accuracy: 0.28125\nVAL : 예측라벨 : [ 42 188  21 192 186 184 190 184 190  83 188 187  42 187  54 184 174  45\n  83  83 183 186  60  24 193 187  83 188 188 188 192 188], 정답 [188 188 188 188 188 188 188 188 188 188 188 188 188 188 188 188 188 188\n 188 188 188 188 188 188 188 188 188 188 188 188 188 188]\n(VAL) Batch 294 Loss : 3.567805767059326, accuracy: 0.1875\nVAL : 예측라벨 : [188 191 177  42  21 188   1 188  39 182 189   0  83 189 189 189 184  45\n 184 189 191 189 189  24 189  90  60 163 189 189 193 189], 정답 [188 188 188 188 188 188 188 188 188 188 189 189 189 189 189 189 189 189\n 189 189 189 189 189 189 189 189 189 189 189 189 189 189]\n(VAL) Batch 295 Loss : 2.8596527576446533, accuracy: 0.4375\nVAL : 예측라벨 : [184  82 183  83 189 189 189  69  45  97 189 189  24 189  90 190 189 189\n 189  36 181  83 189 184 199  77  44 189  34 104   9 190], 정답 [189 189 189 189 189 189 189 189 189 189 189 189 189 189 189 189 189 189\n 189 189 189 189 189 189 189 189 189 189 190 190 190 190]\n(VAL) Batch 296 Loss : 2.943345546722412, accuracy: 0.375\nVAL : 예측라벨 : [190 190 189 177 181 190 177  50 190 190 178 177 181  33 191 190 177 191\n 190 104  41  83  82 182  54 104 190 182 191 182  82 136], 정답 [190 190 190 190 190 190 190 190 190 190 190 190 190 190 190 190 190 190\n 190 190 190 190 190 190 190 190 190 190 190 190 190 190]\n(VAL) Batch 297 Loss : 3.013725757598877, accuracy: 0.25\nVAL : 예측라벨 : [190 192 190 190 190 190 181 191 178 190 188 190 192 142  98 182 177 181\n 191  25 182 190 177 191   1  82 191 190 191 182 192 191], 정답 [190 190 190 190 190 190 190 190 190 190 190 190 190 190 191 191 191 191\n 191 191 191 191 191 191 191 191 191 191 191 191 191 191]\n(VAL) Batch 298 Loss : 2.577979326248169, accuracy: 0.375\nVAL : 예측라벨 : [190 190 177 191 192  65 181 191 190 191  54 179 104 191 136 191 191 196\n 191 126 191 190 104 181 177 191 191 191  82 191 190 191], 정답 [191 191 191 191 191 191 191 191 191 191 191 191 191 191 191 191 191 191\n 191 191 191 191 191 191 191 191 191 191 191 191 191 191]\n(VAL) Batch 299 Loss : 2.539699077606201, accuracy: 0.40625\nVAL : 예측라벨 : [186 186 188 144 183 193  89 192 186 192 192 192 192 181 188 181 193 192\n 183 192 181 192 192 182 182 182  26 193 192 192 193 192], 정답 [192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192\n 192 192 192 192 192 192 192 192 192 192 192 192 192 192]\n(VAL) Batch 300 Loss : 2.526169776916504, accuracy: 0.375\nVAL : 예측라벨 : [192 192  83 182 190 182 181 182 192 192 192 193 184  24 192 192 192  83\n 193 193 193 193 193 192 193 193 193 193 193 193 193  60], 정답 [192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192\n 193 193 193 193 193 193 193 193 193 193 193 193 193 193]\n(VAL) Batch 301 Loss : 2.31732177734375, accuracy: 0.625\nVAL : 예측라벨 : [193 188  26 193 193 193 193 193  24 193  18 179 193 193 193  21 193 193\n  55 193 179 192 193  81 193 193  97 193 193 193 104 193], 정답 [193 193 193 193 193 193 193 193 193 193 193 193 193 193 193 193 193 193\n 193 193 193 193 193 193 193 193 193 193 193 193 193 193]\n(VAL) Batch 302 Loss : 2.2126927375793457, accuracy: 0.625\nVAL : 예측라벨 : [193 193 193 193  23  75 170  55 194 196  31 194   4 194 194  66 194 162\n 194 194 170  41 170 115 198   2 194 194 198  13 194  14], 정답 [193 193 193 193 194 194 194 194 194 194 194 194 194 194 194 194 194 194\n 194 194 194 194 194 194 194 194 194 194 194 194 194 194]\n(VAL) Batch 303 Loss : 2.9085309505462646, accuracy: 0.4375\nVAL : 예측라벨 : [157  96  41  22  78 194 114  53   8 170 194  23  31 152 194   2 170 194\n 118  69  22  22 152  57  55 197 190 145 198  12  11 197], 정답 [194 194 194 194 194 194 194 194 194 194 194 194 194 194 194 194 194 194\n 194 194 194 194 195 195 195 195 195 195 195 195 195 195]\n(VAL) Batch 304 Loss : 3.9381346702575684, accuracy: 0.125\nVAL : 예측라벨 : [104 105  22  11  61   4  66 198  14  14  21 195  57 197  31  12 157  55\n 197 170  55 197  53 197 194 198  55  66 195  12 196 190], 정답 [195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195\n 195 195 195 195 195 195 195 195 195 195 195 195 195 195]\n(VAL) Batch 305 Loss : 4.1075873374938965, accuracy: 0.0625\nVAL : 예측라벨 : [ 55 197 152  42  53  14  55 195  13  23 196  14 196 196 196  14  93 196\n  20  55 196  23  23 183  23 196 196  19  23   2 196  12], 정답 [195 195 195 195 195 195 195 195 196 196 196 196 196 196 196 196 196 196\n 196 196 196 196 196 196 196 196 196 196 196 196 196 196]\n(VAL) Batch 306 Loss : 2.8905293941497803, accuracy: 0.3125\nVAL : 예측라벨 : [196  14  12 196   2 196 196 196  14 196 196 196 152 196  57  19 196  17\n 148  23  14 196 196  93 148  23 114  81 198 197  23 198], 정답 [196 196 196 196 196 196 196 196 196 196 196 196 196 196 196 196 196 196\n 196 196 196 196 196 196 196 196 197 197 197 197 197 197]\n(VAL) Batch 307 Loss : 2.445892572402954, accuracy: 0.40625\nVAL : 예측라벨 : [194  23 197 154 170  66 170 115  40  23 198 197 197  22 157 118 170 118\n  66 170 157 197  87 196 198   2 171 194 107 152  11 152], 정답 [197 197 197 197 197 197 197 197 197 197 197 197 197 197 197 197 197 197\n 197 197 197 197 197 197 197 197 197 197 197 197 197 197]\n(VAL) Batch 308 Loss : 3.2140960693359375, accuracy: 0.125\nVAL : 예측라벨 : [ 12 170   2  27  71  93  23 157  22  78  23 139   2 198 198  11  75 197\n 198 114 198   0 198 198 198  96  89 195 196  11 198 154], 정답 [197 197 197 197 197 197 197 197 197 197 197 197 198 198 198 198 198 198\n 198 198 198 198 198 198 198 198 198 198 198 198 198 198]\n(VAL) Batch 309 Loss : 3.6886513233184814, accuracy: 0.25\nVAL : 예측라벨 : [ 69 198 198 198 194 198 170 198 198  66 193 166  69 198 115 106  78  66\n 170 198 198   4 198 165 196  66 197 198 198 198 199 188], 정답 [198 198 198 198 198 198 198 198 198 198 198 198 198 198 198 198 198 198\n 198 198 198 198 198 198 198 198 198 198 198 198 199 199]\n(VAL) Batch 310 Loss : 3.182082414627075, accuracy: 0.4375\nVAL : 예측라벨 : [ 36  26 184  26  42 199  26  19  39  42  20 181  23 185 184  36  33  81\n   0 184  35  43  26 185  39 199  54 199 199  39 170  42], 정답 [199 199 199 199 199 199 199 199 199 199 199 199 199 199 199 199 199 199\n 199 199 199 199 199 199 199 199 199 199 199 199 199 199]\n(VAL) Batch 311 Loss : 3.445662021636963, accuracy: 0.125\nVAL : 예측라벨 : [  6 184  35 184  42  52 185   2  31 138 199  24 199  55  42  42], 정답 [199 199 199 199 199 199 199 199 199 199 199 199 199 199 199 199]\n(VAL) Batch 312 Loss : 3.4635772705078125, accuracy: 0.125\nepoch 1 Loss/Validate :3.736922735223374 \nepoch 1 Accuracy/Validate : 0.1807\nEpoch : 2, batch 0\n(Train) Batch 0 Loss : 3.73661208152771, 맞은 개수 : 26\nEpoch : 2, batch 1\n(Train) Batch 1 Loss : 3.7039296627044678, 맞은 개수 : 26\nEpoch : 2, batch 2\n(Train) Batch 2 Loss : 3.6581521034240723, 맞은 개수 : 24\nEpoch : 2, batch 3\n(Train) Batch 3 Loss : 3.783647298812866, 맞은 개수 : 22\nEpoch : 2, batch 4\n(Train) Batch 4 Loss : 3.731627941131592, 맞은 개수 : 24\nEpoch : 2, batch 5\n(Train) Batch 5 Loss : 3.5744056701660156, 맞은 개수 : 23\nEpoch : 2, batch 6\n(Train) Batch 6 Loss : 3.6284546852111816, 맞은 개수 : 24\nEpoch : 2, batch 7\n(Train) Batch 7 Loss : 3.9681458473205566, 맞은 개수 : 11\nEpoch : 2, batch 8\n(Train) Batch 8 Loss : 3.749171018600464, 맞은 개수 : 22\nEpoch : 2, batch 9\n(Train) Batch 9 Loss : 3.6899290084838867, 맞은 개수 : 22\nEpoch : 2, batch 10\n(Train) Batch 10 Loss : 3.6241281032562256, 맞은 개수 : 25\nEpoch : 2, batch 11\n(Train) Batch 11 Loss : 3.5077717304229736, 맞은 개수 : 25\nEpoch : 2, batch 12\n(Train) Batch 12 Loss : 3.5166966915130615, 맞은 개수 : 21\nEpoch : 2, batch 13\n(Train) Batch 13 Loss : 3.56795334815979, 맞은 개수 : 23\nEpoch : 2, batch 14\n(Train) Batch 14 Loss : 3.808994770050049, 맞은 개수 : 25\nEpoch : 2, batch 15\n(Train) Batch 15 Loss : 3.8782482147216797, 맞은 개수 : 26\nEpoch : 2, batch 16\n(Train) Batch 16 Loss : 3.5587708950042725, 맞은 개수 : 24\nEpoch : 2, batch 17\n(Train) Batch 17 Loss : 3.6790473461151123, 맞은 개수 : 21\nEpoch : 2, batch 18\n(Train) Batch 18 Loss : 3.8699545860290527, 맞은 개수 : 21\nEpoch : 2, batch 19\n(Train) Batch 19 Loss : 3.9252214431762695, 맞은 개수 : 25\nEpoch : 2, batch 20\n(Train) Batch 20 Loss : 3.962594985961914, 맞은 개수 : 15\nEpoch : 2, batch 21\n(Train) Batch 21 Loss : 3.47562313079834, 맞은 개수 : 30\nEpoch : 2, batch 22\n(Train) Batch 22 Loss : 3.8488664627075195, 맞은 개수 : 17\nEpoch : 2, batch 23\n(Train) Batch 23 Loss : 3.7266616821289062, 맞은 개수 : 22\nEpoch : 2, batch 24\n(Train) Batch 24 Loss : 4.017605304718018, 맞은 개수 : 14\nEpoch : 2, batch 25\n(Train) Batch 25 Loss : 3.7170724868774414, 맞은 개수 : 21\nEpoch : 2, batch 26\n(Train) Batch 26 Loss : 3.8359153270721436, 맞은 개수 : 21\nEpoch : 2, batch 27\n(Train) Batch 27 Loss : 3.615117311477661, 맞은 개수 : 27\nEpoch : 2, batch 28\n(Train) Batch 28 Loss : 3.8735878467559814, 맞은 개수 : 18\nEpoch : 2, batch 29\n(Train) Batch 29 Loss : 3.7374861240386963, 맞은 개수 : 24\nEpoch : 2, batch 30\n(Train) Batch 30 Loss : 3.831904411315918, 맞은 개수 : 21\nEpoch : 2, batch 31\n(Train) Batch 31 Loss : 3.9251644611358643, 맞은 개수 : 17\nEpoch : 2, batch 32\n(Train) Batch 32 Loss : 3.5713329315185547, 맞은 개수 : 23\nEpoch : 2, batch 33\n(Train) Batch 33 Loss : 3.53041672706604, 맞은 개수 : 23\nEpoch : 2, batch 34\n(Train) Batch 34 Loss : 3.6213057041168213, 맞은 개수 : 25\nEpoch : 2, batch 35\n(Train) Batch 35 Loss : 3.9068031311035156, 맞은 개수 : 19\nEpoch : 2, batch 36\n(Train) Batch 36 Loss : 3.9175755977630615, 맞은 개수 : 22\nEpoch : 2, batch 37\n(Train) Batch 37 Loss : 3.8823578357696533, 맞은 개수 : 20\nEpoch : 2, batch 38\n(Train) Batch 38 Loss : 3.512449026107788, 맞은 개수 : 21\nEpoch : 2, batch 39\n(Train) Batch 39 Loss : 3.5710127353668213, 맞은 개수 : 29\nEpoch : 2, batch 40\n(Train) Batch 40 Loss : 3.561053991317749, 맞은 개수 : 35\nEpoch : 2, batch 41\n(Train) Batch 41 Loss : 3.7080349922180176, 맞은 개수 : 22\nEpoch : 2, batch 42\n(Train) Batch 42 Loss : 3.7515923976898193, 맞은 개수 : 23\nEpoch : 2, batch 43\n(Train) Batch 43 Loss : 3.5664498805999756, 맞은 개수 : 24\nEpoch : 2, batch 44\n(Train) Batch 44 Loss : 3.6980795860290527, 맞은 개수 : 21\nEpoch : 2, batch 45\n(Train) Batch 45 Loss : 3.7150466442108154, 맞은 개수 : 18\nEpoch : 2, batch 46\n(Train) Batch 46 Loss : 3.9663054943084717, 맞은 개수 : 16\nEpoch : 2, batch 47\n(Train) Batch 47 Loss : 3.7881062030792236, 맞은 개수 : 25\nEpoch : 2, batch 48\n(Train) Batch 48 Loss : 3.769155979156494, 맞은 개수 : 20\nEpoch : 2, batch 49\n(Train) Batch 49 Loss : 3.4000186920166016, 맞은 개수 : 30\nEpoch : 2, batch 50\n(Train) Batch 50 Loss : 3.812161922454834, 맞은 개수 : 24\nEpoch : 2, batch 51\n(Train) Batch 51 Loss : 3.687148332595825, 맞은 개수 : 24\nEpoch : 2, batch 52\n(Train) Batch 52 Loss : 3.717430830001831, 맞은 개수 : 20\nEpoch : 2, batch 53\n(Train) Batch 53 Loss : 3.7547733783721924, 맞은 개수 : 21\nEpoch : 2, batch 54\n(Train) Batch 54 Loss : 3.788567304611206, 맞은 개수 : 22\nEpoch : 2, batch 55\n(Train) Batch 55 Loss : 3.6179513931274414, 맞은 개수 : 32\nEpoch : 2, batch 56\n(Train) Batch 56 Loss : 3.445772647857666, 맞은 개수 : 23\nEpoch : 2, batch 57\n(Train) Batch 57 Loss : 4.060205459594727, 맞은 개수 : 21\nEpoch : 2, batch 58\n(Train) Batch 58 Loss : 3.6930885314941406, 맞은 개수 : 23\nEpoch : 2, batch 59\n(Train) Batch 59 Loss : 3.917771100997925, 맞은 개수 : 20\nEpoch : 2, batch 60\n(Train) Batch 60 Loss : 3.80560302734375, 맞은 개수 : 24\nEpoch : 2, batch 61\n(Train) Batch 61 Loss : 3.80733323097229, 맞은 개수 : 16\nEpoch : 2, batch 62\n(Train) Batch 62 Loss : 3.768026828765869, 맞은 개수 : 19\nEpoch : 2, batch 63\n(Train) Batch 63 Loss : 3.9882900714874268, 맞은 개수 : 16\nEpoch : 2, batch 64\n(Train) Batch 64 Loss : 3.6809756755828857, 맞은 개수 : 19\nEpoch : 2, batch 65\n(Train) Batch 65 Loss : 3.8203301429748535, 맞은 개수 : 21\nEpoch : 2, batch 66\n(Train) Batch 66 Loss : 3.6594905853271484, 맞은 개수 : 26\nEpoch : 2, batch 67\n(Train) Batch 67 Loss : 3.719636917114258, 맞은 개수 : 13\nEpoch : 2, batch 68\n(Train) Batch 68 Loss : 4.0425310134887695, 맞은 개수 : 14\nEpoch : 2, batch 69\n(Train) Batch 69 Loss : 3.768629789352417, 맞은 개수 : 18\nEpoch : 2, batch 70\n(Train) Batch 70 Loss : 3.6703343391418457, 맞은 개수 : 28\nEpoch : 2, batch 71\n(Train) Batch 71 Loss : 3.899317502975464, 맞은 개수 : 18\nEpoch : 2, batch 72\n(Train) Batch 72 Loss : 3.670522928237915, 맞은 개수 : 30\nEpoch : 2, batch 73\n(Train) Batch 73 Loss : 3.853614091873169, 맞은 개수 : 17\nEpoch : 2, batch 74\n(Train) Batch 74 Loss : 3.8195858001708984, 맞은 개수 : 12\nEpoch : 2, batch 75\n(Train) Batch 75 Loss : 3.7783889770507812, 맞은 개수 : 19\nEpoch : 2, batch 76\n(Train) Batch 76 Loss : 3.6615424156188965, 맞은 개수 : 22\nEpoch : 2, batch 77\n(Train) Batch 77 Loss : 3.885572671890259, 맞은 개수 : 25\nEpoch : 2, batch 78\n(Train) Batch 78 Loss : 3.704637050628662, 맞은 개수 : 24\nEpoch : 2, batch 79\n(Train) Batch 79 Loss : 3.944058656692505, 맞은 개수 : 25\nEpoch : 2, batch 80\n(Train) Batch 80 Loss : 3.749570369720459, 맞은 개수 : 20\nEpoch : 2, batch 81\n(Train) Batch 81 Loss : 3.690716505050659, 맞은 개수 : 20\nEpoch : 2, batch 82\n(Train) Batch 82 Loss : 3.8794147968292236, 맞은 개수 : 23\nEpoch : 2, batch 83\n(Train) Batch 83 Loss : 3.5726592540740967, 맞은 개수 : 28\nEpoch : 2, batch 84\n(Train) Batch 84 Loss : 3.7485146522521973, 맞은 개수 : 20\nEpoch : 2, batch 85\n(Train) Batch 85 Loss : 3.5225026607513428, 맞은 개수 : 28\nEpoch : 2, batch 86\n(Train) Batch 86 Loss : 3.9328832626342773, 맞은 개수 : 23\nEpoch : 2, batch 87\n(Train) Batch 87 Loss : 3.762732982635498, 맞은 개수 : 24\nEpoch : 2, batch 88\n(Train) Batch 88 Loss : 3.6388726234436035, 맞은 개수 : 26\nEpoch : 2, batch 89\n(Train) Batch 89 Loss : 3.9329779148101807, 맞은 개수 : 15\nEpoch : 2, batch 90\n(Train) Batch 90 Loss : 4.003174781799316, 맞은 개수 : 23\nEpoch : 2, batch 91\n(Train) Batch 91 Loss : 3.711397171020508, 맞은 개수 : 27\nEpoch : 2, batch 92\n(Train) Batch 92 Loss : 3.646282196044922, 맞은 개수 : 20\nEpoch : 2, batch 93\n(Train) Batch 93 Loss : 3.561797618865967, 맞은 개수 : 32\nEpoch : 2, batch 94\n(Train) Batch 94 Loss : 4.042443752288818, 맞은 개수 : 14\nEpoch : 2, batch 95\n(Train) Batch 95 Loss : 3.584892988204956, 맞은 개수 : 26\nEpoch : 2, batch 96\n(Train) Batch 96 Loss : 3.7039995193481445, 맞은 개수 : 27\nEpoch : 2, batch 97\n(Train) Batch 97 Loss : 3.7024214267730713, 맞은 개수 : 15\nEpoch : 2, batch 98\n(Train) Batch 98 Loss : 3.7708261013031006, 맞은 개수 : 23\nEpoch : 2, batch 99\n(Train) Batch 99 Loss : 3.814244270324707, 맞은 개수 : 20\nEpoch : 2, batch 100\n(Train) Batch 100 Loss : 3.5318775177001953, 맞은 개수 : 24\nEpoch : 2, batch 101\n(Train) Batch 101 Loss : 3.9907162189483643, 맞은 개수 : 18\nEpoch : 2, batch 102\n(Train) Batch 102 Loss : 3.707563638687134, 맞은 개수 : 33\nEpoch : 2, batch 103\n(Train) Batch 103 Loss : 3.763085126876831, 맞은 개수 : 22\nEpoch : 2, batch 104\n(Train) Batch 104 Loss : 3.6413493156433105, 맞은 개수 : 27\nEpoch : 2, batch 105\n(Train) Batch 105 Loss : 3.756103992462158, 맞은 개수 : 26\nEpoch : 2, batch 106\n(Train) Batch 106 Loss : 3.8454718589782715, 맞은 개수 : 23\nEpoch : 2, batch 107\n(Train) Batch 107 Loss : 3.6104071140289307, 맞은 개수 : 28\nEpoch : 2, batch 108\n(Train) Batch 108 Loss : 3.7633936405181885, 맞은 개수 : 21\nEpoch : 2, batch 109\n(Train) Batch 109 Loss : 4.139737606048584, 맞은 개수 : 17\nEpoch : 2, batch 110\n(Train) Batch 110 Loss : 3.6807188987731934, 맞은 개수 : 21\nEpoch : 2, batch 111\n(Train) Batch 111 Loss : 3.8341410160064697, 맞은 개수 : 17\nEpoch : 2, batch 112\n(Train) Batch 112 Loss : 3.7148828506469727, 맞은 개수 : 23\nEpoch : 2, batch 113\n(Train) Batch 113 Loss : 3.7049167156219482, 맞은 개수 : 25\nEpoch : 2, batch 114\n(Train) Batch 114 Loss : 3.5786421298980713, 맞은 개수 : 17\nEpoch : 2, batch 115\n(Train) Batch 115 Loss : 3.6170108318328857, 맞은 개수 : 22\nEpoch : 2, batch 116\n(Train) Batch 116 Loss : 3.6654436588287354, 맞은 개수 : 27\nEpoch : 2, batch 117\n(Train) Batch 117 Loss : 3.818211317062378, 맞은 개수 : 25\nEpoch : 2, batch 118\n(Train) Batch 118 Loss : 3.9239373207092285, 맞은 개수 : 19\nEpoch : 2, batch 119\n(Train) Batch 119 Loss : 3.802959680557251, 맞은 개수 : 25\nEpoch : 2, batch 120\n(Train) Batch 120 Loss : 3.559321641921997, 맞은 개수 : 27\nEpoch : 2, batch 121\n(Train) Batch 121 Loss : 3.8811166286468506, 맞은 개수 : 18\nEpoch : 2, batch 122\n(Train) Batch 122 Loss : 3.5546555519104004, 맞은 개수 : 22\nEpoch : 2, batch 123\n(Train) Batch 123 Loss : 3.714975118637085, 맞은 개수 : 24\nEpoch : 2, batch 124\n(Train) Batch 124 Loss : 3.7486302852630615, 맞은 개수 : 16\nEpoch : 2, batch 125\n(Train) Batch 125 Loss : 3.9332470893859863, 맞은 개수 : 21\nEpoch : 2, batch 126\n(Train) Batch 126 Loss : 3.8338141441345215, 맞은 개수 : 23\nEpoch : 2, batch 127\n(Train) Batch 127 Loss : 3.6716597080230713, 맞은 개수 : 25\nEpoch : 2, batch 128\n(Train) Batch 128 Loss : 3.740590810775757, 맞은 개수 : 26\nEpoch : 2, batch 129\n(Train) Batch 129 Loss : 3.839750289916992, 맞은 개수 : 20\nEpoch : 2, batch 130\n(Train) Batch 130 Loss : 3.6538283824920654, 맞은 개수 : 24\nEpoch : 2, batch 131\n(Train) Batch 131 Loss : 3.5739145278930664, 맞은 개수 : 22\nEpoch : 2, batch 132\n(Train) Batch 132 Loss : 3.5836637020111084, 맞은 개수 : 28\nEpoch : 2, batch 133\n(Train) Batch 133 Loss : 3.7877449989318848, 맞은 개수 : 20\nEpoch : 2, batch 134\n(Train) Batch 134 Loss : 3.4647161960601807, 맞은 개수 : 25\nEpoch : 2, batch 135\n(Train) Batch 135 Loss : 3.605611562728882, 맞은 개수 : 25\nEpoch : 2, batch 136\n(Train) Batch 136 Loss : 3.730872869491577, 맞은 개수 : 31\nEpoch : 2, batch 137\n(Train) Batch 137 Loss : 3.664116621017456, 맞은 개수 : 24\nEpoch : 2, batch 138\n(Train) Batch 138 Loss : 3.533647298812866, 맞은 개수 : 22\nEpoch : 2, batch 139\n(Train) Batch 139 Loss : 3.820429563522339, 맞은 개수 : 15\nEpoch : 2, batch 140\n(Train) Batch 140 Loss : 3.7823948860168457, 맞은 개수 : 15\nEpoch : 2, batch 141\n(Train) Batch 141 Loss : 3.586247205734253, 맞은 개수 : 27\nEpoch : 2, batch 142\n(Train) Batch 142 Loss : 3.734772205352783, 맞은 개수 : 22\nEpoch : 2, batch 143\n(Train) Batch 143 Loss : 3.542429208755493, 맞은 개수 : 29\nEpoch : 2, batch 144\n(Train) Batch 144 Loss : 3.878713607788086, 맞은 개수 : 24\nEpoch : 2, batch 145\n(Train) Batch 145 Loss : 3.8533456325531006, 맞은 개수 : 19\nEpoch : 2, batch 146\n(Train) Batch 146 Loss : 3.9134652614593506, 맞은 개수 : 25\nEpoch : 2, batch 147\n(Train) Batch 147 Loss : 3.6527910232543945, 맞은 개수 : 23\nEpoch : 2, batch 148\n(Train) Batch 148 Loss : 3.880479574203491, 맞은 개수 : 21\nEpoch : 2, batch 149\n(Train) Batch 149 Loss : 3.570087432861328, 맞은 개수 : 24\nEpoch : 2, batch 150\n(Train) Batch 150 Loss : 3.612973928451538, 맞은 개수 : 22\nEpoch : 2, batch 151\n(Train) Batch 151 Loss : 3.753060817718506, 맞은 개수 : 13\nEpoch : 2, batch 152\n(Train) Batch 152 Loss : 3.8520588874816895, 맞은 개수 : 20\nEpoch : 2, batch 153\n(Train) Batch 153 Loss : 3.7058804035186768, 맞은 개수 : 19\nEpoch : 2, batch 154\n(Train) Batch 154 Loss : 3.8024938106536865, 맞은 개수 : 21\nEpoch : 2, batch 155\n(Train) Batch 155 Loss : 3.5495147705078125, 맞은 개수 : 28\nEpoch : 2, batch 156\n(Train) Batch 156 Loss : 3.6288604736328125, 맞은 개수 : 24\nEpoch : 2, batch 157\n(Train) Batch 157 Loss : 3.601696729660034, 맞은 개수 : 32\nEpoch : 2, batch 158\n(Train) Batch 158 Loss : 3.90494704246521, 맞은 개수 : 22\nEpoch : 2, batch 159\n(Train) Batch 159 Loss : 3.6253843307495117, 맞은 개수 : 22\nEpoch : 2, batch 160\n(Train) Batch 160 Loss : 3.4568769931793213, 맞은 개수 : 35\nEpoch : 2, batch 161\n(Train) Batch 161 Loss : 3.6314802169799805, 맞은 개수 : 20\nEpoch : 2, batch 162\n(Train) Batch 162 Loss : 3.828474521636963, 맞은 개수 : 27\nEpoch : 2, batch 163\n(Train) Batch 163 Loss : 3.7917661666870117, 맞은 개수 : 29\nEpoch : 2, batch 164\n(Train) Batch 164 Loss : 3.715925455093384, 맞은 개수 : 27\nEpoch : 2, batch 165\n(Train) Batch 165 Loss : 3.534646987915039, 맞은 개수 : 20\nEpoch : 2, batch 166\n(Train) Batch 166 Loss : 3.6532559394836426, 맞은 개수 : 26\nEpoch : 2, batch 167\n(Train) Batch 167 Loss : 3.6747405529022217, 맞은 개수 : 27\nEpoch : 2, batch 168\n(Train) Batch 168 Loss : 3.85465669631958, 맞은 개수 : 18\nEpoch : 2, batch 169\n(Train) Batch 169 Loss : 3.7505123615264893, 맞은 개수 : 27\nEpoch : 2, batch 170\n(Train) Batch 170 Loss : 3.6107709407806396, 맞은 개수 : 23\nEpoch : 2, batch 171\n(Train) Batch 171 Loss : 3.4815735816955566, 맞은 개수 : 29\nEpoch : 2, batch 172\n(Train) Batch 172 Loss : 3.7067792415618896, 맞은 개수 : 22\nEpoch : 2, batch 173\n(Train) Batch 173 Loss : 3.8005833625793457, 맞은 개수 : 23\nEpoch : 2, batch 174\n(Train) Batch 174 Loss : 3.80507755279541, 맞은 개수 : 18\nEpoch : 2, batch 175\n(Train) Batch 175 Loss : 3.387972593307495, 맞은 개수 : 26\nEpoch : 2, batch 176\n(Train) Batch 176 Loss : 3.5939693450927734, 맞은 개수 : 30\nEpoch : 2, batch 177\n(Train) Batch 177 Loss : 3.595414876937866, 맞은 개수 : 25\nEpoch : 2, batch 178\n(Train) Batch 178 Loss : 3.704254150390625, 맞은 개수 : 20\nEpoch : 2, batch 179\n(Train) Batch 179 Loss : 3.649138927459717, 맞은 개수 : 24\nEpoch : 2, batch 180\n(Train) Batch 180 Loss : 3.771437168121338, 맞은 개수 : 21\nEpoch : 2, batch 181\n(Train) Batch 181 Loss : 3.5856728553771973, 맞은 개수 : 27\nEpoch : 2, batch 182\n(Train) Batch 182 Loss : 3.7969865798950195, 맞은 개수 : 23\nEpoch : 2, batch 183\n(Train) Batch 183 Loss : 3.8977131843566895, 맞은 개수 : 25\nEpoch : 2, batch 184\n(Train) Batch 184 Loss : 3.7886006832122803, 맞은 개수 : 19\nEpoch : 2, batch 185\n(Train) Batch 185 Loss : 3.623995542526245, 맞은 개수 : 31\nEpoch : 2, batch 186\n(Train) Batch 186 Loss : 3.7796590328216553, 맞은 개수 : 19\nEpoch : 2, batch 187\n(Train) Batch 187 Loss : 3.8627877235412598, 맞은 개수 : 28\nEpoch : 2, batch 188\n(Train) Batch 188 Loss : 3.8696908950805664, 맞은 개수 : 21\nEpoch : 2, batch 189\n(Train) Batch 189 Loss : 3.7554774284362793, 맞은 개수 : 22\nEpoch : 2, batch 190\n(Train) Batch 190 Loss : 3.8348405361175537, 맞은 개수 : 21\nEpoch : 2, batch 191\n(Train) Batch 191 Loss : 3.6113715171813965, 맞은 개수 : 21\nEpoch : 2, batch 192\n(Train) Batch 192 Loss : 3.485649824142456, 맞은 개수 : 27\nEpoch : 2, batch 193\n(Train) Batch 193 Loss : 3.703552722930908, 맞은 개수 : 19\nEpoch : 2, batch 194\n(Train) Batch 194 Loss : 3.6869747638702393, 맞은 개수 : 21\nEpoch : 2, batch 195\n(Train) Batch 195 Loss : 3.9190526008605957, 맞은 개수 : 16\nEpoch : 2, batch 196\n(Train) Batch 196 Loss : 3.8043534755706787, 맞은 개수 : 28\nEpoch : 2, batch 197\n(Train) Batch 197 Loss : 3.603201389312744, 맞은 개수 : 28\nEpoch : 2, batch 198\n(Train) Batch 198 Loss : 3.8439383506774902, 맞은 개수 : 24\nEpoch : 2, batch 199\n(Train) Batch 199 Loss : 3.6041195392608643, 맞은 개수 : 29\nEpoch : 2, batch 200\n(Train) Batch 200 Loss : 3.651848316192627, 맞은 개수 : 27\nEpoch : 2, batch 201\n(Train) Batch 201 Loss : 3.413281202316284, 맞은 개수 : 42\nEpoch : 2, batch 202\n(Train) Batch 202 Loss : 3.279569387435913, 맞은 개수 : 32\nEpoch : 2, batch 203\n(Train) Batch 203 Loss : 3.677724838256836, 맞은 개수 : 20\nEpoch : 2, batch 204\n(Train) Batch 204 Loss : 3.9041473865509033, 맞은 개수 : 20\nEpoch : 2, batch 205\n(Train) Batch 205 Loss : 3.607299327850342, 맞은 개수 : 23\nEpoch : 2, batch 206\n(Train) Batch 206 Loss : 3.9256701469421387, 맞은 개수 : 24\nEpoch : 2, batch 207\n(Train) Batch 207 Loss : 3.609713554382324, 맞은 개수 : 22\nEpoch : 2, batch 208\n(Train) Batch 208 Loss : 3.530268907546997, 맞은 개수 : 26\nEpoch : 2, batch 209\n(Train) Batch 209 Loss : 3.801504135131836, 맞은 개수 : 27\nEpoch : 2, batch 210\n(Train) Batch 210 Loss : 3.5084495544433594, 맞은 개수 : 30\nEpoch : 2, batch 211\n(Train) Batch 211 Loss : 3.5632221698760986, 맞은 개수 : 26\nEpoch : 2, batch 212\n(Train) Batch 212 Loss : 3.5178704261779785, 맞은 개수 : 25\nEpoch : 2, batch 213\n(Train) Batch 213 Loss : 3.5177807807922363, 맞은 개수 : 27\nEpoch : 2, batch 214\n(Train) Batch 214 Loss : 3.775425910949707, 맞은 개수 : 15\nEpoch : 2, batch 215\n(Train) Batch 215 Loss : 3.7086358070373535, 맞은 개수 : 27\nEpoch : 2, batch 216\n(Train) Batch 216 Loss : 3.6423418521881104, 맞은 개수 : 25\nEpoch : 2, batch 217\n(Train) Batch 217 Loss : 3.5834884643554688, 맞은 개수 : 29\nEpoch : 2, batch 218\n(Train) Batch 218 Loss : 3.682710647583008, 맞은 개수 : 24\nEpoch : 2, batch 219\n(Train) Batch 219 Loss : 3.546718120574951, 맞은 개수 : 24\nEpoch : 2, batch 220\n(Train) Batch 220 Loss : 3.5410852432250977, 맞은 개수 : 24\nEpoch : 2, batch 221\n(Train) Batch 221 Loss : 3.654595375061035, 맞은 개수 : 21\nEpoch : 2, batch 222\n(Train) Batch 222 Loss : 3.6595537662506104, 맞은 개수 : 26\nEpoch : 2, batch 223\n(Train) Batch 223 Loss : 3.8516881465911865, 맞은 개수 : 20\nEpoch : 2, batch 224\n(Train) Batch 224 Loss : 3.8374083042144775, 맞은 개수 : 25\nEpoch : 2, batch 225\n(Train) Batch 225 Loss : 3.6397979259490967, 맞은 개수 : 18\nEpoch : 2, batch 226\n(Train) Batch 226 Loss : 3.5575320720672607, 맞은 개수 : 29\nEpoch : 2, batch 227\n(Train) Batch 227 Loss : 3.699262857437134, 맞은 개수 : 22\nEpoch : 2, batch 228\n(Train) Batch 228 Loss : 3.610546350479126, 맞은 개수 : 25\nEpoch : 2, batch 229\n(Train) Batch 229 Loss : 3.541093349456787, 맞은 개수 : 29\nEpoch : 2, batch 230\n(Train) Batch 230 Loss : 3.570692777633667, 맞은 개수 : 27\nEpoch : 2, batch 231\n(Train) Batch 231 Loss : 4.082185745239258, 맞은 개수 : 18\nEpoch : 2, batch 232\n(Train) Batch 232 Loss : 3.3000495433807373, 맞은 개수 : 24\nEpoch : 2, batch 233\n(Train) Batch 233 Loss : 3.458287477493286, 맞은 개수 : 26\nEpoch : 2, batch 234\n(Train) Batch 234 Loss : 3.885220527648926, 맞은 개수 : 15\nEpoch : 2, batch 235\n(Train) Batch 235 Loss : 3.7840094566345215, 맞은 개수 : 17\nEpoch : 2, batch 236\n(Train) Batch 236 Loss : 3.553612232208252, 맞은 개수 : 32\nEpoch : 2, batch 237\n(Train) Batch 237 Loss : 3.615535259246826, 맞은 개수 : 25\nEpoch : 2, batch 238\n(Train) Batch 238 Loss : 3.6979215145111084, 맞은 개수 : 19\nEpoch : 2, batch 239\n(Train) Batch 239 Loss : 3.9717350006103516, 맞은 개수 : 20\nEpoch : 2, batch 240\n(Train) Batch 240 Loss : 3.4179348945617676, 맞은 개수 : 28\nEpoch : 2, batch 241\n(Train) Batch 241 Loss : 3.539208173751831, 맞은 개수 : 26\nEpoch : 2, batch 242\n(Train) Batch 242 Loss : 3.720155715942383, 맞은 개수 : 24\nEpoch : 2, batch 243\n(Train) Batch 243 Loss : 3.84924054145813, 맞은 개수 : 22\nEpoch : 2, batch 244\n(Train) Batch 244 Loss : 3.7433807849884033, 맞은 개수 : 26\nEpoch : 2, batch 245\n(Train) Batch 245 Loss : 3.6208279132843018, 맞은 개수 : 23\nEpoch : 2, batch 246\n(Train) Batch 246 Loss : 3.7124085426330566, 맞은 개수 : 20\nEpoch : 2, batch 247\n(Train) Batch 247 Loss : 3.462998628616333, 맞은 개수 : 38\nEpoch : 2, batch 248\n(Train) Batch 248 Loss : 3.8956971168518066, 맞은 개수 : 21\nEpoch : 2, batch 249\n(Train) Batch 249 Loss : 3.09533953666687, 맞은 개수 : 29\nEpoch : 2, batch 250\n(Train) Batch 250 Loss : 3.7260398864746094, 맞은 개수 : 25\nEpoch : 2, batch 251\n(Train) Batch 251 Loss : 3.6944308280944824, 맞은 개수 : 19\nEpoch : 2, batch 252\n(Train) Batch 252 Loss : 3.601799249649048, 맞은 개수 : 29\nEpoch : 2, batch 253\n(Train) Batch 253 Loss : 3.8428711891174316, 맞은 개수 : 23\nEpoch : 2, batch 254\n(Train) Batch 254 Loss : 3.486330270767212, 맞은 개수 : 31\nEpoch : 2, batch 255\n(Train) Batch 255 Loss : 3.4774463176727295, 맞은 개수 : 30\nEpoch : 2, batch 256\n(Train) Batch 256 Loss : 3.8361263275146484, 맞은 개수 : 23\nEpoch : 2, batch 257\n(Train) Batch 257 Loss : 3.6646993160247803, 맞은 개수 : 21\nEpoch : 2, batch 258\n(Train) Batch 258 Loss : 3.741295337677002, 맞은 개수 : 31\nEpoch : 2, batch 259\n(Train) Batch 259 Loss : 3.4242985248565674, 맞은 개수 : 27\nEpoch : 2, batch 260\n(Train) Batch 260 Loss : 3.9297893047332764, 맞은 개수 : 17\nEpoch : 2, batch 261\n(Train) Batch 261 Loss : 3.6447153091430664, 맞은 개수 : 23\nEpoch : 2, batch 262\n(Train) Batch 262 Loss : 3.809255838394165, 맞은 개수 : 23\nEpoch : 2, batch 263\n(Train) Batch 263 Loss : 3.7443339824676514, 맞은 개수 : 26\nEpoch : 2, batch 264\n(Train) Batch 264 Loss : 3.718134880065918, 맞은 개수 : 21\nEpoch : 2, batch 265\n(Train) Batch 265 Loss : 3.4715983867645264, 맞은 개수 : 30\nEpoch : 2, batch 266\n(Train) Batch 266 Loss : 3.7131357192993164, 맞은 개수 : 29\nEpoch : 2, batch 267\n(Train) Batch 267 Loss : 3.8456499576568604, 맞은 개수 : 21\nEpoch : 2, batch 268\n(Train) Batch 268 Loss : 3.7975618839263916, 맞은 개수 : 26\nEpoch : 2, batch 269\n(Train) Batch 269 Loss : 3.4404549598693848, 맞은 개수 : 31\nEpoch : 2, batch 270\n(Train) Batch 270 Loss : 3.694115400314331, 맞은 개수 : 24\nEpoch : 2, batch 271\n(Train) Batch 271 Loss : 3.5822577476501465, 맞은 개수 : 25\nEpoch : 2, batch 272\n(Train) Batch 272 Loss : 3.7924277782440186, 맞은 개수 : 23\nEpoch : 2, batch 273\n(Train) Batch 273 Loss : 3.472362518310547, 맞은 개수 : 28\nEpoch : 2, batch 274\n(Train) Batch 274 Loss : 3.5640461444854736, 맞은 개수 : 24\nEpoch : 2, batch 275\n(Train) Batch 275 Loss : 3.7399799823760986, 맞은 개수 : 17\nEpoch : 2, batch 276\n(Train) Batch 276 Loss : 3.977267026901245, 맞은 개수 : 18\nEpoch : 2, batch 277\n(Train) Batch 277 Loss : 3.7356812953948975, 맞은 개수 : 18\nEpoch : 2, batch 278\n(Train) Batch 278 Loss : 4.054782867431641, 맞은 개수 : 14\nEpoch : 2, batch 279\n(Train) Batch 279 Loss : 3.6880850791931152, 맞은 개수 : 25\nEpoch : 2, batch 280\n(Train) Batch 280 Loss : 3.992300271987915, 맞은 개수 : 14\nEpoch : 2, batch 281\n(Train) Batch 281 Loss : 3.6853041648864746, 맞은 개수 : 27\nEpoch : 2, batch 282\n(Train) Batch 282 Loss : 3.6110594272613525, 맞은 개수 : 26\nEpoch : 2, batch 283\n(Train) Batch 283 Loss : 3.6368789672851562, 맞은 개수 : 21\nEpoch : 2, batch 284\n(Train) Batch 284 Loss : 3.7385923862457275, 맞은 개수 : 20\nEpoch : 2, batch 285\n(Train) Batch 285 Loss : 3.8270514011383057, 맞은 개수 : 25\nEpoch : 2, batch 286\n(Train) Batch 286 Loss : 3.4289207458496094, 맞은 개수 : 28\nEpoch : 2, batch 287\n(Train) Batch 287 Loss : 4.038549423217773, 맞은 개수 : 20\nEpoch : 2, batch 288\n(Train) Batch 288 Loss : 3.9602365493774414, 맞은 개수 : 21\nEpoch : 2, batch 289\n(Train) Batch 289 Loss : 3.636723756790161, 맞은 개수 : 26\nEpoch : 2, batch 290\n(Train) Batch 290 Loss : 3.7645370960235596, 맞은 개수 : 16\nEpoch : 2, batch 291\n(Train) Batch 291 Loss : 3.6582753658294678, 맞은 개수 : 21\nEpoch : 2, batch 292\n(Train) Batch 292 Loss : 3.5993733406066895, 맞은 개수 : 32\nEpoch : 2, batch 293\n(Train) Batch 293 Loss : 3.9158639907836914, 맞은 개수 : 16\nEpoch : 2, batch 294\n(Train) Batch 294 Loss : 3.8076248168945312, 맞은 개수 : 22\nEpoch : 2, batch 295\n(Train) Batch 295 Loss : 3.6102371215820312, 맞은 개수 : 22\nEpoch : 2, batch 296\n(Train) Batch 296 Loss : 3.828141450881958, 맞은 개수 : 22\nEpoch : 2, batch 297\n(Train) Batch 297 Loss : 3.5636985301971436, 맞은 개수 : 24\nEpoch : 2, batch 298\n(Train) Batch 298 Loss : 3.736964225769043, 맞은 개수 : 26\nEpoch : 2, batch 299\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3924529045.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# 총 batch가 200번 반복됨 , 에포크 5니까 총 1000번 반복됨.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch : {epoch}, batch {batch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch_true_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3267714355.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data, label)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"# 시각화 해보기 \nplt.plot(epoch_train_losses, label='Train')\nplt.plot(epoch_val_losses,label = 'Val')\nplt.xlabel('Epoch')\nplt.ylabel(\"Train Loss\")\nplt.title('Train Loss')\nplt.legend()\nplt.grid(True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T13:21:37.305540Z","iopub.execute_input":"2025-12-04T13:21:37.305989Z","iopub.status.idle":"2025-12-04T13:21:37.633166Z","shell.execute_reply.started":"2025-12-04T13:21:37.305933Z","shell.execute_reply":"2025-12-04T13:21:37.632168Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"wrong_denom = len(small_val_loader) # 8\ncorrect_denom = len(val_dataloader) # 79\ncorrect_denom\n\ncorrection_factor = wrong_denom / correct_denom\n\nepoch_val_losses = [loss * correction_factor for loss in epoch_val_losses]\n\nepoch_val_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T06:57:29.279260Z","iopub.execute_input":"2025-11-28T06:57:29.279536Z","iopub.status.idle":"2025-11-28T06:57:29.285279Z","shell.execute_reply.started":"2025-11-28T06:57:29.279514Z","shell.execute_reply":"2025-11-28T06:57:29.284675Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(epoch_train_losses,label ='Train Loss')\nplt.plot(epoch_val_losses, label = 'Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T13:21:40.232509Z","iopub.execute_input":"2025-12-04T13:21:40.233444Z","iopub.status.idle":"2025-12-04T13:21:40.462199Z","shell.execute_reply.started":"2025-12-04T13:21:40.233412Z","shell.execute_reply":"2025-12-04T13:21:40.461214Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABoYElEQVR4nO3dd1iV9f/H8ec5h42gOFBU3FvRHGkOVNwj06wsJUdlVmquTPOXpVamWbnTXGnLLJt+y1JUcOHArbk37i3gYJ379weBkROEc+DwelwX1xX3uc/Nm48or+5zn/tlMgzDQERERMRBmO09gIiIiEhGUrgRERERh6JwIyIiIg5F4UZEREQcisKNiIiIOBSFGxEREXEoCjciIiLiUJzsPYCtWa1WTp06hZeXFyaTyd7jiIiIyAMwDIPo6GgKFy6M2XzvczM5LtycOnUKf39/e48hIiIi6RAZGUnRokXvuU+OCzdeXl5A0uJ4e3tn6LHj4+NZunQpLVq0wNnZOUOPLbdonW1D62wbWmfb0VrbRmatc1RUFP7+/im/x+8lx4Wb5JeivL29MyXceHh44O3trb84mUjrbBtaZ9vQOtuO1to2MnudH+SSEl1QLCIiIg5F4UZEREQcisKNiIiIOJQcd82NiIg4lsTEROLj4++7X3x8PE5OTty8eZPExEQbTJYzPcw6u7i43Pdt3g9C4UZERLIlwzA4c+YMV65ceeD9CxUqRGRkpO5zlokeZp3NZjMlS5bExcXloWZQuBERkWwpOdj4+vri4eFx31+kVquVmJgYcuXKlSFnB+TO0rvOyTfZPX36NMWKFXuoAKpwIyIi2U5iYmJKsMmXL98DPcdqtRIXF4ebm5vCTSZ6mHUuUKAAp06dIiEh4aHeRq4/XRERyXaSr7Hx8PCw8ySSkZJfjnrYa6IUbkREJNvStTOOJaP+PBVuRERExKEo3IiIiIhDUbgRERHJ5kqUKMHEiRPtPUaWoXCTgcIPXSTBau8pREQkqzKZTPf8GDlyZLqOGxERQa9evR5qtsaNGzNgwICHOkZWobeCZ5DD52N44cvN5Ha2cKPQCZ6tXRxni7KjiIjccvr06ZT//v7773n33XfZt29fyrZcuXKl/LdhGCQmJuLkdP9f1QUKFMjYQbM5/fbNICev3KBALlcux5kY/ttugj4J44eISOITdSpHRMQWDMPgelzCPT9uxCXed5/0fBiG8UAzFipUKOUjd+7cmEymlM/37t2Ll5cXf/75JzVr1sTV1ZU1a9Zw6NAh2rdvT8GCBcmVKxePPvooy5YtS3Xc/74sZTKZmD17Nk8++SQeHh6ULVuWRYsWPdT6/vTTT1SuXBlXV1dKlCjBp59+murxadOmUbZsWTw8PChXrhzPPPNMymM//vgjAQEBuLu7ky9fPpo1a8a1a9ceap570ZmbDBJYtgDLBjbg3a+WsuaCOycu32DITzv4LOwgrzcpS4dHCuOkMzkiIpnmRnwild5dYpevvfu9lni4ZMyv1LfeeotPPvmEUqVK4ePjQ2RkJG3atGH06NG4urry1Vdf0a5dO/bt20exYsXuepxRo0Yxbtw4Pv74Y6ZMmUJwcDDHjh0jb968aZ5p8+bNdOrUiZEjR/Lss88SHh5O7969yZcvHz169GDTpk3069ePr7/+mscee4zIyEi2bt0KJJ2t6ty5M+PGjePJJ58kOjqa1atXP3AgTA+Fmwzk5myhsZ/BqK6B/LDlFJ+vPMSxi9cZvHA7n4UepH/TsrSrVhiLWfdlEBGRO3vvvfdo3rx5yud58+alWrVqKZ+///77/PLLLyxatIi+ffve9Tg9evSgc+fOAHz44YdMnjyZjRs30qpVqzTPNH78eJo2bco777wDQLly5di9ezcff/wxPXr04Pjx43h6evL444/j6emJj48PDRo0AJLCTUJCAh07dqR48eIABAQEpHmGtFC4yQTuLhZ6BpaiS51ifL3uGDNWHebIhWsM+H4bU1YcoH+zcrQN8FPIERHJQO7OFna/1/Kuj1utVqKjovHy9srw+gV3Z0uGHatWrVqpPo+JiWHkyJH88ccfKUHhxo0bHD9+/J7HqVq1asp/e3p64u3tzblz59I10549e2jfvn2qbfXr12fixIkkJibSvHlzihcvTqlSpWjZsiUNGzakS5cu5MqVi2rVqtG0aVMCAgJo2bIlLVq04Omnn8bHxyddszwIvU6SiTxcnHilUWlWDwliSKvy5PFw5tD5a/T7biutJq7ijx2nsVoz77SciEhOYjKZ8HBxuueHu4vlvvuk5yMj75Ts6emZ6vPBgwfzyy+/8OGHH7J69Wq2bdtGQEAAcXFx9zzOf7uZTCYTVmvmXAfq5eXFli1b+O677/Dz82PMmDFUr16dK1euYLFYCAkJ4c8//6RSpUpMmTKF8uXLc+TIkUyZBRRubMLT1YnejcuwekgQbzQvh7ebEwfOxdBn/hbaTF7NX7vOZOprjyIikn2tXbuWHj168OSTTxIQEEChQoU4evSoTWeoWLEia9euvW2ucuXKYbEknbVycnKiWbNmfPTRR6xZs4ajR4+yYsUKIClY1a9fn1GjRrF161ZcXFz45ZdfMm1evSxlQ15uzrzetCzd65fgizVHmLP6CHvPRPPqN5up5OfNwOblaFbRV10pIiKSomzZsvz888+0a9cOk8nEO++8k2lnYM6fP8+2bdtSbfPz8+ONN97g0Ucf5f333+fZZ59l3bp1TJ06lWnTpgHw+++/c/jwYRo2bEju3Ln5+eefsVqtlC9fng0bNrB8+XJatGiBr68vGzZs4Pz581SsWDFTvgfQmRu78HZzZkCzcqwZ2oR+TcqQy9WJ3aejePmrTTwxdS0r9p7VmRwREQGSLub18fGhXr16tGvXjpYtW1KjRo1M+Vrz58+nevXqqT5mzZpFjRo1+OGHH1iwYAFVqlTh3Xff5b333qNHjx4A5MmTh59//pkmTZpQuXJl5s6dy7fffkvlypXx9vZm1apVtGnThnLlyjF8+HA+/fRTWrdunSnfA4DJyGG/RaOiosidOzdXr17F29s7Q48dHx/P4sWLadOmzW2vdd7L5WtxzFp9mHnhR7kel1TzXs0/DwOblaVRuQI6k/Mf6V1nSRuts21ondPn5s2bHDlyhJIlS+Lm5vZAz7FarURFReHt7Z3hFxTLLQ+zzvf6c03L72/96WYBPp4uDGlVgdVDgnilUSncnS1sj7xCj7kRPDU9nDUHLuhMjoiIyANSuMlC8uVyZVjriqweGsTLgSVxdTKz5fgVnp+zgU4z1hF+6IK9RxQREcnyFG6yoPy5XHm7bSVWDwnihfolcHEyE3H0Ml1mbeC5mevYcPiivUcUERHJshRusjBfbzdGtKvMqjeD6F63OC4WM+sPX+LZmesJnr2ezccu2XtEERGRLEfhJhsolNuNUe2rEPZmY55/rBjOFhNrD17kqenr6PbFRrYev2zvEUVERLIMhZtspHAedz7oEEDo4MZ0ru2Pk9nEqv3neXJaOC/M3ciOE1fsPaKIiIjdKdxkQ0V9PBjTsSqhgxvTqVZRLGYTofvO88TUtfT8MoJdJ6/ae0QRERG7UbjJxvzzejDu6WosH9SIp2oUxWyCZXvO8fiUNbzy9Sb2nI6y94giIiI2p3DjAErk9+TTTtVYNqgRHR4pjMkES/4+S+tJq+n97Wb2nYm294giIpKBGjduzIABA+w9RpalcONAShXIxcTnqhMysCHtqiWFnMU7z9Bq0ir6zt/CwXMKOSIi9tSuXTtatWp1x8dWr16NyWRix44dD/115s2bR548eR76ONmVwo0DKuPrxZTO1VkyoCFtA/wwDPh9x2maT1hF/wVbOXQ+xt4jiojkSC+99BIhISGcOHHitsfmzp1LrVq1qFq1qh0mcyx2DTcjR47EZDKl+qhQocI9n7Nw4UIqVKiAm5sbAQEBLF682EbTZj/lCnrxWXAN/uwfSKvKhTAM+G3bKZqPX8mgH7Zx9MI1e48oIpKjPP744xQoUIB58+al2h4TE8PChQt56aWXuHjxIp07d6ZIkSJ4eHgQEBDAd999l6FzHD9+nPbt25MrVy68vb3p1KkTZ8+eTXl8+/btBAUF4eXlhbe3NzVr1mTTpk0AHDt2jHbt2uHj44OnpyeVK1fOcr+Lnew9QOXKlVm2bFnK505Odx8pPDyczp07M2bMGB5//HHmz59Phw4d2LJlC1WqVLHFuNlSRT9vPu9ak10nrzJx2QGW7TnLz1tO8tu2U3SsXoTXm5SlWD4Pe48pIvJwDAPir9/9cas16fE4C2R0caazBzxAybGTkxPdunVj3rx5vP322ynFyAsXLiQxMZHOnTsTExNDzZo1GTp0KN7e3vzxxx907dqV0qVLU7t27Yce1Wq1pgSblStXkpCQQJ8+fXj22WcJCwsDIDg4mOrVqzN9+nQsFgvbtm1LKXbt06cPcXFxrFq1Ck9PT3bv3k2uXLkeeq6MZPdw4+TkRKFChR5o30mTJtGqVSvefPNNAN5//31CQkKYOnUqn3/++R2fExsbS2xsbMrnUVFJ7yCKj48nPj7+IadPLfl4GX3cjFLe14PpXaqx8+RVJq84RNj+CyzcfIJftp6kY/XC9G5ciiJ53O095n1l9XV2FFpn29A6p098fDyGYWC1WrFarUkb465hHlv0rs8xA3kyaR7rWyfAxfOB9u3Rowcff/wxoaGhNG7cGEh6Sapjx454eXnh5eXFoEGDUvbv06cPf/31F99//z21atVK2Z78/d9xnn+23+nxkJAQdu7cyaFDh/D39weSrtEJCAhgw4YNPProoxw/fpw33niDcuXKAVC6dOmU4x0/fpyOHTtSuXJlAEqUKJHqayUXPd9rvruxWq0YhkF8fDwWiyXVY2n5O2L3cHPgwAEKFy6Mm5sbdevWZcyYMRQrVuyO+65bty7VHzhAy5Yt+fXXX+96/DFjxjBq1Kjbti9duhQPj8w5WxESEpIpx81IT+aD6lXgz0gze6+a+WHzSX7acoLHfA2aF7Hi42rvCe8vO6yzI9A624bWOW2S/8c4JiaGuLi4pI3x1zMtvNxPVHQ0OCc+0L6FCxemdu3azJw5kxo1anD48GFWr17N//73P6KiokhMTGT8+PH88ssvnD59mvj4eGJjY3FxcUn5H/SEhATi4uJSPv+vmzdvYhjGHR/ftm0bRYoUIXfu3CmPFy1alNy5c7N161bKly9P79696dWrF19++SWNGjWiQ4cOlCxZEoCePXvyxhtv8Oeff9K4cWPatWt3x1dPoqPT/iaWuLg4bty4wapVq0hISEj12PXr9zgr9x92DTd16tRh3rx5lC9fntOnTzNq1CgCAwPZtWsXXl5et+1/5swZChYsmGpbwYIFOXPmzF2/xrBhw1IFoqioKPz9/WnRogXe3t4Z982QlCpDQkJo3rx5yum7rK43sOX4FSatOEj4oUusPWti4wULz9YqyisNS1LI283eI94mO65zdqR1tg2tc/rcvHmTyMhIcuXKhZvbP/9OGV5JZ1DuwjAMomNi8MqVK+XloIzi/YAvSyV7+eWX6d+/PzNmzODHH3+kdOnStG7dGpPJxEcffcSMGTMYP348AQEBeHp6MnDgQKxWa8rvLScnJ1xcXO76e8zNzQ2TyXTHx93c3DCbzbc9ZjKZcHNzw9vbmw8//JAePXqwePFi/vzzT8aOHcv8+fN58skn6du3L+3bt+ePP/4gJCSEJk2a8Mknn9C3b1/gn3WOjsbLyyvN63zz5k3c3d1p2LDhrT/Xf9wtyN2JXcNN69atU/67atWq1KlTh+LFi/PDDz/w0ksvZcjXcHV1xdX19tMQzs7OmfYPSWYeOzPUKV2A+aULsPHIJSaE7Gfd4Yt8syGSHzafpEvtYvRuXBrfLBhysts6Z1daZ9vQOqdNYmIiJpMJs9mM+d/Xz1hu/x/jZFarFWKtmFxzpX6OHTz33HMMHDiQBQsW8PXXX/Paa6+lvAwTHh5O+/bt6datG5A094EDB6hUqVKquZO//ztJ3n6nxytVqkRkZCQnT55MeVlq9+7dXLlyhSpVqqQ8p0KFClSoUIFBgwbRuXNnvvzyS5566ikAihcvTu/evenduzfDhg1j9uzZ9OvXL2Xe+813N2azGZPJdMe/D2n5+2H3l6X+LU+ePJQrV46DBw/e8fFChQqlupob4OzZsw98zY7cW+2Sefmu12OEH7rAxJADbDx6iXnhR/lu43G6PlacVxqVpoBXNni9SkQki8uVKxfPPvssw4YNIyoqih49eqQ8VrZsWX788UfCw8Px8fFh/PjxnD17lkqVKqXpayQmJrJt27ZU21xdXWnWrBkBAQEEBwczceJEEhIS6N27N40aNaJWrVrcuHGDN998k6effpqSJUty4sQJIiIiUoLNgAEDaN26NeXKlePy5cuEhoZSsWLFh12SDJWl7nMTExPDoUOH8PPzu+PjdevWZfny5am2hYSEULduXVuMl2PUK52f7195jG971qFmcR9iE6zMXnOEwHErGLN4DxdjYu9/EBERuaeXXnqJy5cv07JlSwoXLpyyffjw4dSoUYOWLVvSuHFjChUqRIcOHdJ8/JiYGKpXr57qo127dphMJn777Td8fHxo2LAhzZo1o1SpUnz//fcAWCwWLl68SLdu3ShXrhydOnWidevWKdevJiYm0qdPHypWrEirVq0oV64c06ZNy5A1ySh2PXMzePBg2rVrR/HixTl16hQjRozAYrHQuXNnALp160aRIkUYM2YMAP3796dRo0Z8+umntG3blgULFrBp0yZmzpxpz2/DIZlMJuqXyU+90vlYdeACE0L2sy3yCjNWHebr9cfoUa8ELweWwsfTxd6jiohkS3Xr1k15Z9G/5c2b955vlAFS3rJ9Nz169Eh1Nui/ihUrxm+//XbHx1xcXO55X50pU6bc82tnBXYNNydOnKBz585cvHiRAgUK0KBBA9avX0+BAgWApJsM/fv1unr16jF//nyGDx/O//3f/1G2bFl+/fVX3eMmE5lMJhqVK0DDsvkJ23ee8SH72XnyKtPCDvHVumO8UL8EPRuUIreHrhUQEZGswa7hZsGCBfd8/E7J9JlnnuGZZ57JpInkbkwmE0EVfGlcvgDL95xjfMh+dp+OYsqKg8xbe5QXG5TkxQYlye2ukCMiIvaVpa65kazPZDLRrFJB/ujXgM+fr0mFQl5ExyYwafkBAj9aweTlB4i+qZuRiYiI/SjcSLqYTCZaVSnE4n6BTAuuQbmCuYi6mcD4kP0Ejgvls9CDxMQm3P9AIiIiGUzhRh6K2WyiTYAff/VvyJTO1SldwJMr1+P5eMk+Aj9awecrD3E9TiFHRDLHnS7Ilewro/48FW4kQ5jNJtpVK8zSgY2Y9NwjlMrvyeXr8Yz9cy+BH4Uya9VhbsQ92K3JRUTuJ/mGbmm5Jb9kfclVGv/tlUqrLHUTP8n+LGYT7R8pQtsAP37bdorJKw5w7OJ1Ri/ew4xVh3mtcWmC6xTDzfnhfnBFJGezWCzkyZOHc+fOAeDh4XHfW/1brVbi4uK4efOm3e9Q7MjSu85Wq5Xz58/j4eGBk9PDxROFG8kUThYzT9UsyhOPFOaXrSeZvPwAJy7f4P3fdzNj5SH6BJXh2Uf9FXJEJN2S706fHHDuxzAMbty4gbu7e4Z3S8ktD7POZrOZYsWKPfSfj8KNZCpni5lOtfx5snoRftp8gikrDnLyyg1GLPqbz1ceondQGTrVKoqrk0KOiKSNyWTCz88PX19f4uPv/y7N+Ph4Vq1aRcOGDdXjlYkeZp1dXFwy5Kyawo3YhLPFzHO1i9GxRlF+2BTJZ6EHOX31Ju/8uovPww7Rt0kZnq5ZFGeLThWLSNpYLJYHukbDYrGQkJCAm5ubwk0mygrrrN8kYlMuTmaef6w4YW825r32lSno7crJKzcY9vNOgj4J44eISOITrfYeU0REsjGFG7ELVycL3eqWYOWbQbz7eCUKeLly4vINhvy0g2bjV/Lj5hMkKOSIiEg6KNyIXbk5W3ixQUlWvRnE8LYVyZ/LhWMXrzN44XaaT1jFr1tPkmjVfSxEROTBKdxIluDuYqFnYClWDQliWOsK+Hg4c+TCNQZ8v40WE1ayaPsphRwREXkgCjeSpXi4OPFKo9KsHtqEN1uWJ4+HM4fOX6Pfd1tpNXEVf+w4jVUhR0RE7kHvlpIsKZerE32CytCtbnHmrT3KrNWHOXAuhj7zt1C+YC7q5zbRWrddFxGRO9CZG8nSvNyceb1pWVYPbcKAZmXxcnVi39kYvthvof209YTsPqtuGRERSUXhRrKF3O7ODGhWjjVDm9CncSlcLQZ7zkTz8lebeGLqWlbsVcgREZEkCjeSreT2cGZA0zKMqJ7Iqw1L4uFiYefJq7w4bxMdpoUTtu+cQo6ISA6ncCPZkqczvNG8LKuHBPFKo1K4O1vYHnmFHnMjeGp6OGsOXFDIERHJoRRuJFvLl8uVYa0rsmpIED0blMTVycyW41d4fs4GOs1YR/ihC/YeUUREbEzhRhxCAS9Xhj9eidVDgnihfglcnMxEHL1Ml1kbeG7mOjYcvmjvEUVExEYUbsSh+Hq7MaJdZVa9GUT3usVxsZhZf/gSz85cT/Ds9Ww6esneI4qISCZTuBGHVCi3G6PaVyHszcYE1ymGs8XE2oMXefrzdXSds4Etxy/be0QREckkCjfi0ArncWf0kwGEDm5M59r+OJlNrD5wgY7Twnlh7kZ2nLhi7xFFRCSDKdxIjlDUx4MxHasSOrgxnWoVxWI2EbrvPE9MXUvPLyPYdfKqvUcUEZEMonAjOYp/Xg/GPV2N5YMa8VSNophNsGzPOR6fsoZXvt7EntNR9h5RREQeksKN5Egl8nvyaadqLBvUiA6PFMZkgiV/n6X1pNX0/nYz+85E23tEERFJJ4UbydFKFcjFxOeqEzKwIe2qJYWcxTvP0GrSKvrO38LBcwo5IiLZjcKNCFDG14spnauzZEBD2gb4YRjw+47TNJ+wiv4LtnLofIy9RxQRkQekcCPyL+UKevFZcA3+7B9Iy8oFMQz4bdspmo9fyaAftnH0wjV7jygiIvehcCNyBxX9vJnRtRa/v96AZhULYjXg5y0naTp+JW8u3M7xi9ftPaKIiNyFwo3IPVQpkpvZ3WuxqG99mlTwJdFqsHDzCZp8GsZbP+3gxGWFHBGRrEbhRuQBVC2ahy96PMovvevRsFwBEqwGCyIiCfokjLd/2cmpKzfsPaKIiPxD4UYkDaoX8+GrF2vz02t1aVAmP/GJBt9uOE7jj8N497ddnLl6094jiojkeAo3IulQs3hevulZhx9eqUvdUvmIS7Ty1bpjNPw4lJGL/uZclEKOiIi9KNyIPITaJfPyXa/HmP9yHWqXyEtcgpV54UcJHBfKB7/v5nx0rL1HFBHJcRRuRDJAvdL5+f6Vx/jmpTrULO5DbIKV2WuOEDhuBWMW7+FijEKOiIitKNyIZBCTyUSDsvn58dW6fPlibR7xz8PNeCszVh0mcFwoH/21l8vX4uw9poiIw1O4EclgJpOJRuUK8Evveszt8SgBRXJzPS6R6WGHaPDRCj5duo+r1+PtPaaIiMPKMuFm7NixmEwmBgwYcM/9Jk6cSPny5XF3d8ff35+BAwdy86Yu3pSsx2QyEVTBl0V96zO7Wy0q+XlzLS6RKSsO0uCjFUwI2c/VGwo5IiIZzcneAwBEREQwY8YMqlates/95s+fz1tvvcUXX3xBvXr12L9/Pz169MBkMjF+/HgbTSuSNiaTiWaVCtK0oi9L/j7LxGX72XsmmknLDzB37RF6Bpbihfol8HJztveoIiIOwe5nbmJiYggODmbWrFn4+Pjcc9/w8HDq169Ply5dKFGiBC1atKBz585s3LjRRtOKpJ/JZKJVlUIs7hfItOAalCuYi6ibCYwP2U/guFA+Cz1ITGyCvccUEcn27H7mpk+fPrRt25ZmzZrxwQcf3HPfevXq8c0337Bx40Zq167N4cOHWbx4MV27dr3rc2JjY4mNvfVOlaioKADi4+OJj8/YlwSSj5fRx5XUHGGdm1fIT9Ny+fjz77NMXnGIwxeu8fGSfcxefZieDUrwfB1/PFzs+9fTEdY5O9A6247W2jYya53TcjyTYRhGhn71NFiwYAGjR48mIiICNzc3GjduzCOPPMLEiRPv+pzJkyczePBgDMMgISGBV199lenTp991/5EjRzJq1Kjbts+fPx8PD4+M+DZEHorVgC0XTCw5YebcTRMAuZwMmhax0qCggYvFzgOKiGQB169fp0uXLly9ehVvb+977mu3cBMZGUmtWrUICQlJudbmfuEmLCyM5557jg8++IA6depw8OBB+vfvz8svv8w777xzx+fc6cyNv78/Fy5cuO/ipFV8fDwhISE0b94cZ2ddP5FZHHWdExKt/G/HGaaGHeL4paSuqvy5XOgVWJLOjxbFzdm2KcdR1zmr0TrbjtbaNjJrnaOiosifP/8DhRu7nffevHkz586do0aNGinbEhMTWbVqFVOnTiU2NhaLJfU/5u+88w5du3alZ8+eAAQEBHDt2jV69erF22+/jdl8+yVErq6uuLq63rbd2dk50364M/PYcoujrbOzM3SqXZwna/rzy9aTTF5+gBOXb/Dhn/uYveYofYLK8Oyj/jYPOY62zlmV1tl2tNa2kdHrnJZj2S3cNG3alJ07d6ba9sILL1ChQgWGDh16W7CBpFNS/w0wyfvZ8dU1kQzlbDHTqZY/T1Yvwk+bTzBlxUFOXrnBiEV/8/nKQ/QOKkOnWkVxddLrVSIid2K3cOPl5UWVKlVSbfP09CRfvnwp27t160aRIkUYM2YMAO3atWP8+PFUr1495WWpd955h3bt2t0xDIlkZ84WM8/VLkbHGkX5YVMkn4Ue5PTVm7zz6y4+DztE3yZleLpmUZwtdn/To4hIlmL3d0vdy/Hjx1OdqRk+fDgmk4nhw4dz8uRJChQoQLt27Rg9erQdpxTJXC5OZp5/rDhP1yzK9xGRTAtLOpMz7OedfBZ6kH5NyvJkjSIKOSIi/8hS4SYsLOyenzs5OTFixAhGjBhhu6FEsgg3Zwvd65Xg2Uf9mb/hONPCDnHi8g2G/LSDz8IO8nqTsnR4pDBOCjkiksPpX0GRbMbN2cKLDUqyekgQw9tWJJ+nC8cuXmfwwu00n7CKX7eeJNGqa9BEJOdSuBHJptxdLPQMLMXqoUEMa10BHw9njly4xoDvt9FiwkoWbT+lkCMiOZLCjUg25+HixCuNSrN6aBPebFmePB7OHDp/jX7fbaXVxFX8seM0VoUcEclBFG5EHEQuVyf6BJVh9ZAg3mheDm83Jw6ci6HP/C20mbyav3Yp5IhIzqBwI+JgvNyceb1pWVYPbcKAZmXxcnVi75loXv1mC49PWUPI7rO6L5SIODSFGxEHldvdmQHNyrFmaBNeb1KGXK5O7D4dxctfbeKJqWtZsVchR0Qck8KNiIPL7eHMGy3Ks3pIEL0bl8bDxcLOk1d5cd4mOkwLJ2zfOYUcEXEoCjciOYSPpwtDWlVg9ZAgXmlUCndnC9sjr9BjbgRPTQ9nzYELCjki4hAUbkRymHy5XBnWuiKrhgTRs0FJXJ3MbDl+hefnbKDTjHWEH7pg7xFFRB6Kwo1IDlXAy5Xhj1di9ZAgXqhfAhcnMxFHL9Nl1gaem7mOjUcv2XtEEZF0UbgRyeF8vd0Y0a4yq94Monvd4rhYzKw/fIngOZv4bLeZzccu23tEEZE0UbgREQAK5XZjVPsqhL3ZmOA6xXC2mNh/1cxzsyPoOmcDW44r5IhI9qBwIyKpFM7jzugnAwgZ0IC6vlaczCZWH7hAx2nhvDB3IztOXLH3iCIi96RwIyJ3VCSPO8+VtrJ0QH061SqKxWwidN95npi6lp5fRrDr5FV7jygickcKNyJyT/4+Hox7uhrLBzWiY40imE2wbM85Hp+yhle+3sSe01H2HlFEJBWFGxF5ICXyezK+0yMsG9SIDo8UxmSCJX+fpfWk1fT+djP7zkTbe0QREUDhRkTSqFSBXEx8rjohAxvSrlpSyFm88wytJq2i7/wtHDynkCMi9qVwIyLpUsbXiymdq7NkQEPaBvhhGPD7jtM0n7CK/gu2cuh8jL1HFJEcSuFGRB5KuYJefBZcgz/7B9KyckEMA37bdorm41cy6IdtHL1wzd4jikgOo3AjIhmiop83M7rW4vfXG9CsYkGsBvy85SRNx6/kzYXbOX7xur1HFJEcQuFGRDJUlSK5md29Fov61ieofAESrQYLN5+gyadhvPXTDk5cVsgRkcylcCMimaJq0TzMfaE2v/SuR8NyBUiwGiyIiCTokzDe/mUnp67csPeIIuKgFG5EJFNVL+bDVy/W5qfX6tKgTH7iEw2+3XCcxh+H8e5vuzhz9aa9RxQRB6NwIyI2UbN4Xr7pWYcfXqlL3VL5iEu08tW6YzT8OJSRi/7mXJRCjohkDIUbEbGp2iXz8l2vx5j/ch1ql8hLXIKVeeFHCRwXyge/7+Z8dKy9RxSRbE7hRkTsol7p/Hz/ymN881IdahTLQ2yCldlrjhA4bgVjFu/hYoxCjoikj8KNiNiNyWSiQdn8/PRaPb58sTbV/PNwM97KjFWHCRwXykd/7eXytTh7jyki2YzCjYjYnclkolG5Avzaux5zezxKQJHcXI9LZHrYIRp8tIJPl+7j6vV4e48pItmEwo2IZBkmk4mgCr4s6lufWd1qUcnPm2txiUxZcZAGH61gQsh+rt5QyBGRe1O4EZEsx2Qy0bxSQf7o14DPn69JhUJeRMcmMGn5AQI/WsHk5QeIvqmQIyJ3pnAjIlmWyWSiVZVCLO4XyLTgGpQrmIuomwmMD9lP4LhQPgs9SExsgr3HFJEsRuFGRLI8s9lEmwA//urfkCmdq1O6gCdXrsfz8ZJ9BH60gs9XHuJ6nEKOiCRRuBGRbMNsNtGuWmGWDmzExGcfoWR+Ty5fj2fsn3sJ/CiUWasOcyMu0d5jioidKdyISLZjMZvoUL0IIQMb8ukz1Siez4OL1+IYvXgPgeNCmbPmCDfjFXJEciqFGxHJtpwsZp6qWZRlgxox7umqFPVx50JMLO//vpuG40L5MvyoQo5IDqRwIyLZnrPFTKda/qx4ozFjOgZQJI8756JjGbHob4I+CePr9ceITVDIEckpFG5ExGG4OJnpXLsYoYMb80GHKvjlduP01Zu88+sumnyyku82Hic+0WrvMUUkkynciIjDcXEy8/xjxQkd3JhRT1SmoLcrJ6/cYNjPOwn6JIwfIiIVckQcmMKNiDgsN2cL3euVYOWbQbz7eCXy53LlxOUbDPlpB83Gr+THzSdIUMgRcThZJtyMHTsWk8nEgAED7rnflStX6NOnD35+fri6ulKuXDkWL15smyFFJFtyc7bwYoOSrB4SxPC2Fcnn6cKxi9cZvHA7zSes4tetJ0m0GvYeU0QyiJO9BwCIiIhgxowZVK1a9Z77xcXF0bx5c3x9ffnxxx8pUqQIx44dI0+ePLYZVESyNXcXCz0DS9GlTjG+WneMGSsPceTCNQZ8v40pKw7Qv1k52gb4YTGb7D2qiDwEu4ebmJgYgoODmTVrFh988ME99/3iiy+4dOkS4eHhODs7A1CiRIl7Pic2NpbY2NiUz6OiogCIj48nPj5ju2mSj5fRx5XUtM624cjr7GyCl+oV49mahflm/XFmrz3KofPX6PfdViYv20+/JqVpWakgZhuEHEde56xGa20bmbXOaTmeyTAMu56L7d69O3nz5mXChAk0btyYRx55hIkTJ95x3zZt2pA3b148PDz47bffKFCgAF26dGHo0KFYLJY7PmfkyJGMGjXqtu3z58/Hw8MjI78VEcmmbibAyjMmQk+ZuZGYFGj8PAxaF7USkNdAJ3JE7O/69et06dKFq1ev4u3tfc997XrmZsGCBWzZsoWIiIgH2v/w4cOsWLGC4OBgFi9ezMGDB+nduzfx8fGMGDHijs8ZNmwYgwYNSvk8KioKf39/WrRocd/FSav4+HhCQkJo3rx5ypklyXhaZ9vIaevcEYi6Ec+X647zRfgxTl9P4Iv9FioW8qJ/k9I0qVAAkynjU05OW2d70lrbRmatc/IrLw/CbuEmMjKS/v37ExISgpub2wM9x2q14uvry8yZM7FYLNSsWZOTJ0/y8ccf3zXcuLq64urqett2Z2fnTPvhzsxjyy1aZ9vISeucz9mZQS0r8FJgaWavOczctUfZcyaaV+dvI6BIbgY2L0tQed9MCTk5aZ3tTWttGxm9zmk5lt3eLbV582bOnTtHjRo1cHJywsnJiZUrVzJ58mScnJxITLz9bqJ+fn6UK1cu1UtQFStW5MyZM8TFxdlyfBFxYLk9nHmjRXlWDwmid+PSeLhY2HnyKi/O20SHaeGE7TuHnV/RF5F7sFu4adq0KTt37mTbtm0pH7Vq1SI4OJht27bd8Rqa+vXrc/DgQazWW/el2L9/P35+fri4uNhyfBHJAXw8XRjSqgKrhwTxSqNSuDtb2B55hR5zI3hqejhrDlxQyBHJguwWbry8vKhSpUqqD09PT/Lly0eVKlUA6NatG8OGDUt5zmuvvcalS5fo378/+/fv548//uDDDz+kT58+9vo2Uku4ae8JRCQT5MvlyrDWFVk1JIieDUri6mRmy/ErPD9nA51mrCP80AV7jygi/5JlbuJ3J8ePH+f06dMpn/v7+7NkyRIiIiKoWrUq/fr1o3///rz11lt2nPIfN6NwmlqDase/gIsH7D2NiGSCAl6uDH+8EquHBPFC/RK4OJmJOHqZLrM28NzMdWw4fNHeI4oIWeA+N/8WFhZ2z88B6taty/r1620zUFrsW4zp2jlKXDuH8Xk9qNAW6vWDYnXsPZmIZDBfbzdGtKvMKw1LMz3sIN9tjGT94Us8O3M99cvkY2CzctQqkdfeY4rkWFn6zE22UvVZErr9zmnv6pgwYO/v8EULmNMS9v4BVvXXiDiaQrndGNW+CmFvNia4TjGcLSbWHrzI05+vo+ucDWw5ftneI4rkSAo3GcVkwvB/jI2lBxL/SjhU7woWF4hcDwu6wGe1YfOXEK/rckQcTeE87ox+MoDQwY3pXNsfJ7OJ1Qcu0HFaOC/M3ciOE1fsPaJIjqJwkxnyl4P2U2HATmgwEFxzJ12H879+MDEAVn0CN/R/dCKOpqiPB2M6ViV0cGM61SqKxWwidN95npi6lp5fRrDr5FV7jyiSIyjcZCavQtBsJAz6G1qMBu8icO0crHgfxleGv4bBlUh7TykiGcw/rwfjnq7G8kGN6FijCGYTLNtzjsenrOGVrzex5/SD32lVRNJO4cYWXL2gXl/ovx2enAm+lSH+GqyfBpOqwU8vw5md9p5SRDJYifyejO/0CCGDGtHhkcKYTLDk77O0nrSa3t9uZt+ZaHuPKOKQFG5syeIM1Z6F19bC8z9ByUZgJMLOH+DzBvBVBzgUCropmIhDKV0gFxOfq07IwIa0q5YUchbvPEOrSasY8P0Ozly394QijkXhxh5MJijTDLovgl4rocpTYDLD4VD4ugPMCIQdCyExwd6TikgGKuPrxZTO1fmrf0PaBBTCMOCPXWcYu93CoIU7OHQ+xt4jijgEhRt7K/wIPP0F9NsKtV8BZ4+kl6h+7gmTq8P66RCrf/BEHEn5Ql5MC67Jn/0DaV7RFwMT/9txhubjVzLoh20cvXDN3iOKZGsKN1mFTwloMw4G/g1Bw8EjP1w9Dn+9BRMqw/L3IeacvacUkQxU0c+baV0e4c2qCTStUACrAT9vOUnT8St5c+F2jl/U61Ui6aFwk9V45IVGb8LAXfD4BMhbGm5egdWfwIQqsKgfXFC9g4gjKeoJnwdXZ1Hf+gSVL0Ci1WDh5hM0+TSMt37awYnLCjkiaaFwk1U5u0OtF6FvBHT6Goo+ComxsOVLmPooLAiG4xvsPaWIZKCqRfMw94Xa/NK7Hg3LFSDBarAgIpKgT8J4+5ednLpyw94jimQLCjdZndkClZ6Al0Lghb+gXGtQvYOIQ6tezIevXqzNT6/VpUGZ/MQnGny74TiNPw7j3d92ceaq7nQuci8KN9mFyQTF60KXBdBnI1R/XvUOIg6uZvG8fNOzDt/3eozHSuUlLtHKV+uO0fDjUEYu+ptzUfr7LnInCjfZUYHy0P6zu9c7rP5U9Q4iDqROqXws6FWX+S/XoXaJvMQlWJkXfpTAcaF88PtuzkfH2ntEkSxF4SY7u1u9w/L3VO8g4oDqlc7P9688xjcv1aFGsTzEJliZveYIgeNWMGbxHi7GKOSIgMKNY1C9g0iOYTKZaFA2Pz+9Vo8vX6xNNf883Iy3MmPVYQLHhfLRX3u5fC3O3mOK2JXCjSO5rd6hYep6h6+fVL2DiIMwmUw0KleAX3vXY26PRwkokpvrcYlMDztEg49W8OnSfVy9Hm/vMUXsQuHGEaXUO/wvdb3DoRW36h12/qh6BxEHYDKZCKrgy6K+9ZnVrRaV/Ly5FpfIlBUHafDRCiaE7OfqDYUcyVkUbhzd3eodfnpJ9Q4iDsRkMtG8UkF+f70Bnz9fkwqFvIiOTWDS8gMEfrSCycsPEH1TIUdyBoWbnCJVvcPbqncQcVBms4lWVQqxuF8g04JrUNY3F1E3Exgfsp/AcaF8FnqQmFidtRXHpnCT03jkhUZD/lXvUCp1vcP/+sOFg/aeUkQektlsok2AH38NaMjkztUpXcCTK9fj+XjJPgI/WsHnKw9xPU4hRxyTwk1OlVLvsCmp3qFIraR6h83zYGqtpHqHyI32nlJEHpLFbOKJaoVZOrARE599hJL5Pbl8PZ6xf+4l8KNQZq06zI24RHuPKZKhFG5yuuR6h57L4IU/U9c7zGmuegcRB2Exm+hQvQghAxvy6TPVKJ7Pg4vX4hi9eA+B40KZs+YIN+MVcsQxKNxIEpMJitdLqnfovUH1DiIOysli5qmaRVk2qBHjnq5KUR93LsTE8v7vu2k4LpQvw48q5Ei2p3Ajt/OtkFTv0H8H1B+gegcRB+RsMdOplj8r3mjMmI4BFMnjzrnoWEYs+pugT8L4ev0xYhMUciR7UriRu/P2g+ajki4+Vr2DiENycTLTuXYxVgxuxAcdquCX243TV2/yzq+7aPLJSr7beJz4RL0sLdmLwo3cn5v3v+odZqjeQcQBuTpZeP6x4oQObsyoJyrj6+XKySs3GPbzToI+CeOHiEiFHMk20hVuIiMjOXHiRMrnGzduZMCAAcycOTPDBpMsyOIM1Z5LqncIVr2DiCNyc7bQvV4JVg0J4t3HK5E/lysnLt9gyE87aDZ+JT9uPkGCQo5kcekKN126dCE0NBSAM2fO0Lx5czZu3Mjbb7/Ne++9l6EDShZkMkHZ5HqHMKjc8T/1Dg1V7yCSzbk5W3ixQUlWDwlieNuK5PN04djF6wxeuJ3mE1bx69aTJFr1PzKSNaUr3OzatYvatWsD8MMPP1ClShXCw8P59ttvmTdvXkbOJ1ld4erwzNz/1Dvs+Fe9w+eqdxDJxtxdLPQMLMXqoUG81boCPh7OHLlwjQHfb6PFhJUs2n5KIUeynHSFm/j4eFxdXQFYtmwZTzzxBAAVKlTg9OnTGTedZB93rXcYqnoHEQfg4eLEq41Ks3poE95sWZ7c7s4cOn+Nft9tpdXEVfyx4zRWhRzJItIVbipXrsznn3/O6tWrCQkJoVWrVgCcOnWKfPnyZeiAks38u96h7XjVO4g4mFyuTvQJKsOaoUG80bwc3m5OHDgXQ5/5W2gzeTV/7VLIEftLV7j56KOPmDFjBo0bN6Zz585Uq1YNgEWLFqW8XCU5nLM7PPqS6h1EHJSXmzOvNy3L6qFN6N+0LF6uTuw9E82r32zh8SlrCNl9FkNvLhA7cUrPkxo3bsyFCxeIiorCx8cnZXuvXr3w8PDIsOHEASTXO1RsB8fXwdrJsP/PpHqHvb+D/2NQvz+UawVm3ZlAJLvJ7e7MwObleLF+SWavOcwXa46w+3QUL3+1iYAiuRnYvCxB5X0xmUz2HlVykHT9Nrlx4waxsbEpwebYsWNMnDiRffv24evrm6EDioO4U72D2fmfeofOMK0ObPkKEmLtPamIpENuD2feaFGeNUOb0LtxaTxcLOw8eZUX522iw7Rwwvad05kcsZl0hZv27dvz1VdfAXDlyhXq1KnDp59+SocOHZg+fXqGDigOKLneYcDOW/UOF/bDotdV7yCSzfl4ujCkVQVWDwnilUalcHe2sD3yCj3mRvDU9HDWHLigkCOZLl3hZsuWLQQGBgLw448/UrBgQY4dO8ZXX33F5MmTM3RAcWB3qneIOZtU7zChCvz1f6p3EMmm8uVyZVjriqwaEkTPBiVxdTKz5fgVnp+zgU4z1hF+6IK9RxQHlq5wc/36dby8vABYunQpHTt2xGw289hjj3Hs2LEMHVBygOR6h37bbtU7xMXA+s9g8iPwcy/VO4hkUwW8XBn+eCVWDwnihfolcHEyE3H0Ml1mbeC5mevYcPiivUcUB5SucFOmTBl+/fVXIiMjWbJkCS1atADg3LlzeHt7p2uQsWPHYjKZGDBgwAPtv2DBAkwmEx06dEjX15MsyMnl9noHawLs+P5WvcPhMNU7iGRDvt5ujGhXmVVvBtGtbnFcLGbWH77EszPXEzx7PZuOXrL3iOJA0hVu3n33XQYPHkyJEiWoXbs2devWBZLO4lSvXj3Nx4uIiGDGjBlUrVr1gfY/evQogwcPTnlpTBzMveodvmqP05wmFLm0Lin4iEi2Uii3G++1r0LYm40JrlMMZ4uJtQcv8vTn6+g6ZwNbjut6O3l46Qo3Tz/9NMePH2fTpk0sWbIkZXvTpk2ZMGFCmo4VExNDcHAws2bNSvW28rtJTEwkODiYUaNGUapUqTTPLtlMqnqHXuDkjunsTmodm47TtNpJ9Q5x1+w9pYikUeE87ox+MoDQwY3pXNsfJ7OJ1Qcu0HFaOC/M3ciOE1fsPaJkY+m6zw1AoUKFKFSoUEo7eNGiRdN1A78+ffrQtm1bmjVrxgcffHDf/d977z18fX156aWXWL169X33j42NJTb21tuLo6KigKQKifj4+DTPey/Jx8vo4wqQqwg0/xDqD8aImIWx/nNc/6l3MMLGYK35EtZaL0Eu3Yogo+jn2TZy+joXzOXMe+0q0rN+caatPMyv204Tuu88ofvO06R8Afo1KU3lwum73OG/cvpa20pmrXNajpeucGO1Wvnggw/49NNPiYlJKkX08vLijTfe4O2338b8gDdjW7BgAVu2bCEiIuKB9l+zZg1z5sxh27ZtDzzrmDFjGDVq1G3bly5dmmk3HAwJCcmU40qyAMyVJ1Ds0hpKn/uTXDfPYln7KYRPJjJvAw76tuKam5+9h3QY+nm2Da0zNHSFilVhyUkzm86bWLHvPCv2nadqXiutilop4pkxX0drbRsZvc7Xr19/4H3TFW7efvtt5syZw9ixY6lfvz6QFDxGjhzJzZs3GT169H2PERkZSf/+/QkJCcHNze2++0dHR9O1a1dmzZpF/vz5H3jWYcOGMWjQoJTPo6Ki8Pf3p0WLFum++Plu4uPjCQkJoXnz5jg7O2foseWW5HUu23kMzpaPSNi/GPO6qVhObabExVCKXwzDKN8G62N9MYo+au9xsy39PNuG1vl23YHD56/xWdhh/rfzNDsumdlxyUyrygV5PagU5Qp6peu4WmvbyKx1Tn7l5UGkK9x8+eWXzJ49O6UNHKBq1aoUKVKE3r17P1C42bx5M+fOnaNGjRop2xITE1m1ahVTp04lNjYWi8WS8tihQ4c4evQo7dq1S9lmtVqTvgknJ/bt20fp0qVv+zqurq4pDeb/5uzsnGk/3Jl5bLklZZ0DOkKVJ/+pd5iEaf9fmPb9gXnfH1CsLtTrp3qHh6CfZ9vQOqdWvnAeJnepQb9z0UxafpDfd5zir7/PsmT3WdoG+DGgWVnK+KYv5GitbSOj1zktx0pXuLl06RIVKlS4bXuFChW4dOnB3s7XtGlTdu5Mfe+SF154gQoVKjB06NBUwSb52P/df/jw4URHRzNp0iT8/f3T+F2IQ0mudyheD87thXVTYPv3SYHn+DrIXw7qvQ5VnwWn28OuiGRNZXy9mNK5On2DyjBp+X4W7zzD7ztO88fO0zxRrTD9mpaldIFc9h5Tsph0hZtq1aoxderU2+5GPHXq1Ad+O7eXlxdVqlRJtc3T05N8+fKlbO/WrRtFihRhzJgxuLm53bZ/njx5AG7bLjlccr1D0HDY8Dlsmnur3mHFB1DnFaj1Irjf/915IpI1lC/kxbTgmuw+FcWk5ftZ8vdZftt2iv9tP0WH6kXo16QsJfJn0EU5ku2lK9yMGzeOtm3bsmzZspR73Kxbt47IyEgWL16cYcMdP378gS9OFrlNcr1D4Buw5UtYPx2iTibVO6weDzW6w2OvQR6d9RPJLioV9mZG11rsOnmVicv2s2zPOX7ecpLftp2iY/UivN6kLMXyZc6bRST7SFe4adSoEfv37+ezzz5j7969AHTs2JFevXrxwQcfpPvmemFhYff8/L/mzZuXrq8jOYybd9JLUrVfgb9/hrWT4NzupHqHjTOgylNJ1+UU0hlAkeyiSpHczO7+KNsjrzBx2X5C951n4eYT/LL1JE/XLErfJmUo6qOQk1Ol+z43hQsXvu3C4e3btzNnzhxmzpz50IOJZLjkeoeqz8LB5RA+CY6sSqp32PE9lG4C9ftDyUZJ1/CISJZXzT8Pc1+ozdbjl5mw7ACr9p9nQUQkP205wTO1/OkbVIbCedztPabYmF7zkZznPvUOzGgIO3+ERNU7iGQX1Yv58NWLtfnptbo0KJOf+ESD+RuO0/jjMN79bRdnrt6094hiQwo3krMl1zu8viWl3oEzO+Cnl2BKddU7iGQzNYvn5Zuedfi+12M8ViovcYlWvlp3jIYfh/L+H3u5GmfvCcUWFG5EAPKWhDYfw6DdEPQ2eOSDK0n1DoyvlPQuq5jz9p5SRB5QnVL5WNCrLvNfrsOjJXyIS7Dy1frjvL/Fwpg/93E+Ovb+B5FsK03X3HTs2PGej1+5cuVhZhGxP4+80GhI0gXI2+ZD+BS4fARWfQxrJ8MjXZIey3f7DSNFJOupVzo/dUvlY+3Bi3y6dC9bI6/yRfgx5kdE0r1uCXo1LEW+XLr3laNJU7jJnTv3fR/v1q3bQw0kkiU4u8OjL0HNHrD396R3WJ3cDJvnwuZ5UKEt1B8A/qp3EMnqTCYTDcrmp3bx2oz/7i/Co33YcSKKGasO8/X6Y3SvV4JegaXw8XSx96iSQdIUbubOnZtZc4hkTWYLVGoPFZ+AY+EQPhn2/5UUePb+rnoHkWzEZDJRMY/BoM51WHP4MhNCDrDz5FWmhx3iq/CjvNigJD0blCK3h6oZsjv9ayzyIEwmKFEfunwPvTfAI8+D2Tmp2mFBZ5hWB7Z8BQl6HV8kqzOZTDSpUJBFfeszq1stKvl5cy0ukSkrDtLgoxVMCNnP1Rvx9h5THoLCjUha+VaADp/BgJ1JL025et+qd5gYkHT34xtX7D2liNyHyWSieaWC/P56Az5/viYVCnkRHZvApOUHCPxoBZOXHyD6pkJOdqRwI5JeyfUOA/+GFh+AdxGIOQvLR8GEyvDX/8GVSHtPKSL3YTabaFWlEIv7BTItuAZlfXMRdTOB8SH7CRwXymehB4mJ1X2vshOFG5GHlVzv0G8bdPgcfCtBXExSvcPkR+DnXnBml72nFJH7MJtNtAnw468BDZncuTqlCnhy5Xo8Hy/ZR+BHK/h85SGuxynkZAcKNyIZxckFHukMr4VD8E9QIhCsCUnVDp/Xh687wuEwMAx7Tyoi92Axm3iiWmFCBjZi4rOPUDK/J5evxzP2z70EfhTKrFWHuRGXaO8x5R4UbkQyWnK9Q4/f4eXQf9U7LE+qd5jZSPUOItmAxWyiQ/UihAxsyKfPVKNYXg8uXotj9OI9BI4LZc6aI9yMV8jJihRuRDJTkRq31zuc3q56B5FsxMli5qmaRVn+RiPGPVWVoj7uXIiJ5f3fd9NwXChfhh9VyMliFG5EbCG53mHg39D4/1LXO0yorHoHkWzA2WKm06P+rHijMWM6BlAkjzvnomMZsehvgj4J4+v1x4hNUMjJChRuRGzJMx80HpoUctqOB5+ScONyUr3DhMrwvwFw8ZC9pxSRe3BxMtO5djFWDG7EBx2q4JfbjdNXb/LOr7to8slKvtt4nPhEq73HzNEUbkTsIbne4fXN0OkrKFITEmOT6h2m1IQFwRAZYe8pReQeXJ0sPP9YcUIHN2bUE5Xx9XLl5JUbDPt5J0GfhPFDRKRCjp0o3IjYU3K9Q8/l0GNxUo0DRlK1w5xm8EUr2PcnWPUPpEhW5eZsoXu9EqwaEsS7j1cify5XTly+wZCfdtD005X8uPkECQo5NqVwI5IV3Kve4bvnVO8gkg24OVt4sUFJVg8J4u02Fcnn6cLxS9cZvHA7zSes4tetJ0m06lYQtqBwI5LVqN5BJFtzd7HwcsNSrB4axFutK+Dj4cyRC9cY8P02WkxYyaLtpxRyMpnCjUhW9d96B6/CqesdlrwNV0/Ye0oRuQsPFydebVSa1UOb8GbL8uR2d+bQ+Wv0+24rrSau4o8dp7Eq5GQKhRuRrC653qH/9tT1DuumwqRqqncQyeJyuTrRJ6gMa4YG8Ubzcni7OXHgXAx95m+hzeTV/LVLISejKdyIZBep6h1+VL2DSDbj5ebM603LsnpoE/o3LYuXqxN7z0Tz6jdbeHzKGkJ2n8XQ398MoXAjkt2YTFC2+b/qHZ5UvYNINpLb3ZmBzcuxZmgTXm9SBk8XC7tPR/HyV5t4YupaVuxVyHlYCjci2VmRGvDMvLvXO2yYoXoHkSwqt4czb7Qoz5qhTejduDQeLhZ2nrzKi/M20WFaOGH7zinkpJPCjYgjuFu9w59DVO8gksX5eLowpFUFVg8J4pWGpXB3trA98go95kbw1PRw1hy4oJCTRgo3Io4kud5hwC5o+2nqeoeJVVTvIJKF5cvlyrA2FVk1JIieDUri6mRmy/ErPD9nA51mrCP80AV7j5htKNyIOCIXD3i0Z+p6h4Sbt+odvn9e9Q4iWVQBL1eGP16J1UOCeKF+CVyczEQcvUyXWRt4buY6Nhy+aO8RszyFGxFHdrd6hz3/U72DSBbn6+3GiHaVWfVmEN3qFsfFYmb94Us8O3M9wbPXs+noJXuPmGUp3IjkBA9U7/C16h1EsqBCud14r30Vwt5sTHCdYjhbTKw9eJGnP19H1zkb2HL8sr1HzHIUbkRympR6hx1Qv/+/6h36wsSqqncQyaIK53Fn9JMBhA5uTOfa/jiZTaw+cIGO08J5Ye5Gdpy4Yu8RswyFG5GcyrswNH/vP/UOZ1LqHczL3sEtTq/ti2Q1RX08GNOxKiveaMwzNYtiMZsI3XeeJ6aupeeXEew6edXeI9qdwo1ITneXegfLhuk0/3swlt9eU72DSBZULJ8HHz9TjeWDGtGxRhHMJli25xyPT1lDr682sftUlL1HtBuFGxFJ8p96B2vxBphJxLxr4b/qHVaq3kEkiymR35PxnR4hZFAjOjxSGJMJlu4+S5vJq+n97Wb2nYm294g2p3AjIqn9U++Q+PyvrCw/EmvF9v+qd3hC9Q4iWVTpArmY+Fx1lg5oyONV/TCZYPHOM7SatIq+87dw8FzOCTkKNyJyV1c8SpHYcU5SvcOjL6veQSQbKFvQi6ldavBX/4a0CSiEYcDvO07TfMIq+i/YyqHzMfYeMdMp3IjI/eUtCW0/uUe9w2jVO4hkMeULeTEtuCaL+wXSsnJBDAN+23aK5uNXMuiHbRy94Lj/Y6JwIyIP7q71DuNU7yCSRVUq7M2MrrX4/fUGNKvoi9WAn7ecpOn4lby5cDvHL16394gZTuFGRNLu3/UOz3ypegeRbKBKkdzM7v4ov/WpT1D5AiRaDRZuPkGTT8N466cdnLjsOCEny4SbsWPHYjKZGDBgwF33mTVrFoGBgfj4+ODj40OzZs3YuHGj7YYUkdTMFqjc4Va9Q9mWpK53aK16B5Esppp/Hua+UJufe9ejYbkCJFgNFkREEvRJGP/3y05OXblh7xEfWpYINxEREcyYMYOqVavec7+wsDA6d+5MaGgo69atw9/fnxYtWnDy5EkbTSoid5Rc7xD8A/Re/696h/B/6h0eU72DSBZTo5gPX71Ymx9frUv9MvmITzSYv+E4jT8O493fdnHm6k17j5huTvYeICYmhuDgYGbNmsUHH3xwz32//fbbVJ/Pnj2bn376ieXLl9OtW7c7Pic2NpbY2Fv/oEZFJd3UKD4+nvj4+IecPrXk42X0cSU1rbNtpHudfcpA24kQOBTzppmYt8zDdGEfLOqLseJ9rI/2wlqjB7jlzvCZsyP9PNuO1vrOqhXxYl73mmw8eonJKw6x4chlvlp3jAURkTxXqyivNCyJr5frAx8vs9Y5LcczGYZ978jVvXt38ubNy4QJE2jcuDGPPPIIEydOfKDnRkdH4+vry8KFC3n88cfvuM/IkSMZNWrUbdvnz5+Ph4fHw4wuIg/AKfEGxS+EUvr8Etzjkwr+EsxuHM3XmEO+Lbnpks/OE4rIvx24amJxpJnD0SYAnE0G9QsZNC1sxdvFfnNdv36dLl26cPXqVby9ve+5r13DzYIFCxg9ejQRERG4ubmlOdz07t2bJUuW8Pfff+Pm5nbHfe505sbf358LFy7cd3HSKj4+npCQEJo3b46zs3OGHltu0TrbRoavc2Icpr9/wbJ+KqbzewAwzE4YlTuSWKcPFKz88F8jG9LPs+1orR+cYRiEH77EpOUH2RqZ1FXl5mzm+TrF6NmgBPk8755yMmudo6KiyJ8//wOFG7u9LBUZGUn//v0JCQm5azC5l7Fjx7JgwQLCwsLu+XxXV1dcXW8/nebs7JxpP9yZeWy5RetsGxm2zs7OUPN5qBEMB5fB2kmYjq7GtPMHzDt/gNJNk1rKSzZMuoYnh9HPs+1orR9M4wqFaFS+IKsOXGB8yH62R15h9pqjzN8YSfd6JegVWAqfe4ScjF7ntBzLbhcUb968mXPnzlGjRg2cnJxwcnJi5cqVTJ48GScnJxITE+/63E8++YSxY8eydOnS+16ELCJZzD/1DvT4HV4OhcpP3l7vsOsn1TuIZAEmk4lG5Qrwa+96fNGjFgFFcnM9LpHpYYdo8NEKPl26j6vXs941THY7c9O0aVN27tyZatsLL7xAhQoVGDp0KBaL5Y7PGzduHKNHj2bJkiXUqlXLFqOKSGYpUgOemQeXjsC6z2DrN0n1Dj++CHmKQd2+UP15cPG096QiOZrJZKJJhYIElfdl2Z5zTAjZz+7TUUxZcZB5a4/yYoOSvNigJLnds8YZMbuFGy8vL6pUqZJqm6enJ/ny5UvZ3q1bN4oUKcKYMWMA+Oijj3j33XeZP38+JUqU4MyZMwDkypWLXLly2fYbEJGMk1zv0HgYRMyGjTNu1TuEjUnqtardC3IVsPekIjmayWSieaWCNK3gy9LdZ5m4bD97z0QzafkB5q49Qs/AUjxfu6i9x8wa97m5m+PHj3P69OmUz6dPn05cXBxPP/00fn5+KR+ffPKJHacUkQyjegeRbMFsNtGqSiEW9wvksy41KOubi6ibCYwP2U+T8atZesJEfKL9bt5p9/vc/FtYWNg9Pz969KjNZhERO0qud6j5QtLdjtdOglNbkuodNs+Dio9D/QFQVC9Ni9iT2WyibVU/WlUpxB87TzNx2X4On7/G9ktmnMz2e2NAlgo3IiKpJNc7VGoPx9bC2slwYElS4NnzPyhWD+r3S6p9MGfpE9EiDs1iNvFEtcK0DfDjly2RHN29DZMd3/Wofw1EJOszmaBEg3/VOwSr3kEkC7KYTbSv5kfZ3Ha9P7DCjYhkM74VocM0GLAj6b44rt7wT70DE6vCmglw44q9pxQRO1K4EZHsybswNH8PBu6C5u+DV2GIOQPLRsKEyrDkbbh6wt5TiogdKNyISPbmljvpupv+26HDdChQEeJiYN1UmFQNfn4Fzuyy95QiYkMKNyLiGJxc4JEu0HsddFkIJQLBmgA7FsDn9eGbp+DwSrBvV7CI2IDCjYg4FpMJyrX4p95hBVTqkFTvcHCZ6h1EcgiFGxFxXEVqQqcv4fXNSXc5dnK/Ve8wpTpsmAFx1+w9pYhkMIUbEXF8eUsl1TsM/Dup4sEj3616hwmVYcVoiDlv7ylFJIMo3IhIzuGZDxq/lVTv0OYT8CmRut7h94GqdxBxAAo3IpLzuHhA7Zfh9S3wzJdQuAYk3IRNX8CUmvD983Bik72nFJF0UrgRkZwrud7h5RXQ44+kGgeMpGqH2U3hi9aw7y+w2q8AUETSTt1SIiLJ9Q4lGsC5PRA+BXb8kFTvcDwc8peHeq9D1U7g5GrvaUXkPnTmRkTk3/5d71Cvn+odRLIhhRsRkTvxLgwt3v9XvYPfv+odqvxT73DS3lOKyB0o3IiI3EtKvcOOf9U7RP9T71A1qd7h7N/2nlJE/kXhRkTkQdyr3mF6vaR6hyOrVO8gkgUo3IiIpMW96h2+bAczG6veQcTOFG5ERNLrjvUO2/6pd6gBG2aq3kHEDhRuREQe1h3rHY7Bn2+q3kHEDhRuREQyiuodRLIEhRsRkYyWqt5h3h3qHbqq3kEkE+kOxSIimcVsgcpPJl10fGwtrJ0EB5bCnkVJH8XrY6rTGwzVO4hkJIUbEZHMdrd6h2NrcTq2liZuhTEVuQrVO6veQSQD6GUpERFb+k+9g+HqhdfNUzj90V/1DiIZROFGRMQe/ql3SOi7nb8LP4uRq5DqHUQyiMKNiIg9uXlzsGBbEvpuuXO9wy+vqt5BJI0UbkREsgLLXeodtn+negeRNFK4ERHJSlTvIPLQFG5ERLKqVPUOPVXvIPKAFG5ERLK6vKWg7ae36h3c86audwj9EK5dsPeUIlmGwo2ISHaRXO8w8O/U9Q4rP0oKOap3EAEUbkREsh/VO4jck8KNiEh2lVzv8PIK6P47lG0BGEnVDrObwtw2sO8vsKreQXIW1S+IiGR3JhOUDEz6+E+9A8fWQv7yUL8fBDyjegfJEXTmRkTEkfyn3gFXb7iwD37ro3oHyTEUbkREHNE/9Q4M3AXN3wMvP9U7SI6hcCMi4sjcckP9/tB/B7SfpnoHyRGyTLgZO3YsJpOJAQMG3HO/hQsXUqFCBdzc3AgICGDx4sW2GVBEJDtzcoHqwbfqHYo3+E+9w9OqdxCHkSXCTUREBDNmzKBq1ar33C88PJzOnTvz0ksvsXXrVjp06ECHDh3YtWuXjSYVEcnmkusdXvjjP/UOIap3EIdh93ATExNDcHAws2bNwsfH5577Tpo0iVatWvHmm29SsWJF3n//fWrUqMHUqVNtNK2IiANRvYM4KLu/FbxPnz60bduWZs2a8cEHH9xz33Xr1jFo0KBU21q2bMmvv/561+fExsYSGxub8nlUVBQA8fHxxMfHp3/wO0g+XkYfV1LTOtuG1tk2ssQ6e/lDi7FQfzDmzXMwb5qD6Z96ByNsDNaaL2Kt1RM889tvxgyQJdY6B8isdU7L8ewabhYsWMCWLVuIiIh4oP3PnDlDwYIFU20rWLAgZ86cuetzxowZw6hRo27bvnTpUjw8PNI28AMKCQnJlONKalpn29A620bWWecALOU+wv/iasqc+wvPG+ewrPkE1k7ieL5ADvm25pprwfsfJgvLOmvt2DJ6na9fv/7A+9ot3ERGRtK/f39CQkJwc3PLtK8zbNiwVGd7oqKi8Pf3p0WLFnh7e2fo14qPjyckJITmzZvj7OycoceWW7TOtqF1to2su85PgvVjEvb9jnndVCynt1LywgpKXAjFqPA41sf6YhSpae8h0yTrrrVjyax1Tn7l5UHYLdxs3ryZc+fOUaNGjZRtiYmJrFq1iqlTpxIbG4vFYkn1nEKFCnH27NlU286ePUuhQoXu+nVcXV1xdb39jpzOzs6Z9sOdmceWW7TOtqF1to2suc7OUPVpCHgKjq6B8MmYDizFtPd/mPf+D4rXT7pRYNkWYLb7JZwPLGuutePJ6HVOy7Hs9tPYtGlTdu7cybZt21I+atWqRXBwMNu2bbst2ADUrVuX5cuXp9oWEhJC3bp1bTW2iEjOk1zvELwQXlsH1bqA2Tmp2uG7Z2F6Xdj6DSTE3v9YIjZgtzM3Xl5eVKlSJdU2T09P8uXLl7K9W7duFClShDFjxgDQv39/GjVqxKeffkrbtm1ZsGABmzZtYubMmTafX0QkRypYCZ6cDk3fgfXTYdNcOL83qd5h+fvw2GtQ64WkmweK2EmWPo94/PhxTp8+nfJ5vXr1mD9/PjNnzqRatWr8+OOP/Prrr7eFJBERyWTJ9Q6D/v5PvcMIGF9Z9Q5iV3Z/K/i/hYWF3fNzgGeeeYZnnnnGNgOJiMi9Jdc71HkNdi5MaiQ/vyep3mHD50lN5PVeh4KV7T2p5CBZ+syNiIhkE8n1Dq+FQ5cfVO8gdqVwIyIiGcdshnIt71Pv8LPqHSRTKdyIiEjmuGu9wwuqd5BMpXAjIiKZK28paPspDNwFjd4C97zwT70DE6pA6Idw7YK9pxQHonAjIiK24ZkfgobBwL+hzSfgUwJuXIKVH8GEyvD7ILh4yN5TigNQuBEREdty8YDaL8PrW+CZeVC4OiTchE1zYEpN+KEbnNhs7yklG1O4ERER+zBboPKT8HIodP89qcYBA3b/BrObwNw2sO8vsFrtPalkM1nqPjciIpIDJdc7lAyEs7uT7pWzc2FSvcOxtVCgQtK9cgKeAafbuwJF/ktnbkREJOtIrnfovz0p0Lh43ap3mFQN1kyEm1ftPaVkcQo3IiKS9eQuAi0+SF3vEH36Vr3D0uGqd5C7UrgREZGsK7neof8OaD8NClSEuOikl64mVYVfXoWzf9t7SsliFG5ERCTrU72DpIHCjYiIZB//rnfouQIqtU9d7zArSPUOonAjIiLZVNGa0Omrf9U7uMGprbfqHTbOgrjr9p5S7EDhRkREsreUeoe/U9c7LB6cdOdj1TvkOAo3IiLiGO5T72D+8008Y8/ae0qxAd3ET0REHEtyvUPNF2DPIgifDKe2Ytkyl6aYMKyrocGApJe1xCHpzI2IiDgmixNU6ZhS72At3QwTBua9i27VO+xfonoHB6RwIyIiju2feofE5xawosKHWKs+B2bnpGqH+Z1gel3Y+g0kxNp7UskgCjciIpJjRLsXJbHdVNU7ODiFGxERyXn+Xe/QbJTqHRyMwo2IiORcbrmTLi5OqXeo8J96h9eSmsolW1G4ERERSal3WPefeof5SdfkqN4hW1G4ERERSaZ6B4egcCMiInInyfUOfTdBrZdS1ztMral6hyxM4UZERORe8pWGx8enrne4fPRf9Q5jVO+QxSjciIiIPIi71juMTQo5vw+Ci4fsPaWgcCMiIpI2yfUOfTfD03OhcHVIuAmb5sDUWvBDNzix2d5T5mgKNyIiIunxn3oHyjQHwwq7f/un3qGt6h3sRMWZIiIiD+OfegdKBibdEyd8Cuz8AY6tSfooUAHq9YOAZ5Leci6ZTmduREREMkrBSvDk9KSbAqaqd+iddFNA1TvYhMKNiIhIRlO9g10p3IiIiGSWlHqH7dD+s//UO1RTvUMmUbgRERHJbE6uUP35f9U71Adr/H/qHVar3iGDKNyIiIjYSkq9w+Jb9Q6Y/ql3eFz1DhlE4UZERMQekusdXt+seocMpnAjIiJiT6nqHYaq3iEDKNyIiIhkBZ75Iej/YOAuaP0x5Cmeut7hjzfg0mF7T5ktKNyIiIhkJS6eUKcXvL4lqd7B75GkeoeI2TClpuodHoBdw8306dOpWrUq3t7eeHt7U7duXf788897PmfixImUL18ed3d3/P39GThwIDdv3rTRxCIiIjaSXO/QKwy6/0/1Dmlg1/qFokWLMnbsWMqWLYthGHz55Ze0b9+erVu3Urly5dv2nz9/Pm+99RZffPEF9erVY//+/fTo0QOTycT48ePt8B2IiIhkMpMJSjZM+jj79z/1Dgv/Ve9QMeluyKp3SGHXcNOuXbtUn48ePZrp06ezfv36O4ab8PBw6tevT5cuXQAoUaIEnTt3ZsOGDXf9GrGxscTGxqZ8HhUVBUB8fDzx8fEZ8W2kSD5eRh9XUtM624bW2Ta0zrbjEGudtxw8PgUavoV54wzMW7/CdH4P/NYbY/l7WGu/grV6d3DzttuImbXOaTmeyTCyxh2DEhMTWbhwId27d2fr1q1UqlTptn3mz59P7969Wbp0KbVr1+bw4cO0bduWrl278n//9393PO7IkSMZNWrUHY/l4eGR4d+HiIiIrTglXqfEhVBKn1uCW8IVAOLNbhzNH8ThAi256ZLXvgNmoOvXr9OlSxeuXr2Kt/e9w5vdw83OnTupW7cuN2/eJFeuXMyfP582bdrcdf/JkyczePBgDMMgISGBV199lenTp991/zudufH39+fChQv3XZy0io+PJyQkhObNm+Ps7Jyhx5ZbtM62oXW2Da2z7Tj0WifEYvr7JyzrP8N0YR8AhtkZo8pTJNbpA74VbTZKZq1zVFQU+fPnf6BwY9eXpQDKly/Ptm3buHr1Kj/++CPdu3dn5cqVdzxzExYWxocffsi0adOoU6cOBw8epH///rz//vu88847dzy+q6srrq6ut213dnbOtB/uzDy23KJ1tg2ts21onW3HIdfa2RlqdYcaXZPudrx2EqZjazHtWIB5xwIo2wLq9YMSDZKu4bHJSBm7zmk5lt3DjYuLC2XKlAGgZs2aREREMGnSJGbMmHHbvu+88w5du3alZ8+eAAQEBHDt2jV69erF22+/jdmsd7aLiEgOllzvUK4lnNgEayfBnv/BgaVJH4WrQ/3+UPEJMFvsPW2myXJpwGq1pnoZ6d+uX79+W4CxWJL+cLLIpUMiIiJZQ9Fa8OzXt9c7LOwBU2o4dL2DXcPNsGHDWLVqFUePHmXnzp0MGzaMsLAwgoODAejWrRvDhg1L2b9du3ZMnz6dBQsWcOTIEUJCQnjnnXdo165dSsgRERGRf0mudxiw6/Z6h4lVHLLewa4vS507d45u3bpx+vRpcufOTdWqVVmyZAnNmzcH4Pjx46nO1AwfPhyTycTw4cM5efIkBQoUoF27dowePdpe34KIiEj2kKtAUr1D/f6w9VtYNxWuHEuqd1g7CaoHQ90+kLeUvSd9aHYNN3PmzLnn42FhYak+d3JyYsSIEYwYMSITpxIREXFgyfUOtV6EPYuSgs3pbUn1Dpu+SLoep34/KFLT3pOmW5a75kZERERs4K71Dr/CrOR6h6WQDa9ptfu7pURERMSOHLDeQWduREREJEnByvDk59B/O9TtCy5e8E+9A5OqJb2EdfOqvae8L4UbERERSS13UWg5GgbugmajIFchiD4FIe/ChCqw9B2IOmXvKe9K4UZERETuzD0PNBgAA3ZA+8+gQAWIjYLwyTCxKvzyGpzdbe8pb6NwIyIiIvfm5ArVn4fX1kHn76F4fbDGw/b5ML0ufPsMHFmdZS4+1gXFIiIi8mDMZijfKunjjvUONTA91gcM+95YV2duREREJO1S1Tu8+E+9wxacfn6JJnv/DxLj7Taawo2IiIikX77S8PiElHoHw92HSx5lwGK/5nWFGxEREXl4/9Q7JPTdxu7Cnew6isKNiIiIZBwXT+Kcve06gsKNiIiIOBSFGxEREXEoCjciIiLiUBRuRERExKEo3IiIiIhDUbgRERERh6JwIyIiIg5F4UZEREQcisKNiIiIOBSFGxEREXEoCjciIiLiUBRuRERExKEo3IiIiIhDcbL3ALZmGAYAUVFRGX7s+Ph4rl+/TlRUFM7Ozhl+fEmidbYNrbNtaJ1tR2ttG5m1zsm/t5N/j99Ljgs30dHRAPj7+9t5EhEREUmr6OhocufOfc99TMaDRCAHYrVaOXXqFF5eXphMpgw9dlRUFP7+/kRGRuLt7Z2hx5ZbtM62oXW2Da2z7WitbSOz1tkwDKKjoylcuDBm872vqslxZ27MZjNFixbN1K/h7e2tvzg2oHW2Da2zbWidbUdrbRuZsc73O2OTTBcUi4iIiENRuBERERGHonCTgVxdXRkxYgSurq72HsWhaZ1tQ+tsG1pn29Fa20ZWWOccd0GxiIiIODaduRERERGHonAjIiIiDkXhRkRERByKwo2IiIg4FIWbNPrss88oUaIEbm5u1KlTh40bN95z/4ULF1KhQgXc3NwICAhg8eLFNpo0e0vLOs+aNYvAwEB8fHzw8fGhWbNm9/1zkSRp/XlOtmDBAkwmEx06dMjcAR1EWtf5ypUr9OnTBz8/P1xdXSlXrpz+7XgAaV3niRMnUr58edzd3fH392fgwIHcvHnTRtNmT6tWraJdu3YULlwYk8nEr7/+et/nhIWFUaNGDVxdXSlTpgzz5s3L9Dkx5IEtWLDAcHFxMb744gvj77//Nl5++WUjT548xtmzZ++4/9q1aw2LxWKMGzfO2L17tzF8+HDD2dnZ2Llzp40nz17Sus5dunQxPvvsM2Pr1q3Gnj17jB49ehi5c+c2Tpw4YePJs5e0rnOyI0eOGEWKFDECAwON9u3b22bYbCyt6xwbG2vUqlXLaNOmjbFmzRrjyJEjRlhYmLFt2zYbT569pHWdv/32W8PV1dX49ttvjSNHjhhLliwx/Pz8jIEDB9p48uxl8eLFxttvv238/PPPBmD88ssv99z/8OHDhoeHhzFo0CBj9+7dxpQpUwyLxWL89ddfmTqnwk0a1K5d2+jTp0/K54mJiUbhwoWNMWPG3HH/Tp06GW3btk21rU6dOsYrr7ySqXNmd2ld5/9KSEgwvLy8jC+//DKzRnQI6VnnhIQEo169esbs2bON7t27K9w8gLSu8/Tp041SpUoZcXFxthrRIaR1nfv06WM0adIk1bZBgwYZ9evXz9Q5HcmDhJshQ4YYlStXTrXt2WefNVq2bJmJkxmGXpZ6QHFxcWzevJlmzZqlbDObzTRr1ox169bd8Tnr1q1LtT9Ay5Yt77q/pG+d/+v69evEx8eTN2/ezBoz20vvOr/33nv4+vry0ksv2WLMbC8967xo0SLq1q1Lnz59KFiwIFWqVOHDDz8kMTHRVmNnO+lZ53r16rF58+aUl64OHz7M4sWLadOmjU1mzins9XswxxVnpteFCxdITEykYMGCqbYXLFiQvXv33vE5Z86cueP+Z86cybQ5s7v0rPN/DR06lMKFC9/2F0puSc86r1mzhjlz5rBt2zYbTOgY0rPOhw8fZsWKFQQHB7N48WIOHjxI7969iY+PZ8SIEbYYO9tJzzp36dKFCxcu0KBBAwzDICEhgVdffZX/+7//s8XIOcbdfg9GRUVx48YN3N3dM+Xr6syNOJSxY8eyYMECfvnlF9zc3Ow9jsOIjo6ma9euzJo1i/z589t7HIdmtVrx9fVl5syZ1KxZk2effZa3336bzz//3N6jOZSwsDA+/PBDpk2bxpYtW/j555/5448/eP/99+09mmQAnbl5QPnz58disXD27NlU28+ePUuhQoXu+JxChQqlaX9J3zon++STTxg7dizLli2jatWqmTlmtpfWdT506BBHjx6lXbt2KdusVisATk5O7Nu3j9KlS2fu0NlQen6e/fz8cHZ2xmKxpGyrWLEiZ86cIS4uDhcXl0ydOTtKzzq/8847dO3alZ49ewIQEBDAtWvX6NWrF2+//TZms/7fPyPc7fegt7d3pp21AZ25eWAuLi7UrFmT5cuXp2yzWq0sX76cunXr3vE5devWTbU/QEhIyF33l/StM8C4ceN4//33+euvv6hVq5YtRs3W0rrOFSpUYOfOnWzbti3l44knniAoKIht27bh7+9vy/GzjfT8PNevX5+DBw+mhEeA/fv34+fnp2BzF+lZ5+vXr98WYJIDpaHKxQxjt9+DmXq5soNZsGCB4erqasybN8/YvXu30atXLyNPnjzGmTNnDMMwjK5duxpvvfVWyv5r1641nJycjE8++cTYs2ePMWLECL0V/AGkdZ3Hjh1ruLi4GD/++KNx+vTplI/o6Gh7fQvZQlrX+b/0bqkHk9Z1Pn78uOHl5WX07dvX2Ldvn/H7778bvr6+xgcffGCvbyFbSOs6jxgxwvDy8jK+++474/Dhw8bSpUuN0qVLG506dbLXt5AtREdHG1u3bjW2bt1qAMb48eONrVu3GseOHTMMwzDeeusto2vXrin7J78V/M033zT27NljfPbZZ3oreFY0ZcoUo1ixYoaLi4tRu3ZtY/369SmPNWrUyOjevXuq/X/44QejXLlyhouLi1G5cmXjjz/+sPHE2VNa1rl48eIGcNvHiBEjbD94NpPWn+d/U7h5cGld5/DwcKNOnTqGq6urUapUKWP06NFGQkKCjafOftKyzvHx8cbIkSON0qVLG25uboa/v7/Ru3dv4/Lly7YfPBsJDQ2947+3yWvbvXt3o1GjRrc955FHHjFcXFyMUqVKGXPnzs30OU2GofNvIiIi4jh0zY2IiIg4FIUbERERcSgKNyIiIuJQFG5ERETEoSjciIiIiENRuBERERGHonAjIiIiDkXhRkRERByKwo2I5Hgmk4lff/3V3mOISAZRuBERu+rRowcmk+m2j1atWtl7NBHJppzsPYCISKtWrZg7d26qba6urnaaRkSyO525ERG7c3V1pVChQqk+fHx8gKSXjKZPn07r1q1xd3enVKlS/Pjjj6mev3PnTpo0aYK7uzv58uWjV69exMTEpNrniy++oHLlyri6uuLn50ffvn1TPX7hwgWefPJJPDw8KFu2LIsWLcrcb1pEMo3CjYhkee+88w5PPfUU27dvJzg4mOeee449e/YAcO3aNVq2bImPjw8REREsXLiQZcuWpQov06dPp0+fPvTq1YudO3eyaNEiypQpk+prjBo1ik6dOrFjxw7atGlDcHAwly5dsun3KSIZJNN7x0VE7qF79+6GxWIxPD09U32MHj3aMAzDAIxXX3011XPq1KljvPbaa4ZhGMbMmTMNHx8fIyYmJuXxP/74wzCbzcaZM2cMwzCMwoULG2+//fZdZwCM4cOHp3weExNjAMaff/6ZYd+niNiOrrkREbsLCgpi+vTpqbblzZs35b/r1q2b6rG6deuybds2APbs2UO1atXw9PRMebx+/fpYrVb27duHyWTi1KlTNG3a9J4zVK1aNeW/PT098fb25ty5c+n9lkTEjhRuRMTuPD09b3uZKKO4u7s/0H7Ozs6pPjeZTFit1swYSUQyma65EZEsb/369bd9XrFiRQAqVqzI9u3buXbtWsrja9euxWw2U758eby8vChRogTLly+36cwiYj86cyMidhcbG8uZM2dSbXNyciJ//vwALFy4kFq1atGgQQO+/fZbNm7cyJw5cwAIDg5mxIgRdO/enZEjR3L+/Hlef/11unbtSsGCBQEYOXIkr776Kr6+vrRu3Zro6GjWrl3L66+/bttvVERsQuFGROzur7/+ws/PL9W28uXLs3fvXiDpnUwLFiygd+/e+Pn58d1331GpUiUAPDw8WLJkCf379+fRRx/Fw8ODp556ivHjx6ccq3v37ty8eZMJEyYwePBg8ufPz9NPP227b1BEbMpkGIZh7yFERO7GZDLxyy+/0KFDB3uPIiLZhK65EREREYeicCMiIiIORdfciEiWplfORSStdOZGREREHIrCjYiIiDgUhRsRERFxKAo3IiIi4lAUbkRERMShKNyIiIiIQ1G4EREREYeicCMiIiIO5f8BH4LcCEdrW5oAAAAASUVORK5CYII=\n"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"plt.plot(epoch_train_accuracies, label = 'Train Accuracy')\nplt.plot(epoch_val_accuracies, label = 'Val Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel(\"Train Accuracy\")\nplt.legend()\nplt.grid(True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T13:21:42.986160Z","iopub.execute_input":"2025-12-04T13:21:42.986505Z","iopub.status.idle":"2025-12-04T13:21:43.234469Z","shell.execute_reply.started":"2025-12-04T13:21:42.986480Z","shell.execute_reply":"2025-12-04T13:21:43.233478Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"final_acc = epoch_train_accuracies[-1]\n\nfinal_loss = epoch_train_losses[-1]\n\nfinal_val_acc = epoch_val_accuracies[-1]\nfinal_val_loss = epoch_val_losses[-1]\n\nmax_train_acc = max(epoch_train_accuracies)\n\nprint(f'최종 학습 정확도 : {final_acc*100}')\nprint(f'최대 학습 정확도 : {max_train_acc*100}')\nprint(f'최종 검증 정확도 : {final_val_acc * 100}')\nprint(f'최대 검증 정확도 : {max(epoch_val_accuracies) * 100}')\n\n\nprint(f'최종 학습 Loss : {final_loss}')\nprint(f'최종 검증 loss : {final_val_loss}')\nprint(f'최소 학습 Loss : {min(epoch_train_losses)}')\nprint(f'최소 검증 Loss : {min(epoch_val_losses)}')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T13:21:46.917281Z","iopub.execute_input":"2025-12-04T13:21:46.917589Z","iopub.status.idle":"2025-12-04T13:21:46.925714Z","shell.execute_reply.started":"2025-12-04T13:21:46.917567Z","shell.execute_reply":"2025-12-04T13:21:46.924701Z"}},"outputs":[{"name":"stdout","text":"최종 학습 정확도 : 11.725\n최대 학습 정확도 : 11.725\n최종 검증 정확도 : 18.07\n최대 검증 정확도 : 18.07\n최종 학습 Loss : 4.171926782593388\n최종 검증 loss : 3.736922735223374\n최소 학습 Loss : 4.171926782593388\n최소 검증 Loss : 3.736922735223374\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# 1. 앞 1000개만 쓰는 subset 정의\nnum_small = 1000\nsmall_indices = list(range(num_small))\nsmall_train_dataset = Subset(train_dataset, small_indices)\n\n# 2. 작은 dataloader\nsmall_train_loader = DataLoader(\n    small_train_dataset,\n    batch_size=64,\n    shuffle=True,\n)\n\nsmall_val_dataset = Subset(val_dataset, small_indices)\n\n# 2. 작은 dataloader\nsmall_val_loader = DataLoader(\n    small_val_dataset,\n    batch_size=16,\n    shuffle=False,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:39:16.927961Z","iopub.execute_input":"2025-12-04T02:39:16.928417Z","iopub.status.idle":"2025-12-04T02:39:16.935213Z","shell.execute_reply.started":"2025-12-04T02:39:16.928391Z","shell.execute_reply":"2025-12-04T02:39:16.934240Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from collections import Counter\nimport numpy as np\n\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for x, y in small_train_loader:\n        x = x.to(device)\n        logits = model(x)\n        preds = logits.argmax(1).cpu().numpy()\n        all_preds.extend(list(preds))\n        all_labels.extend(list(y.numpy()))\n\nprint(\"pred label dist:\", Counter(all_preds))\nprint(\"true label dist:\", Counter(all_labels))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T05:42:29.560027Z","iopub.execute_input":"2025-12-01T05:42:29.560306Z","iopub.status.idle":"2025-12-01T05:42:31.898545Z","shell.execute_reply.started":"2025-12-01T05:42:29.560286Z","shell.execute_reply":"2025-12-01T05:42:31.897900Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. 3~5 epoch만 빠르게 돌려보기\nepoch_losses = []\nepoch_accuracies = []\nfor epoch in range(3):\n    epoch_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for imgs, labels in small_train_loader:\n        model.train()\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logits = model(imgs)\n        loss = loss_fn(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item() * imgs.size(0)\n        preds = logits.argmax(1)\n\n        print(f'preds : {preds}')\n        correct += (preds == labels).sum().item()\n        total += imgs.size(0)\n\n    with torch.no_grad():\n        for img,labels in small_val_loader:\n            model.eval()\n            # validate \n            # print(f'val img.shape : {img.shape }')\n            # B, ncrops, C, H, W = images.shape\n            # img = img.view(-1,C, H, W )\n            val_loss , val_true_prediction = validate(model, img,labels)\n            print(f'val loss :{val_loss}, val_accuracy : {val_true_prediction/labels.shape[0]}')\n        \n    print(f'[small] epoch {epoch} loss: {epoch_loss/total:.3f}, acc: {correct/total:.3f}')\n    epoch_losses.append(epoch_loss/total)\n    epoch_accuracies.append(correct/total)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:50:37.018698Z","iopub.execute_input":"2025-12-04T02:50:37.019032Z","iopub.status.idle":"2025-12-04T03:00:20.435790Z","shell.execute_reply.started":"2025-12-04T02:50:37.019007Z","shell.execute_reply":"2025-12-04T03:00:20.434420Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"preds : tensor([ 95, 153, 156,  29,  96, 140, 123,  93,  31,  93,  96,  70,  93, 154,\n        143,  31, 123,  84,  93,  48, 126, 156,  81,  93, 141,  70, 102, 172,\n         98, 129, 118,  21,  82, 129, 116,  31,  23,  59,  54, 123,  83,  96,\n         48, 118, 129,  29,  82,  96,  86, 159,  48,  60,  70, 145,  97,  78,\n        123, 147, 147,  43,  11,  43, 153, 172])\npreds : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\npreds : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\npreds : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\npreds : tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\npreds : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\npreds : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\npreds : tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\npreds : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\npreds : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\npreds : tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0])\npreds : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\npreds : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\npreds : tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0])\npreds : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0])\npreds : tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nval loss :1.0415995121002197, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nval loss :1.0420546531677246, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nval loss :1.0417444705963135, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\nval loss :0.5116612315177917, val_accuracy : 0.875\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\nval loss :0.43536102771759033, val_accuracy : 1.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\nval loss :0.43543195724487305, val_accuracy : 1.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2]\nval loss :17.8847599029541, val_accuracy : 0.25\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\nval loss :23.700605392456055, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\nval loss :23.700897216796875, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3]\nval loss :23.31629753112793, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\nval loss :23.085880279541016, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\nval loss :23.085935592651367, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4]\nval loss :23.586423873901367, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\nval loss :24.087797164916992, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\nval loss :24.087841033935547, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5]\nval loss :23.975698471069336, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\nval loss :23.788665771484375, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\nval loss :23.78803825378418, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6]\nval loss :23.87175178527832, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\nval loss :24.122352600097656, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\nval loss :24.121891021728516, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7]\nval loss :24.11477279663086, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\nval loss :24.06171417236328, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\nval loss :24.061969757080078, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\nval loss :24.06121063232422, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\nval loss :22.992847442626953, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\nval loss :22.992124557495117, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\nval loss :22.992223739624023, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [8 8 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\nval loss :23.974979400634766, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\nval loss :24.115097045898438, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\nval loss :24.115100860595703, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [ 9  9  9  9 10 10 10 10 10 10 10 10 10 10 10 10]\nval loss :23.883121490478516, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\nval loss :23.8060245513916, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\nval loss :23.8057861328125, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11]\nval loss :23.873395919799805, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\nval loss :23.91497230529785, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\nval loss :23.914522171020508, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [11 11 11 11 11 11 11 11 12 12 12 12 12 12 12 12]\nval loss :23.951162338256836, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12]\nval loss :23.986896514892578, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12]\nval loss :23.986886978149414, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13]\nval loss :23.96840476989746, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13]\nval loss :23.937864303588867, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13]\nval loss :23.937726974487305, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [13 13 13 13 13 13 13 13 13 13 13 13 14 14 14 14]\nval loss :23.855144500732422, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]\nval loss :23.608003616333008, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]\nval loss :23.607666015625, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 15 15]\nval loss :23.71078109741211, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15]\nval loss :24.431976318359375, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15]\nval loss :24.432235717773438, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15]\nval loss :24.43100929260254, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16]\nval loss :24.376537322998047, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16]\nval loss :24.376760482788086, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16]\nval loss :24.376548767089844, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [16 16 17 17 17 17 17 17 17 17 17 17 17 17 17 17]\nval loss :25.691762924194336, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17]\nval loss :25.880041122436523, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17]\nval loss :25.879701614379883, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [17 17 17 17 18 18 18 18 18 18 18 18 18 18 18 18]\nval loss :24.457122802734375, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18]\nval loss :23.981550216674805, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18]\nval loss :23.98171615600586, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [18 18 18 18 18 18 19 19 19 19 19 19 19 19 19 19]\nval loss :24.84672737121582, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19]\nval loss :25.364564895629883, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19]\nval loss :25.363624572753906, val_accuracy : 0.0\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1], 정답 [19 19 19 19 19 19 19 19]\nval loss :25.364994049072266, val_accuracy : 0.0\n[small] epoch 0 loss: 2.381, acc: 0.457\npreds : tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1])\npreds : tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0])\npreds : tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1])\npreds : tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0])\npreds : tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1])\npreds : tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0])\npreds : tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1])\npreds : tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1])\npreds : tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0])\npreds : tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\npreds : tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0])\npreds : tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1])\npreds : tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1])\npreds : tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1])\npreds : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1])\npreds : tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1])\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nval loss :0.24142783880233765, val_accuracy : 1.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nval loss :0.24130120873451233, val_accuracy : 1.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nval loss :0.24130681157112122, val_accuracy : 1.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\nval loss :1.3754568099975586, val_accuracy : 0.125\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\nval loss :1.5377880334854126, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\nval loss :1.5378365516662598, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2]\nval loss :18.090436935424805, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\nval loss :23.60748291015625, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\nval loss :23.607444763183594, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3]\nval loss :23.445838928222656, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\nval loss :23.349220275878906, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\nval loss :23.349136352539062, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4]\nval loss :23.696821212768555, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\nval loss :24.044591903686523, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\nval loss :24.044532775878906, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5]\nval loss :23.97722625732422, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\nval loss :23.864723205566406, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\nval loss :23.864465713500977, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6]\nval loss :23.95903205871582, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\nval loss :24.2424373626709, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\nval loss :24.242122650146484, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7]\nval loss :24.243000030517578, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\nval loss :24.249767303466797, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\nval loss :24.249820709228516, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\nval loss :24.24942398071289, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\nval loss :23.193044662475586, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\nval loss :23.193103790283203, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\nval loss :23.19277572631836, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [8 8 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\nval loss :24.33271026611328, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\nval loss :24.495182037353516, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\nval loss :24.4952449798584, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [ 9  9  9  9 10 10 10 10 10 10 10 10 10 10 10 10]\nval loss :23.88022232055664, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\nval loss :23.67572593688965, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\nval loss :23.67559051513672, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11]\nval loss :24.009340286254883, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\nval loss :24.20977210998535, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\nval loss :24.209814071655273, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [11 11 11 11 11 11 11 11 12 12 12 12 12 12 12 12]\nval loss :24.122663497924805, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12]\nval loss :24.034912109375, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12]\nval loss :24.03490447998047, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13]\nval loss :23.98097801208496, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13]\nval loss :23.891357421875, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13]\nval loss :23.891571044921875, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [13 13 13 13 13 13 13 13 13 13 13 13 14 14 14 14]\nval loss :23.87908172607422, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]\nval loss :23.841197967529297, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]\nval loss :23.841054916381836, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 15 15]\nval loss :23.968381881713867, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15]\nval loss :24.85947036743164, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15]\nval loss :24.859901428222656, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15]\nval loss :24.858642578125, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16]\nval loss :24.42965316772461, val_accuracy : 0.0\nVAL : 예측라벨 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], 정답 [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16]\nval loss :24.42942237854004, val_accuracy : 0.0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2033316765.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# B, ncrops, C, H, W = images.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# img = img.view(-1,C, H, W )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mval_loss\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mval_true_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'val loss :{val_loss}, val_accuracy : {val_true_prediction/labels.shape[0]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/828456332.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, data, label)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# bx10 , num_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m# print(f'logits1.shape : {logits.shape}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/537886857.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# print(f'x : {x.shape}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;31m#  Response Normalization 적용하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mlrn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLocalResponseNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"device = 'cuda' if( torch.cuda.is_available) else 'cpu'\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T05:40:31.414030Z","iopub.execute_input":"2025-12-03T05:40:31.414290Z","iopub.status.idle":"2025-12-03T05:40:31.418071Z","shell.execute_reply.started":"2025-12-03T05:40:31.414274Z","shell.execute_reply":"2025-12-03T05:40:31.417288Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"plt.plot(epoch_losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title(\"Adam + lr : 1e-4\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:21:50.280060Z","iopub.status.idle":"2025-11-30T13:21:50.280361Z","shell.execute_reply.started":"2025-11-30T13:21:50.280191Z","shell.execute_reply":"2025-11-30T13:21:50.280207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(epoch_accuracies)\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title(\"Adam + lr : 1e-4\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T13:49:36.555935Z","iopub.execute_input":"2025-11-27T13:49:36.556736Z","iopub.status.idle":"2025-11-27T13:49:36.738311Z","shell.execute_reply.started":"2025-11-27T13:49:36.556708Z","shell.execute_reply":"2025-11-27T13:49:36.737547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for imgs, labels in small_train_loader:\n    print(np.unique(labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T13:25:20.531133Z","iopub.execute_input":"2025-11-27T13:25:20.531772Z","iopub.status.idle":"2025-11-27T13:25:22.378769Z","shell.execute_reply.started":"2025-11-27T13:25:20.531748Z","shell.execute_reply":"2025-11-27T13:25:22.377779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epoch_train_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:42:10.845332Z","iopub.execute_input":"2025-12-01T13:42:10.845575Z","iopub.status.idle":"2025-12-01T13:42:10.849120Z","shell.execute_reply.started":"2025-12-01T13:42:10.845558Z","shell.execute_reply":"2025-12-01T13:42:10.848322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x0, y0 = train_dataset[0]      # #train_dataset[0] # 레이블 0인, 이미지들 \n\nx0 = x0.unsqueeze(0).to(device)\ny0 = torch.tensor([0], device=device)\nmodel.to(device)\nfor step in range(200):\n    optimizer.zero_grad()\n    logits = model(x0)\n    loss = loss_fn(logits, y0)\n    loss.backward()\n    optimizer.step()\n\n    pred = logits.argmax(1).item()\n    if step % 20 == 0:\n        print(step, \"loss:\", loss.item(), \"pred:\", pred)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T13:19:16.417314Z","iopub.execute_input":"2025-11-27T13:19:16.417675Z","iopub.status.idle":"2025-11-27T13:19:18.037418Z","shell.execute_reply.started":"2025-11-27T13:19:16.417643Z","shell.execute_reply":"2025-11-27T13:19:18.036753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images, labels = next(iter(train_dataloader))\nprint(\"batch label dist:\", np.bincount(labels.numpy()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T12:44:51.811219Z","iopub.execute_input":"2025-11-27T12:44:51.811610Z","iopub.status.idle":"2025-11-27T12:44:52.438935Z","shell.execute_reply.started":"2025-11-27T12:44:51.811576Z","shell.execute_reply":"2025-11-27T12:44:52.437946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nalex = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\nalex.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T12:46:35.202285Z","iopub.execute_input":"2025-11-27T12:46:35.203083Z","iopub.status.idle":"2025-11-27T12:46:35.957954Z","shell.execute_reply.started":"2025-11-27T12:46:35.203016Z","shell.execute_reply":"2025-11-27T12:46:35.957211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"alex.classifier[6].out_features = 200\n\nalex","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T12:48:30.092725Z","iopub.execute_input":"2025-11-27T12:48:30.093037Z","iopub.status.idle":"2025-11-27T12:48:30.099022Z","shell.execute_reply.started":"2025-11-27T12:48:30.093016Z","shell.execute_reply":"2025-11-27T12:48:30.098307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(val_dataloader.batch_size) # batch의 크기 33  , batch_size = 120\nlen(train_dataloader) # batch_size = 120 \n\nepoch_val_accuracies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:42:33.566609Z","iopub.execute_input":"2025-12-01T13:42:33.566856Z","iopub.status.idle":"2025-12-01T13:42:33.570870Z","shell.execute_reply.started":"2025-12-01T13:42:33.566831Z","shell.execute_reply":"2025-12-01T13:42:33.570180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(\n    {'epoch' : 2, # 2의 배치 298까지 돌아감\n     'batch' : 298,# 299부터 돌려야함\n    'model_state_dict':model.state_dict(),\n    'optimizer' : optimizer.state_dict(),\n    'train_loss': epoch_train_losses,\n    'val_loss': epoch_val_losses,\n     'train_accuracy' : epoch_train_accuracies,\n     'val_accuracy' : epoch_val_accuracies\n    }\n   ,'./alexnet_ckpt.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T13:22:14.726944Z","iopub.execute_input":"2025-12-04T13:22:14.727264Z","iopub.status.idle":"2025-12-04T13:22:16.241490Z","shell.execute_reply.started":"2025-12-04T13:22:14.727242Z","shell.execute_reply":"2025-12-04T13:22:16.240460Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"checkpoint = torch.load('./alexnet_ckpt.pth',map_location = torch.device('cpu'),weights_only=False)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer'])\nstart_epoch = checkpoint['epoch']+1\nepoch_train_losses = checkpoint['train_loss']\nepoch_val_losses = checkpoint['val_loss']\nepoch_train_accuracies = checkpoint['train_accuracy']\nepoch_val_accuracies = checkpoint['val_accuracy']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T13:23:05.457539Z","iopub.execute_input":"2025-12-04T13:23:05.457839Z","iopub.status.idle":"2025-12-04T13:23:06.211654Z","shell.execute_reply.started":"2025-12-04T13:23:05.457819Z","shell.execute_reply":"2025-12-04T13:23:06.210672Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"epoch_train_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T13:23:29.675950Z","iopub.execute_input":"2025-12-04T13:23:29.676310Z","iopub.status.idle":"2025-12-04T13:23:29.682924Z","shell.execute_reply.started":"2025-12-04T13:23:29.676281Z","shell.execute_reply":"2025-12-04T13:23:29.681994Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"[5.095089932244631, 4.171926782593388]"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T12:04:51.689828Z","iopub.execute_input":"2025-12-01T12:04:51.690125Z","iopub.status.idle":"2025-12-01T12:04:51.693863Z","shell.execute_reply.started":"2025-12-01T12:04:51.690104Z","shell.execute_reply":"2025-12-01T12:04:51.693105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model2 =  Alexnet()\nmodel2.load_state_dict(torch.load('./model', weights_only=True))\nmodel2","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}