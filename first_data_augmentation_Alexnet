{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":109264,"sourceType":"datasetVersion","datasetId":56828,"isSourceIdPinned":false},{"sourceId":998277,"sourceType":"datasetVersion","datasetId":547506,"isSourceIdPinned":false}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 필요한 라이브러리 불러오기(import)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import DataLoader, Dataset, random_split,Subset\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import v2\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import ConcatDataset\n\nfrom sklearn import decomposition","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:57:05.580219Z","iopub.execute_input":"2025-12-01T11:57:05.580449Z","iopub.status.idle":"2025-12-01T11:57:49.950439Z","shell.execute_reply.started":"2025-12-01T11:57:05.580429Z","shell.execute_reply":"2025-12-01T11:57:49.949472Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"akash2sharma/tiny-imagenet\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:58:27.612780Z","iopub.execute_input":"2025-11-27T10:58:27.613119Z","iopub.status.idle":"2025-11-27T10:58:28.041396Z","shell.execute_reply.started":"2025-11-27T10:58:27.613082Z","shell.execute_reply":"2025-11-27T10:58:28.040565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transform 만들기 ","metadata":{}},{"cell_type":"code","source":"# 사용자 정의 transform 만들기\n\n# class MeanSubstractionTransform:\n#     def __call__(self,img):\n        \n#       mean = img.mean(dim=[1,2])  # 각 채널별 행, 열의 평균을 구할 수 있다 1행, 3열 형태이다.\n#       print(torch.mean(torch.tensor(img),dim=0))\n#       img -= mean.view(3,1,1) # mean을 C : 3개 , H : 1, W : 1의 3x1x1 형태로 바꿔주고 원본 이미지에서 빼기 연산을 수행해준다.\n#       return img\n\nclass AugmentDataPCA():\n  def __call__(self,img):\n    img_array = np.array(img)\n    img_array = img_array.reshape(-1,3)\n    pca = decomposition.PCA(n_components=3)\n    pca.fit(img_array)\n    pca_result = pca.transform(img_array)\n    # 주성분 벡터(고유벡터) , 각 행이 [R,G,B]\n    principal_components = torch.tensor(pca.components_, dtype = torch.float32)\n    # 분산(고유값)\n    explained_variance = torch.tensor(pca.explained_variance_,dtype=torch.float32)\n\n    alpha = torch.randn(3) * 0.1\n    delta = explained_variance * alpha\n    color_shift = torch.matmul(principal_components,delta) # principal_components와 delta.T와의 내적합을 구해준다.\n\n    augmented_img = torch.from_numpy(img_array) + color_shift[:,None, None] # img_array에 채널별 계산값을 더해준다.\n\n    return augmented_img\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T02:18:45.262697Z","iopub.execute_input":"2025-12-01T02:18:45.263002Z","iopub.status.idle":"2025-12-01T02:18:45.269041Z","shell.execute_reply.started":"2025-12-01T02:18:45.262980Z","shell.execute_reply":"2025-12-01T02:18:45.268122Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize(size =(256,256)),\n    transforms.CenterCrop(size= (224,224)),\n   # 전체 데이터의 각 채널별 평균 빼주기 \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nflip_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize(size =(256,256)),\n    transforms.CenterCrop(size= (224,224)),\n    transforms.RandomHorizontalFlip(p=1.0),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]\n)\n\ntest_transform = transforms.Compose([\n    transforms.Resize(size =(256,256)),\n    transforms.TenCrop(224),#10개 crop을 tuple로 반환    \n    \n    # (10,3,224,224)형태로 stack\n    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n    transforms.Lambda(lambda crops: torch.stack([\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(c) for c in crops\n    ]))\n    ]\n)\n\n\ntrain_dir = '/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/'\nval_dir = '/kaggle/input/tiny-imagenet/tiny-imagenet-200/val/'\nval_reorg_dir='/kaggle/working/tiny-imagenet/val_reorganized'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:58:07.926356Z","iopub.execute_input":"2025-12-01T11:58:07.926930Z","iopub.status.idle":"2025-12-01T11:58:07.932616Z","shell.execute_reply.started":"2025-12-01T11:58:07.926909Z","shell.execute_reply":"2025-12-01T11:58:07.931767Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from IPython.utils.path import shutil\nimport os\nimport pandas as pd\n\nval_img_dir = os.path.join(val_dir, 'images')\nval_annotations = os.path.join(val_dir, 'val_annotations.txt')\n\nval_reorg_dir =  '/kaggle/working/tiny-imagenet/val_reorganized'\n\n# annotation 파일 읽기\n\nval_df = pd.read_csv(val_annotations, sep='\\t', header=None,names=['filename','class_id','x','y','w','h'])\n\nfor _,row in val_df.iterrows():\n    filename = row['filename']\n    class_id = row['class_id']\n\n    # 클래스 폴더 생성\n    class_folder = os.path.join(val_reorg_dir, class_id)\n    os.makedirs(class_folder, exist_ok=True)\n\n    # 이미지 복사\n    src = os.path.join(val_img_dir, filename)\n    dst = os.path.join(class_folder, filename)\n\n    shutil.copy(src,dst)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T06:11:03.591786Z","iopub.execute_input":"2025-12-01T06:11:03.592072Z","iopub.status.idle":"2025-12-01T06:12:40.017148Z","shell.execute_reply.started":"2025-12-01T06:11:03.592051Z","shell.execute_reply":"2025-12-01T06:12:40.016513Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# 데이터 불러오기 ","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 128\ntrain_dataset = datasets.ImageFolder(train_dir, transform=transform)\ntrain_flip_dataset = datasets.ImageFolder(train_dir, transform=flip_transform)\n\nval_dataset = datasets.ImageFolder(val_reorg_dir, transform = test_transform)\n\nfull_train_dataset = ConcatDataset([train_dataset, train_flip_dataset])\n\ntrain_dataloader = DataLoader(full_train_dataset, batch_size = BATCH_SIZE, shuffle = True)#  shuffle: 에포크마다 배치 순서를 섞을지 여부\n\nVAL_BATCH_SIZE = 32\nval_dataloader = DataLoader(val_dataset, batch_size= VAL_BATCH_SIZE, shuffle = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:58:13.888249Z","iopub.execute_input":"2025-12-01T11:58:13.888554Z","iopub.status.idle":"2025-12-01T11:58:53.859041Z","shell.execute_reply.started":"2025-12-01T11:58:13.888532Z","shell.execute_reply":"2025-12-01T11:58:53.858168Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(len(val_dataloader.dataset))# 200000\nlen(val_dataset) # 10000\n\nfor images, labels in val_dataloader:\n    print(images.shape)#torch.Size([128, 10, 3, 224, 224])\n\nbreak","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T06:04:06.475983Z","iopub.execute_input":"2025-12-01T06:04:06.476253Z","iopub.status.idle":"2025-12-01T06:04:09.148485Z","shell.execute_reply.started":"2025-12-01T06:04:06.476231Z","shell.execute_reply":"2025-12-01T06:04:09.147479Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"10000\ntorch.Size([32, 10, 3, 224, 224])\ntorch.Size([32, 10, 3, 224, 224])\ntorch.Size([32, 10, 3, 224, 224])\ntorch.Size([32, 10, 3, 224, 224])\ntorch.Size([32, 10, 3, 224, 224])\ntorch.Size([32, 10, 3, 224, 224])\ntorch.Size([32, 10, 3, 224, 224])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1516153130.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#torch.Size([128, 10, 3, 224, 224])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/1405829277.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(crops)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# (10,3,224,224)형태로 stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcrops\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcrop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     transforms.Lambda(lambda crops: torch.stack([\n\u001b[1;32m     27\u001b[0m         \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":74},{"cell_type":"code","source":"print(val_dataset[0][0].shape) # torch.Size([10, 3, 224, 224])\n   \n\n# torch.Size([3, 224, 224])\n# tiny dataset : torch.Size([3, 64, 64]) -> torch.Size([3, 224, 224])로 수정\n\n\nplt.imshow(train_dataloader.dataset[0][0].permute(1,2,0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:59:47.642090Z","iopub.execute_input":"2025-12-01T11:59:47.642370Z","iopub.status.idle":"2025-12-01T11:59:48.627243Z","shell.execute_reply.started":"2025-12-01T11:59:47.642350Z","shell.execute_reply":"2025-12-01T11:59:48.626497Z"}},"outputs":[{"name":"stderr","text":"Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.9825114..2.64].\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([10, 3, 224, 224])\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7e49c9b84080>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Tiny버전 모델 정의하기","metadata":{}},{"cell_type":"code","source":"class Alexnet_Tiny(nn.Module):\n    def __init__(self):\n        super().__init__()\n        dropout = nn.Dropout(p=0.5),\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64,kernel_size=(11,11),stride=4,padding=2)\n        nn.init.normal_(self.conv1.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv1.bias,val=0)\n        self.lrn1 = nn.LocalResponseNorm(size = 5, alpha=10**(-4),beta=0.75,k = 2)\n\n\n        self.conv2= nn.Conv2d(in_channels=64, out_channels=192, kernel_size=(5,5),padding = 2)\n        nn.init.normal_(self.conv2.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv2.bias,val=1)\n        self.lrn2 = nn.LocalResponseNorm(size = 5, alpha=10**(-4),beta=0.75,k = 2)\n\n        self.conv3= nn.Conv2d(in_channels=192, out_channels=384, kernel_size=(3,3),padding=1)\n        nn.init.normal_(self.conv3.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv3.bias,val=0)\n        \n\n        self.conv4= nn.Conv2d(in_channels=384, out_channels=256, kernel_size=(3,3),padding=1)\n        nn.init.normal_(self.conv4.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv4.bias,val=1)\n\n        self.conv5= nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3),padding = 1)\n        nn.init.normal_(self.conv5.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv5.bias,val=1)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((6,6))\n        self.fc1= nn.Linear(in_features=256*6*6, out_features=4096)\n        nn.init.constant_(self.fc1.bias,val=1)\n        self.fc2 = nn.Linear(in_features = 4096, out_features=4096)\n        nn.init.constant_(self.fc2.bias,val=1)\n        self.fc3 = nn.Linear(in_features = 4096, out_features=200)\n        nn.init.constant_(self.fc3.bias,val=0)\n        nn.init.normal_(self.fc1.weight, mean=0.0, std=0.01)\n        nn.init.normal_(self.fc2.weight, mean=0.0, std=0.01)\n        nn.init.normal_(self.fc3.weight, mean=0.0, std=0.01)\n\n\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            self.fc1,\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            self.fc2,\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            self.fc3\n        )\n\n\n\n    def forward(self,x):\n        # ---- Conv1 ----\n        # ReLU\n\n        # print(f'x : {x.shape}')\n        out = F.relu(self.conv1(x))\n        #  Response Normalization 적용하기\n        lrn = nn.LocalResponseNorm(size = 5, alpha=10**(-4),beta=0.75,k = 2)\n        out = self.lrn1(out)\n        # MaxPooling 적용 (window = 3, stride = 2)\n        out = F.max_pool2d(input = out, kernel_size=3, stride=2)\n\n       # print(f'conv1 후 : {out.shape}')\n\n\n\n        # ---- Conv2 ----\n        out = F.relu(self.conv2(out))\n        #  Response Normalization 적용하기\n        out = self.lrn2(out)\n\n        # MaxPooling 적용 (window = 3, stride = 2)\n        out = F.max_pool2d(input = out,kernel_size=3, stride=2)\n\n        # print(f'conv2 후 : {out.shape}')\n\n\n        # ---- Conv3 ----\n        out = F.relu(self.conv3(out))\n\n        # print(f'conv3 후 : {out.shape}')\n\n\n        # ---- Conv4 ----\n        out = F.relu(self.conv4(out))\n        # print(f'conv4 후 : {out.shape}')\n\n\n        # ---- Conv5 ----\n        out = F.relu(self.conv5(out))\n        out = F.max_pool2d(input = out, kernel_size = 3, stride= 2 )\n\n        # print(f'conv5 후 : {out.shape}')\n\n\n        # ---- Fully Connected layer ----\n        logits = self.classifier(out)\n\n        # print(f'logits : {logits.shape}')\n\n        # ---- softmax ----\n        #probs = F.softmax(logits, dim=1)\n\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:59:51.892941Z","iopub.execute_input":"2025-12-01T11:59:51.893244Z","iopub.status.idle":"2025-12-01T11:59:51.902102Z","shell.execute_reply.started":"2025-12-01T11:59:51.893226Z","shell.execute_reply":"2025-12-01T11:59:51.901294Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Alexnet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        dropout = nn.Dropout(p=0.5),\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96,kernel_size=(11,11),stride=4,padding=2)\n        nn.init.normal_(self.conv1.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv1.bias,val=0)\n\n        self.conv2= nn.Conv2d(in_channels=96, out_channels=256, kernel_size=(5,5),padding = 2)\n        nn.init.normal_(self.conv2.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv2.bias,val=1)\n        self.conv3= nn.Conv2d(in_channels=256, out_channels=384, kernel_size=(3,3),padding=1)\n        nn.init.normal_(self.conv3.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv3.bias,val=0)\n\n        self.conv4= nn.Conv2d(in_channels=384, out_channels=384, kernel_size=(3,3),padding=1)\n        nn.init.normal_(self.conv4.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv4.bias,val=1)\n\n        self.conv5= nn.Conv2d(in_channels=384, out_channels=256, kernel_size=(3,3),padding = 1)\n        nn.init.normal_(self.conv5.weight, mean = 0.0, std = 0.01)\n        nn.init.constant_(self.conv5.bias,val=1)\n\n        self.fc1= nn.Linear(in_features=256*6*6, out_features=4096)\n        nn.init.constant_(self.fc1.bias,val=1)\n        self.fc2 = nn.Linear(in_features = 4096, out_features=4096)\n        nn.init.constant_(self.fc2.bias,val=1)\n        self.fc3 = nn.Linear(in_features = 4096, out_features=200)\n        nn.init.constant_(self.fc3.bias,val=0)\n\n\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            self.fc1,\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            self.fc2,\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            self.fc3\n        )\n\n\n\n    def forward(self,x):\n        # ---- Conv1 ----\n        # ReLU\n\n        print(f'x : {x.shape}')\n        out = F.relu(self.conv1(x))\n        #  Response Normalization 적용하기\n        lrn = nn.LocalResponseNorm(size = 5, alpha=10**(-4),beta=0.75,k = 2)\n        out = lrn(out)\n        # MaxPooling 적용 (window = 3, stride = 2)\n        out = F.max_pool2d(input = out, kernel_size=3, stride=2)\n\n        print(f'conv1 후 : {out.shape}')\n\n\n\n        # ---- Conv2 ----\n        out = F.relu(self.conv2(out))\n        #  Response Normalization 적용하기\n        out = lrn(out)\n\n        # MaxPooling 적용 (window = 3, stride = 2)\n        out = F.max_pool2d(input = out,kernel_size=3, stride=2)\n\n        print(f'conv2 후 : {out.shape}')\n\n\n        # ---- Conv3 ----\n        out = F.relu(self.conv3(out))\n\n        print(f'conv3 후 : {out.shape}')\n\n\n        # ---- Conv4 ----\n        out = F.relu(self.conv4(out))\n        print(f'conv4 후 : {out.shape}')\n\n\n        # ---- Conv5 ----\n        out = F.relu(self.conv5(out))\n        out = F.max_pool2d(input = out, kernel_size = 3, stride= 2 )\n\n        print(f'conv5 후 : {out.shape}')\n\n\n        # ---- Fully Connected layer ----\n        logits = self.classifier(out)\n\n        print(f'logits : {logits.shape}')\n\n        # ---- softmax ----\n        probs = F.softmax(logits, dim=1)\n\n        return logits, probs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T11:20:14.844860Z","iopub.execute_input":"2025-11-27T11:20:14.845155Z","iopub.status.idle":"2025-11-27T11:20:14.859359Z","shell.execute_reply.started":"2025-11-27T11:20:14.845135Z","shell.execute_reply":"2025-11-27T11:20:14.858595Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = Alexnet_Tiny().to(device)# 먼저 모델을 디바이스로 보내기 \n\nEPOCH = 40\n\nLEARNING_RATE = 1e-4 # validation loss에 따라 조절하기\n\nloss_fn = torch.nn.CrossEntropyLoss()\n#optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9) # 그 다음 optimizer 만들기 \n# # 또는\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n#schedular = StepLR(optimizer=optimizer, step_size=1, gamma=0.0001)\n\nwriter = SummaryWriter()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T12:00:02.430939Z","iopub.execute_input":"2025-12-01T12:00:02.431202Z","iopub.status.idle":"2025-12-01T12:00:02.931623Z","shell.execute_reply.started":"2025-12-01T12:00:02.431184Z","shell.execute_reply":"2025-12-01T12:00:02.930654Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train_model(model, data,label):\n  \n  model.train()\n\n  running_loss = 0.0\n  last_loss = 0.0\n  true_prediction = 0\n\n\n  data ,label = data.to(device),label.to(device)\n\n  optimizer.zero_grad()\n\n  logits = model(data)\n  loss = loss_fn(logits,label)\n\n \n  loss.backward()\n  optimizer.step()\n\n # 정확도 계산\n  label, logits = label.cpu().detach().numpy(), logits.cpu().detach().numpy()\n\n  label_predict = np.argmax(logits,axis=1)\n\n  true_prediction = np.sum(label == label_predict)\n  train_loss = loss.item()\n\n  return train_loss , true_prediction\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T12:00:04.736864Z","iopub.execute_input":"2025-12-01T12:00:04.737131Z","iopub.status.idle":"2025-12-01T12:00:04.741072Z","shell.execute_reply.started":"2025-12-01T12:00:04.737114Z","shell.execute_reply":"2025-12-01T12:00:04.740316Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def validate(model,data,label):\n\n    model.eval()\n\n    running_loss = 0.0\n    last_loss= 0.0\n    true_prediction = 0\n\n    print(f'val - before view : {data.shape}')\n\n    B,flip,C,H,W = data.shape\n    \n    data = data.view(-1, C,H,W)\n    print(f'val after view : {data.shape}')\n\n    data ,label = data.to(device),  label.to(device)\n    logits = model(data) # bx10 , num_classes\n    print(f'logits1.shape : {logits.shape}')\n    logits = logits.view(B,flip,-1)\n    print(f'logits2.shape : {logits.shape}')\n\n    logits_mean = logits.mean(1)\n\n    \n    print(f'logits_mean.shape : {logits_mean.shape}')\n\n    loss = loss_fn(logits_mean, label)\n\n    label = label.to('cpu').detach().numpy()\n\n    logits_mean = logits_mean.to('cpu').detach().numpy()\n\n    # print(f'logits : {logits}')\n\n    prob_label = np.argmax(logits_mean,axis = 1)\n    print(f'VAL : 예측라벨 : {prob_label}, 정답 {label}')\n\n    true_prediction = np.sum(label == prob_label)\n    val_loss = loss.item()\n    \n    return val_loss , true_prediction\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T12:00:06.434806Z","iopub.execute_input":"2025-12-01T12:00:06.435042Z","iopub.status.idle":"2025-12-01T12:00:06.439873Z","shell.execute_reply.started":"2025-12-01T12:00:06.435023Z","shell.execute_reply":"2025-12-01T12:00:06.439156Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def valAugmentation(img):\n    crop_size = 224\n    five_crop = v2.TenCrop(crop_size)\n    ten_cropped_img = five_crop(img)\n\n    plt.imshow(ten_cropped_img)\n    return ten_cropped_img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:27:56.581088Z","iopub.execute_input":"2025-11-30T13:27:56.581678Z","iopub.status.idle":"2025-11-30T13:27:56.585458Z","shell.execute_reply.started":"2025-11-30T13:27:56.581654Z","shell.execute_reply":"2025-11-30T13:27:56.584729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = torch.rand(3,3)\nprint(test)\nprint(test.sum())\nprint(test.sum(dim=[0]))\n\nprint(test.size(0))\nprint(test.sum(dim=[0])/test.size(0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T12:54:45.420874Z","iopub.execute_input":"2025-11-26T12:54:45.421624Z","iopub.status.idle":"2025-11-26T12:54:45.435192Z","shell.execute_reply.started":"2025-11-26T12:54:45.421597Z","shell.execute_reply":"2025-11-26T12:54:45.434598Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_img_value = torch.zeros(3)\ntotal_pixels =  0\n\nfor batch,(data,label) in enumerate(val_dataloader):\n\n    # print(data.shape) # torch.Size([120, 3, 224, 224])\n\n    total_img_value += data.sum(dim=[0,2,3]) # dim을 한꺼번에 모두 더함 \n    total_pixels += data.size(0) * data.size(2) * data.size(3)\n\nmean = total_img_value/total_pixels\n\nmean","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:18:37.723789Z","iopub.execute_input":"2025-11-26T06:18:37.724518Z","iopub.status.idle":"2025-11-26T06:19:34.811753Z","shell.execute_reply.started":"2025-11-26T06:18:37.724490Z","shell.execute_reply":"2025-11-26T06:19:34.810982Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def unnormalize(img, mean, std):\n    img = img.clone()\n    for t, m, s in zip(img, mean, std):\n        t.mul_(s).add_(m)\n    return img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:39:56.259128Z","iopub.execute_input":"2025-11-26T08:39:56.259678Z","iopub.status.idle":"2025-11-26T08:39:56.263674Z","shell.execute_reply.started":"2025-11-26T08:39:56.259655Z","shell.execute_reply":"2025-11-26T08:39:56.262934Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_dataset.transform)\n\nimg = unnormalize(train_dataset[0][0], [0.4777, 0.4520, 0.4032], [1.0,1.0,1.0])\n\n# 또는\nprint(img.shape, img.min(), img.max())\n\nplt.imshow(img.permute(1,2,0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T06:05:03.632319Z","iopub.execute_input":"2025-12-01T06:05:03.632927Z","iopub.status.idle":"2025-12-01T06:05:03.652761Z","shell.execute_reply.started":"2025-12-01T06:05:03.632880Z","shell.execute_reply":"2025-12-01T06:05:03.651854Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"Compose(\n    ToTensor()\n    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n    CenterCrop(size=(224, 224))\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2617827487.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.4777\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4520\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4032\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 또는\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'unnormalize' is not defined"],"ename":"NameError","evalue":"name 'unnormalize' is not defined","output_type":"error"}],"execution_count":80},{"cell_type":"code","source":"for data, label in train_dataloader:\n    print(label.min(), label.max())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T12:56:11.014193Z","iopub.execute_input":"2025-11-26T12:56:11.014501Z","iopub.status.idle":"2025-11-26T12:56:18.984007Z","shell.execute_reply.started":"2025-11-26T12:56:11.014462Z","shell.execute_reply":"2025-11-26T12:56:18.982965Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Early Stopping 구현하기 \n\n- 4시간 넘게 돌렸는데 val accuracy가 좋아지지 않았다 - early stopping 추가해서 더이상 올라가지 않으면 종료하기 ","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self,patience = 10, verbose=False, delta=0,path = 'checkpoing.pt',trace_func=print):\n        self.patience= patience\n        self.verbose = verbose # True이면 개선 시마다 메시지 출력 \n        self.delta = delta # 개선으로 인정할 최소 변화량 \n        self.path = path\n        self.trace_func = trace_func\n\n        self.best_val_accuracy = None\n        self.early_stop = False\n        self.val_accuracy_max = - np.inf\n        self.counter = 0\n    def __call__(self, val_accuracy, model):\n        if np.isnan(val_accuracy):\n            self.trace_func(\"Validation accuracy is NaN. Ignoring this epoch\")\n            return \n        if self.best_val_accuracy is None :\n            self.best_val_accuracy = val_accuracy\n            self.save_checkpoint(self.best_val_accuracy, model)\n        elif val_accuracy >  self.best_val_accuracy + self.delta:\n            self.best_val_accuracy = val_accuracy\n            self.save_checkpoint(val_accuracy,model)\n            self.counter = 0 \n        else:\n            self.counter += 1\n            self.trace_func(f'EarlyStopping counter : {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n    def save_checkpoint(self,val_accuracy, model):\n        if self.verbose:\n            self.trace_func(f'Validation accuracy increased {self.best_val_accuracy} -> {val_accuracy}. Saving Model...' \n                           )\n            torch.save(model.state_dict(),self.path)\n            self.best_val_accuracy = val_accuracy\n\nearly_stopping = EarlyStopping(patience = 15,verbose = True, delta = 1e-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T02:20:22.224849Z","iopub.execute_input":"2025-11-29T02:20:22.225647Z","iopub.status.idle":"2025-11-29T02:20:22.232198Z","shell.execute_reply.started":"2025-11-29T02:20:22.225610Z","shell.execute_reply":"2025-11-29T02:20:22.231426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_train_losses = []\nbatch_val_losses = []\nbatch_train_accuracies = []\nbatch_val_accuracies = []\n\n\nepoch_train_losses = []\nepoch_val_losses = []\nepoch_train_accuracies = []\nepoch_val_accuracies = []\n\nstart_epoch = start_epoch\n\nfor epoch in range(start_epoch, EPOCH):\n\n      epoch_train_loss = 0.0\n      epoch_train_accuracy = 0.0\n      epoch_true_prediction= 0\n      epoch_total = 0\n\n      epoch_val_loss = 0.0\n      epoch_val_accuracy = 0.0\n      epoch_val_true_prediction= 0\n      epoch_val_total = 0\n\n      for batch , (train_data,train_label) in enumerate(train_dataloader):\n\n        global_step = epoch * len(train_dataloader) + batch\n        # 총 batch가 200번 반복됨 , 에포크 5니까 총 1000번 반복됨.\n        print(f'Epoch : {epoch}, batch {batch}')\n        train_loss , batch_true_prediction = train_model(model,train_data,train_label)\n\n        batch_size = train_label.shape[0]\n\n        epoch_train_loss += train_loss\n        epoch_true_prediction += batch_true_prediction\n        epoch_total += batch_size\n\n        # 각 배치별 스칼라 기록\n        batch_train_accuracy = batch_true_prediction / batch_size\n\n        batch_train_losses.append(train_loss)\n        batch_train_accuracies.append(batch_train_accuracy)\n\n        print(f'(Train) Batch {batch} Loss : {train_loss}, 맞은 개수 : {batch_true_prediction}')\n        writer.add_scalar(\"Batch별 Loss/Train\", train_loss, global_step)\n        writer.add_scalar(\"Batch별 Accuracy/Train\", batch_train_accuracy,global_step)\n\n      # 각 에포크별 스칼라 기록\n      epoch_train_losses.append(epoch_train_loss / len(train_dataloader))#배치의 개수로 나누면 됨\n      epoch_train_accuracies.append(epoch_true_prediction / epoch_total)\n    \n      writer.add_scalar(\"Epoch별 Loss/Train\", epoch_train_loss / len(train_dataloader), epoch)\n      writer.add_scalar(\"Epoch별 Accuracy/Train\", epoch_true_prediction / epoch_total,epoch)\n      print(f'epoch {epoch} Loss/Train :{epoch_train_loss / len(train_dataloader)} ')\n      print(f'epoch {epoch} Accuracy/Train : {epoch_true_prediction / epoch_total}')\n\n\n    # 정확도 추가하자\n      with torch.no_grad():\n        for batch,(val_data,val_label) in enumerate(val_dataloader):\n          # print(\"data shape:\", val_data.shape, \"label shape:\", val_label.shape)\n          global_step = epoch * len(val_dataloader) + batch\n          val_loss , val_true_predction = validate(model,val_data,val_label)\n          batch_size = val_label.shape[0]\n          epoch_val_loss+= val_loss\n          epoch_val_true_prediction += val_true_predction\n          epoch_val_total += batch_size\n\n          print(f'(VAL) Batch {batch} Loss : {val_loss}, accuracy: {val_true_predction/batch_size}')\n          batch_val_losses.append(val_loss)\n          batch_val_accuracies.append(val_true_predction/batch_size)\n          writer.add_scalar(\"Batch별 Loss/Validate\", val_loss, global_step)\n          writer.add_scalar(\"Batch별 Accuracy/Validate Batch\", val_true_predction/batch_size,global_step)\n\n        epoch_val_losses.append(epoch_val_loss/len(val_dataloader))\n        epoch_val_accuracies.append(epoch_val_true_prediction/epoch_val_total)\n\n       # early_stopping(val_accuracy = epoch_val_true_prediction/epoch_val_total, model = model)\n     \n        writer.add_scalar(\"Epoch별 Loss/Validate\", epoch_val_loss/len(val_dataloader), epoch)\n        writer.add_scalar(\"Epoch별 Accuracy/Validate\", epoch_val_true_prediction/epoch_val_total,epoch)\n        print(f'epoch {epoch} Loss/Validate :{epoch_val_loss/len(val_dataloader)} ')\n        print(f'epoch {epoch} Accuracy/Validate : {epoch_val_true_prediction/epoch_val_total}')\n        \n        torch.save(\n    {'epoch' : EPOCH,\n    'model_state_dict':model.state_dict(),\n    'optimizer' : optimizer.state_dict()}\n   ,'alexnet_ckpt.pth')\n\n        # if early_stopping.early_stop:\n        #     print(f'Early Stopped : {epoch} ')\n        #     break\n\n\nwriter.flush()\n\n\n%load_ext tensorboard\n%tensorboard --logdir=runs\n\nwriter.close()\n\n# https://ysg2997.tistory.com/16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T12:05:42.228147Z","iopub.execute_input":"2025-12-01T12:05:42.228396Z","iopub.status.idle":"2025-12-01T13:39:36.813541Z","shell.execute_reply.started":"2025-12-01T12:05:42.228378Z","shell.execute_reply":"2025-12-01T13:39:36.812424Z"}},"outputs":[{"name":"stdout","text":"Epoch : 20, batch 0\n(Train) Batch 0 Loss : 0.2314506471157074, 맞은 개수 : 115\nEpoch : 20, batch 1\n(Train) Batch 1 Loss : 0.3105165362358093, 맞은 개수 : 118\nEpoch : 20, batch 2\n(Train) Batch 2 Loss : 0.48168376088142395, 맞은 개수 : 107\nEpoch : 20, batch 3\n(Train) Batch 3 Loss : 0.2901264727115631, 맞은 개수 : 114\nEpoch : 20, batch 4\n(Train) Batch 4 Loss : 0.30565527081489563, 맞은 개수 : 115\nEpoch : 20, batch 5\n(Train) Batch 5 Loss : 0.24733883142471313, 맞은 개수 : 119\nEpoch : 20, batch 6\n(Train) Batch 6 Loss : 0.30131348967552185, 맞은 개수 : 115\nEpoch : 20, batch 7\n(Train) Batch 7 Loss : 0.21465492248535156, 맞은 개수 : 123\nEpoch : 20, batch 8\n(Train) Batch 8 Loss : 0.19990837574005127, 맞은 개수 : 121\nEpoch : 20, batch 9\n(Train) Batch 9 Loss : 0.2315373718738556, 맞은 개수 : 118\nEpoch : 20, batch 10\n(Train) Batch 10 Loss : 0.2080351710319519, 맞은 개수 : 119\nEpoch : 20, batch 11\n(Train) Batch 11 Loss : 0.19836416840553284, 맞은 개수 : 121\nEpoch : 20, batch 12\n(Train) Batch 12 Loss : 0.21692518889904022, 맞은 개수 : 117\nEpoch : 20, batch 13\n(Train) Batch 13 Loss : 0.1579921543598175, 맞은 개수 : 123\nEpoch : 20, batch 14\n(Train) Batch 14 Loss : 0.20037034153938293, 맞은 개수 : 119\nEpoch : 20, batch 15\n(Train) Batch 15 Loss : 0.19657500088214874, 맞은 개수 : 121\nEpoch : 20, batch 16\n(Train) Batch 16 Loss : 0.18782493472099304, 맞은 개수 : 120\nEpoch : 20, batch 17\n(Train) Batch 17 Loss : 0.3162968158721924, 맞은 개수 : 115\nEpoch : 20, batch 18\n(Train) Batch 18 Loss : 0.1478598713874817, 맞은 개수 : 124\nEpoch : 20, batch 19\n(Train) Batch 19 Loss : 0.23910009860992432, 맞은 개수 : 117\nEpoch : 20, batch 20\n(Train) Batch 20 Loss : 0.19625554978847504, 맞은 개수 : 121\nEpoch : 20, batch 21\n(Train) Batch 21 Loss : 0.31772908568382263, 맞은 개수 : 117\nEpoch : 20, batch 22\n(Train) Batch 22 Loss : 0.28011640906333923, 맞은 개수 : 119\nEpoch : 20, batch 23\n(Train) Batch 23 Loss : 0.21848273277282715, 맞은 개수 : 118\nEpoch : 20, batch 24\n(Train) Batch 24 Loss : 0.27770134806632996, 맞은 개수 : 120\nEpoch : 20, batch 25\n(Train) Batch 25 Loss : 0.29711028933525085, 맞은 개수 : 119\nEpoch : 20, batch 26\n(Train) Batch 26 Loss : 0.2838507890701294, 맞은 개수 : 116\nEpoch : 20, batch 27\n(Train) Batch 27 Loss : 0.13318124413490295, 맞은 개수 : 122\nEpoch : 20, batch 28\n(Train) Batch 28 Loss : 0.20593732595443726, 맞은 개수 : 118\nEpoch : 20, batch 29\n(Train) Batch 29 Loss : 0.2226771116256714, 맞은 개수 : 119\nEpoch : 20, batch 30\n(Train) Batch 30 Loss : 0.2981973886489868, 맞은 개수 : 113\nEpoch : 20, batch 31\n(Train) Batch 31 Loss : 0.2517223358154297, 맞은 개수 : 117\nEpoch : 20, batch 32\n(Train) Batch 32 Loss : 0.21011848747730255, 맞은 개수 : 119\nEpoch : 20, batch 33\n(Train) Batch 33 Loss : 0.28491827845573425, 맞은 개수 : 118\nEpoch : 20, batch 34\n(Train) Batch 34 Loss : 0.24468006193637848, 맞은 개수 : 119\nEpoch : 20, batch 35\n(Train) Batch 35 Loss : 0.35623207688331604, 맞은 개수 : 112\nEpoch : 20, batch 36\n(Train) Batch 36 Loss : 0.342414915561676, 맞은 개수 : 114\nEpoch : 20, batch 37\n(Train) Batch 37 Loss : 0.45642420649528503, 맞은 개수 : 109\nEpoch : 20, batch 38\n(Train) Batch 38 Loss : 0.1999882012605667, 맞은 개수 : 120\nEpoch : 20, batch 39\n(Train) Batch 39 Loss : 0.2621304988861084, 맞은 개수 : 117\nEpoch : 20, batch 40\n(Train) Batch 40 Loss : 0.25690722465515137, 맞은 개수 : 119\nEpoch : 20, batch 41\n(Train) Batch 41 Loss : 0.16628235578536987, 맞은 개수 : 118\nEpoch : 20, batch 42\n(Train) Batch 42 Loss : 0.3233197331428528, 맞은 개수 : 114\nEpoch : 20, batch 43\n(Train) Batch 43 Loss : 0.22807767987251282, 맞은 개수 : 116\nEpoch : 20, batch 44\n(Train) Batch 44 Loss : 0.3146391212940216, 맞은 개수 : 118\nEpoch : 20, batch 45\n(Train) Batch 45 Loss : 0.21627026796340942, 맞은 개수 : 120\nEpoch : 20, batch 46\n(Train) Batch 46 Loss : 0.20685523748397827, 맞은 개수 : 119\nEpoch : 20, batch 47\n(Train) Batch 47 Loss : 0.2554699778556824, 맞은 개수 : 115\nEpoch : 20, batch 48\n(Train) Batch 48 Loss : 0.15750110149383545, 맞은 개수 : 122\nEpoch : 20, batch 49\n(Train) Batch 49 Loss : 0.39419013261795044, 맞은 개수 : 111\nEpoch : 20, batch 50\n(Train) Batch 50 Loss : 0.21763703227043152, 맞은 개수 : 119\nEpoch : 20, batch 51\n(Train) Batch 51 Loss : 0.2566637098789215, 맞은 개수 : 119\nEpoch : 20, batch 52\n(Train) Batch 52 Loss : 0.18457511067390442, 맞은 개수 : 120\nEpoch : 20, batch 53\n(Train) Batch 53 Loss : 0.2356709986925125, 맞은 개수 : 120\nEpoch : 20, batch 54\n(Train) Batch 54 Loss : 0.16723547875881195, 맞은 개수 : 123\nEpoch : 20, batch 55\n(Train) Batch 55 Loss : 0.33482393622398376, 맞은 개수 : 117\nEpoch : 20, batch 56\n(Train) Batch 56 Loss : 0.19483943283557892, 맞은 개수 : 117\nEpoch : 20, batch 57\n(Train) Batch 57 Loss : 0.16832968592643738, 맞은 개수 : 119\nEpoch : 20, batch 58\n(Train) Batch 58 Loss : 0.2731878459453583, 맞은 개수 : 117\nEpoch : 20, batch 59\n(Train) Batch 59 Loss : 0.18839497864246368, 맞은 개수 : 121\nEpoch : 20, batch 60\n(Train) Batch 60 Loss : 0.34623661637306213, 맞은 개수 : 112\nEpoch : 20, batch 61\n(Train) Batch 61 Loss : 0.0938010960817337, 맞은 개수 : 126\nEpoch : 20, batch 62\n(Train) Batch 62 Loss : 0.23326325416564941, 맞은 개수 : 121\nEpoch : 20, batch 63\n(Train) Batch 63 Loss : 0.1712118536233902, 맞은 개수 : 122\nEpoch : 20, batch 64\n(Train) Batch 64 Loss : 0.3951689898967743, 맞은 개수 : 112\nEpoch : 20, batch 65\n(Train) Batch 65 Loss : 0.23719556629657745, 맞은 개수 : 122\nEpoch : 20, batch 66\n(Train) Batch 66 Loss : 0.15514691174030304, 맞은 개수 : 122\nEpoch : 20, batch 67\n(Train) Batch 67 Loss : 0.2978973090648651, 맞은 개수 : 114\nEpoch : 20, batch 68\n(Train) Batch 68 Loss : 0.2821425795555115, 맞은 개수 : 118\nEpoch : 20, batch 69\n(Train) Batch 69 Loss : 0.20645372569561005, 맞은 개수 : 119\nEpoch : 20, batch 70\n(Train) Batch 70 Loss : 0.3174280822277069, 맞은 개수 : 113\nEpoch : 20, batch 71\n(Train) Batch 71 Loss : 0.18515639007091522, 맞은 개수 : 117\nEpoch : 20, batch 72\n(Train) Batch 72 Loss : 0.2526216506958008, 맞은 개수 : 117\nEpoch : 20, batch 73\n(Train) Batch 73 Loss : 0.3539634048938751, 맞은 개수 : 114\nEpoch : 20, batch 74\n(Train) Batch 74 Loss : 0.3058289885520935, 맞은 개수 : 115\nEpoch : 20, batch 75\n(Train) Batch 75 Loss : 0.298380970954895, 맞은 개수 : 115\nEpoch : 20, batch 76\n(Train) Batch 76 Loss : 0.14987719058990479, 맞은 개수 : 123\nEpoch : 20, batch 77\n(Train) Batch 77 Loss : 0.2507951557636261, 맞은 개수 : 121\nEpoch : 20, batch 78\n(Train) Batch 78 Loss : 0.21715740859508514, 맞은 개수 : 123\nEpoch : 20, batch 79\n(Train) Batch 79 Loss : 0.2132326364517212, 맞은 개수 : 121\nEpoch : 20, batch 80\n(Train) Batch 80 Loss : 0.19365249574184418, 맞은 개수 : 120\nEpoch : 20, batch 81\n(Train) Batch 81 Loss : 0.23343797028064728, 맞은 개수 : 117\nEpoch : 20, batch 82\n(Train) Batch 82 Loss : 0.2790105640888214, 맞은 개수 : 119\nEpoch : 20, batch 83\n(Train) Batch 83 Loss : 0.22888042032718658, 맞은 개수 : 118\nEpoch : 20, batch 84\n(Train) Batch 84 Loss : 0.3357250392436981, 맞은 개수 : 116\nEpoch : 20, batch 85\n(Train) Batch 85 Loss : 0.30290457606315613, 맞은 개수 : 115\nEpoch : 20, batch 86\n(Train) Batch 86 Loss : 0.19132529199123383, 맞은 개수 : 121\nEpoch : 20, batch 87\n(Train) Batch 87 Loss : 0.39198046922683716, 맞은 개수 : 112\nEpoch : 20, batch 88\n(Train) Batch 88 Loss : 0.3549143671989441, 맞은 개수 : 116\nEpoch : 20, batch 89\n(Train) Batch 89 Loss : 0.30053430795669556, 맞은 개수 : 117\nEpoch : 20, batch 90\n(Train) Batch 90 Loss : 0.2604815661907196, 맞은 개수 : 121\nEpoch : 20, batch 91\n(Train) Batch 91 Loss : 0.15415459871292114, 맞은 개수 : 123\nEpoch : 20, batch 92\n(Train) Batch 92 Loss : 0.10021065920591354, 맞은 개수 : 124\nEpoch : 20, batch 93\n(Train) Batch 93 Loss : 0.24713747203350067, 맞은 개수 : 119\nEpoch : 20, batch 94\n(Train) Batch 94 Loss : 0.3782494068145752, 맞은 개수 : 115\nEpoch : 20, batch 95\n(Train) Batch 95 Loss : 0.3117537200450897, 맞은 개수 : 120\nEpoch : 20, batch 96\n(Train) Batch 96 Loss : 0.17665113508701324, 맞은 개수 : 120\nEpoch : 20, batch 97\n(Train) Batch 97 Loss : 0.2959541976451874, 맞은 개수 : 116\nEpoch : 20, batch 98\n(Train) Batch 98 Loss : 0.13914141058921814, 맞은 개수 : 125\nEpoch : 20, batch 99\n(Train) Batch 99 Loss : 0.34612298011779785, 맞은 개수 : 116\nEpoch : 20, batch 100\n(Train) Batch 100 Loss : 0.2380695343017578, 맞은 개수 : 118\nEpoch : 20, batch 101\n(Train) Batch 101 Loss : 0.27143654227256775, 맞은 개수 : 118\nEpoch : 20, batch 102\n(Train) Batch 102 Loss : 0.3314659893512726, 맞은 개수 : 113\nEpoch : 20, batch 103\n(Train) Batch 103 Loss : 0.2695452868938446, 맞은 개수 : 112\nEpoch : 20, batch 104\n(Train) Batch 104 Loss : 0.28157907724380493, 맞은 개수 : 114\nEpoch : 20, batch 105\n(Train) Batch 105 Loss : 0.19725090265274048, 맞은 개수 : 120\nEpoch : 20, batch 106\n(Train) Batch 106 Loss : 0.37592172622680664, 맞은 개수 : 114\nEpoch : 20, batch 107\n(Train) Batch 107 Loss : 0.12897889316082, 맞은 개수 : 122\nEpoch : 20, batch 108\n(Train) Batch 108 Loss : 0.378553181886673, 맞은 개수 : 114\nEpoch : 20, batch 109\n(Train) Batch 109 Loss : 0.3260994553565979, 맞은 개수 : 112\nEpoch : 20, batch 110\n(Train) Batch 110 Loss : 0.3916224539279938, 맞은 개수 : 111\nEpoch : 20, batch 111\n(Train) Batch 111 Loss : 0.2958729565143585, 맞은 개수 : 117\nEpoch : 20, batch 112\n(Train) Batch 112 Loss : 0.237156942486763, 맞은 개수 : 119\nEpoch : 20, batch 113\n(Train) Batch 113 Loss : 0.41611409187316895, 맞은 개수 : 117\nEpoch : 20, batch 114\n(Train) Batch 114 Loss : 0.20785140991210938, 맞은 개수 : 118\nEpoch : 20, batch 115\n(Train) Batch 115 Loss : 0.42308154702186584, 맞은 개수 : 114\nEpoch : 20, batch 116\n(Train) Batch 116 Loss : 0.32591769099235535, 맞은 개수 : 112\nEpoch : 20, batch 117\n(Train) Batch 117 Loss : 0.12497497349977493, 맞은 개수 : 123\nEpoch : 20, batch 118\n(Train) Batch 118 Loss : 0.3210214078426361, 맞은 개수 : 115\nEpoch : 20, batch 119\n(Train) Batch 119 Loss : 0.3348625898361206, 맞은 개수 : 112\nEpoch : 20, batch 120\n(Train) Batch 120 Loss : 0.30197733640670776, 맞은 개수 : 119\nEpoch : 20, batch 121\n(Train) Batch 121 Loss : 0.1559520810842514, 맞은 개수 : 121\nEpoch : 20, batch 122\n(Train) Batch 122 Loss : 0.28587082028388977, 맞은 개수 : 118\nEpoch : 20, batch 123\n(Train) Batch 123 Loss : 0.21450453996658325, 맞은 개수 : 117\nEpoch : 20, batch 124\n(Train) Batch 124 Loss : 0.2564043402671814, 맞은 개수 : 114\nEpoch : 20, batch 125\n(Train) Batch 125 Loss : 0.3882397711277008, 맞은 개수 : 112\nEpoch : 20, batch 126\n(Train) Batch 126 Loss : 0.11582055687904358, 맞은 개수 : 124\nEpoch : 20, batch 127\n(Train) Batch 127 Loss : 0.28119948506355286, 맞은 개수 : 119\nEpoch : 20, batch 128\n(Train) Batch 128 Loss : 0.2802579998970032, 맞은 개수 : 116\nEpoch : 20, batch 129\n(Train) Batch 129 Loss : 0.29652485251426697, 맞은 개수 : 117\nEpoch : 20, batch 130\n(Train) Batch 130 Loss : 0.3815042972564697, 맞은 개수 : 111\nEpoch : 20, batch 131\n(Train) Batch 131 Loss : 0.258145809173584, 맞은 개수 : 118\nEpoch : 20, batch 132\n(Train) Batch 132 Loss : 0.3202819228172302, 맞은 개수 : 115\nEpoch : 20, batch 133\n(Train) Batch 133 Loss : 0.1996123045682907, 맞은 개수 : 120\nEpoch : 20, batch 134\n(Train) Batch 134 Loss : 0.3521859347820282, 맞은 개수 : 113\nEpoch : 20, batch 135\n(Train) Batch 135 Loss : 0.3827493488788605, 맞은 개수 : 117\nEpoch : 20, batch 136\n(Train) Batch 136 Loss : 0.11446300148963928, 맞은 개수 : 124\nEpoch : 20, batch 137\n(Train) Batch 137 Loss : 0.2844056189060211, 맞은 개수 : 116\nEpoch : 20, batch 138\n(Train) Batch 138 Loss : 0.25347593426704407, 맞은 개수 : 118\nEpoch : 20, batch 139\n(Train) Batch 139 Loss : 0.2362498939037323, 맞은 개수 : 122\nEpoch : 20, batch 140\n(Train) Batch 140 Loss : 0.25162726640701294, 맞은 개수 : 115\nEpoch : 20, batch 141\n(Train) Batch 141 Loss : 0.23443663120269775, 맞은 개수 : 117\nEpoch : 20, batch 142\n(Train) Batch 142 Loss : 0.33095377683639526, 맞은 개수 : 114\nEpoch : 20, batch 143\n(Train) Batch 143 Loss : 0.31394943594932556, 맞은 개수 : 114\nEpoch : 20, batch 144\n(Train) Batch 144 Loss : 0.2610386908054352, 맞은 개수 : 117\nEpoch : 20, batch 145\n(Train) Batch 145 Loss : 0.21787656843662262, 맞은 개수 : 119\nEpoch : 20, batch 146\n(Train) Batch 146 Loss : 0.3097161650657654, 맞은 개수 : 118\nEpoch : 20, batch 147\n(Train) Batch 147 Loss : 0.2543542981147766, 맞은 개수 : 114\nEpoch : 20, batch 148\n(Train) Batch 148 Loss : 0.2127782553434372, 맞은 개수 : 119\nEpoch : 20, batch 149\n(Train) Batch 149 Loss : 0.3041747212409973, 맞은 개수 : 116\nEpoch : 20, batch 150\n(Train) Batch 150 Loss : 0.21295222640037537, 맞은 개수 : 121\nEpoch : 20, batch 151\n(Train) Batch 151 Loss : 0.2825932502746582, 맞은 개수 : 116\nEpoch : 20, batch 152\n(Train) Batch 152 Loss : 0.23598287999629974, 맞은 개수 : 120\nEpoch : 20, batch 153\n(Train) Batch 153 Loss : 0.2770920991897583, 맞은 개수 : 116\nEpoch : 20, batch 154\n(Train) Batch 154 Loss : 0.309230774641037, 맞은 개수 : 115\nEpoch : 20, batch 155\n(Train) Batch 155 Loss : 0.25106823444366455, 맞은 개수 : 114\nEpoch : 20, batch 156\n(Train) Batch 156 Loss : 0.24421165883541107, 맞은 개수 : 117\nEpoch : 20, batch 157\n(Train) Batch 157 Loss : 0.1972779780626297, 맞은 개수 : 120\nEpoch : 20, batch 158\n(Train) Batch 158 Loss : 0.22234372794628143, 맞은 개수 : 116\nEpoch : 20, batch 159\n(Train) Batch 159 Loss : 0.2505495250225067, 맞은 개수 : 113\nEpoch : 20, batch 160\n(Train) Batch 160 Loss : 0.3505358099937439, 맞은 개수 : 119\nEpoch : 20, batch 161\n(Train) Batch 161 Loss : 0.3225927948951721, 맞은 개수 : 115\nEpoch : 20, batch 162\n(Train) Batch 162 Loss : 0.2735363245010376, 맞은 개수 : 115\nEpoch : 20, batch 163\n(Train) Batch 163 Loss : 0.2822161018848419, 맞은 개수 : 116\nEpoch : 20, batch 164\n(Train) Batch 164 Loss : 0.20918703079223633, 맞은 개수 : 119\nEpoch : 20, batch 165\n(Train) Batch 165 Loss : 0.288728803396225, 맞은 개수 : 117\nEpoch : 20, batch 166\n(Train) Batch 166 Loss : 0.3051918148994446, 맞은 개수 : 116\nEpoch : 20, batch 167\n(Train) Batch 167 Loss : 0.30318889021873474, 맞은 개수 : 117\nEpoch : 20, batch 168\n(Train) Batch 168 Loss : 0.15944994986057281, 맞은 개수 : 121\nEpoch : 20, batch 169\n(Train) Batch 169 Loss : 0.16049997508525848, 맞은 개수 : 121\nEpoch : 20, batch 170\n(Train) Batch 170 Loss : 0.3141513764858246, 맞은 개수 : 117\nEpoch : 20, batch 171\n(Train) Batch 171 Loss : 0.24313566088676453, 맞은 개수 : 120\nEpoch : 20, batch 172\n(Train) Batch 172 Loss : 0.31696000695228577, 맞은 개수 : 115\nEpoch : 20, batch 173\n(Train) Batch 173 Loss : 0.2547655999660492, 맞은 개수 : 116\nEpoch : 20, batch 174\n(Train) Batch 174 Loss : 0.23548460006713867, 맞은 개수 : 121\nEpoch : 20, batch 175\n(Train) Batch 175 Loss : 0.29750898480415344, 맞은 개수 : 117\nEpoch : 20, batch 176\n(Train) Batch 176 Loss : 0.21076028048992157, 맞은 개수 : 119\nEpoch : 20, batch 177\n(Train) Batch 177 Loss : 0.27345457673072815, 맞은 개수 : 119\nEpoch : 20, batch 178\n(Train) Batch 178 Loss : 0.3557647466659546, 맞은 개수 : 115\nEpoch : 20, batch 179\n(Train) Batch 179 Loss : 0.2972644865512848, 맞은 개수 : 114\nEpoch : 20, batch 180\n(Train) Batch 180 Loss : 0.34568047523498535, 맞은 개수 : 115\nEpoch : 20, batch 181\n(Train) Batch 181 Loss : 0.22367669641971588, 맞은 개수 : 117\nEpoch : 20, batch 182\n(Train) Batch 182 Loss : 0.32465416193008423, 맞은 개수 : 118\nEpoch : 20, batch 183\n(Train) Batch 183 Loss : 0.25390973687171936, 맞은 개수 : 117\nEpoch : 20, batch 184\n(Train) Batch 184 Loss : 0.24025437235832214, 맞은 개수 : 118\nEpoch : 20, batch 185\n(Train) Batch 185 Loss : 0.2606689929962158, 맞은 개수 : 118\nEpoch : 20, batch 186\n(Train) Batch 186 Loss : 0.26886871457099915, 맞은 개수 : 117\nEpoch : 20, batch 187\n(Train) Batch 187 Loss : 0.20140625536441803, 맞은 개수 : 121\nEpoch : 20, batch 188\n(Train) Batch 188 Loss : 0.3026365637779236, 맞은 개수 : 115\nEpoch : 20, batch 189\n(Train) Batch 189 Loss : 0.2778737545013428, 맞은 개수 : 114\nEpoch : 20, batch 190\n(Train) Batch 190 Loss : 0.39776501059532166, 맞은 개수 : 110\nEpoch : 20, batch 191\n(Train) Batch 191 Loss : 0.2622116208076477, 맞은 개수 : 116\nEpoch : 20, batch 192\n(Train) Batch 192 Loss : 0.23080430924892426, 맞은 개수 : 120\nEpoch : 20, batch 193\n(Train) Batch 193 Loss : 0.2668362557888031, 맞은 개수 : 118\nEpoch : 20, batch 194\n(Train) Batch 194 Loss : 0.2541317045688629, 맞은 개수 : 118\nEpoch : 20, batch 195\n(Train) Batch 195 Loss : 0.21095727384090424, 맞은 개수 : 118\nEpoch : 20, batch 196\n(Train) Batch 196 Loss : 0.4106690585613251, 맞은 개수 : 113\nEpoch : 20, batch 197\n(Train) Batch 197 Loss : 0.29264912009239197, 맞은 개수 : 114\nEpoch : 20, batch 198\n(Train) Batch 198 Loss : 0.21701262891292572, 맞은 개수 : 120\nEpoch : 20, batch 199\n(Train) Batch 199 Loss : 0.2668583393096924, 맞은 개수 : 117\nEpoch : 20, batch 200\n(Train) Batch 200 Loss : 0.2747865915298462, 맞은 개수 : 119\nEpoch : 20, batch 201\n(Train) Batch 201 Loss : 0.25343722105026245, 맞은 개수 : 115\nEpoch : 20, batch 202\n(Train) Batch 202 Loss : 0.322061151266098, 맞은 개수 : 117\nEpoch : 20, batch 203\n(Train) Batch 203 Loss : 0.33437687158584595, 맞은 개수 : 113\nEpoch : 20, batch 204\n(Train) Batch 204 Loss : 0.2927257716655731, 맞은 개수 : 118\nEpoch : 20, batch 205\n(Train) Batch 205 Loss : 0.2693057656288147, 맞은 개수 : 121\nEpoch : 20, batch 206\n(Train) Batch 206 Loss : 0.29117608070373535, 맞은 개수 : 116\nEpoch : 20, batch 207\n(Train) Batch 207 Loss : 0.3120361566543579, 맞은 개수 : 113\nEpoch : 20, batch 208\n(Train) Batch 208 Loss : 0.25150159001350403, 맞은 개수 : 118\nEpoch : 20, batch 209\n(Train) Batch 209 Loss : 0.4056219160556793, 맞은 개수 : 115\nEpoch : 20, batch 210\n(Train) Batch 210 Loss : 0.22803722321987152, 맞은 개수 : 118\nEpoch : 20, batch 211\n(Train) Batch 211 Loss : 0.2619525194168091, 맞은 개수 : 116\nEpoch : 20, batch 212\n(Train) Batch 212 Loss : 0.28401002287864685, 맞은 개수 : 118\nEpoch : 20, batch 213\n(Train) Batch 213 Loss : 0.27181610465049744, 맞은 개수 : 117\nEpoch : 20, batch 214\n(Train) Batch 214 Loss : 0.4142896234989166, 맞은 개수 : 117\nEpoch : 20, batch 215\n(Train) Batch 215 Loss : 0.2575433850288391, 맞은 개수 : 115\nEpoch : 20, batch 216\n(Train) Batch 216 Loss : 0.24845893681049347, 맞은 개수 : 118\nEpoch : 20, batch 217\n(Train) Batch 217 Loss : 0.13720600306987762, 맞은 개수 : 125\nEpoch : 20, batch 218\n(Train) Batch 218 Loss : 0.282654345035553, 맞은 개수 : 118\nEpoch : 20, batch 219\n(Train) Batch 219 Loss : 0.24806664884090424, 맞은 개수 : 121\nEpoch : 20, batch 220\n(Train) Batch 220 Loss : 0.3320674002170563, 맞은 개수 : 119\nEpoch : 20, batch 221\n(Train) Batch 221 Loss : 0.30775123834609985, 맞은 개수 : 120\nEpoch : 20, batch 222\n(Train) Batch 222 Loss : 0.2943756878376007, 맞은 개수 : 119\nEpoch : 20, batch 223\n(Train) Batch 223 Loss : 0.21478299796581268, 맞은 개수 : 118\nEpoch : 20, batch 224\n(Train) Batch 224 Loss : 0.26495397090911865, 맞은 개수 : 116\nEpoch : 20, batch 225\n(Train) Batch 225 Loss : 0.30197814106941223, 맞은 개수 : 117\nEpoch : 20, batch 226\n(Train) Batch 226 Loss : 0.25387364625930786, 맞은 개수 : 118\nEpoch : 20, batch 227\n(Train) Batch 227 Loss : 0.24646686017513275, 맞은 개수 : 112\nEpoch : 20, batch 228\n(Train) Batch 228 Loss : 0.31437239050865173, 맞은 개수 : 114\nEpoch : 20, batch 229\n(Train) Batch 229 Loss : 0.2867482900619507, 맞은 개수 : 117\nEpoch : 20, batch 230\n(Train) Batch 230 Loss : 0.31678059697151184, 맞은 개수 : 116\nEpoch : 20, batch 231\n(Train) Batch 231 Loss : 0.33435600996017456, 맞은 개수 : 117\nEpoch : 20, batch 232\n(Train) Batch 232 Loss : 0.2930055558681488, 맞은 개수 : 115\nEpoch : 20, batch 233\n(Train) Batch 233 Loss : 0.20334294438362122, 맞은 개수 : 123\nEpoch : 20, batch 234\n(Train) Batch 234 Loss : 0.24809525907039642, 맞은 개수 : 118\nEpoch : 20, batch 235\n(Train) Batch 235 Loss : 0.22641903162002563, 맞은 개수 : 117\nEpoch : 20, batch 236\n(Train) Batch 236 Loss : 0.27256956696510315, 맞은 개수 : 114\nEpoch : 20, batch 237\n(Train) Batch 237 Loss : 0.12600640952587128, 맞은 개수 : 121\nEpoch : 20, batch 238\n(Train) Batch 238 Loss : 0.17275139689445496, 맞은 개수 : 122\nEpoch : 20, batch 239\n(Train) Batch 239 Loss : 0.18543869256973267, 맞은 개수 : 122\nEpoch : 20, batch 240\n(Train) Batch 240 Loss : 0.16341538727283478, 맞은 개수 : 120\nEpoch : 20, batch 241\n(Train) Batch 241 Loss : 0.22145049273967743, 맞은 개수 : 119\nEpoch : 20, batch 242\n(Train) Batch 242 Loss : 0.17756760120391846, 맞은 개수 : 121\nEpoch : 20, batch 243\n(Train) Batch 243 Loss : 0.16868409514427185, 맞은 개수 : 122\nEpoch : 20, batch 244\n(Train) Batch 244 Loss : 0.24061799049377441, 맞은 개수 : 122\nEpoch : 20, batch 245\n(Train) Batch 245 Loss : 0.2260723114013672, 맞은 개수 : 121\nEpoch : 20, batch 246\n(Train) Batch 246 Loss : 0.0795229896903038, 맞은 개수 : 126\nEpoch : 20, batch 247\n(Train) Batch 247 Loss : 0.25533053278923035, 맞은 개수 : 116\nEpoch : 20, batch 248\n(Train) Batch 248 Loss : 0.2502119541168213, 맞은 개수 : 120\nEpoch : 20, batch 249\n(Train) Batch 249 Loss : 0.2590588331222534, 맞은 개수 : 118\nEpoch : 20, batch 250\n(Train) Batch 250 Loss : 0.15246228873729706, 맞은 개수 : 122\nEpoch : 20, batch 251\n(Train) Batch 251 Loss : 0.2856748402118683, 맞은 개수 : 115\nEpoch : 20, batch 252\n(Train) Batch 252 Loss : 0.12612253427505493, 맞은 개수 : 122\nEpoch : 20, batch 253\n(Train) Batch 253 Loss : 0.20875953137874603, 맞은 개수 : 118\nEpoch : 20, batch 254\n(Train) Batch 254 Loss : 0.29375046491622925, 맞은 개수 : 114\nEpoch : 20, batch 255\n(Train) Batch 255 Loss : 0.2985956370830536, 맞은 개수 : 113\nEpoch : 20, batch 256\n(Train) Batch 256 Loss : 0.19391220808029175, 맞은 개수 : 117\nEpoch : 20, batch 257\n(Train) Batch 257 Loss : 0.3028453290462494, 맞은 개수 : 117\nEpoch : 20, batch 258\n(Train) Batch 258 Loss : 0.2035820037126541, 맞은 개수 : 118\nEpoch : 20, batch 259\n(Train) Batch 259 Loss : 0.28328487277030945, 맞은 개수 : 116\nEpoch : 20, batch 260\n(Train) Batch 260 Loss : 0.33176612854003906, 맞은 개수 : 115\nEpoch : 20, batch 261\n(Train) Batch 261 Loss : 0.3749285340309143, 맞은 개수 : 113\nEpoch : 20, batch 262\n(Train) Batch 262 Loss : 0.2644774317741394, 맞은 개수 : 117\nEpoch : 20, batch 263\n(Train) Batch 263 Loss : 0.2063591480255127, 맞은 개수 : 121\nEpoch : 20, batch 264\n(Train) Batch 264 Loss : 0.24153871834278107, 맞은 개수 : 118\nEpoch : 20, batch 265\n(Train) Batch 265 Loss : 0.20715056359767914, 맞은 개수 : 117\nEpoch : 20, batch 266\n(Train) Batch 266 Loss : 0.295879065990448, 맞은 개수 : 119\nEpoch : 20, batch 267\n(Train) Batch 267 Loss : 0.25119730830192566, 맞은 개수 : 116\nEpoch : 20, batch 268\n(Train) Batch 268 Loss : 0.25465187430381775, 맞은 개수 : 116\nEpoch : 20, batch 269\n(Train) Batch 269 Loss : 0.27336007356643677, 맞은 개수 : 115\nEpoch : 20, batch 270\n(Train) Batch 270 Loss : 0.15716464817523956, 맞은 개수 : 123\nEpoch : 20, batch 271\n(Train) Batch 271 Loss : 0.1809128075838089, 맞은 개수 : 122\nEpoch : 20, batch 272\n(Train) Batch 272 Loss : 0.3179563283920288, 맞은 개수 : 111\nEpoch : 20, batch 273\n(Train) Batch 273 Loss : 0.24479572474956512, 맞은 개수 : 118\nEpoch : 20, batch 274\n(Train) Batch 274 Loss : 0.2712675929069519, 맞은 개수 : 119\nEpoch : 20, batch 275\n(Train) Batch 275 Loss : 0.28690406680107117, 맞은 개수 : 113\nEpoch : 20, batch 276\n(Train) Batch 276 Loss : 0.15619005262851715, 맞은 개수 : 120\nEpoch : 20, batch 277\n(Train) Batch 277 Loss : 0.2710684537887573, 맞은 개수 : 117\nEpoch : 20, batch 278\n(Train) Batch 278 Loss : 0.2449507713317871, 맞은 개수 : 120\nEpoch : 20, batch 279\n(Train) Batch 279 Loss : 0.23663237690925598, 맞은 개수 : 119\nEpoch : 20, batch 280\n(Train) Batch 280 Loss : 0.4434148073196411, 맞은 개수 : 112\nEpoch : 20, batch 281\n(Train) Batch 281 Loss : 0.26431360840797424, 맞은 개수 : 115\nEpoch : 20, batch 282\n(Train) Batch 282 Loss : 0.36038896441459656, 맞은 개수 : 117\nEpoch : 20, batch 283\n(Train) Batch 283 Loss : 0.19950462877750397, 맞은 개수 : 123\nEpoch : 20, batch 284\n(Train) Batch 284 Loss : 0.3393649160861969, 맞은 개수 : 117\nEpoch : 20, batch 285\n(Train) Batch 285 Loss : 0.2168005108833313, 맞은 개수 : 120\nEpoch : 20, batch 286\n(Train) Batch 286 Loss : 0.21411439776420593, 맞은 개수 : 119\nEpoch : 20, batch 287\n(Train) Batch 287 Loss : 0.26971712708473206, 맞은 개수 : 116\nEpoch : 20, batch 288\n(Train) Batch 288 Loss : 0.3118121325969696, 맞은 개수 : 118\nEpoch : 20, batch 289\n(Train) Batch 289 Loss : 0.21968738734722137, 맞은 개수 : 116\nEpoch : 20, batch 290\n(Train) Batch 290 Loss : 0.22413145005702972, 맞은 개수 : 119\nEpoch : 20, batch 291\n(Train) Batch 291 Loss : 0.46829041838645935, 맞은 개수 : 109\nEpoch : 20, batch 292\n(Train) Batch 292 Loss : 0.2531501352787018, 맞은 개수 : 117\nEpoch : 20, batch 293\n(Train) Batch 293 Loss : 0.20740953087806702, 맞은 개수 : 119\nEpoch : 20, batch 294\n(Train) Batch 294 Loss : 0.26035264134407043, 맞은 개수 : 118\nEpoch : 20, batch 295\n(Train) Batch 295 Loss : 0.34664276242256165, 맞은 개수 : 117\nEpoch : 20, batch 296\n(Train) Batch 296 Loss : 0.32615962624549866, 맞은 개수 : 115\nEpoch : 20, batch 297\n(Train) Batch 297 Loss : 0.25684690475463867, 맞은 개수 : 119\nEpoch : 20, batch 298\n(Train) Batch 298 Loss : 0.28741806745529175, 맞은 개수 : 115\nEpoch : 20, batch 299\n(Train) Batch 299 Loss : 0.1970495879650116, 맞은 개수 : 117\nEpoch : 20, batch 300\n(Train) Batch 300 Loss : 0.2670029401779175, 맞은 개수 : 116\nEpoch : 20, batch 301\n(Train) Batch 301 Loss : 0.3191279172897339, 맞은 개수 : 118\nEpoch : 20, batch 302\n(Train) Batch 302 Loss : 0.1717294603586197, 맞은 개수 : 123\nEpoch : 20, batch 303\n(Train) Batch 303 Loss : 0.40295732021331787, 맞은 개수 : 111\nEpoch : 20, batch 304\n(Train) Batch 304 Loss : 0.12072654068470001, 맞은 개수 : 121\nEpoch : 20, batch 305\n(Train) Batch 305 Loss : 0.4269619286060333, 맞은 개수 : 115\nEpoch : 20, batch 306\n(Train) Batch 306 Loss : 0.22074545919895172, 맞은 개수 : 120\nEpoch : 20, batch 307\n(Train) Batch 307 Loss : 0.282377153635025, 맞은 개수 : 117\nEpoch : 20, batch 308\n(Train) Batch 308 Loss : 0.2673538327217102, 맞은 개수 : 116\nEpoch : 20, batch 309\n(Train) Batch 309 Loss : 0.20995600521564484, 맞은 개수 : 118\nEpoch : 20, batch 310\n(Train) Batch 310 Loss : 0.27576005458831787, 맞은 개수 : 118\nEpoch : 20, batch 311\n(Train) Batch 311 Loss : 0.1652628779411316, 맞은 개수 : 121\nEpoch : 20, batch 312\n(Train) Batch 312 Loss : 0.20025964081287384, 맞은 개수 : 118\nEpoch : 20, batch 313\n(Train) Batch 313 Loss : 0.30675557255744934, 맞은 개수 : 116\nEpoch : 20, batch 314\n(Train) Batch 314 Loss : 0.3284280002117157, 맞은 개수 : 115\nEpoch : 20, batch 315\n(Train) Batch 315 Loss : 0.29643121361732483, 맞은 개수 : 114\nEpoch : 20, batch 316\n(Train) Batch 316 Loss : 0.261525958776474, 맞은 개수 : 115\nEpoch : 20, batch 317\n(Train) Batch 317 Loss : 0.39063194394111633, 맞은 개수 : 113\nEpoch : 20, batch 318\n(Train) Batch 318 Loss : 0.2871460020542145, 맞은 개수 : 119\nEpoch : 20, batch 319\n(Train) Batch 319 Loss : 0.3628897964954376, 맞은 개수 : 114\nEpoch : 20, batch 320\n(Train) Batch 320 Loss : 0.24198105931282043, 맞은 개수 : 117\nEpoch : 20, batch 321\n(Train) Batch 321 Loss : 0.15656371414661407, 맞은 개수 : 121\nEpoch : 20, batch 322\n(Train) Batch 322 Loss : 0.20938950777053833, 맞은 개수 : 121\nEpoch : 20, batch 323\n(Train) Batch 323 Loss : 0.23437222838401794, 맞은 개수 : 119\nEpoch : 20, batch 324\n(Train) Batch 324 Loss : 0.26220107078552246, 맞은 개수 : 122\nEpoch : 20, batch 325\n(Train) Batch 325 Loss : 0.25990626215934753, 맞은 개수 : 117\nEpoch : 20, batch 326\n(Train) Batch 326 Loss : 0.34757745265960693, 맞은 개수 : 115\nEpoch : 20, batch 327\n(Train) Batch 327 Loss : 0.24400901794433594, 맞은 개수 : 123\nEpoch : 20, batch 328\n(Train) Batch 328 Loss : 0.2884891629219055, 맞은 개수 : 117\nEpoch : 20, batch 329\n(Train) Batch 329 Loss : 0.2755938470363617, 맞은 개수 : 121\nEpoch : 20, batch 330\n(Train) Batch 330 Loss : 0.43580162525177, 맞은 개수 : 114\nEpoch : 20, batch 331\n(Train) Batch 331 Loss : 0.23815101385116577, 맞은 개수 : 119\nEpoch : 20, batch 332\n(Train) Batch 332 Loss : 0.1959628313779831, 맞은 개수 : 120\nEpoch : 20, batch 333\n(Train) Batch 333 Loss : 0.3684014678001404, 맞은 개수 : 119\nEpoch : 20, batch 334\n(Train) Batch 334 Loss : 0.1644764095544815, 맞은 개수 : 120\nEpoch : 20, batch 335\n(Train) Batch 335 Loss : 0.3648829758167267, 맞은 개수 : 114\nEpoch : 20, batch 336\n(Train) Batch 336 Loss : 0.2317790538072586, 맞은 개수 : 118\nEpoch : 20, batch 337\n(Train) Batch 337 Loss : 0.1120917871594429, 맞은 개수 : 125\nEpoch : 20, batch 338\n(Train) Batch 338 Loss : 0.237875297665596, 맞은 개수 : 118\nEpoch : 20, batch 339\n(Train) Batch 339 Loss : 0.2516566216945648, 맞은 개수 : 119\nEpoch : 20, batch 340\n(Train) Batch 340 Loss : 0.18752186000347137, 맞은 개수 : 121\nEpoch : 20, batch 341\n(Train) Batch 341 Loss : 0.17603175342082977, 맞은 개수 : 121\nEpoch : 20, batch 342\n(Train) Batch 342 Loss : 0.16405798494815826, 맞은 개수 : 122\nEpoch : 20, batch 343\n(Train) Batch 343 Loss : 0.21234560012817383, 맞은 개수 : 121\nEpoch : 20, batch 344\n(Train) Batch 344 Loss : 0.1643146574497223, 맞은 개수 : 122\nEpoch : 20, batch 345\n(Train) Batch 345 Loss : 0.18297290802001953, 맞은 개수 : 120\nEpoch : 20, batch 346\n(Train) Batch 346 Loss : 0.15998490154743195, 맞은 개수 : 120\nEpoch : 20, batch 347\n(Train) Batch 347 Loss : 0.26365432143211365, 맞은 개수 : 115\nEpoch : 20, batch 348\n(Train) Batch 348 Loss : 0.3087884187698364, 맞은 개수 : 117\nEpoch : 20, batch 349\n(Train) Batch 349 Loss : 0.2985198199748993, 맞은 개수 : 119\nEpoch : 20, batch 350\n(Train) Batch 350 Loss : 0.27225950360298157, 맞은 개수 : 111\nEpoch : 20, batch 351\n(Train) Batch 351 Loss : 0.3203123211860657, 맞은 개수 : 116\nEpoch : 20, batch 352\n(Train) Batch 352 Loss : 0.2930699586868286, 맞은 개수 : 116\nEpoch : 20, batch 353\n(Train) Batch 353 Loss : 0.31614699959754944, 맞은 개수 : 114\nEpoch : 20, batch 354\n(Train) Batch 354 Loss : 0.31917911767959595, 맞은 개수 : 112\nEpoch : 20, batch 355\n(Train) Batch 355 Loss : 0.25820687413215637, 맞은 개수 : 117\nEpoch : 20, batch 356\n(Train) Batch 356 Loss : 0.3928406536579132, 맞은 개수 : 114\nEpoch : 20, batch 357\n(Train) Batch 357 Loss : 0.13138191401958466, 맞은 개수 : 121\nEpoch : 20, batch 358\n(Train) Batch 358 Loss : 0.3433028757572174, 맞은 개수 : 118\nEpoch : 20, batch 359\n(Train) Batch 359 Loss : 0.20806042850017548, 맞은 개수 : 119\nEpoch : 20, batch 360\n(Train) Batch 360 Loss : 0.38025930523872375, 맞은 개수 : 116\nEpoch : 20, batch 361\n(Train) Batch 361 Loss : 0.18089546263217926, 맞은 개수 : 119\nEpoch : 20, batch 362\n(Train) Batch 362 Loss : 0.33295920491218567, 맞은 개수 : 117\nEpoch : 20, batch 363\n(Train) Batch 363 Loss : 0.15377387404441833, 맞은 개수 : 121\nEpoch : 20, batch 364\n(Train) Batch 364 Loss : 0.30565667152404785, 맞은 개수 : 113\nEpoch : 20, batch 365\n(Train) Batch 365 Loss : 0.19315078854560852, 맞은 개수 : 122\nEpoch : 20, batch 366\n(Train) Batch 366 Loss : 0.18314872682094574, 맞은 개수 : 121\nEpoch : 20, batch 367\n(Train) Batch 367 Loss : 0.16021941602230072, 맞은 개수 : 120\nEpoch : 20, batch 368\n(Train) Batch 368 Loss : 0.24293987452983856, 맞은 개수 : 117\nEpoch : 20, batch 369\n(Train) Batch 369 Loss : 0.24902792274951935, 맞은 개수 : 117\nEpoch : 20, batch 370\n(Train) Batch 370 Loss : 0.3071722984313965, 맞은 개수 : 116\nEpoch : 20, batch 371\n(Train) Batch 371 Loss : 0.3842933177947998, 맞은 개수 : 114\nEpoch : 20, batch 372\n(Train) Batch 372 Loss : 0.23620127141475677, 맞은 개수 : 122\nEpoch : 20, batch 373\n(Train) Batch 373 Loss : 0.4025023579597473, 맞은 개수 : 115\nEpoch : 20, batch 374\n(Train) Batch 374 Loss : 0.23547221720218658, 맞은 개수 : 119\nEpoch : 20, batch 375\n(Train) Batch 375 Loss : 0.2729019522666931, 맞은 개수 : 116\nEpoch : 20, batch 376\n(Train) Batch 376 Loss : 0.16446764767169952, 맞은 개수 : 122\nEpoch : 20, batch 377\n(Train) Batch 377 Loss : 0.2186405211687088, 맞은 개수 : 120\nEpoch : 20, batch 378\n(Train) Batch 378 Loss : 0.18524670600891113, 맞은 개수 : 119\nEpoch : 20, batch 379\n(Train) Batch 379 Loss : 0.2420465052127838, 맞은 개수 : 116\nEpoch : 20, batch 380\n(Train) Batch 380 Loss : 0.43396157026290894, 맞은 개수 : 112\nEpoch : 20, batch 381\n(Train) Batch 381 Loss : 0.2675454914569855, 맞은 개수 : 119\nEpoch : 20, batch 382\n(Train) Batch 382 Loss : 0.3574799597263336, 맞은 개수 : 110\nEpoch : 20, batch 383\n(Train) Batch 383 Loss : 0.3210970163345337, 맞은 개수 : 118\nEpoch : 20, batch 384\n(Train) Batch 384 Loss : 0.2764357328414917, 맞은 개수 : 117\nEpoch : 20, batch 385\n(Train) Batch 385 Loss : 0.24832263588905334, 맞은 개수 : 116\nEpoch : 20, batch 386\n(Train) Batch 386 Loss : 0.2601865231990814, 맞은 개수 : 117\nEpoch : 20, batch 387\n(Train) Batch 387 Loss : 0.27691617608070374, 맞은 개수 : 116\nEpoch : 20, batch 388\n(Train) Batch 388 Loss : 0.16764403879642487, 맞은 개수 : 121\nEpoch : 20, batch 389\n(Train) Batch 389 Loss : 0.23588573932647705, 맞은 개수 : 116\nEpoch : 20, batch 390\n(Train) Batch 390 Loss : 0.20512980222702026, 맞은 개수 : 120\nEpoch : 20, batch 391\n(Train) Batch 391 Loss : 0.27148354053497314, 맞은 개수 : 120\nEpoch : 20, batch 392\n(Train) Batch 392 Loss : 0.2591463625431061, 맞은 개수 : 119\nEpoch : 20, batch 393\n(Train) Batch 393 Loss : 0.15261252224445343, 맞은 개수 : 123\nEpoch : 20, batch 394\n(Train) Batch 394 Loss : 0.213934063911438, 맞은 개수 : 120\nEpoch : 20, batch 395\n(Train) Batch 395 Loss : 0.273758202791214, 맞은 개수 : 119\nEpoch : 20, batch 396\n(Train) Batch 396 Loss : 0.3012600243091583, 맞은 개수 : 113\nEpoch : 20, batch 397\n(Train) Batch 397 Loss : 0.25998738408088684, 맞은 개수 : 116\nEpoch : 20, batch 398\n(Train) Batch 398 Loss : 0.29895302653312683, 맞은 개수 : 114\nEpoch : 20, batch 399\n(Train) Batch 399 Loss : 0.36132633686065674, 맞은 개수 : 117\nEpoch : 20, batch 400\n(Train) Batch 400 Loss : 0.3122332692146301, 맞은 개수 : 116\nEpoch : 20, batch 401\n(Train) Batch 401 Loss : 0.23745398223400116, 맞은 개수 : 118\nEpoch : 20, batch 402\n(Train) Batch 402 Loss : 0.37298402190208435, 맞은 개수 : 111\nEpoch : 20, batch 403\n(Train) Batch 403 Loss : 0.20808401703834534, 맞은 개수 : 118\nEpoch : 20, batch 404\n(Train) Batch 404 Loss : 0.22791902720928192, 맞은 개수 : 117\nEpoch : 20, batch 405\n(Train) Batch 405 Loss : 0.1876213252544403, 맞은 개수 : 122\nEpoch : 20, batch 406\n(Train) Batch 406 Loss : 0.29516279697418213, 맞은 개수 : 116\nEpoch : 20, batch 407\n(Train) Batch 407 Loss : 0.15889686346054077, 맞은 개수 : 122\nEpoch : 20, batch 408\n(Train) Batch 408 Loss : 0.2274506539106369, 맞은 개수 : 118\nEpoch : 20, batch 409\n(Train) Batch 409 Loss : 0.26760613918304443, 맞은 개수 : 118\nEpoch : 20, batch 410\n(Train) Batch 410 Loss : 0.33415862917900085, 맞은 개수 : 116\nEpoch : 20, batch 411\n(Train) Batch 411 Loss : 0.17749638855457306, 맞은 개수 : 121\nEpoch : 20, batch 412\n(Train) Batch 412 Loss : 0.2675263285636902, 맞은 개수 : 117\nEpoch : 20, batch 413\n(Train) Batch 413 Loss : 0.38131052255630493, 맞은 개수 : 115\nEpoch : 20, batch 414\n(Train) Batch 414 Loss : 0.2473846673965454, 맞은 개수 : 118\nEpoch : 20, batch 415\n(Train) Batch 415 Loss : 0.26286089420318604, 맞은 개수 : 115\nEpoch : 20, batch 416\n(Train) Batch 416 Loss : 0.2500670850276947, 맞은 개수 : 119\nEpoch : 20, batch 417\n(Train) Batch 417 Loss : 0.33283790946006775, 맞은 개수 : 115\nEpoch : 20, batch 418\n(Train) Batch 418 Loss : 0.320524662733078, 맞은 개수 : 114\nEpoch : 20, batch 419\n(Train) Batch 419 Loss : 0.11083553731441498, 맞은 개수 : 125\nEpoch : 20, batch 420\n(Train) Batch 420 Loss : 0.33491796255111694, 맞은 개수 : 115\nEpoch : 20, batch 421\n(Train) Batch 421 Loss : 0.30381423234939575, 맞은 개수 : 120\nEpoch : 20, batch 422\n(Train) Batch 422 Loss : 0.36001116037368774, 맞은 개수 : 115\nEpoch : 20, batch 423\n(Train) Batch 423 Loss : 0.3554782271385193, 맞은 개수 : 117\nEpoch : 20, batch 424\n(Train) Batch 424 Loss : 0.3602423071861267, 맞은 개수 : 113\nEpoch : 20, batch 425\n(Train) Batch 425 Loss : 0.2567913234233856, 맞은 개수 : 118\nEpoch : 20, batch 426\n(Train) Batch 426 Loss : 0.22850506007671356, 맞은 개수 : 121\nEpoch : 20, batch 427\n(Train) Batch 427 Loss : 0.2532426714897156, 맞은 개수 : 119\nEpoch : 20, batch 428\n(Train) Batch 428 Loss : 0.3804675042629242, 맞은 개수 : 113\nEpoch : 20, batch 429\n(Train) Batch 429 Loss : 0.21286241710186005, 맞은 개수 : 117\nEpoch : 20, batch 430\n(Train) Batch 430 Loss : 0.2628327012062073, 맞은 개수 : 117\nEpoch : 20, batch 431\n(Train) Batch 431 Loss : 0.15609203279018402, 맞은 개수 : 120\nEpoch : 20, batch 432\n(Train) Batch 432 Loss : 0.29862913489341736, 맞은 개수 : 118\nEpoch : 20, batch 433\n(Train) Batch 433 Loss : 0.3620309829711914, 맞은 개수 : 116\nEpoch : 20, batch 434\n(Train) Batch 434 Loss : 0.21313606202602386, 맞은 개수 : 119\nEpoch : 20, batch 435\n(Train) Batch 435 Loss : 0.3045324385166168, 맞은 개수 : 118\nEpoch : 20, batch 436\n(Train) Batch 436 Loss : 0.49771660566329956, 맞은 개수 : 114\nEpoch : 20, batch 437\n(Train) Batch 437 Loss : 0.38082724809646606, 맞은 개수 : 118\nEpoch : 20, batch 438\n(Train) Batch 438 Loss : 0.47019162774086, 맞은 개수 : 113\nEpoch : 20, batch 439\n(Train) Batch 439 Loss : 0.23467810451984406, 맞은 개수 : 118\nEpoch : 20, batch 440\n(Train) Batch 440 Loss : 0.1690477430820465, 맞은 개수 : 120\nEpoch : 20, batch 441\n(Train) Batch 441 Loss : 0.39659732580184937, 맞은 개수 : 116\nEpoch : 20, batch 442\n(Train) Batch 442 Loss : 0.20685778558254242, 맞은 개수 : 122\nEpoch : 20, batch 443\n(Train) Batch 443 Loss : 0.1460840404033661, 맞은 개수 : 123\nEpoch : 20, batch 444\n(Train) Batch 444 Loss : 0.2666245996952057, 맞은 개수 : 119\nEpoch : 20, batch 445\n(Train) Batch 445 Loss : 0.16977208852767944, 맞은 개수 : 119\nEpoch : 20, batch 446\n(Train) Batch 446 Loss : 0.21640042960643768, 맞은 개수 : 118\nEpoch : 20, batch 447\n(Train) Batch 447 Loss : 0.20682962238788605, 맞은 개수 : 121\nEpoch : 20, batch 448\n(Train) Batch 448 Loss : 0.33400899171829224, 맞은 개수 : 117\nEpoch : 20, batch 449\n(Train) Batch 449 Loss : 0.23588775098323822, 맞은 개수 : 118\nEpoch : 20, batch 450\n(Train) Batch 450 Loss : 0.2666996717453003, 맞은 개수 : 119\nEpoch : 20, batch 451\n(Train) Batch 451 Loss : 0.3424239456653595, 맞은 개수 : 110\nEpoch : 20, batch 452\n(Train) Batch 452 Loss : 0.18644681572914124, 맞은 개수 : 118\nEpoch : 20, batch 453\n(Train) Batch 453 Loss : 0.15800124406814575, 맞은 개수 : 123\nEpoch : 20, batch 454\n(Train) Batch 454 Loss : 0.26339834928512573, 맞은 개수 : 118\nEpoch : 20, batch 455\n(Train) Batch 455 Loss : 0.2966718077659607, 맞은 개수 : 116\nEpoch : 20, batch 456\n(Train) Batch 456 Loss : 0.2622376084327698, 맞은 개수 : 116\nEpoch : 20, batch 457\n(Train) Batch 457 Loss : 0.3468998968601227, 맞은 개수 : 112\nEpoch : 20, batch 458\n(Train) Batch 458 Loss : 0.2279222160577774, 맞은 개수 : 117\nEpoch : 20, batch 459\n(Train) Batch 459 Loss : 0.1776067316532135, 맞은 개수 : 119\nEpoch : 20, batch 460\n(Train) Batch 460 Loss : 0.2770518660545349, 맞은 개수 : 117\nEpoch : 20, batch 461\n(Train) Batch 461 Loss : 0.36077821254730225, 맞은 개수 : 115\nEpoch : 20, batch 462\n(Train) Batch 462 Loss : 0.22404475510120392, 맞은 개수 : 115\nEpoch : 20, batch 463\n(Train) Batch 463 Loss : 0.33331048488616943, 맞은 개수 : 118\nEpoch : 20, batch 464\n(Train) Batch 464 Loss : 0.269869327545166, 맞은 개수 : 118\nEpoch : 20, batch 465\n(Train) Batch 465 Loss : 0.1581142395734787, 맞은 개수 : 122\nEpoch : 20, batch 466\n(Train) Batch 466 Loss : 0.21525926887989044, 맞은 개수 : 120\nEpoch : 20, batch 467\n(Train) Batch 467 Loss : 0.2248038798570633, 맞은 개수 : 117\nEpoch : 20, batch 468\n(Train) Batch 468 Loss : 0.29362690448760986, 맞은 개수 : 118\nEpoch : 20, batch 469\n(Train) Batch 469 Loss : 0.31026753783226013, 맞은 개수 : 115\nEpoch : 20, batch 470\n(Train) Batch 470 Loss : 0.26038655638694763, 맞은 개수 : 113\nEpoch : 20, batch 471\n(Train) Batch 471 Loss : 0.3465665280818939, 맞은 개수 : 114\nEpoch : 20, batch 472\n(Train) Batch 472 Loss : 0.39628171920776367, 맞은 개수 : 115\nEpoch : 20, batch 473\n(Train) Batch 473 Loss : 0.16763348877429962, 맞은 개수 : 118\nEpoch : 20, batch 474\n(Train) Batch 474 Loss : 0.240403413772583, 맞은 개수 : 117\nEpoch : 20, batch 475\n(Train) Batch 475 Loss : 0.3679138422012329, 맞은 개수 : 117\nEpoch : 20, batch 476\n(Train) Batch 476 Loss : 0.24930673837661743, 맞은 개수 : 119\nEpoch : 20, batch 477\n(Train) Batch 477 Loss : 0.21799041330814362, 맞은 개수 : 119\nEpoch : 20, batch 478\n(Train) Batch 478 Loss : 0.2994573414325714, 맞은 개수 : 114\nEpoch : 20, batch 479\n(Train) Batch 479 Loss : 0.19708047807216644, 맞은 개수 : 123\nEpoch : 20, batch 480\n(Train) Batch 480 Loss : 0.2133132666349411, 맞은 개수 : 115\nEpoch : 20, batch 481\n(Train) Batch 481 Loss : 0.16021451354026794, 맞은 개수 : 121\nEpoch : 20, batch 482\n(Train) Batch 482 Loss : 0.39502137899398804, 맞은 개수 : 114\nEpoch : 20, batch 483\n(Train) Batch 483 Loss : 0.39159777760505676, 맞은 개수 : 109\nEpoch : 20, batch 484\n(Train) Batch 484 Loss : 0.38149645924568176, 맞은 개수 : 115\nEpoch : 20, batch 485\n(Train) Batch 485 Loss : 0.4606209397315979, 맞은 개수 : 117\nEpoch : 20, batch 486\n(Train) Batch 486 Loss : 0.3256581723690033, 맞은 개수 : 114\nEpoch : 20, batch 487\n(Train) Batch 487 Loss : 0.40235286951065063, 맞은 개수 : 114\nEpoch : 20, batch 488\n(Train) Batch 488 Loss : 0.26503026485443115, 맞은 개수 : 117\nEpoch : 20, batch 489\n(Train) Batch 489 Loss : 0.3099111318588257, 맞은 개수 : 115\nEpoch : 20, batch 490\n(Train) Batch 490 Loss : 0.20610466599464417, 맞은 개수 : 115\nEpoch : 20, batch 491\n(Train) Batch 491 Loss : 0.24298153817653656, 맞은 개수 : 118\nEpoch : 20, batch 492\n(Train) Batch 492 Loss : 0.2659400403499603, 맞은 개수 : 115\nEpoch : 20, batch 493\n(Train) Batch 493 Loss : 0.3934728503227234, 맞은 개수 : 116\nEpoch : 20, batch 494\n(Train) Batch 494 Loss : 0.3090822994709015, 맞은 개수 : 114\nEpoch : 20, batch 495\n(Train) Batch 495 Loss : 0.2602565586566925, 맞은 개수 : 115\nEpoch : 20, batch 496\n(Train) Batch 496 Loss : 0.1981891542673111, 맞은 개수 : 121\nEpoch : 20, batch 497\n(Train) Batch 497 Loss : 0.2194937765598297, 맞은 개수 : 119\nEpoch : 20, batch 498\n(Train) Batch 498 Loss : 0.3040597140789032, 맞은 개수 : 115\nEpoch : 20, batch 499\n(Train) Batch 499 Loss : 0.31690022349357605, 맞은 개수 : 116\nEpoch : 20, batch 500\n(Train) Batch 500 Loss : 0.3460529148578644, 맞은 개수 : 116\nEpoch : 20, batch 501\n(Train) Batch 501 Loss : 0.24286875128746033, 맞은 개수 : 120\nEpoch : 20, batch 502\n(Train) Batch 502 Loss : 0.23580366373062134, 맞은 개수 : 116\nEpoch : 20, batch 503\n(Train) Batch 503 Loss : 0.28047025203704834, 맞은 개수 : 116\nEpoch : 20, batch 504\n(Train) Batch 504 Loss : 0.41760507225990295, 맞은 개수 : 114\nEpoch : 20, batch 505\n(Train) Batch 505 Loss : 0.25104284286499023, 맞은 개수 : 115\nEpoch : 20, batch 506\n(Train) Batch 506 Loss : 0.2728324234485626, 맞은 개수 : 112\nEpoch : 20, batch 507\n(Train) Batch 507 Loss : 0.307400107383728, 맞은 개수 : 115\nEpoch : 20, batch 508\n(Train) Batch 508 Loss : 0.2708263397216797, 맞은 개수 : 119\nEpoch : 20, batch 509\n(Train) Batch 509 Loss : 0.16129356622695923, 맞은 개수 : 122\nEpoch : 20, batch 510\n(Train) Batch 510 Loss : 0.2789919078350067, 맞은 개수 : 118\nEpoch : 20, batch 511\n(Train) Batch 511 Loss : 0.2472287267446518, 맞은 개수 : 119\nEpoch : 20, batch 512\n(Train) Batch 512 Loss : 0.27172547578811646, 맞은 개수 : 116\nEpoch : 20, batch 513\n(Train) Batch 513 Loss : 0.2928471863269806, 맞은 개수 : 117\nEpoch : 20, batch 514\n(Train) Batch 514 Loss : 0.18656495213508606, 맞은 개수 : 119\nEpoch : 20, batch 515\n(Train) Batch 515 Loss : 0.24396643042564392, 맞은 개수 : 114\nEpoch : 20, batch 516\n(Train) Batch 516 Loss : 0.2683612108230591, 맞은 개수 : 113\nEpoch : 20, batch 517\n(Train) Batch 517 Loss : 0.31672215461730957, 맞은 개수 : 113\nEpoch : 20, batch 518\n(Train) Batch 518 Loss : 0.3993299901485443, 맞은 개수 : 111\nEpoch : 20, batch 519\n(Train) Batch 519 Loss : 0.29133912920951843, 맞은 개수 : 113\nEpoch : 20, batch 520\n(Train) Batch 520 Loss : 0.42608368396759033, 맞은 개수 : 112\nEpoch : 20, batch 521\n(Train) Batch 521 Loss : 0.2988942265510559, 맞은 개수 : 121\nEpoch : 20, batch 522\n(Train) Batch 522 Loss : 0.30746638774871826, 맞은 개수 : 114\nEpoch : 20, batch 523\n(Train) Batch 523 Loss : 0.2512250542640686, 맞은 개수 : 119\nEpoch : 20, batch 524\n(Train) Batch 524 Loss : 0.2914895713329315, 맞은 개수 : 114\nEpoch : 20, batch 525\n(Train) Batch 525 Loss : 0.3312053382396698, 맞은 개수 : 113\nEpoch : 20, batch 526\n(Train) Batch 526 Loss : 0.3470962643623352, 맞은 개수 : 114\nEpoch : 20, batch 527\n(Train) Batch 527 Loss : 0.21261462569236755, 맞은 개수 : 119\nEpoch : 20, batch 528\n(Train) Batch 528 Loss : 0.27967533469200134, 맞은 개수 : 116\nEpoch : 20, batch 529\n(Train) Batch 529 Loss : 0.2116588056087494, 맞은 개수 : 119\nEpoch : 20, batch 530\n(Train) Batch 530 Loss : 0.3245532512664795, 맞은 개수 : 115\nEpoch : 20, batch 531\n(Train) Batch 531 Loss : 0.38823193311691284, 맞은 개수 : 111\nEpoch : 20, batch 532\n(Train) Batch 532 Loss : 0.2953299582004547, 맞은 개수 : 116\nEpoch : 20, batch 533\n(Train) Batch 533 Loss : 0.24407117068767548, 맞은 개수 : 120\nEpoch : 20, batch 534\n(Train) Batch 534 Loss : 0.27351289987564087, 맞은 개수 : 116\nEpoch : 20, batch 535\n(Train) Batch 535 Loss : 0.39261752367019653, 맞은 개수 : 109\nEpoch : 20, batch 536\n(Train) Batch 536 Loss : 0.26802653074264526, 맞은 개수 : 115\nEpoch : 20, batch 537\n(Train) Batch 537 Loss : 0.39564186334609985, 맞은 개수 : 115\nEpoch : 20, batch 538\n(Train) Batch 538 Loss : 0.21984802186489105, 맞은 개수 : 116\nEpoch : 20, batch 539\n(Train) Batch 539 Loss : 0.28123709559440613, 맞은 개수 : 119\nEpoch : 20, batch 540\n(Train) Batch 540 Loss : 0.34063661098480225, 맞은 개수 : 116\nEpoch : 20, batch 541\n(Train) Batch 541 Loss : 0.3150038421154022, 맞은 개수 : 115\nEpoch : 20, batch 542\n(Train) Batch 542 Loss : 0.18961669504642487, 맞은 개수 : 122\nEpoch : 20, batch 543\n(Train) Batch 543 Loss : 0.24122613668441772, 맞은 개수 : 117\nEpoch : 20, batch 544\n(Train) Batch 544 Loss : 0.23424436151981354, 맞은 개수 : 118\nEpoch : 20, batch 545\n(Train) Batch 545 Loss : 0.28538793325424194, 맞은 개수 : 118\nEpoch : 20, batch 546\n(Train) Batch 546 Loss : 0.42507922649383545, 맞은 개수 : 112\nEpoch : 20, batch 547\n(Train) Batch 547 Loss : 0.39482104778289795, 맞은 개수 : 115\nEpoch : 20, batch 548\n(Train) Batch 548 Loss : 0.1957974135875702, 맞은 개수 : 121\nEpoch : 20, batch 549\n(Train) Batch 549 Loss : 0.3168364465236664, 맞은 개수 : 112\nEpoch : 20, batch 550\n(Train) Batch 550 Loss : 0.22438353300094604, 맞은 개수 : 116\nEpoch : 20, batch 551\n(Train) Batch 551 Loss : 0.31155386567115784, 맞은 개수 : 117\nEpoch : 20, batch 552\n(Train) Batch 552 Loss : 0.16558721661567688, 맞은 개수 : 122\nEpoch : 20, batch 553\n(Train) Batch 553 Loss : 0.269700288772583, 맞은 개수 : 117\nEpoch : 20, batch 554\n(Train) Batch 554 Loss : 0.36991968750953674, 맞은 개수 : 122\nEpoch : 20, batch 555\n(Train) Batch 555 Loss : 0.14893509447574615, 맞은 개수 : 122\nEpoch : 20, batch 556\n(Train) Batch 556 Loss : 0.35452762246131897, 맞은 개수 : 114\nEpoch : 20, batch 557\n(Train) Batch 557 Loss : 0.24728600680828094, 맞은 개수 : 119\nEpoch : 20, batch 558\n(Train) Batch 558 Loss : 0.34442099928855896, 맞은 개수 : 119\nEpoch : 20, batch 559\n(Train) Batch 559 Loss : 0.3185388743877411, 맞은 개수 : 118\nEpoch : 20, batch 560\n(Train) Batch 560 Loss : 0.2644699811935425, 맞은 개수 : 116\nEpoch : 20, batch 561\n(Train) Batch 561 Loss : 0.36022332310676575, 맞은 개수 : 115\nEpoch : 20, batch 562\n(Train) Batch 562 Loss : 0.29604390263557434, 맞은 개수 : 115\nEpoch : 20, batch 563\n(Train) Batch 563 Loss : 0.4001083970069885, 맞은 개수 : 116\nEpoch : 20, batch 564\n(Train) Batch 564 Loss : 0.24924375116825104, 맞은 개수 : 117\nEpoch : 20, batch 565\n(Train) Batch 565 Loss : 0.4180372357368469, 맞은 개수 : 114\nEpoch : 20, batch 566\n(Train) Batch 566 Loss : 0.28860047459602356, 맞은 개수 : 118\nEpoch : 20, batch 567\n(Train) Batch 567 Loss : 0.21942774951457977, 맞은 개수 : 119\nEpoch : 20, batch 568\n(Train) Batch 568 Loss : 0.14718197286128998, 맞은 개수 : 123\nEpoch : 20, batch 569\n(Train) Batch 569 Loss : 0.29864877462387085, 맞은 개수 : 118\nEpoch : 20, batch 570\n(Train) Batch 570 Loss : 0.2022593915462494, 맞은 개수 : 120\nEpoch : 20, batch 571\n(Train) Batch 571 Loss : 0.295556902885437, 맞은 개수 : 117\nEpoch : 20, batch 572\n(Train) Batch 572 Loss : 0.26939573884010315, 맞은 개수 : 121\nEpoch : 20, batch 573\n(Train) Batch 573 Loss : 0.2640193998813629, 맞은 개수 : 118\nEpoch : 20, batch 574\n(Train) Batch 574 Loss : 0.40080463886260986, 맞은 개수 : 115\nEpoch : 20, batch 575\n(Train) Batch 575 Loss : 0.2660462558269501, 맞은 개수 : 118\nEpoch : 20, batch 576\n(Train) Batch 576 Loss : 0.23963555693626404, 맞은 개수 : 118\nEpoch : 20, batch 577\n(Train) Batch 577 Loss : 0.21959079802036285, 맞은 개수 : 120\nEpoch : 20, batch 578\n(Train) Batch 578 Loss : 0.408088356256485, 맞은 개수 : 109\nEpoch : 20, batch 579\n(Train) Batch 579 Loss : 0.3641458749771118, 맞은 개수 : 115\nEpoch : 20, batch 580\n(Train) Batch 580 Loss : 0.17163114249706268, 맞은 개수 : 121\nEpoch : 20, batch 581\n(Train) Batch 581 Loss : 0.30836841464042664, 맞은 개수 : 114\nEpoch : 20, batch 582\n(Train) Batch 582 Loss : 0.4318660795688629, 맞은 개수 : 111\nEpoch : 20, batch 583\n(Train) Batch 583 Loss : 0.2569720447063446, 맞은 개수 : 118\nEpoch : 20, batch 584\n(Train) Batch 584 Loss : 0.2675784230232239, 맞은 개수 : 119\nEpoch : 20, batch 585\n(Train) Batch 585 Loss : 0.28700849413871765, 맞은 개수 : 112\nEpoch : 20, batch 586\n(Train) Batch 586 Loss : 0.16836650669574738, 맞은 개수 : 120\nEpoch : 20, batch 587\n(Train) Batch 587 Loss : 0.3483213782310486, 맞은 개수 : 116\nEpoch : 20, batch 588\n(Train) Batch 588 Loss : 0.25429314374923706, 맞은 개수 : 117\nEpoch : 20, batch 589\n(Train) Batch 589 Loss : 0.21757085621356964, 맞은 개수 : 117\nEpoch : 20, batch 590\n(Train) Batch 590 Loss : 0.4064670205116272, 맞은 개수 : 112\nEpoch : 20, batch 591\n(Train) Batch 591 Loss : 0.29140505194664, 맞은 개수 : 118\nEpoch : 20, batch 592\n(Train) Batch 592 Loss : 0.3578583002090454, 맞은 개수 : 114\nEpoch : 20, batch 593\n(Train) Batch 593 Loss : 0.21254552900791168, 맞은 개수 : 114\nEpoch : 20, batch 594\n(Train) Batch 594 Loss : 0.3065144717693329, 맞은 개수 : 118\nEpoch : 20, batch 595\n(Train) Batch 595 Loss : 0.31066328287124634, 맞은 개수 : 116\nEpoch : 20, batch 596\n(Train) Batch 596 Loss : 0.2659739553928375, 맞은 개수 : 117\nEpoch : 20, batch 597\n(Train) Batch 597 Loss : 0.2384878247976303, 맞은 개수 : 116\nEpoch : 20, batch 598\n(Train) Batch 598 Loss : 0.25814366340637207, 맞은 개수 : 116\nEpoch : 20, batch 599\n(Train) Batch 599 Loss : 0.30191898345947266, 맞은 개수 : 115\nEpoch : 20, batch 600\n(Train) Batch 600 Loss : 0.230171799659729, 맞은 개수 : 120\nEpoch : 20, batch 601\n(Train) Batch 601 Loss : 0.30090782046318054, 맞은 개수 : 114\nEpoch : 20, batch 602\n(Train) Batch 602 Loss : 0.36136025190353394, 맞은 개수 : 117\nEpoch : 20, batch 603\n(Train) Batch 603 Loss : 0.36134693026542664, 맞은 개수 : 112\nEpoch : 20, batch 604\n(Train) Batch 604 Loss : 0.4182552099227905, 맞은 개수 : 118\nEpoch : 20, batch 605\n(Train) Batch 605 Loss : 0.37468624114990234, 맞은 개수 : 111\nEpoch : 20, batch 606\n(Train) Batch 606 Loss : 0.3939838409423828, 맞은 개수 : 114\nEpoch : 20, batch 607\n(Train) Batch 607 Loss : 0.20284128189086914, 맞은 개수 : 120\nEpoch : 20, batch 608\n(Train) Batch 608 Loss : 0.42055490612983704, 맞은 개수 : 110\nEpoch : 20, batch 609\n(Train) Batch 609 Loss : 0.3027842938899994, 맞은 개수 : 114\nEpoch : 20, batch 610\n(Train) Batch 610 Loss : 0.28569790720939636, 맞은 개수 : 115\nEpoch : 20, batch 611\n(Train) Batch 611 Loss : 0.29948991537094116, 맞은 개수 : 116\nEpoch : 20, batch 612\n(Train) Batch 612 Loss : 0.25183454155921936, 맞은 개수 : 121\nEpoch : 20, batch 613\n(Train) Batch 613 Loss : 0.43090203404426575, 맞은 개수 : 113\nEpoch : 20, batch 614\n(Train) Batch 614 Loss : 0.3199433982372284, 맞은 개수 : 113\nEpoch : 20, batch 615\n(Train) Batch 615 Loss : 0.30828797817230225, 맞은 개수 : 115\nEpoch : 20, batch 616\n(Train) Batch 616 Loss : 0.3527442216873169, 맞은 개수 : 115\nEpoch : 20, batch 617\n(Train) Batch 617 Loss : 0.2922595739364624, 맞은 개수 : 117\nEpoch : 20, batch 618\n(Train) Batch 618 Loss : 0.36184802651405334, 맞은 개수 : 114\nEpoch : 20, batch 619\n(Train) Batch 619 Loss : 0.3370787501335144, 맞은 개수 : 115\nEpoch : 20, batch 620\n(Train) Batch 620 Loss : 0.2582128345966339, 맞은 개수 : 117\nEpoch : 20, batch 621\n(Train) Batch 621 Loss : 0.17166249454021454, 맞은 개수 : 122\nEpoch : 20, batch 622\n(Train) Batch 622 Loss : 0.31471455097198486, 맞은 개수 : 113\nEpoch : 20, batch 623\n(Train) Batch 623 Loss : 0.3440990746021271, 맞은 개수 : 117\nEpoch : 20, batch 624\n(Train) Batch 624 Loss : 0.209042489528656, 맞은 개수 : 120\nEpoch : 20, batch 625\n(Train) Batch 625 Loss : 0.19057568907737732, 맞은 개수 : 118\nEpoch : 20, batch 626\n(Train) Batch 626 Loss : 0.2194153368473053, 맞은 개수 : 116\nEpoch : 20, batch 627\n(Train) Batch 627 Loss : 0.27866703271865845, 맞은 개수 : 113\nEpoch : 20, batch 628\n(Train) Batch 628 Loss : 0.25676673650741577, 맞은 개수 : 115\nEpoch : 20, batch 629\n(Train) Batch 629 Loss : 0.41260796785354614, 맞은 개수 : 113\nEpoch : 20, batch 630\n(Train) Batch 630 Loss : 0.24773620069026947, 맞은 개수 : 120\nEpoch : 20, batch 631\n(Train) Batch 631 Loss : 0.14125299453735352, 맞은 개수 : 120\nEpoch : 20, batch 632\n(Train) Batch 632 Loss : 0.3472025692462921, 맞은 개수 : 112\nEpoch : 20, batch 633\n(Train) Batch 633 Loss : 0.23916283249855042, 맞은 개수 : 115\nEpoch : 20, batch 634\n(Train) Batch 634 Loss : 0.19013507664203644, 맞은 개수 : 122\nEpoch : 20, batch 635\n(Train) Batch 635 Loss : 0.18338274955749512, 맞은 개수 : 122\nEpoch : 20, batch 636\n(Train) Batch 636 Loss : 0.2840617895126343, 맞은 개수 : 115\nEpoch : 20, batch 637\n(Train) Batch 637 Loss : 0.30777472257614136, 맞은 개수 : 114\nEpoch : 20, batch 638\n(Train) Batch 638 Loss : 0.22614292800426483, 맞은 개수 : 115\nEpoch : 20, batch 639\n(Train) Batch 639 Loss : 0.22417938709259033, 맞은 개수 : 117\nEpoch : 20, batch 640\n(Train) Batch 640 Loss : 0.43920743465423584, 맞은 개수 : 111\nEpoch : 20, batch 641\n(Train) Batch 641 Loss : 0.24250102043151855, 맞은 개수 : 119\nEpoch : 20, batch 642\n(Train) Batch 642 Loss : 0.36128464341163635, 맞은 개수 : 115\nEpoch : 20, batch 643\n(Train) Batch 643 Loss : 0.4578631818294525, 맞은 개수 : 114\nEpoch : 20, batch 644\n(Train) Batch 644 Loss : 0.12596581876277924, 맞은 개수 : 122\nEpoch : 20, batch 645\n(Train) Batch 645 Loss : 0.244825080037117, 맞은 개수 : 118\nEpoch : 20, batch 646\n(Train) Batch 646 Loss : 0.2032143473625183, 맞은 개수 : 123\nEpoch : 20, batch 647\n(Train) Batch 647 Loss : 0.2993667423725128, 맞은 개수 : 114\nEpoch : 20, batch 648\n(Train) Batch 648 Loss : 0.2343195527791977, 맞은 개수 : 120\nEpoch : 20, batch 649\n(Train) Batch 649 Loss : 0.19732657074928284, 맞은 개수 : 120\nEpoch : 20, batch 650\n(Train) Batch 650 Loss : 0.3016316890716553, 맞은 개수 : 118\nEpoch : 20, batch 651\n(Train) Batch 651 Loss : 0.3405221104621887, 맞은 개수 : 113\nEpoch : 20, batch 652\n(Train) Batch 652 Loss : 0.19385427236557007, 맞은 개수 : 121\nEpoch : 20, batch 653\n(Train) Batch 653 Loss : 0.3764069974422455, 맞은 개수 : 111\nEpoch : 20, batch 654\n(Train) Batch 654 Loss : 0.28248128294944763, 맞은 개수 : 116\nEpoch : 20, batch 655\n(Train) Batch 655 Loss : 0.2805807888507843, 맞은 개수 : 119\nEpoch : 20, batch 656\n(Train) Batch 656 Loss : 0.2527564465999603, 맞은 개수 : 118\nEpoch : 20, batch 657\n(Train) Batch 657 Loss : 0.3362686038017273, 맞은 개수 : 117\nEpoch : 20, batch 658\n(Train) Batch 658 Loss : 0.37164175510406494, 맞은 개수 : 118\nEpoch : 20, batch 659\n(Train) Batch 659 Loss : 0.3455210030078888, 맞은 개수 : 115\nEpoch : 20, batch 660\n(Train) Batch 660 Loss : 0.3764220178127289, 맞은 개수 : 114\nEpoch : 20, batch 661\n(Train) Batch 661 Loss : 0.3194456696510315, 맞은 개수 : 113\nEpoch : 20, batch 662\n(Train) Batch 662 Loss : 0.2797117531299591, 맞은 개수 : 119\nEpoch : 20, batch 663\n(Train) Batch 663 Loss : 0.37109819054603577, 맞은 개수 : 114\nEpoch : 20, batch 664\n(Train) Batch 664 Loss : 0.3074823319911957, 맞은 개수 : 115\nEpoch : 20, batch 665\n(Train) Batch 665 Loss : 0.2271987348794937, 맞은 개수 : 120\nEpoch : 20, batch 666\n(Train) Batch 666 Loss : 0.23719489574432373, 맞은 개수 : 118\nEpoch : 20, batch 667\n(Train) Batch 667 Loss : 0.3149542212486267, 맞은 개수 : 118\nEpoch : 20, batch 668\n(Train) Batch 668 Loss : 0.3167881965637207, 맞은 개수 : 115\nEpoch : 20, batch 669\n(Train) Batch 669 Loss : 0.2330189049243927, 맞은 개수 : 118\nEpoch : 20, batch 670\n(Train) Batch 670 Loss : 0.26305991411209106, 맞은 개수 : 118\nEpoch : 20, batch 671\n(Train) Batch 671 Loss : 0.17825107276439667, 맞은 개수 : 120\nEpoch : 20, batch 672\n(Train) Batch 672 Loss : 0.43778517842292786, 맞은 개수 : 112\nEpoch : 20, batch 673\n(Train) Batch 673 Loss : 0.18652668595314026, 맞은 개수 : 121\nEpoch : 20, batch 674\n(Train) Batch 674 Loss : 0.30040714144706726, 맞은 개수 : 118\nEpoch : 20, batch 675\n(Train) Batch 675 Loss : 0.42758700251579285, 맞은 개수 : 110\nEpoch : 20, batch 676\n(Train) Batch 676 Loss : 0.2696491479873657, 맞은 개수 : 119\nEpoch : 20, batch 677\n(Train) Batch 677 Loss : 0.33421313762664795, 맞은 개수 : 113\nEpoch : 20, batch 678\n(Train) Batch 678 Loss : 0.22370575368404388, 맞은 개수 : 117\nEpoch : 20, batch 679\n(Train) Batch 679 Loss : 0.18862998485565186, 맞은 개수 : 120\nEpoch : 20, batch 680\n(Train) Batch 680 Loss : 0.30536481738090515, 맞은 개수 : 114\nEpoch : 20, batch 681\n(Train) Batch 681 Loss : 0.29811549186706543, 맞은 개수 : 114\nEpoch : 20, batch 682\n(Train) Batch 682 Loss : 0.35910797119140625, 맞은 개수 : 117\nEpoch : 20, batch 683\n(Train) Batch 683 Loss : 0.2768251299858093, 맞은 개수 : 118\nEpoch : 20, batch 684\n(Train) Batch 684 Loss : 0.2013978511095047, 맞은 개수 : 121\nEpoch : 20, batch 685\n(Train) Batch 685 Loss : 0.21238337457180023, 맞은 개수 : 120\nEpoch : 20, batch 686\n(Train) Batch 686 Loss : 0.2397136390209198, 맞은 개수 : 120\nEpoch : 20, batch 687\n(Train) Batch 687 Loss : 0.32986319065093994, 맞은 개수 : 115\nEpoch : 20, batch 688\n(Train) Batch 688 Loss : 0.3025139272212982, 맞은 개수 : 119\nEpoch : 20, batch 689\n(Train) Batch 689 Loss : 0.33270859718322754, 맞은 개수 : 112\nEpoch : 20, batch 690\n(Train) Batch 690 Loss : 0.3741830885410309, 맞은 개수 : 118\nEpoch : 20, batch 691\n(Train) Batch 691 Loss : 0.3018241226673126, 맞은 개수 : 117\nEpoch : 20, batch 692\n(Train) Batch 692 Loss : 0.27860331535339355, 맞은 개수 : 117\nEpoch : 20, batch 693\n(Train) Batch 693 Loss : 0.2962091267108917, 맞은 개수 : 117\nEpoch : 20, batch 694\n(Train) Batch 694 Loss : 0.3987172245979309, 맞은 개수 : 114\nEpoch : 20, batch 695\n(Train) Batch 695 Loss : 0.22980622947216034, 맞은 개수 : 121\nEpoch : 20, batch 696\n(Train) Batch 696 Loss : 0.22275088727474213, 맞은 개수 : 119\nEpoch : 20, batch 697\n(Train) Batch 697 Loss : 0.24370411038398743, 맞은 개수 : 119\nEpoch : 20, batch 698\n(Train) Batch 698 Loss : 0.3557712435722351, 맞은 개수 : 118\nEpoch : 20, batch 699\n(Train) Batch 699 Loss : 0.40677791833877563, 맞은 개수 : 111\nEpoch : 20, batch 700\n(Train) Batch 700 Loss : 0.373579740524292, 맞은 개수 : 114\nEpoch : 20, batch 701\n(Train) Batch 701 Loss : 0.32787591218948364, 맞은 개수 : 117\nEpoch : 20, batch 702\n(Train) Batch 702 Loss : 0.27840468287467957, 맞은 개수 : 116\nEpoch : 20, batch 703\n(Train) Batch 703 Loss : 0.35943469405174255, 맞은 개수 : 119\nEpoch : 20, batch 704\n(Train) Batch 704 Loss : 0.23824477195739746, 맞은 개수 : 114\nEpoch : 20, batch 705\n(Train) Batch 705 Loss : 0.4301813542842865, 맞은 개수 : 110\nEpoch : 20, batch 706\n(Train) Batch 706 Loss : 0.24979284405708313, 맞은 개수 : 118\nEpoch : 20, batch 707\n(Train) Batch 707 Loss : 0.28893300890922546, 맞은 개수 : 118\nEpoch : 20, batch 708\n(Train) Batch 708 Loss : 0.32527270913124084, 맞은 개수 : 112\nEpoch : 20, batch 709\n(Train) Batch 709 Loss : 0.2933657765388489, 맞은 개수 : 117\nEpoch : 20, batch 710\n(Train) Batch 710 Loss : 0.34688568115234375, 맞은 개수 : 118\nEpoch : 20, batch 711\n(Train) Batch 711 Loss : 0.19749896228313446, 맞은 개수 : 121\nEpoch : 20, batch 712\n(Train) Batch 712 Loss : 0.2686822712421417, 맞은 개수 : 114\nEpoch : 20, batch 713\n(Train) Batch 713 Loss : 0.2477019727230072, 맞은 개수 : 120\nEpoch : 20, batch 714\n(Train) Batch 714 Loss : 0.2594327926635742, 맞은 개수 : 119\nEpoch : 20, batch 715\n(Train) Batch 715 Loss : 0.3049957752227783, 맞은 개수 : 115\nEpoch : 20, batch 716\n(Train) Batch 716 Loss : 0.15740391612052917, 맞은 개수 : 121\nEpoch : 20, batch 717\n(Train) Batch 717 Loss : 0.3628857731819153, 맞은 개수 : 112\nEpoch : 20, batch 718\n(Train) Batch 718 Loss : 0.37983447313308716, 맞은 개수 : 113\nEpoch : 20, batch 719\n(Train) Batch 719 Loss : 0.27765512466430664, 맞은 개수 : 117\nEpoch : 20, batch 720\n(Train) Batch 720 Loss : 0.4480069577693939, 맞은 개수 : 113\nEpoch : 20, batch 721\n(Train) Batch 721 Loss : 0.4289729595184326, 맞은 개수 : 111\nEpoch : 20, batch 722\n(Train) Batch 722 Loss : 0.22709816694259644, 맞은 개수 : 117\nEpoch : 20, batch 723\n(Train) Batch 723 Loss : 0.3962846100330353, 맞은 개수 : 113\nEpoch : 20, batch 724\n(Train) Batch 724 Loss : 0.19575922191143036, 맞은 개수 : 119\nEpoch : 20, batch 725\n(Train) Batch 725 Loss : 0.24699440598487854, 맞은 개수 : 119\nEpoch : 20, batch 726\n(Train) Batch 726 Loss : 0.26490262150764465, 맞은 개수 : 115\nEpoch : 20, batch 727\n(Train) Batch 727 Loss : 0.4067559838294983, 맞은 개수 : 115\nEpoch : 20, batch 728\n(Train) Batch 728 Loss : 0.34762221574783325, 맞은 개수 : 113\nEpoch : 20, batch 729\n(Train) Batch 729 Loss : 0.3720604479312897, 맞은 개수 : 111\nEpoch : 20, batch 730\n(Train) Batch 730 Loss : 0.2192787379026413, 맞은 개수 : 119\nEpoch : 20, batch 731\n(Train) Batch 731 Loss : 0.44851961731910706, 맞은 개수 : 109\nEpoch : 20, batch 732\n(Train) Batch 732 Loss : 0.5026110410690308, 맞은 개수 : 113\nEpoch : 20, batch 733\n(Train) Batch 733 Loss : 0.3269953727722168, 맞은 개수 : 114\nEpoch : 20, batch 734\n(Train) Batch 734 Loss : 0.3167951703071594, 맞은 개수 : 116\nEpoch : 20, batch 735\n(Train) Batch 735 Loss : 0.34079235792160034, 맞은 개수 : 115\nEpoch : 20, batch 736\n(Train) Batch 736 Loss : 0.26193466782569885, 맞은 개수 : 114\nEpoch : 20, batch 737\n(Train) Batch 737 Loss : 0.2120109647512436, 맞은 개수 : 120\nEpoch : 20, batch 738\n(Train) Batch 738 Loss : 0.19502952694892883, 맞은 개수 : 121\nEpoch : 20, batch 739\n(Train) Batch 739 Loss : 0.3109641671180725, 맞은 개수 : 115\nEpoch : 20, batch 740\n(Train) Batch 740 Loss : 0.5804298520088196, 맞은 개수 : 109\nEpoch : 20, batch 741\n(Train) Batch 741 Loss : 0.19681352376937866, 맞은 개수 : 120\nEpoch : 20, batch 742\n(Train) Batch 742 Loss : 0.44055455923080444, 맞은 개수 : 116\nEpoch : 20, batch 743\n(Train) Batch 743 Loss : 0.25316759943962097, 맞은 개수 : 119\nEpoch : 20, batch 744\n(Train) Batch 744 Loss : 0.3119003474712372, 맞은 개수 : 115\nEpoch : 20, batch 745\n(Train) Batch 745 Loss : 0.2384817898273468, 맞은 개수 : 119\nEpoch : 20, batch 746\n(Train) Batch 746 Loss : 0.33987024426460266, 맞은 개수 : 114\nEpoch : 20, batch 747\n(Train) Batch 747 Loss : 0.2975468635559082, 맞은 개수 : 115\nEpoch : 20, batch 748\n(Train) Batch 748 Loss : 0.2550913095474243, 맞은 개수 : 117\nEpoch : 20, batch 749\n(Train) Batch 749 Loss : 0.48394307494163513, 맞은 개수 : 115\nEpoch : 20, batch 750\n(Train) Batch 750 Loss : 0.2897223234176636, 맞은 개수 : 119\nEpoch : 20, batch 751\n(Train) Batch 751 Loss : 0.23213991522789001, 맞은 개수 : 117\nEpoch : 20, batch 752\n(Train) Batch 752 Loss : 0.2887824475765228, 맞은 개수 : 113\nEpoch : 20, batch 753\n(Train) Batch 753 Loss : 0.3792126774787903, 맞은 개수 : 113\nEpoch : 20, batch 754\n(Train) Batch 754 Loss : 0.29215893149375916, 맞은 개수 : 115\nEpoch : 20, batch 755\n(Train) Batch 755 Loss : 0.21218836307525635, 맞은 개수 : 120\nEpoch : 20, batch 756\n(Train) Batch 756 Loss : 0.21842022240161896, 맞은 개수 : 115\nEpoch : 20, batch 757\n(Train) Batch 757 Loss : 0.3873691260814667, 맞은 개수 : 116\nEpoch : 20, batch 758\n(Train) Batch 758 Loss : 0.4009355902671814, 맞은 개수 : 111\nEpoch : 20, batch 759\n(Train) Batch 759 Loss : 0.2754223942756653, 맞은 개수 : 114\nEpoch : 20, batch 760\n(Train) Batch 760 Loss : 0.25372514128685, 맞은 개수 : 119\nEpoch : 20, batch 761\n(Train) Batch 761 Loss : 0.22994464635849, 맞은 개수 : 118\nEpoch : 20, batch 762\n(Train) Batch 762 Loss : 0.35024917125701904, 맞은 개수 : 117\nEpoch : 20, batch 763\n(Train) Batch 763 Loss : 0.16945233941078186, 맞은 개수 : 119\nEpoch : 20, batch 764\n(Train) Batch 764 Loss : 0.1777465045452118, 맞은 개수 : 123\nEpoch : 20, batch 765\n(Train) Batch 765 Loss : 0.296221524477005, 맞은 개수 : 115\nEpoch : 20, batch 766\n(Train) Batch 766 Loss : 0.3633711338043213, 맞은 개수 : 113\nEpoch : 20, batch 767\n(Train) Batch 767 Loss : 0.3655208349227905, 맞은 개수 : 117\nEpoch : 20, batch 768\n(Train) Batch 768 Loss : 0.2413916438817978, 맞은 개수 : 119\nEpoch : 20, batch 769\n(Train) Batch 769 Loss : 0.40431588888168335, 맞은 개수 : 116\nEpoch : 20, batch 770\n(Train) Batch 770 Loss : 0.4228697121143341, 맞은 개수 : 114\nEpoch : 20, batch 771\n(Train) Batch 771 Loss : 0.34394606947898865, 맞은 개수 : 109\nEpoch : 20, batch 772\n(Train) Batch 772 Loss : 0.2278991937637329, 맞은 개수 : 119\nEpoch : 20, batch 773\n(Train) Batch 773 Loss : 0.3417779505252838, 맞은 개수 : 113\nEpoch : 20, batch 774\n(Train) Batch 774 Loss : 0.25687870383262634, 맞은 개수 : 116\nEpoch : 20, batch 775\n(Train) Batch 775 Loss : 0.3191094398498535, 맞은 개수 : 114\nEpoch : 20, batch 776\n(Train) Batch 776 Loss : 0.21751563251018524, 맞은 개수 : 119\nEpoch : 20, batch 777\n(Train) Batch 777 Loss : 0.23561708629131317, 맞은 개수 : 118\nEpoch : 20, batch 778\n(Train) Batch 778 Loss : 0.2215346097946167, 맞은 개수 : 118\nEpoch : 20, batch 779\n(Train) Batch 779 Loss : 0.27527710795402527, 맞은 개수 : 118\nEpoch : 20, batch 780\n(Train) Batch 780 Loss : 0.3543784022331238, 맞은 개수 : 112\nEpoch : 20, batch 781\n(Train) Batch 781 Loss : 0.3461277484893799, 맞은 개수 : 118\nEpoch : 20, batch 782\n(Train) Batch 782 Loss : 0.2555728852748871, 맞은 개수 : 120\nEpoch : 20, batch 783\n(Train) Batch 783 Loss : 0.2551691234111786, 맞은 개수 : 118\nEpoch : 20, batch 784\n(Train) Batch 784 Loss : 0.18699626624584198, 맞은 개수 : 122\nEpoch : 20, batch 785\n(Train) Batch 785 Loss : 0.2051497995853424, 맞은 개수 : 121\nEpoch : 20, batch 786\n(Train) Batch 786 Loss : 0.4249013364315033, 맞은 개수 : 113\nEpoch : 20, batch 787\n(Train) Batch 787 Loss : 0.3912370800971985, 맞은 개수 : 116\nEpoch : 20, batch 788\n(Train) Batch 788 Loss : 0.21150247752666473, 맞은 개수 : 116\nEpoch : 20, batch 789\n(Train) Batch 789 Loss : 0.4064576029777527, 맞은 개수 : 116\nEpoch : 20, batch 790\n(Train) Batch 790 Loss : 0.2668287456035614, 맞은 개수 : 115\nEpoch : 20, batch 791\n(Train) Batch 791 Loss : 0.3246202766895294, 맞은 개수 : 115\nEpoch : 20, batch 792\n(Train) Batch 792 Loss : 0.20408548414707184, 맞은 개수 : 120\nEpoch : 20, batch 793\n(Train) Batch 793 Loss : 0.37497082352638245, 맞은 개수 : 114\nEpoch : 20, batch 794\n(Train) Batch 794 Loss : 0.2216782420873642, 맞은 개수 : 119\nEpoch : 20, batch 795\n(Train) Batch 795 Loss : 0.30640366673469543, 맞은 개수 : 115\nEpoch : 20, batch 796\n(Train) Batch 796 Loss : 0.2212136685848236, 맞은 개수 : 119\nEpoch : 20, batch 797\n(Train) Batch 797 Loss : 0.40734100341796875, 맞은 개수 : 113\nEpoch : 20, batch 798\n(Train) Batch 798 Loss : 0.21605545282363892, 맞은 개수 : 117\nEpoch : 20, batch 799\n(Train) Batch 799 Loss : 0.319973349571228, 맞은 개수 : 114\nEpoch : 20, batch 800\n(Train) Batch 800 Loss : 0.28050583600997925, 맞은 개수 : 114\nEpoch : 20, batch 801\n(Train) Batch 801 Loss : 0.3409431278705597, 맞은 개수 : 116\nEpoch : 20, batch 802\n(Train) Batch 802 Loss : 0.3593391478061676, 맞은 개수 : 115\nEpoch : 20, batch 803\n(Train) Batch 803 Loss : 0.2619713842868805, 맞은 개수 : 114\nEpoch : 20, batch 804\n(Train) Batch 804 Loss : 0.3557329773902893, 맞은 개수 : 116\nEpoch : 20, batch 805\n(Train) Batch 805 Loss : 0.33097049593925476, 맞은 개수 : 114\nEpoch : 20, batch 806\n(Train) Batch 806 Loss : 0.23504841327667236, 맞은 개수 : 119\nEpoch : 20, batch 807\n(Train) Batch 807 Loss : 0.35200878977775574, 맞은 개수 : 116\nEpoch : 20, batch 808\n(Train) Batch 808 Loss : 0.3921569287776947, 맞은 개수 : 112\nEpoch : 20, batch 809\n(Train) Batch 809 Loss : 0.20357011258602142, 맞은 개수 : 120\nEpoch : 20, batch 810\n(Train) Batch 810 Loss : 0.33400219678878784, 맞은 개수 : 115\nEpoch : 20, batch 811\n(Train) Batch 811 Loss : 0.4024489223957062, 맞은 개수 : 116\nEpoch : 20, batch 812\n(Train) Batch 812 Loss : 0.17701636254787445, 맞은 개수 : 122\nEpoch : 20, batch 813\n(Train) Batch 813 Loss : 0.18351688981056213, 맞은 개수 : 120\nEpoch : 20, batch 814\n(Train) Batch 814 Loss : 0.19194336235523224, 맞은 개수 : 119\nEpoch : 20, batch 815\n(Train) Batch 815 Loss : 0.30940568447113037, 맞은 개수 : 113\nEpoch : 20, batch 816\n(Train) Batch 816 Loss : 0.3021710515022278, 맞은 개수 : 115\nEpoch : 20, batch 817\n(Train) Batch 817 Loss : 0.3334306478500366, 맞은 개수 : 116\nEpoch : 20, batch 818\n(Train) Batch 818 Loss : 0.3349319100379944, 맞은 개수 : 117\nEpoch : 20, batch 819\n(Train) Batch 819 Loss : 0.22152002155780792, 맞은 개수 : 119\nEpoch : 20, batch 820\n(Train) Batch 820 Loss : 0.3649873435497284, 맞은 개수 : 116\nEpoch : 20, batch 821\n(Train) Batch 821 Loss : 0.2969980239868164, 맞은 개수 : 114\nEpoch : 20, batch 822\n(Train) Batch 822 Loss : 0.21182170510292053, 맞은 개수 : 120\nEpoch : 20, batch 823\n(Train) Batch 823 Loss : 0.2849188446998596, 맞은 개수 : 114\nEpoch : 20, batch 824\n(Train) Batch 824 Loss : 0.2101958692073822, 맞은 개수 : 120\nEpoch : 20, batch 825\n(Train) Batch 825 Loss : 0.384601354598999, 맞은 개수 : 115\nEpoch : 20, batch 826\n(Train) Batch 826 Loss : 0.24342985451221466, 맞은 개수 : 115\nEpoch : 20, batch 827\n(Train) Batch 827 Loss : 0.2518766522407532, 맞은 개수 : 118\nEpoch : 20, batch 828\n(Train) Batch 828 Loss : 0.3067561686038971, 맞은 개수 : 115\nEpoch : 20, batch 829\n(Train) Batch 829 Loss : 0.2122652232646942, 맞은 개수 : 117\nEpoch : 20, batch 830\n(Train) Batch 830 Loss : 0.4403265416622162, 맞은 개수 : 111\nEpoch : 20, batch 831\n(Train) Batch 831 Loss : 0.3514253795146942, 맞은 개수 : 115\nEpoch : 20, batch 832\n(Train) Batch 832 Loss : 0.4569314420223236, 맞은 개수 : 110\nEpoch : 20, batch 833\n(Train) Batch 833 Loss : 0.3948684334754944, 맞은 개수 : 109\nEpoch : 20, batch 834\n(Train) Batch 834 Loss : 0.2318394035100937, 맞은 개수 : 119\nEpoch : 20, batch 835\n(Train) Batch 835 Loss : 0.2535755932331085, 맞은 개수 : 118\nEpoch : 20, batch 836\n(Train) Batch 836 Loss : 0.32887130975723267, 맞은 개수 : 113\nEpoch : 20, batch 837\n(Train) Batch 837 Loss : 0.5091537237167358, 맞은 개수 : 111\nEpoch : 20, batch 838\n(Train) Batch 838 Loss : 0.2531174421310425, 맞은 개수 : 121\nEpoch : 20, batch 839\n(Train) Batch 839 Loss : 0.18358273804187775, 맞은 개수 : 118\nEpoch : 20, batch 840\n(Train) Batch 840 Loss : 0.3302243649959564, 맞은 개수 : 116\nEpoch : 20, batch 841\n(Train) Batch 841 Loss : 0.3670659363269806, 맞은 개수 : 115\nEpoch : 20, batch 842\n(Train) Batch 842 Loss : 0.3874743580818176, 맞은 개수 : 112\nEpoch : 20, batch 843\n(Train) Batch 843 Loss : 0.38435137271881104, 맞은 개수 : 112\nEpoch : 20, batch 844\n(Train) Batch 844 Loss : 0.47311222553253174, 맞은 개수 : 113\nEpoch : 20, batch 845\n(Train) Batch 845 Loss : 0.25192391872406006, 맞은 개수 : 120\nEpoch : 20, batch 846\n(Train) Batch 846 Loss : 0.322245329618454, 맞은 개수 : 114\nEpoch : 20, batch 847\n(Train) Batch 847 Loss : 0.33656808733940125, 맞은 개수 : 115\nEpoch : 20, batch 848\n(Train) Batch 848 Loss : 0.26078829169273376, 맞은 개수 : 112\nEpoch : 20, batch 849\n(Train) Batch 849 Loss : 0.35569071769714355, 맞은 개수 : 112\nEpoch : 20, batch 850\n(Train) Batch 850 Loss : 0.30905044078826904, 맞은 개수 : 112\nEpoch : 20, batch 851\n(Train) Batch 851 Loss : 0.25673559308052063, 맞은 개수 : 119\nEpoch : 20, batch 852\n(Train) Batch 852 Loss : 0.4650825560092926, 맞은 개수 : 111\nEpoch : 20, batch 853\n(Train) Batch 853 Loss : 0.2885611653327942, 맞은 개수 : 115\nEpoch : 20, batch 854\n(Train) Batch 854 Loss : 0.3481504023075104, 맞은 개수 : 110\nEpoch : 20, batch 855\n(Train) Batch 855 Loss : 0.4432535469532013, 맞은 개수 : 113\nEpoch : 20, batch 856\n(Train) Batch 856 Loss : 0.2879261076450348, 맞은 개수 : 120\nEpoch : 20, batch 857\n(Train) Batch 857 Loss : 0.4081467390060425, 맞은 개수 : 112\nEpoch : 20, batch 858\n(Train) Batch 858 Loss : 0.3212299644947052, 맞은 개수 : 115\nEpoch : 20, batch 859\n(Train) Batch 859 Loss : 0.2433009147644043, 맞은 개수 : 120\nEpoch : 20, batch 860\n(Train) Batch 860 Loss : 0.23812156915664673, 맞은 개수 : 119\nEpoch : 20, batch 861\n(Train) Batch 861 Loss : 0.31750625371932983, 맞은 개수 : 116\nEpoch : 20, batch 862\n(Train) Batch 862 Loss : 0.2760545313358307, 맞은 개수 : 116\nEpoch : 20, batch 863\n(Train) Batch 863 Loss : 0.29782170057296753, 맞은 개수 : 116\nEpoch : 20, batch 864\n(Train) Batch 864 Loss : 0.2965252101421356, 맞은 개수 : 116\nEpoch : 20, batch 865\n(Train) Batch 865 Loss : 0.32445091009140015, 맞은 개수 : 114\nEpoch : 20, batch 866\n(Train) Batch 866 Loss : 0.2778080403804779, 맞은 개수 : 114\nEpoch : 20, batch 867\n(Train) Batch 867 Loss : 0.40254247188568115, 맞은 개수 : 113\nEpoch : 20, batch 868\n(Train) Batch 868 Loss : 0.4000591039657593, 맞은 개수 : 112\nEpoch : 20, batch 869\n(Train) Batch 869 Loss : 0.4110310673713684, 맞은 개수 : 115\nEpoch : 20, batch 870\n(Train) Batch 870 Loss : 0.2930108308792114, 맞은 개수 : 116\nEpoch : 20, batch 871\n(Train) Batch 871 Loss : 0.35870394110679626, 맞은 개수 : 115\nEpoch : 20, batch 872\n(Train) Batch 872 Loss : 0.328174352645874, 맞은 개수 : 112\nEpoch : 20, batch 873\n(Train) Batch 873 Loss : 0.30624428391456604, 맞은 개수 : 116\nEpoch : 20, batch 874\n(Train) Batch 874 Loss : 0.42453691363334656, 맞은 개수 : 115\nEpoch : 20, batch 875\n(Train) Batch 875 Loss : 0.23264235258102417, 맞은 개수 : 117\nEpoch : 20, batch 876\n(Train) Batch 876 Loss : 0.3091362714767456, 맞은 개수 : 117\nEpoch : 20, batch 877\n(Train) Batch 877 Loss : 0.2288261502981186, 맞은 개수 : 115\nEpoch : 20, batch 878\n(Train) Batch 878 Loss : 0.25740405917167664, 맞은 개수 : 116\nEpoch : 20, batch 879\n(Train) Batch 879 Loss : 0.2791476547718048, 맞은 개수 : 117\nEpoch : 20, batch 880\n(Train) Batch 880 Loss : 0.3836462199687958, 맞은 개수 : 115\nEpoch : 20, batch 881\n(Train) Batch 881 Loss : 0.13468608260154724, 맞은 개수 : 123\nEpoch : 20, batch 882\n(Train) Batch 882 Loss : 0.3154788613319397, 맞은 개수 : 113\nEpoch : 20, batch 883\n(Train) Batch 883 Loss : 0.3103444278240204, 맞은 개수 : 115\nEpoch : 20, batch 884\n(Train) Batch 884 Loss : 0.3726208806037903, 맞은 개수 : 113\nEpoch : 20, batch 885\n(Train) Batch 885 Loss : 0.4305339753627777, 맞은 개수 : 116\nEpoch : 20, batch 886\n(Train) Batch 886 Loss : 0.3452056646347046, 맞은 개수 : 115\nEpoch : 20, batch 887\n(Train) Batch 887 Loss : 0.27675801515579224, 맞은 개수 : 114\nEpoch : 20, batch 888\n(Train) Batch 888 Loss : 0.39295920729637146, 맞은 개수 : 113\nEpoch : 20, batch 889\n(Train) Batch 889 Loss : 0.3849235475063324, 맞은 개수 : 113\nEpoch : 20, batch 890\n(Train) Batch 890 Loss : 0.2737213671207428, 맞은 개수 : 117\nEpoch : 20, batch 891\n(Train) Batch 891 Loss : 0.3729117810726166, 맞은 개수 : 114\nEpoch : 20, batch 892\n(Train) Batch 892 Loss : 0.2830459773540497, 맞은 개수 : 116\nEpoch : 20, batch 893\n(Train) Batch 893 Loss : 0.29863008856773376, 맞은 개수 : 115\nEpoch : 20, batch 894\n(Train) Batch 894 Loss : 0.26658010482788086, 맞은 개수 : 120\nEpoch : 20, batch 895\n(Train) Batch 895 Loss : 0.35743439197540283, 맞은 개수 : 112\nEpoch : 20, batch 896\n(Train) Batch 896 Loss : 0.3813881576061249, 맞은 개수 : 119\nEpoch : 20, batch 897\n(Train) Batch 897 Loss : 0.40593621134757996, 맞은 개수 : 112\nEpoch : 20, batch 898\n(Train) Batch 898 Loss : 0.30275341868400574, 맞은 개수 : 117\nEpoch : 20, batch 899\n(Train) Batch 899 Loss : 0.2012983113527298, 맞은 개수 : 119\nEpoch : 20, batch 900\n(Train) Batch 900 Loss : 0.26736655831336975, 맞은 개수 : 116\nEpoch : 20, batch 901\n(Train) Batch 901 Loss : 0.3792703151702881, 맞은 개수 : 112\nEpoch : 20, batch 902\n(Train) Batch 902 Loss : 0.3296583294868469, 맞은 개수 : 119\nEpoch : 20, batch 903\n(Train) Batch 903 Loss : 0.2483063042163849, 맞은 개수 : 118\nEpoch : 20, batch 904\n(Train) Batch 904 Loss : 0.2798652946949005, 맞은 개수 : 118\nEpoch : 20, batch 905\n(Train) Batch 905 Loss : 0.2125776708126068, 맞은 개수 : 121\nEpoch : 20, batch 906\n(Train) Batch 906 Loss : 0.25528088212013245, 맞은 개수 : 119\nEpoch : 20, batch 907\n(Train) Batch 907 Loss : 0.27303239703178406, 맞은 개수 : 118\nEpoch : 20, batch 908\n(Train) Batch 908 Loss : 0.339830219745636, 맞은 개수 : 112\nEpoch : 20, batch 909\n(Train) Batch 909 Loss : 0.20323972404003143, 맞은 개수 : 121\nEpoch : 20, batch 910\n(Train) Batch 910 Loss : 0.4290642738342285, 맞은 개수 : 112\nEpoch : 20, batch 911\n(Train) Batch 911 Loss : 0.23760069906711578, 맞은 개수 : 121\nEpoch : 20, batch 912\n(Train) Batch 912 Loss : 0.19594800472259521, 맞은 개수 : 118\nEpoch : 20, batch 913\n(Train) Batch 913 Loss : 0.1655041128396988, 맞은 개수 : 122\nEpoch : 20, batch 914\n(Train) Batch 914 Loss : 0.2963652014732361, 맞은 개수 : 113\nEpoch : 20, batch 915\n(Train) Batch 915 Loss : 0.27024462819099426, 맞은 개수 : 115\nEpoch : 20, batch 916\n(Train) Batch 916 Loss : 0.3150084316730499, 맞은 개수 : 116\nEpoch : 20, batch 917\n(Train) Batch 917 Loss : 0.2622070610523224, 맞은 개수 : 119\nEpoch : 20, batch 918\n(Train) Batch 918 Loss : 0.44197210669517517, 맞은 개수 : 108\nEpoch : 20, batch 919\n(Train) Batch 919 Loss : 0.37549084424972534, 맞은 개수 : 112\nEpoch : 20, batch 920\n(Train) Batch 920 Loss : 0.1385403573513031, 맞은 개수 : 121\nEpoch : 20, batch 921\n(Train) Batch 921 Loss : 0.2781701385974884, 맞은 개수 : 116\nEpoch : 20, batch 922\n(Train) Batch 922 Loss : 0.35905078053474426, 맞은 개수 : 116\nEpoch : 20, batch 923\n(Train) Batch 923 Loss : 0.3708319664001465, 맞은 개수 : 113\nEpoch : 20, batch 924\n(Train) Batch 924 Loss : 0.3736257553100586, 맞은 개수 : 114\nEpoch : 20, batch 925\n(Train) Batch 925 Loss : 0.24515697360038757, 맞은 개수 : 115\nEpoch : 20, batch 926\n(Train) Batch 926 Loss : 0.2901639938354492, 맞은 개수 : 117\nEpoch : 20, batch 927\n(Train) Batch 927 Loss : 0.31203415989875793, 맞은 개수 : 115\nEpoch : 20, batch 928\n(Train) Batch 928 Loss : 0.34150025248527527, 맞은 개수 : 115\nEpoch : 20, batch 929\n(Train) Batch 929 Loss : 0.2936796545982361, 맞은 개수 : 117\nEpoch : 20, batch 930\n(Train) Batch 930 Loss : 0.3270147740840912, 맞은 개수 : 117\nEpoch : 20, batch 931\n(Train) Batch 931 Loss : 0.31384700536727905, 맞은 개수 : 115\nEpoch : 20, batch 932\n(Train) Batch 932 Loss : 0.15880046784877777, 맞은 개수 : 122\nEpoch : 20, batch 933\n(Train) Batch 933 Loss : 0.2519015371799469, 맞은 개수 : 115\nEpoch : 20, batch 934\n(Train) Batch 934 Loss : 0.35124877095222473, 맞은 개수 : 110\nEpoch : 20, batch 935\n(Train) Batch 935 Loss : 0.26705893874168396, 맞은 개수 : 119\nEpoch : 20, batch 936\n(Train) Batch 936 Loss : 0.33631521463394165, 맞은 개수 : 113\nEpoch : 20, batch 937\n(Train) Batch 937 Loss : 0.31092339754104614, 맞은 개수 : 110\nEpoch : 20, batch 938\n(Train) Batch 938 Loss : 0.4358014762401581, 맞은 개수 : 111\nEpoch : 20, batch 939\n(Train) Batch 939 Loss : 0.2626696228981018, 맞은 개수 : 117\nEpoch : 20, batch 940\n(Train) Batch 940 Loss : 0.2816043198108673, 맞은 개수 : 117\nEpoch : 20, batch 941\n(Train) Batch 941 Loss : 0.3068997859954834, 맞은 개수 : 117\nEpoch : 20, batch 942\n(Train) Batch 942 Loss : 0.4114615023136139, 맞은 개수 : 115\nEpoch : 20, batch 943\n(Train) Batch 943 Loss : 0.3282564580440521, 맞은 개수 : 115\nEpoch : 20, batch 944\n(Train) Batch 944 Loss : 0.277997761964798, 맞은 개수 : 118\nEpoch : 20, batch 945\n(Train) Batch 945 Loss : 0.29509422183036804, 맞은 개수 : 113\nEpoch : 20, batch 946\n(Train) Batch 946 Loss : 0.19396542012691498, 맞은 개수 : 122\nEpoch : 20, batch 947\n(Train) Batch 947 Loss : 0.37641382217407227, 맞은 개수 : 109\nEpoch : 20, batch 948\n(Train) Batch 948 Loss : 0.4744309186935425, 맞은 개수 : 107\nEpoch : 20, batch 949\n(Train) Batch 949 Loss : 0.40872085094451904, 맞은 개수 : 111\nEpoch : 20, batch 950\n(Train) Batch 950 Loss : 0.4700793921947479, 맞은 개수 : 114\nEpoch : 20, batch 951\n(Train) Batch 951 Loss : 0.30793750286102295, 맞은 개수 : 118\nEpoch : 20, batch 952\n(Train) Batch 952 Loss : 0.3519306778907776, 맞은 개수 : 115\nEpoch : 20, batch 953\n(Train) Batch 953 Loss : 0.4291725158691406, 맞은 개수 : 112\nEpoch : 20, batch 954\n(Train) Batch 954 Loss : 0.29642340540885925, 맞은 개수 : 117\nEpoch : 20, batch 955\n(Train) Batch 955 Loss : 0.44300952553749084, 맞은 개수 : 113\nEpoch : 20, batch 956\n(Train) Batch 956 Loss : 0.36242812871932983, 맞은 개수 : 108\nEpoch : 20, batch 957\n(Train) Batch 957 Loss : 0.3499525189399719, 맞은 개수 : 115\nEpoch : 20, batch 958\n(Train) Batch 958 Loss : 0.32256948947906494, 맞은 개수 : 114\nEpoch : 20, batch 959\n(Train) Batch 959 Loss : 0.4641328752040863, 맞은 개수 : 107\nEpoch : 20, batch 960\n(Train) Batch 960 Loss : 0.3003823757171631, 맞은 개수 : 115\nEpoch : 20, batch 961\n(Train) Batch 961 Loss : 0.4100889563560486, 맞은 개수 : 115\nEpoch : 20, batch 962\n(Train) Batch 962 Loss : 0.2923416793346405, 맞은 개수 : 117\nEpoch : 20, batch 963\n(Train) Batch 963 Loss : 0.23820272088050842, 맞은 개수 : 116\nEpoch : 20, batch 964\n(Train) Batch 964 Loss : 0.41531234979629517, 맞은 개수 : 114\nEpoch : 20, batch 965\n(Train) Batch 965 Loss : 0.33222416043281555, 맞은 개수 : 113\nEpoch : 20, batch 966\n(Train) Batch 966 Loss : 0.2762306332588196, 맞은 개수 : 117\nEpoch : 20, batch 967\n(Train) Batch 967 Loss : 0.44817569851875305, 맞은 개수 : 112\nEpoch : 20, batch 968\n(Train) Batch 968 Loss : 0.24572627246379852, 맞은 개수 : 120\nEpoch : 20, batch 969\n(Train) Batch 969 Loss : 0.24501483142375946, 맞은 개수 : 119\nEpoch : 20, batch 970\n(Train) Batch 970 Loss : 0.25725018978118896, 맞은 개수 : 118\nEpoch : 20, batch 971\n(Train) Batch 971 Loss : 0.30805453658103943, 맞은 개수 : 118\nEpoch : 20, batch 972\n(Train) Batch 972 Loss : 0.34847861528396606, 맞은 개수 : 114\nEpoch : 20, batch 973\n(Train) Batch 973 Loss : 0.2900412678718567, 맞은 개수 : 114\nEpoch : 20, batch 974\n(Train) Batch 974 Loss : 0.3781082034111023, 맞은 개수 : 112\nEpoch : 20, batch 975\n(Train) Batch 975 Loss : 0.12573744356632233, 맞은 개수 : 124\nEpoch : 20, batch 976\n(Train) Batch 976 Loss : 0.38965556025505066, 맞은 개수 : 115\nEpoch : 20, batch 977\n(Train) Batch 977 Loss : 0.49659502506256104, 맞은 개수 : 106\nEpoch : 20, batch 978\n(Train) Batch 978 Loss : 0.3572927415370941, 맞은 개수 : 115\nEpoch : 20, batch 979\n(Train) Batch 979 Loss : 0.4067152738571167, 맞은 개수 : 115\nEpoch : 20, batch 980\n(Train) Batch 980 Loss : 0.3366241157054901, 맞은 개수 : 115\nEpoch : 20, batch 981\n(Train) Batch 981 Loss : 0.3109014928340912, 맞은 개수 : 115\nEpoch : 20, batch 982\n(Train) Batch 982 Loss : 0.37223830819129944, 맞은 개수 : 111\nEpoch : 20, batch 983\n(Train) Batch 983 Loss : 0.29545435309410095, 맞은 개수 : 116\nEpoch : 20, batch 984\n(Train) Batch 984 Loss : 0.2746748626232147, 맞은 개수 : 115\nEpoch : 20, batch 985\n(Train) Batch 985 Loss : 0.366238534450531, 맞은 개수 : 115\nEpoch : 20, batch 986\n(Train) Batch 986 Loss : 0.3716176450252533, 맞은 개수 : 111\nEpoch : 20, batch 987\n(Train) Batch 987 Loss : 0.36920130252838135, 맞은 개수 : 114\nEpoch : 20, batch 988\n(Train) Batch 988 Loss : 0.4977439045906067, 맞은 개수 : 113\nEpoch : 20, batch 989\n(Train) Batch 989 Loss : 0.31075650453567505, 맞은 개수 : 118\nEpoch : 20, batch 990\n(Train) Batch 990 Loss : 0.21689166128635406, 맞은 개수 : 120\nEpoch : 20, batch 991\n(Train) Batch 991 Loss : 0.21421438455581665, 맞은 개수 : 120\nEpoch : 20, batch 992\n(Train) Batch 992 Loss : 0.23981158435344696, 맞은 개수 : 119\nEpoch : 20, batch 993\n(Train) Batch 993 Loss : 0.2423420250415802, 맞은 개수 : 115\nEpoch : 20, batch 994\n(Train) Batch 994 Loss : 0.30740562081336975, 맞은 개수 : 115\nEpoch : 20, batch 995\n(Train) Batch 995 Loss : 0.19241662323474884, 맞은 개수 : 119\nEpoch : 20, batch 996\n(Train) Batch 996 Loss : 0.3778563439846039, 맞은 개수 : 115\nEpoch : 20, batch 997\n(Train) Batch 997 Loss : 0.34514346718788147, 맞은 개수 : 116\nEpoch : 20, batch 998\n(Train) Batch 998 Loss : 0.32588937878608704, 맞은 개수 : 116\nEpoch : 20, batch 999\n(Train) Batch 999 Loss : 0.2694736123085022, 맞은 개수 : 116\nEpoch : 20, batch 1000\n(Train) Batch 1000 Loss : 0.2791094481945038, 맞은 개수 : 118\nEpoch : 20, batch 1001\n(Train) Batch 1001 Loss : 0.34189876914024353, 맞은 개수 : 117\nEpoch : 20, batch 1002\n(Train) Batch 1002 Loss : 0.21092472970485687, 맞은 개수 : 120\nEpoch : 20, batch 1003\n(Train) Batch 1003 Loss : 0.35042816400527954, 맞은 개수 : 114\nEpoch : 20, batch 1004\n(Train) Batch 1004 Loss : 0.5571929812431335, 맞은 개수 : 112\nEpoch : 20, batch 1005\n(Train) Batch 1005 Loss : 0.226253479719162, 맞은 개수 : 120\nEpoch : 20, batch 1006\n(Train) Batch 1006 Loss : 0.23010121285915375, 맞은 개수 : 120\nEpoch : 20, batch 1007\n(Train) Batch 1007 Loss : 0.19888359308242798, 맞은 개수 : 120\nEpoch : 20, batch 1008\n(Train) Batch 1008 Loss : 0.4141136705875397, 맞은 개수 : 111\nEpoch : 20, batch 1009\n(Train) Batch 1009 Loss : 0.2640969455242157, 맞은 개수 : 116\nEpoch : 20, batch 1010\n(Train) Batch 1010 Loss : 0.4317456781864166, 맞은 개수 : 111\nEpoch : 20, batch 1011\n(Train) Batch 1011 Loss : 0.3016306161880493, 맞은 개수 : 120\nEpoch : 20, batch 1012\n(Train) Batch 1012 Loss : 0.28466588258743286, 맞은 개수 : 117\nEpoch : 20, batch 1013\n(Train) Batch 1013 Loss : 0.2673659026622772, 맞은 개수 : 118\nEpoch : 20, batch 1014\n(Train) Batch 1014 Loss : 0.26559340953826904, 맞은 개수 : 118\nEpoch : 20, batch 1015\n(Train) Batch 1015 Loss : 0.3843245208263397, 맞은 개수 : 114\nEpoch : 20, batch 1016\n(Train) Batch 1016 Loss : 0.4066297113895416, 맞은 개수 : 112\nEpoch : 20, batch 1017\n(Train) Batch 1017 Loss : 0.3907139301300049, 맞은 개수 : 114\nEpoch : 20, batch 1018\n(Train) Batch 1018 Loss : 0.42537418007850647, 맞은 개수 : 113\nEpoch : 20, batch 1019\n(Train) Batch 1019 Loss : 0.24341100454330444, 맞은 개수 : 118\nEpoch : 20, batch 1020\n(Train) Batch 1020 Loss : 0.3088427782058716, 맞은 개수 : 115\nEpoch : 20, batch 1021\n(Train) Batch 1021 Loss : 0.17014500498771667, 맞은 개수 : 120\nEpoch : 20, batch 1022\n(Train) Batch 1022 Loss : 0.26633068919181824, 맞은 개수 : 117\nEpoch : 20, batch 1023\n(Train) Batch 1023 Loss : 0.276959091424942, 맞은 개수 : 116\nEpoch : 20, batch 1024\n(Train) Batch 1024 Loss : 0.2771493196487427, 맞은 개수 : 119\nEpoch : 20, batch 1025\n(Train) Batch 1025 Loss : 0.30042701959609985, 맞은 개수 : 115\nEpoch : 20, batch 1026\n(Train) Batch 1026 Loss : 0.3326875865459442, 맞은 개수 : 111\nEpoch : 20, batch 1027\n(Train) Batch 1027 Loss : 0.28575363755226135, 맞은 개수 : 119\nEpoch : 20, batch 1028\n(Train) Batch 1028 Loss : 0.47432881593704224, 맞은 개수 : 106\nEpoch : 20, batch 1029\n(Train) Batch 1029 Loss : 0.2389439046382904, 맞은 개수 : 117\nEpoch : 20, batch 1030\n(Train) Batch 1030 Loss : 0.31738927960395813, 맞은 개수 : 115\nEpoch : 20, batch 1031\n(Train) Batch 1031 Loss : 0.2509763836860657, 맞은 개수 : 119\nEpoch : 20, batch 1032\n(Train) Batch 1032 Loss : 0.5134162306785583, 맞은 개수 : 112\nEpoch : 20, batch 1033\n(Train) Batch 1033 Loss : 0.3105921745300293, 맞은 개수 : 116\nEpoch : 20, batch 1034\n(Train) Batch 1034 Loss : 0.23082946240901947, 맞은 개수 : 118\nEpoch : 20, batch 1035\n(Train) Batch 1035 Loss : 0.2715023458003998, 맞은 개수 : 113\nEpoch : 20, batch 1036\n(Train) Batch 1036 Loss : 0.27218085527420044, 맞은 개수 : 122\nEpoch : 20, batch 1037\n(Train) Batch 1037 Loss : 0.31308162212371826, 맞은 개수 : 116\nEpoch : 20, batch 1038\n(Train) Batch 1038 Loss : 0.2872704863548279, 맞은 개수 : 116\nEpoch : 20, batch 1039\n(Train) Batch 1039 Loss : 0.28818845748901367, 맞은 개수 : 116\nEpoch : 20, batch 1040\n(Train) Batch 1040 Loss : 0.32526499032974243, 맞은 개수 : 117\nEpoch : 20, batch 1041\n(Train) Batch 1041 Loss : 0.34886977076530457, 맞은 개수 : 113\nEpoch : 20, batch 1042\n(Train) Batch 1042 Loss : 0.22666189074516296, 맞은 개수 : 120\nEpoch : 20, batch 1043\n(Train) Batch 1043 Loss : 0.21018223464488983, 맞은 개수 : 117\nEpoch : 20, batch 1044\n(Train) Batch 1044 Loss : 0.3112615644931793, 맞은 개수 : 113\nEpoch : 20, batch 1045\n(Train) Batch 1045 Loss : 0.35187891125679016, 맞은 개수 : 114\nEpoch : 20, batch 1046\n(Train) Batch 1046 Loss : 0.33728155493736267, 맞은 개수 : 113\nEpoch : 20, batch 1047\n(Train) Batch 1047 Loss : 0.44171780347824097, 맞은 개수 : 111\nEpoch : 20, batch 1048\n(Train) Batch 1048 Loss : 0.24082982540130615, 맞은 개수 : 118\nEpoch : 20, batch 1049\n(Train) Batch 1049 Loss : 0.26777520775794983, 맞은 개수 : 121\nEpoch : 20, batch 1050\n(Train) Batch 1050 Loss : 0.32972443103790283, 맞은 개수 : 116\nEpoch : 20, batch 1051\n(Train) Batch 1051 Loss : 0.28990161418914795, 맞은 개수 : 117\nEpoch : 20, batch 1052\n(Train) Batch 1052 Loss : 0.4148457646369934, 맞은 개수 : 111\nEpoch : 20, batch 1053\n(Train) Batch 1053 Loss : 0.3698948919773102, 맞은 개수 : 113\nEpoch : 20, batch 1054\n(Train) Batch 1054 Loss : 0.21466265618801117, 맞은 개수 : 117\nEpoch : 20, batch 1055\n(Train) Batch 1055 Loss : 0.26678216457366943, 맞은 개수 : 117\nEpoch : 20, batch 1056\n(Train) Batch 1056 Loss : 0.27860742807388306, 맞은 개수 : 114\nEpoch : 20, batch 1057\n(Train) Batch 1057 Loss : 0.22591261565685272, 맞은 개수 : 119\nEpoch : 20, batch 1058\n(Train) Batch 1058 Loss : 0.26689186692237854, 맞은 개수 : 117\nEpoch : 20, batch 1059\n(Train) Batch 1059 Loss : 0.24529488384723663, 맞은 개수 : 118\nEpoch : 20, batch 1060\n(Train) Batch 1060 Loss : 0.1860862821340561, 맞은 개수 : 123\nEpoch : 20, batch 1061\n(Train) Batch 1061 Loss : 0.3591945171356201, 맞은 개수 : 112\nEpoch : 20, batch 1062\n(Train) Batch 1062 Loss : 0.19516010582447052, 맞은 개수 : 118\nEpoch : 20, batch 1063\n(Train) Batch 1063 Loss : 0.3157908022403717, 맞은 개수 : 113\nEpoch : 20, batch 1064\n(Train) Batch 1064 Loss : 0.2866533696651459, 맞은 개수 : 115\nEpoch : 20, batch 1065\n(Train) Batch 1065 Loss : 0.25382518768310547, 맞은 개수 : 121\nEpoch : 20, batch 1066\n(Train) Batch 1066 Loss : 0.42275822162628174, 맞은 개수 : 112\nEpoch : 20, batch 1067\n(Train) Batch 1067 Loss : 0.30589988827705383, 맞은 개수 : 121\nEpoch : 20, batch 1068\n(Train) Batch 1068 Loss : 0.39047178626060486, 맞은 개수 : 115\nEpoch : 20, batch 1069\n(Train) Batch 1069 Loss : 0.26395857334136963, 맞은 개수 : 119\nEpoch : 20, batch 1070\n(Train) Batch 1070 Loss : 0.33164528012275696, 맞은 개수 : 111\nEpoch : 20, batch 1071\n(Train) Batch 1071 Loss : 0.13713498413562775, 맞은 개수 : 118\nEpoch : 20, batch 1072\n(Train) Batch 1072 Loss : 0.33905741572380066, 맞은 개수 : 116\nEpoch : 20, batch 1073\n(Train) Batch 1073 Loss : 0.19504223763942719, 맞은 개수 : 118\nEpoch : 20, batch 1074\n(Train) Batch 1074 Loss : 0.24876923859119415, 맞은 개수 : 119\nEpoch : 20, batch 1075\n(Train) Batch 1075 Loss : 0.3921642303466797, 맞은 개수 : 116\nEpoch : 20, batch 1076\n(Train) Batch 1076 Loss : 0.3231579065322876, 맞은 개수 : 114\nEpoch : 20, batch 1077\n(Train) Batch 1077 Loss : 0.31262293457984924, 맞은 개수 : 114\nEpoch : 20, batch 1078\n(Train) Batch 1078 Loss : 0.29628729820251465, 맞은 개수 : 117\nEpoch : 20, batch 1079\n(Train) Batch 1079 Loss : 0.2756027579307556, 맞은 개수 : 118\nEpoch : 20, batch 1080\n(Train) Batch 1080 Loss : 0.30521780252456665, 맞은 개수 : 117\nEpoch : 20, batch 1081\n(Train) Batch 1081 Loss : 0.2152622491121292, 맞은 개수 : 121\nEpoch : 20, batch 1082\n(Train) Batch 1082 Loss : 0.2900394797325134, 맞은 개수 : 119\nEpoch : 20, batch 1083\n(Train) Batch 1083 Loss : 0.31484168767929077, 맞은 개수 : 116\nEpoch : 20, batch 1084\n(Train) Batch 1084 Loss : 0.24786096811294556, 맞은 개수 : 117\nEpoch : 20, batch 1085\n(Train) Batch 1085 Loss : 0.24433918297290802, 맞은 개수 : 115\nEpoch : 20, batch 1086\n(Train) Batch 1086 Loss : 0.2870537042617798, 맞은 개수 : 116\nEpoch : 20, batch 1087\n(Train) Batch 1087 Loss : 0.37431901693344116, 맞은 개수 : 112\nEpoch : 20, batch 1088\n(Train) Batch 1088 Loss : 0.2507631480693817, 맞은 개수 : 120\nEpoch : 20, batch 1089\n(Train) Batch 1089 Loss : 0.3065396845340729, 맞은 개수 : 115\nEpoch : 20, batch 1090\n(Train) Batch 1090 Loss : 0.19038449227809906, 맞은 개수 : 121\nEpoch : 20, batch 1091\n(Train) Batch 1091 Loss : 0.33236461877822876, 맞은 개수 : 114\nEpoch : 20, batch 1092\n(Train) Batch 1092 Loss : 0.13239866495132446, 맞은 개수 : 125\nEpoch : 20, batch 1093\n(Train) Batch 1093 Loss : 0.14732320606708527, 맞은 개수 : 123\nEpoch : 20, batch 1094\n(Train) Batch 1094 Loss : 0.38063961267471313, 맞은 개수 : 114\nEpoch : 20, batch 1095\n(Train) Batch 1095 Loss : 0.27718284726142883, 맞은 개수 : 118\nEpoch : 20, batch 1096\n(Train) Batch 1096 Loss : 0.31127819418907166, 맞은 개수 : 117\nEpoch : 20, batch 1097\n(Train) Batch 1097 Loss : 0.49479320645332336, 맞은 개수 : 113\nEpoch : 20, batch 1098\n(Train) Batch 1098 Loss : 0.2203962504863739, 맞은 개수 : 119\nEpoch : 20, batch 1099\n(Train) Batch 1099 Loss : 0.286388635635376, 맞은 개수 : 117\nEpoch : 20, batch 1100\n(Train) Batch 1100 Loss : 0.4005541503429413, 맞은 개수 : 111\nEpoch : 20, batch 1101\n(Train) Batch 1101 Loss : 0.2996620833873749, 맞은 개수 : 116\nEpoch : 20, batch 1102\n(Train) Batch 1102 Loss : 0.4338703453540802, 맞은 개수 : 108\nEpoch : 20, batch 1103\n(Train) Batch 1103 Loss : 0.21314150094985962, 맞은 개수 : 119\nEpoch : 20, batch 1104\n(Train) Batch 1104 Loss : 0.3323197662830353, 맞은 개수 : 117\nEpoch : 20, batch 1105\n(Train) Batch 1105 Loss : 0.294053316116333, 맞은 개수 : 113\nEpoch : 20, batch 1106\n(Train) Batch 1106 Loss : 0.3722148537635803, 맞은 개수 : 115\nEpoch : 20, batch 1107\n(Train) Batch 1107 Loss : 0.5529494285583496, 맞은 개수 : 110\nEpoch : 20, batch 1108\n(Train) Batch 1108 Loss : 0.402310848236084, 맞은 개수 : 112\nEpoch : 20, batch 1109\n(Train) Batch 1109 Loss : 0.37281888723373413, 맞은 개수 : 113\nEpoch : 20, batch 1110\n(Train) Batch 1110 Loss : 0.3371616005897522, 맞은 개수 : 116\nEpoch : 20, batch 1111\n(Train) Batch 1111 Loss : 0.30472496151924133, 맞은 개수 : 116\nEpoch : 20, batch 1112\n(Train) Batch 1112 Loss : 0.32389166951179504, 맞은 개수 : 114\nEpoch : 20, batch 1113\n(Train) Batch 1113 Loss : 0.1937144696712494, 맞은 개수 : 121\nEpoch : 20, batch 1114\n(Train) Batch 1114 Loss : 0.39295366406440735, 맞은 개수 : 111\nEpoch : 20, batch 1115\n(Train) Batch 1115 Loss : 0.2344333678483963, 맞은 개수 : 118\nEpoch : 20, batch 1116\n(Train) Batch 1116 Loss : 0.18497468531131744, 맞은 개수 : 121\nEpoch : 20, batch 1117\n(Train) Batch 1117 Loss : 0.3556206226348877, 맞은 개수 : 113\nEpoch : 20, batch 1118\n(Train) Batch 1118 Loss : 0.452044814825058, 맞은 개수 : 107\nEpoch : 20, batch 1119\n(Train) Batch 1119 Loss : 0.2800554037094116, 맞은 개수 : 116\nEpoch : 20, batch 1120\n(Train) Batch 1120 Loss : 0.19930894672870636, 맞은 개수 : 120\nEpoch : 20, batch 1121\n(Train) Batch 1121 Loss : 0.20369452238082886, 맞은 개수 : 120\nEpoch : 20, batch 1122\n(Train) Batch 1122 Loss : 0.3139648139476776, 맞은 개수 : 116\nEpoch : 20, batch 1123\n(Train) Batch 1123 Loss : 0.16437840461730957, 맞은 개수 : 123\nEpoch : 20, batch 1124\n(Train) Batch 1124 Loss : 0.2881978750228882, 맞은 개수 : 115\nEpoch : 20, batch 1125\n(Train) Batch 1125 Loss : 0.266873300075531, 맞은 개수 : 117\nEpoch : 20, batch 1126\n(Train) Batch 1126 Loss : 0.2910535931587219, 맞은 개수 : 116\nEpoch : 20, batch 1127\n(Train) Batch 1127 Loss : 0.4637086093425751, 맞은 개수 : 109\nEpoch : 20, batch 1128\n(Train) Batch 1128 Loss : 0.31334805488586426, 맞은 개수 : 114\nEpoch : 20, batch 1129\n(Train) Batch 1129 Loss : 0.5281115770339966, 맞은 개수 : 107\nEpoch : 20, batch 1130\n(Train) Batch 1130 Loss : 0.30360400676727295, 맞은 개수 : 114\nEpoch : 20, batch 1131\n(Train) Batch 1131 Loss : 0.39903953671455383, 맞은 개수 : 114\nEpoch : 20, batch 1132\n(Train) Batch 1132 Loss : 0.4603196680545807, 맞은 개수 : 107\nEpoch : 20, batch 1133\n(Train) Batch 1133 Loss : 0.3103432357311249, 맞은 개수 : 115\nEpoch : 20, batch 1134\n(Train) Batch 1134 Loss : 0.28304412961006165, 맞은 개수 : 118\nEpoch : 20, batch 1135\n(Train) Batch 1135 Loss : 0.3212946653366089, 맞은 개수 : 116\nEpoch : 20, batch 1136\n(Train) Batch 1136 Loss : 0.42488226294517517, 맞은 개수 : 112\nEpoch : 20, batch 1137\n(Train) Batch 1137 Loss : 0.3146001398563385, 맞은 개수 : 117\nEpoch : 20, batch 1138\n(Train) Batch 1138 Loss : 0.37221577763557434, 맞은 개수 : 115\nEpoch : 20, batch 1139\n(Train) Batch 1139 Loss : 0.29107552766799927, 맞은 개수 : 118\nEpoch : 20, batch 1140\n(Train) Batch 1140 Loss : 0.2794090509414673, 맞은 개수 : 112\nEpoch : 20, batch 1141\n(Train) Batch 1141 Loss : 0.28318190574645996, 맞은 개수 : 115\nEpoch : 20, batch 1142\n(Train) Batch 1142 Loss : 0.3608912229537964, 맞은 개수 : 115\nEpoch : 20, batch 1143\n(Train) Batch 1143 Loss : 0.1328570544719696, 맞은 개수 : 124\nEpoch : 20, batch 1144\n(Train) Batch 1144 Loss : 0.29173043370246887, 맞은 개수 : 116\nEpoch : 20, batch 1145\n(Train) Batch 1145 Loss : 0.3548837900161743, 맞은 개수 : 115\nEpoch : 20, batch 1146\n(Train) Batch 1146 Loss : 0.3778786361217499, 맞은 개수 : 115\nEpoch : 20, batch 1147\n(Train) Batch 1147 Loss : 0.30473798513412476, 맞은 개수 : 115\nEpoch : 20, batch 1148\n(Train) Batch 1148 Loss : 0.28679218888282776, 맞은 개수 : 117\nEpoch : 20, batch 1149\n(Train) Batch 1149 Loss : 0.3018999397754669, 맞은 개수 : 116\nEpoch : 20, batch 1150\n(Train) Batch 1150 Loss : 0.3146347999572754, 맞은 개수 : 119\nEpoch : 20, batch 1151\n(Train) Batch 1151 Loss : 0.4159435033798218, 맞은 개수 : 111\nEpoch : 20, batch 1152\n(Train) Batch 1152 Loss : 0.3304100036621094, 맞은 개수 : 114\nEpoch : 20, batch 1153\n(Train) Batch 1153 Loss : 0.41493114829063416, 맞은 개수 : 114\nEpoch : 20, batch 1154\n(Train) Batch 1154 Loss : 0.38655510544776917, 맞은 개수 : 111\nEpoch : 20, batch 1155\n(Train) Batch 1155 Loss : 0.3609616458415985, 맞은 개수 : 114\nEpoch : 20, batch 1156\n(Train) Batch 1156 Loss : 0.2968722879886627, 맞은 개수 : 116\nEpoch : 20, batch 1157\n(Train) Batch 1157 Loss : 0.36087048053741455, 맞은 개수 : 115\nEpoch : 20, batch 1158\n(Train) Batch 1158 Loss : 0.29266250133514404, 맞은 개수 : 119\nEpoch : 20, batch 1159\n(Train) Batch 1159 Loss : 0.5304321646690369, 맞은 개수 : 112\nEpoch : 20, batch 1160\n(Train) Batch 1160 Loss : 0.24111902713775635, 맞은 개수 : 121\nEpoch : 20, batch 1161\n(Train) Batch 1161 Loss : 0.2867354154586792, 맞은 개수 : 114\nEpoch : 20, batch 1162\n(Train) Batch 1162 Loss : 0.20596948266029358, 맞은 개수 : 121\nEpoch : 20, batch 1163\n(Train) Batch 1163 Loss : 0.3488227427005768, 맞은 개수 : 116\nEpoch : 20, batch 1164\n(Train) Batch 1164 Loss : 0.3745240867137909, 맞은 개수 : 112\nEpoch : 20, batch 1165\n(Train) Batch 1165 Loss : 0.4218309819698334, 맞은 개수 : 111\nEpoch : 20, batch 1166\n(Train) Batch 1166 Loss : 0.2565266191959381, 맞은 개수 : 118\nEpoch : 20, batch 1167\n(Train) Batch 1167 Loss : 0.1603437215089798, 맞은 개수 : 123\nEpoch : 20, batch 1168\n(Train) Batch 1168 Loss : 0.34050536155700684, 맞은 개수 : 117\nEpoch : 20, batch 1169\n(Train) Batch 1169 Loss : 0.4501871168613434, 맞은 개수 : 116\nEpoch : 20, batch 1170\n(Train) Batch 1170 Loss : 0.2748939096927643, 맞은 개수 : 116\nEpoch : 20, batch 1171\n(Train) Batch 1171 Loss : 0.3367163836956024, 맞은 개수 : 115\nEpoch : 20, batch 1172\n(Train) Batch 1172 Loss : 0.2535634934902191, 맞은 개수 : 119\nEpoch : 20, batch 1173\n(Train) Batch 1173 Loss : 0.3045961856842041, 맞은 개수 : 113\nEpoch : 20, batch 1174\n(Train) Batch 1174 Loss : 0.25417593121528625, 맞은 개수 : 118\nEpoch : 20, batch 1175\n(Train) Batch 1175 Loss : 0.2648833990097046, 맞은 개수 : 114\nEpoch : 20, batch 1176\n(Train) Batch 1176 Loss : 0.2273857742547989, 맞은 개수 : 119\nEpoch : 20, batch 1177\n(Train) Batch 1177 Loss : 0.19342395663261414, 맞은 개수 : 120\nEpoch : 20, batch 1178\n(Train) Batch 1178 Loss : 0.401201993227005, 맞은 개수 : 113\nEpoch : 20, batch 1179\n(Train) Batch 1179 Loss : 0.4710235595703125, 맞은 개수 : 113\nEpoch : 20, batch 1180\n(Train) Batch 1180 Loss : 0.28389620780944824, 맞은 개수 : 117\nEpoch : 20, batch 1181\n(Train) Batch 1181 Loss : 0.29074302315711975, 맞은 개수 : 118\nEpoch : 20, batch 1182\n(Train) Batch 1182 Loss : 0.2996157109737396, 맞은 개수 : 117\nEpoch : 20, batch 1183\n(Train) Batch 1183 Loss : 0.3593127727508545, 맞은 개수 : 114\nEpoch : 20, batch 1184\n(Train) Batch 1184 Loss : 0.2876923680305481, 맞은 개수 : 118\nEpoch : 20, batch 1185\n(Train) Batch 1185 Loss : 0.23477418720722198, 맞은 개수 : 118\nEpoch : 20, batch 1186\n(Train) Batch 1186 Loss : 0.2681509554386139, 맞은 개수 : 114\nEpoch : 20, batch 1187\n(Train) Batch 1187 Loss : 0.4598531723022461, 맞은 개수 : 113\nEpoch : 20, batch 1188\n(Train) Batch 1188 Loss : 0.2601858973503113, 맞은 개수 : 119\nEpoch : 20, batch 1189\n(Train) Batch 1189 Loss : 0.3482275903224945, 맞은 개수 : 114\nEpoch : 20, batch 1190\n(Train) Batch 1190 Loss : 0.15114343166351318, 맞은 개수 : 121\nEpoch : 20, batch 1191\n(Train) Batch 1191 Loss : 0.32421112060546875, 맞은 개수 : 116\nEpoch : 20, batch 1192\n(Train) Batch 1192 Loss : 0.26444604992866516, 맞은 개수 : 115\nEpoch : 20, batch 1193\n(Train) Batch 1193 Loss : 0.314211905002594, 맞은 개수 : 114\nEpoch : 20, batch 1194\n(Train) Batch 1194 Loss : 0.44618478417396545, 맞은 개수 : 112\nEpoch : 20, batch 1195\n(Train) Batch 1195 Loss : 0.5307000279426575, 맞은 개수 : 108\nEpoch : 20, batch 1196\n(Train) Batch 1196 Loss : 0.37001001834869385, 맞은 개수 : 114\nEpoch : 20, batch 1197\n(Train) Batch 1197 Loss : 0.27835866808891296, 맞은 개수 : 117\nEpoch : 20, batch 1198\n(Train) Batch 1198 Loss : 0.222628653049469, 맞은 개수 : 118\nEpoch : 20, batch 1199\n(Train) Batch 1199 Loss : 0.29613184928894043, 맞은 개수 : 117\nEpoch : 20, batch 1200\n(Train) Batch 1200 Loss : 0.27860650420188904, 맞은 개수 : 115\nEpoch : 20, batch 1201\n(Train) Batch 1201 Loss : 0.22603867948055267, 맞은 개수 : 118\nEpoch : 20, batch 1202\n(Train) Batch 1202 Loss : 0.26412123441696167, 맞은 개수 : 115\nEpoch : 20, batch 1203\n(Train) Batch 1203 Loss : 0.25857892632484436, 맞은 개수 : 117\nEpoch : 20, batch 1204\n(Train) Batch 1204 Loss : 0.23643870651721954, 맞은 개수 : 121\nEpoch : 20, batch 1205\n(Train) Batch 1205 Loss : 0.2659546434879303, 맞은 개수 : 116\nEpoch : 20, batch 1206\n(Train) Batch 1206 Loss : 0.22332142293453217, 맞은 개수 : 120\nEpoch : 20, batch 1207\n(Train) Batch 1207 Loss : 0.3322317600250244, 맞은 개수 : 116\nEpoch : 20, batch 1208\n(Train) Batch 1208 Loss : 0.29276877641677856, 맞은 개수 : 116\nEpoch : 20, batch 1209\n(Train) Batch 1209 Loss : 0.3644849359989166, 맞은 개수 : 117\nEpoch : 20, batch 1210\n(Train) Batch 1210 Loss : 0.23670583963394165, 맞은 개수 : 121\nEpoch : 20, batch 1211\n(Train) Batch 1211 Loss : 0.24827410280704498, 맞은 개수 : 118\nEpoch : 20, batch 1212\n(Train) Batch 1212 Loss : 0.23440416157245636, 맞은 개수 : 119\nEpoch : 20, batch 1213\n(Train) Batch 1213 Loss : 0.3591483235359192, 맞은 개수 : 113\nEpoch : 20, batch 1214\n(Train) Batch 1214 Loss : 0.340129554271698, 맞은 개수 : 114\nEpoch : 20, batch 1215\n(Train) Batch 1215 Loss : 0.200595885515213, 맞은 개수 : 118\nEpoch : 20, batch 1216\n(Train) Batch 1216 Loss : 0.3282828629016876, 맞은 개수 : 113\nEpoch : 20, batch 1217\n(Train) Batch 1217 Loss : 0.4221138656139374, 맞은 개수 : 115\nEpoch : 20, batch 1218\n(Train) Batch 1218 Loss : 0.4394826591014862, 맞은 개수 : 113\nEpoch : 20, batch 1219\n(Train) Batch 1219 Loss : 0.34325504302978516, 맞은 개수 : 114\nEpoch : 20, batch 1220\n(Train) Batch 1220 Loss : 0.18300789594650269, 맞은 개수 : 121\nEpoch : 20, batch 1221\n(Train) Batch 1221 Loss : 0.5060709714889526, 맞은 개수 : 109\nEpoch : 20, batch 1222\n(Train) Batch 1222 Loss : 0.2754833698272705, 맞은 개수 : 113\nEpoch : 20, batch 1223\n(Train) Batch 1223 Loss : 0.28394219279289246, 맞은 개수 : 117\nEpoch : 20, batch 1224\n(Train) Batch 1224 Loss : 0.2787892520427704, 맞은 개수 : 118\nEpoch : 20, batch 1225\n(Train) Batch 1225 Loss : 0.3380102515220642, 맞은 개수 : 117\nEpoch : 20, batch 1226\n(Train) Batch 1226 Loss : 0.3400384187698364, 맞은 개수 : 114\nEpoch : 20, batch 1227\n(Train) Batch 1227 Loss : 0.3384591042995453, 맞은 개수 : 118\nEpoch : 20, batch 1228\n(Train) Batch 1228 Loss : 0.455253928899765, 맞은 개수 : 114\nEpoch : 20, batch 1229\n(Train) Batch 1229 Loss : 0.21098598837852478, 맞은 개수 : 120\nEpoch : 20, batch 1230\n(Train) Batch 1230 Loss : 0.3155518174171448, 맞은 개수 : 119\nEpoch : 20, batch 1231\n(Train) Batch 1231 Loss : 0.5657553672790527, 맞은 개수 : 114\nEpoch : 20, batch 1232\n(Train) Batch 1232 Loss : 0.28421688079833984, 맞은 개수 : 116\nEpoch : 20, batch 1233\n(Train) Batch 1233 Loss : 0.31724125146865845, 맞은 개수 : 117\nEpoch : 20, batch 1234\n(Train) Batch 1234 Loss : 0.2500225305557251, 맞은 개수 : 118\nEpoch : 20, batch 1235\n(Train) Batch 1235 Loss : 0.4150087833404541, 맞은 개수 : 116\nEpoch : 20, batch 1236\n(Train) Batch 1236 Loss : 0.5612912178039551, 맞은 개수 : 106\nEpoch : 20, batch 1237\n(Train) Batch 1237 Loss : 0.32528942823410034, 맞은 개수 : 118\nEpoch : 20, batch 1238\n(Train) Batch 1238 Loss : 0.28936848044395447, 맞은 개수 : 119\nEpoch : 20, batch 1239\n(Train) Batch 1239 Loss : 0.2787732183933258, 맞은 개수 : 119\nEpoch : 20, batch 1240\n(Train) Batch 1240 Loss : 0.2581009268760681, 맞은 개수 : 118\nEpoch : 20, batch 1241\n(Train) Batch 1241 Loss : 0.2877255976200104, 맞은 개수 : 114\nEpoch : 20, batch 1242\n(Train) Batch 1242 Loss : 0.355957955121994, 맞은 개수 : 111\nEpoch : 20, batch 1243\n(Train) Batch 1243 Loss : 0.2913757562637329, 맞은 개수 : 116\nEpoch : 20, batch 1244\n(Train) Batch 1244 Loss : 0.3147994875907898, 맞은 개수 : 118\nEpoch : 20, batch 1245\n(Train) Batch 1245 Loss : 0.19940103590488434, 맞은 개수 : 118\nEpoch : 20, batch 1246\n(Train) Batch 1246 Loss : 0.4255672097206116, 맞은 개수 : 111\nEpoch : 20, batch 1247\n(Train) Batch 1247 Loss : 0.28288066387176514, 맞은 개수 : 117\nEpoch : 20, batch 1248\n(Train) Batch 1248 Loss : 0.265110582113266, 맞은 개수 : 119\nEpoch : 20, batch 1249\n(Train) Batch 1249 Loss : 0.39515650272369385, 맞은 개수 : 112\nEpoch : 20, batch 1250\n(Train) Batch 1250 Loss : 0.3153894245624542, 맞은 개수 : 114\nEpoch : 20, batch 1251\n(Train) Batch 1251 Loss : 0.37791648507118225, 맞은 개수 : 110\nEpoch : 20, batch 1252\n(Train) Batch 1252 Loss : 0.3701992630958557, 맞은 개수 : 110\nEpoch : 20, batch 1253\n(Train) Batch 1253 Loss : 0.34889066219329834, 맞은 개수 : 112\nEpoch : 20, batch 1254\n(Train) Batch 1254 Loss : 0.3958803415298462, 맞은 개수 : 114\nEpoch : 20, batch 1255\n(Train) Batch 1255 Loss : 0.24435308575630188, 맞은 개수 : 122\nEpoch : 20, batch 1256\n(Train) Batch 1256 Loss : 0.19678227603435516, 맞은 개수 : 118\nEpoch : 20, batch 1257\n(Train) Batch 1257 Loss : 0.21607480943202972, 맞은 개수 : 121\nEpoch : 20, batch 1258\n(Train) Batch 1258 Loss : 0.33733677864074707, 맞은 개수 : 118\nEpoch : 20, batch 1259\n(Train) Batch 1259 Loss : 0.3261944651603699, 맞은 개수 : 115\nEpoch : 20, batch 1260\n(Train) Batch 1260 Loss : 0.4541245996952057, 맞은 개수 : 115\nEpoch : 20, batch 1261\n(Train) Batch 1261 Loss : 0.25852102041244507, 맞은 개수 : 117\nEpoch : 20, batch 1262\n(Train) Batch 1262 Loss : 0.27762630581855774, 맞은 개수 : 119\nEpoch : 20, batch 1263\n(Train) Batch 1263 Loss : 0.3360522985458374, 맞은 개수 : 115\nEpoch : 20, batch 1264\n(Train) Batch 1264 Loss : 0.3689561188220978, 맞은 개수 : 117\nEpoch : 20, batch 1265\n(Train) Batch 1265 Loss : 0.3109792470932007, 맞은 개수 : 114\nEpoch : 20, batch 1266\n(Train) Batch 1266 Loss : 0.21834322810173035, 맞은 개수 : 117\nEpoch : 20, batch 1267\n(Train) Batch 1267 Loss : 0.1928512006998062, 맞은 개수 : 122\nEpoch : 20, batch 1268\n(Train) Batch 1268 Loss : 0.3679821491241455, 맞은 개수 : 112\nEpoch : 20, batch 1269\n(Train) Batch 1269 Loss : 0.24022996425628662, 맞은 개수 : 117\nEpoch : 20, batch 1270\n(Train) Batch 1270 Loss : 0.2940897047519684, 맞은 개수 : 116\nEpoch : 20, batch 1271\n(Train) Batch 1271 Loss : 0.2721676230430603, 맞은 개수 : 119\nEpoch : 20, batch 1272\n(Train) Batch 1272 Loss : 0.22990518808364868, 맞은 개수 : 119\nEpoch : 20, batch 1273\n(Train) Batch 1273 Loss : 0.5534287691116333, 맞은 개수 : 107\nEpoch : 20, batch 1274\n(Train) Batch 1274 Loss : 0.4471542537212372, 맞은 개수 : 111\nEpoch : 20, batch 1275\n(Train) Batch 1275 Loss : 0.3720746338367462, 맞은 개수 : 114\nEpoch : 20, batch 1276\n(Train) Batch 1276 Loss : 0.35612019896507263, 맞은 개수 : 110\nEpoch : 20, batch 1277\n(Train) Batch 1277 Loss : 0.3359149992465973, 맞은 개수 : 113\nEpoch : 20, batch 1278\n(Train) Batch 1278 Loss : 0.27025532722473145, 맞은 개수 : 117\nEpoch : 20, batch 1279\n(Train) Batch 1279 Loss : 0.36225268244743347, 맞은 개수 : 115\nEpoch : 20, batch 1280\n(Train) Batch 1280 Loss : 0.3707714378833771, 맞은 개수 : 109\nEpoch : 20, batch 1281\n(Train) Batch 1281 Loss : 0.26040002703666687, 맞은 개수 : 118\nEpoch : 20, batch 1282\n(Train) Batch 1282 Loss : 0.2908641993999481, 맞은 개수 : 116\nEpoch : 20, batch 1283\n(Train) Batch 1283 Loss : 0.3736066222190857, 맞은 개수 : 113\nEpoch : 20, batch 1284\n(Train) Batch 1284 Loss : 0.32653552293777466, 맞은 개수 : 115\nEpoch : 20, batch 1285\n(Train) Batch 1285 Loss : 0.2301124781370163, 맞은 개수 : 119\nEpoch : 20, batch 1286\n(Train) Batch 1286 Loss : 0.26560452580451965, 맞은 개수 : 115\nEpoch : 20, batch 1287\n(Train) Batch 1287 Loss : 0.3688773214817047, 맞은 개수 : 112\nEpoch : 20, batch 1288\n(Train) Batch 1288 Loss : 0.3402935862541199, 맞은 개수 : 115\nEpoch : 20, batch 1289\n(Train) Batch 1289 Loss : 0.46780237555503845, 맞은 개수 : 110\nEpoch : 20, batch 1290\n(Train) Batch 1290 Loss : 0.2781815826892853, 맞은 개수 : 117\nEpoch : 20, batch 1291\n(Train) Batch 1291 Loss : 0.3992392122745514, 맞은 개수 : 119\nEpoch : 20, batch 1292\n(Train) Batch 1292 Loss : 0.27161115407943726, 맞은 개수 : 116\nEpoch : 20, batch 1293\n(Train) Batch 1293 Loss : 0.4507780969142914, 맞은 개수 : 111\nEpoch : 20, batch 1294\n(Train) Batch 1294 Loss : 0.43724778294563293, 맞은 개수 : 111\nEpoch : 20, batch 1295\n(Train) Batch 1295 Loss : 0.4461215138435364, 맞은 개수 : 115\nEpoch : 20, batch 1296\n(Train) Batch 1296 Loss : 0.35465243458747864, 맞은 개수 : 117\nEpoch : 20, batch 1297\n(Train) Batch 1297 Loss : 0.324818879365921, 맞은 개수 : 117\nEpoch : 20, batch 1298\n(Train) Batch 1298 Loss : 0.3285864293575287, 맞은 개수 : 119\nEpoch : 20, batch 1299\n(Train) Batch 1299 Loss : 0.14949384331703186, 맞은 개수 : 122\nEpoch : 20, batch 1300\n(Train) Batch 1300 Loss : 0.3995678126811981, 맞은 개수 : 113\nEpoch : 20, batch 1301\n(Train) Batch 1301 Loss : 0.28029394149780273, 맞은 개수 : 119\nEpoch : 20, batch 1302\n(Train) Batch 1302 Loss : 0.39366310834884644, 맞은 개수 : 115\nEpoch : 20, batch 1303\n(Train) Batch 1303 Loss : 0.27426305413246155, 맞은 개수 : 115\nEpoch : 20, batch 1304\n(Train) Batch 1304 Loss : 0.49191397428512573, 맞은 개수 : 109\nEpoch : 20, batch 1305\n(Train) Batch 1305 Loss : 0.3471508324146271, 맞은 개수 : 114\nEpoch : 20, batch 1306\n(Train) Batch 1306 Loss : 0.35784465074539185, 맞은 개수 : 113\nEpoch : 20, batch 1307\n(Train) Batch 1307 Loss : 0.23412378132343292, 맞은 개수 : 117\nEpoch : 20, batch 1308\n(Train) Batch 1308 Loss : 0.30752691626548767, 맞은 개수 : 119\nEpoch : 20, batch 1309\n(Train) Batch 1309 Loss : 0.2798146605491638, 맞은 개수 : 115\nEpoch : 20, batch 1310\n(Train) Batch 1310 Loss : 0.26979655027389526, 맞은 개수 : 116\nEpoch : 20, batch 1311\n(Train) Batch 1311 Loss : 0.25998586416244507, 맞은 개수 : 117\nEpoch : 20, batch 1312\n(Train) Batch 1312 Loss : 0.3327397108078003, 맞은 개수 : 114\nEpoch : 20, batch 1313\n(Train) Batch 1313 Loss : 0.2757928669452667, 맞은 개수 : 118\nEpoch : 20, batch 1314\n(Train) Batch 1314 Loss : 0.35079464316368103, 맞은 개수 : 114\nEpoch : 20, batch 1315\n(Train) Batch 1315 Loss : 0.2774316072463989, 맞은 개수 : 118\nEpoch : 20, batch 1316\n(Train) Batch 1316 Loss : 0.28721532225608826, 맞은 개수 : 117\nEpoch : 20, batch 1317\n(Train) Batch 1317 Loss : 0.34859299659729004, 맞은 개수 : 116\nEpoch : 20, batch 1318\n(Train) Batch 1318 Loss : 0.5765793323516846, 맞은 개수 : 104\nEpoch : 20, batch 1319\n(Train) Batch 1319 Loss : 0.2899627983570099, 맞은 개수 : 116\nEpoch : 20, batch 1320\n(Train) Batch 1320 Loss : 0.41815486550331116, 맞은 개수 : 112\nEpoch : 20, batch 1321\n(Train) Batch 1321 Loss : 0.41447561979293823, 맞은 개수 : 113\nEpoch : 20, batch 1322\n(Train) Batch 1322 Loss : 0.34875622391700745, 맞은 개수 : 116\nEpoch : 20, batch 1323\n(Train) Batch 1323 Loss : 0.18863147497177124, 맞은 개수 : 123\nEpoch : 20, batch 1324\n(Train) Batch 1324 Loss : 0.27882716059684753, 맞은 개수 : 116\nEpoch : 20, batch 1325\n(Train) Batch 1325 Loss : 0.19316190481185913, 맞은 개수 : 119\nEpoch : 20, batch 1326\n(Train) Batch 1326 Loss : 0.38484030961990356, 맞은 개수 : 110\nEpoch : 20, batch 1327\n(Train) Batch 1327 Loss : 0.21493114531040192, 맞은 개수 : 121\nEpoch : 20, batch 1328\n(Train) Batch 1328 Loss : 0.2630951702594757, 맞은 개수 : 117\nEpoch : 20, batch 1329\n(Train) Batch 1329 Loss : 0.3976406753063202, 맞은 개수 : 117\nEpoch : 20, batch 1330\n(Train) Batch 1330 Loss : 0.47462961077690125, 맞은 개수 : 111\nEpoch : 20, batch 1331\n(Train) Batch 1331 Loss : 0.39824724197387695, 맞은 개수 : 112\nEpoch : 20, batch 1332\n(Train) Batch 1332 Loss : 0.39308568835258484, 맞은 개수 : 112\nEpoch : 20, batch 1333\n(Train) Batch 1333 Loss : 0.353863388299942, 맞은 개수 : 113\nEpoch : 20, batch 1334\n(Train) Batch 1334 Loss : 0.2820802927017212, 맞은 개수 : 116\nEpoch : 20, batch 1335\n(Train) Batch 1335 Loss : 0.4016827344894409, 맞은 개수 : 117\nEpoch : 20, batch 1336\n(Train) Batch 1336 Loss : 0.20948855578899384, 맞은 개수 : 121\nEpoch : 20, batch 1337\n(Train) Batch 1337 Loss : 0.25782451033592224, 맞은 개수 : 117\nEpoch : 20, batch 1338\n(Train) Batch 1338 Loss : 0.40470585227012634, 맞은 개수 : 115\nEpoch : 20, batch 1339\n(Train) Batch 1339 Loss : 0.3476773500442505, 맞은 개수 : 113\nEpoch : 20, batch 1340\n(Train) Batch 1340 Loss : 0.31069549918174744, 맞은 개수 : 116\nEpoch : 20, batch 1341\n(Train) Batch 1341 Loss : 0.26555582880973816, 맞은 개수 : 118\nEpoch : 20, batch 1342\n(Train) Batch 1342 Loss : 0.2885051667690277, 맞은 개수 : 118\nEpoch : 20, batch 1343\n(Train) Batch 1343 Loss : 0.32017046213150024, 맞은 개수 : 117\nEpoch : 20, batch 1344\n(Train) Batch 1344 Loss : 0.5391178727149963, 맞은 개수 : 108\nEpoch : 20, batch 1345\n(Train) Batch 1345 Loss : 0.28569287061691284, 맞은 개수 : 114\nEpoch : 20, batch 1346\n(Train) Batch 1346 Loss : 0.37546634674072266, 맞은 개수 : 116\nEpoch : 20, batch 1347\n(Train) Batch 1347 Loss : 0.2538105547428131, 맞은 개수 : 116\nEpoch : 20, batch 1348\n(Train) Batch 1348 Loss : 0.2815999686717987, 맞은 개수 : 118\nEpoch : 20, batch 1349\n(Train) Batch 1349 Loss : 0.26524868607521057, 맞은 개수 : 117\nEpoch : 20, batch 1350\n(Train) Batch 1350 Loss : 0.5524808168411255, 맞은 개수 : 104\nEpoch : 20, batch 1351\n(Train) Batch 1351 Loss : 0.2747991979122162, 맞은 개수 : 114\nEpoch : 20, batch 1352\n(Train) Batch 1352 Loss : 0.34668946266174316, 맞은 개수 : 116\nEpoch : 20, batch 1353\n(Train) Batch 1353 Loss : 0.27552804350852966, 맞은 개수 : 116\nEpoch : 20, batch 1354\n(Train) Batch 1354 Loss : 0.2519207000732422, 맞은 개수 : 115\nEpoch : 20, batch 1355\n(Train) Batch 1355 Loss : 0.3461715579032898, 맞은 개수 : 117\nEpoch : 20, batch 1356\n(Train) Batch 1356 Loss : 0.2551921010017395, 맞은 개수 : 117\nEpoch : 20, batch 1357\n(Train) Batch 1357 Loss : 0.462177574634552, 맞은 개수 : 115\nEpoch : 20, batch 1358\n(Train) Batch 1358 Loss : 0.27085772156715393, 맞은 개수 : 118\nEpoch : 20, batch 1359\n(Train) Batch 1359 Loss : 0.36507534980773926, 맞은 개수 : 113\nEpoch : 20, batch 1360\n(Train) Batch 1360 Loss : 0.25087183713912964, 맞은 개수 : 120\nEpoch : 20, batch 1361\n(Train) Batch 1361 Loss : 0.3290596902370453, 맞은 개수 : 112\nEpoch : 20, batch 1362\n(Train) Batch 1362 Loss : 0.2815295457839966, 맞은 개수 : 116\nEpoch : 20, batch 1363\n(Train) Batch 1363 Loss : 0.33590856194496155, 맞은 개수 : 113\nEpoch : 20, batch 1364\n(Train) Batch 1364 Loss : 0.3157604932785034, 맞은 개수 : 116\nEpoch : 20, batch 1365\n(Train) Batch 1365 Loss : 0.23658357560634613, 맞은 개수 : 120\nEpoch : 20, batch 1366\n(Train) Batch 1366 Loss : 0.44549912214279175, 맞은 개수 : 110\nEpoch : 20, batch 1367\n(Train) Batch 1367 Loss : 0.43663090467453003, 맞은 개수 : 110\nEpoch : 20, batch 1368\n(Train) Batch 1368 Loss : 0.3042832314968109, 맞은 개수 : 116\nEpoch : 20, batch 1369\n(Train) Batch 1369 Loss : 0.22260572016239166, 맞은 개수 : 119\nEpoch : 20, batch 1370\n(Train) Batch 1370 Loss : 0.2564108669757843, 맞은 개수 : 116\nEpoch : 20, batch 1371\n(Train) Batch 1371 Loss : 0.28741738200187683, 맞은 개수 : 113\nEpoch : 20, batch 1372\n(Train) Batch 1372 Loss : 0.29740095138549805, 맞은 개수 : 117\nEpoch : 20, batch 1373\n(Train) Batch 1373 Loss : 0.2601400315761566, 맞은 개수 : 115\nEpoch : 20, batch 1374\n(Train) Batch 1374 Loss : 0.42940300703048706, 맞은 개수 : 113\nEpoch : 20, batch 1375\n(Train) Batch 1375 Loss : 0.47319164872169495, 맞은 개수 : 110\nEpoch : 20, batch 1376\n(Train) Batch 1376 Loss : 0.17978079617023468, 맞은 개수 : 119\nEpoch : 20, batch 1377\n(Train) Batch 1377 Loss : 0.3441183269023895, 맞은 개수 : 114\nEpoch : 20, batch 1378\n(Train) Batch 1378 Loss : 0.21072179079055786, 맞은 개수 : 119\nEpoch : 20, batch 1379\n(Train) Batch 1379 Loss : 0.24296079576015472, 맞은 개수 : 117\nEpoch : 20, batch 1380\n(Train) Batch 1380 Loss : 0.39477473497390747, 맞은 개수 : 110\nEpoch : 20, batch 1381\n(Train) Batch 1381 Loss : 0.29426267743110657, 맞은 개수 : 115\nEpoch : 20, batch 1382\n(Train) Batch 1382 Loss : 0.2533734142780304, 맞은 개수 : 113\nEpoch : 20, batch 1383\n(Train) Batch 1383 Loss : 0.2577532231807709, 맞은 개수 : 118\nEpoch : 20, batch 1384\n(Train) Batch 1384 Loss : 0.23470868170261383, 맞은 개수 : 120\nEpoch : 20, batch 1385\n(Train) Batch 1385 Loss : 0.3617464601993561, 맞은 개수 : 117\nEpoch : 20, batch 1386\n(Train) Batch 1386 Loss : 0.2002268135547638, 맞은 개수 : 123\nEpoch : 20, batch 1387\n(Train) Batch 1387 Loss : 0.28288570046424866, 맞은 개수 : 114\nEpoch : 20, batch 1388\n(Train) Batch 1388 Loss : 0.20660583674907684, 맞은 개수 : 120\nEpoch : 20, batch 1389\n(Train) Batch 1389 Loss : 0.4020339548587799, 맞은 개수 : 112\nEpoch : 20, batch 1390\n(Train) Batch 1390 Loss : 0.34151917695999146, 맞은 개수 : 115\nEpoch : 20, batch 1391\n(Train) Batch 1391 Loss : 0.23457349836826324, 맞은 개수 : 117\nEpoch : 20, batch 1392\n(Train) Batch 1392 Loss : 0.30665335059165955, 맞은 개수 : 114\nEpoch : 20, batch 1393\n(Train) Batch 1393 Loss : 0.21091552078723907, 맞은 개수 : 120\nEpoch : 20, batch 1394\n(Train) Batch 1394 Loss : 0.26604771614074707, 맞은 개수 : 118\nEpoch : 20, batch 1395\n(Train) Batch 1395 Loss : 0.31815624237060547, 맞은 개수 : 113\nEpoch : 20, batch 1396\n(Train) Batch 1396 Loss : 0.2791939675807953, 맞은 개수 : 116\nEpoch : 20, batch 1397\n(Train) Batch 1397 Loss : 0.4173201620578766, 맞은 개수 : 109\nEpoch : 20, batch 1398\n(Train) Batch 1398 Loss : 0.2735143303871155, 맞은 개수 : 118\nEpoch : 20, batch 1399\n(Train) Batch 1399 Loss : 0.3338391184806824, 맞은 개수 : 115\nEpoch : 20, batch 1400\n(Train) Batch 1400 Loss : 0.3013123869895935, 맞은 개수 : 115\nEpoch : 20, batch 1401\n(Train) Batch 1401 Loss : 0.24288944900035858, 맞은 개수 : 121\nEpoch : 20, batch 1402\n(Train) Batch 1402 Loss : 0.35990092158317566, 맞은 개수 : 114\nEpoch : 20, batch 1403\n(Train) Batch 1403 Loss : 0.30263787508010864, 맞은 개수 : 114\nEpoch : 20, batch 1404\n(Train) Batch 1404 Loss : 0.2505219280719757, 맞은 개수 : 118\nEpoch : 20, batch 1405\n(Train) Batch 1405 Loss : 0.3392530381679535, 맞은 개수 : 111\nEpoch : 20, batch 1406\n(Train) Batch 1406 Loss : 0.2990894913673401, 맞은 개수 : 114\nEpoch : 20, batch 1407\n(Train) Batch 1407 Loss : 0.3848733603954315, 맞은 개수 : 112\nEpoch : 20, batch 1408\n(Train) Batch 1408 Loss : 0.21454037725925446, 맞은 개수 : 119\nEpoch : 20, batch 1409\n(Train) Batch 1409 Loss : 0.2732584476470947, 맞은 개수 : 119\nEpoch : 20, batch 1410\n(Train) Batch 1410 Loss : 0.30083125829696655, 맞은 개수 : 115\nEpoch : 20, batch 1411\n(Train) Batch 1411 Loss : 0.2567792236804962, 맞은 개수 : 117\nEpoch : 20, batch 1412\n(Train) Batch 1412 Loss : 0.29044121503829956, 맞은 개수 : 117\nEpoch : 20, batch 1413\n(Train) Batch 1413 Loss : 0.31714707612991333, 맞은 개수 : 116\nEpoch : 20, batch 1414\n(Train) Batch 1414 Loss : 0.326996773481369, 맞은 개수 : 116\nEpoch : 20, batch 1415\n(Train) Batch 1415 Loss : 0.41321977972984314, 맞은 개수 : 113\nEpoch : 20, batch 1416\n(Train) Batch 1416 Loss : 0.35368239879608154, 맞은 개수 : 113\nEpoch : 20, batch 1417\n(Train) Batch 1417 Loss : 0.2314515858888626, 맞은 개수 : 116\nEpoch : 20, batch 1418\n(Train) Batch 1418 Loss : 0.2861202657222748, 맞은 개수 : 119\nEpoch : 20, batch 1419\n(Train) Batch 1419 Loss : 0.30834099650382996, 맞은 개수 : 115\nEpoch : 20, batch 1420\n(Train) Batch 1420 Loss : 0.41861703991889954, 맞은 개수 : 109\nEpoch : 20, batch 1421\n(Train) Batch 1421 Loss : 0.32090669870376587, 맞은 개수 : 116\nEpoch : 20, batch 1422\n(Train) Batch 1422 Loss : 0.4800940155982971, 맞은 개수 : 113\nEpoch : 20, batch 1423\n(Train) Batch 1423 Loss : 0.24949264526367188, 맞은 개수 : 119\nEpoch : 20, batch 1424\n(Train) Batch 1424 Loss : 0.2272234857082367, 맞은 개수 : 121\nEpoch : 20, batch 1425\n(Train) Batch 1425 Loss : 0.2707459628582001, 맞은 개수 : 119\nEpoch : 20, batch 1426\n(Train) Batch 1426 Loss : 0.2879076302051544, 맞은 개수 : 115\nEpoch : 20, batch 1427\n(Train) Batch 1427 Loss : 0.27327513694763184, 맞은 개수 : 120\nEpoch : 20, batch 1428\n(Train) Batch 1428 Loss : 0.25636303424835205, 맞은 개수 : 116\nEpoch : 20, batch 1429\n(Train) Batch 1429 Loss : 0.3307214379310608, 맞은 개수 : 116\nEpoch : 20, batch 1430\n(Train) Batch 1430 Loss : 0.33774301409721375, 맞은 개수 : 111\nEpoch : 20, batch 1431\n(Train) Batch 1431 Loss : 0.2524661421775818, 맞은 개수 : 118\nEpoch : 20, batch 1432\n(Train) Batch 1432 Loss : 0.24890129268169403, 맞은 개수 : 118\nEpoch : 20, batch 1433\n(Train) Batch 1433 Loss : 0.28493866324424744, 맞은 개수 : 117\nEpoch : 20, batch 1434\n(Train) Batch 1434 Loss : 0.37088543176651, 맞은 개수 : 118\nEpoch : 20, batch 1435\n(Train) Batch 1435 Loss : 0.23980046808719635, 맞은 개수 : 119\nEpoch : 20, batch 1436\n(Train) Batch 1436 Loss : 0.2887780964374542, 맞은 개수 : 116\nEpoch : 20, batch 1437\n(Train) Batch 1437 Loss : 0.2520473003387451, 맞은 개수 : 117\nEpoch : 20, batch 1438\n(Train) Batch 1438 Loss : 0.1963295042514801, 맞은 개수 : 119\nEpoch : 20, batch 1439\n(Train) Batch 1439 Loss : 0.21969588100910187, 맞은 개수 : 119\nEpoch : 20, batch 1440\n(Train) Batch 1440 Loss : 0.2805076837539673, 맞은 개수 : 117\nEpoch : 20, batch 1441\n(Train) Batch 1441 Loss : 0.4107408821582794, 맞은 개수 : 113\nEpoch : 20, batch 1442\n(Train) Batch 1442 Loss : 0.26792746782302856, 맞은 개수 : 115\nEpoch : 20, batch 1443\n(Train) Batch 1443 Loss : 0.25445690751075745, 맞은 개수 : 115\nEpoch : 20, batch 1444\n(Train) Batch 1444 Loss : 0.2065877765417099, 맞은 개수 : 121\nEpoch : 20, batch 1445\n(Train) Batch 1445 Loss : 0.4585345387458801, 맞은 개수 : 114\nEpoch : 20, batch 1446\n(Train) Batch 1446 Loss : 0.2972647249698639, 맞은 개수 : 119\nEpoch : 20, batch 1447\n(Train) Batch 1447 Loss : 0.26068201661109924, 맞은 개수 : 117\nEpoch : 20, batch 1448\n(Train) Batch 1448 Loss : 0.2743101418018341, 맞은 개수 : 116\nEpoch : 20, batch 1449\n(Train) Batch 1449 Loss : 0.4601711630821228, 맞은 개수 : 110\nEpoch : 20, batch 1450\n(Train) Batch 1450 Loss : 0.37092605233192444, 맞은 개수 : 114\nEpoch : 20, batch 1451\n(Train) Batch 1451 Loss : 0.3957066833972931, 맞은 개수 : 114\nEpoch : 20, batch 1452\n(Train) Batch 1452 Loss : 0.31687334179878235, 맞은 개수 : 114\nEpoch : 20, batch 1453\n(Train) Batch 1453 Loss : 0.31291353702545166, 맞은 개수 : 117\nEpoch : 20, batch 1454\n(Train) Batch 1454 Loss : 0.3975260853767395, 맞은 개수 : 114\nEpoch : 20, batch 1455\n(Train) Batch 1455 Loss : 0.189436674118042, 맞은 개수 : 120\nEpoch : 20, batch 1456\n(Train) Batch 1456 Loss : 0.3200753927230835, 맞은 개수 : 115\nEpoch : 20, batch 1457\n(Train) Batch 1457 Loss : 0.3514877259731293, 맞은 개수 : 111\nEpoch : 20, batch 1458\n(Train) Batch 1458 Loss : 0.31603115797042847, 맞은 개수 : 115\nEpoch : 20, batch 1459\n(Train) Batch 1459 Loss : 0.36612632870674133, 맞은 개수 : 115\nEpoch : 20, batch 1460\n(Train) Batch 1460 Loss : 0.3866996765136719, 맞은 개수 : 115\nEpoch : 20, batch 1461\n(Train) Batch 1461 Loss : 0.29418036341667175, 맞은 개수 : 116\nEpoch : 20, batch 1462\n(Train) Batch 1462 Loss : 0.2828344702720642, 맞은 개수 : 117\nEpoch : 20, batch 1463\n(Train) Batch 1463 Loss : 0.34357166290283203, 맞은 개수 : 114\nEpoch : 20, batch 1464\n(Train) Batch 1464 Loss : 0.2771283984184265, 맞은 개수 : 118\nEpoch : 20, batch 1465\n(Train) Batch 1465 Loss : 0.41521328687667847, 맞은 개수 : 111\nEpoch : 20, batch 1466\n(Train) Batch 1466 Loss : 0.2513009309768677, 맞은 개수 : 119\nEpoch : 20, batch 1467\n(Train) Batch 1467 Loss : 0.403690367937088, 맞은 개수 : 115\nEpoch : 20, batch 1468\n(Train) Batch 1468 Loss : 0.2982710301876068, 맞은 개수 : 116\nEpoch : 20, batch 1469\n(Train) Batch 1469 Loss : 0.3890970051288605, 맞은 개수 : 112\nEpoch : 20, batch 1470\n(Train) Batch 1470 Loss : 0.4126625955104828, 맞은 개수 : 113\nEpoch : 20, batch 1471\n(Train) Batch 1471 Loss : 0.32433557510375977, 맞은 개수 : 112\nEpoch : 20, batch 1472\n(Train) Batch 1472 Loss : 0.2731444537639618, 맞은 개수 : 118\nEpoch : 20, batch 1473\n(Train) Batch 1473 Loss : 0.3253447711467743, 맞은 개수 : 118\nEpoch : 20, batch 1474\n(Train) Batch 1474 Loss : 0.2982816696166992, 맞은 개수 : 117\nEpoch : 20, batch 1475\n(Train) Batch 1475 Loss : 0.40100306272506714, 맞은 개수 : 112\nEpoch : 20, batch 1476\n(Train) Batch 1476 Loss : 0.378020316362381, 맞은 개수 : 116\nEpoch : 20, batch 1477\n(Train) Batch 1477 Loss : 0.3476106822490692, 맞은 개수 : 116\nEpoch : 20, batch 1478\n(Train) Batch 1478 Loss : 0.2918124198913574, 맞은 개수 : 116\nEpoch : 20, batch 1479\n(Train) Batch 1479 Loss : 0.33389198780059814, 맞은 개수 : 110\nEpoch : 20, batch 1480\n(Train) Batch 1480 Loss : 0.26615530252456665, 맞은 개수 : 118\nEpoch : 20, batch 1481\n(Train) Batch 1481 Loss : 0.20383965969085693, 맞은 개수 : 120\nEpoch : 20, batch 1482\n(Train) Batch 1482 Loss : 0.39926180243492126, 맞은 개수 : 111\nEpoch : 20, batch 1483\n(Train) Batch 1483 Loss : 0.3707883059978485, 맞은 개수 : 111\nEpoch : 20, batch 1484\n(Train) Batch 1484 Loss : 0.24890145659446716, 맞은 개수 : 118\nEpoch : 20, batch 1485\n(Train) Batch 1485 Loss : 0.23261119425296783, 맞은 개수 : 119\nEpoch : 20, batch 1486\n(Train) Batch 1486 Loss : 0.3572404980659485, 맞은 개수 : 115\nEpoch : 20, batch 1487\n(Train) Batch 1487 Loss : 0.29263293743133545, 맞은 개수 : 117\nEpoch : 20, batch 1488\n(Train) Batch 1488 Loss : 0.2895551323890686, 맞은 개수 : 116\nEpoch : 20, batch 1489\n(Train) Batch 1489 Loss : 0.2814742922782898, 맞은 개수 : 116\nEpoch : 20, batch 1490\n(Train) Batch 1490 Loss : 0.332166463136673, 맞은 개수 : 112\nEpoch : 20, batch 1491\n(Train) Batch 1491 Loss : 0.2404605597257614, 맞은 개수 : 118\nEpoch : 20, batch 1492\n(Train) Batch 1492 Loss : 0.34355518221855164, 맞은 개수 : 115\nEpoch : 20, batch 1493\n(Train) Batch 1493 Loss : 0.3393559455871582, 맞은 개수 : 116\nEpoch : 20, batch 1494\n(Train) Batch 1494 Loss : 0.49085649847984314, 맞은 개수 : 107\nEpoch : 20, batch 1495\n(Train) Batch 1495 Loss : 0.2855806052684784, 맞은 개수 : 117\nEpoch : 20, batch 1496\n(Train) Batch 1496 Loss : 0.2444867342710495, 맞은 개수 : 114\nEpoch : 20, batch 1497\n(Train) Batch 1497 Loss : 0.4811587631702423, 맞은 개수 : 110\nEpoch : 20, batch 1498\n(Train) Batch 1498 Loss : 0.2133835107088089, 맞은 개수 : 119\nEpoch : 20, batch 1499\n(Train) Batch 1499 Loss : 0.3407227396965027, 맞은 개수 : 114\nEpoch : 20, batch 1500\n(Train) Batch 1500 Loss : 0.3300488293170929, 맞은 개수 : 117\nEpoch : 20, batch 1501\n(Train) Batch 1501 Loss : 0.23659953474998474, 맞은 개수 : 120\nEpoch : 20, batch 1502\n(Train) Batch 1502 Loss : 0.2778129279613495, 맞은 개수 : 118\nEpoch : 20, batch 1503\n(Train) Batch 1503 Loss : 0.26840516924858093, 맞은 개수 : 115\nEpoch : 20, batch 1504\n(Train) Batch 1504 Loss : 0.29938581585884094, 맞은 개수 : 117\nEpoch : 20, batch 1505\n(Train) Batch 1505 Loss : 0.27524787187576294, 맞은 개수 : 115\nEpoch : 20, batch 1506\n(Train) Batch 1506 Loss : 0.26369401812553406, 맞은 개수 : 118\nEpoch : 20, batch 1507\n(Train) Batch 1507 Loss : 0.42160478234291077, 맞은 개수 : 107\nEpoch : 20, batch 1508\n(Train) Batch 1508 Loss : 0.2848132252693176, 맞은 개수 : 116\nEpoch : 20, batch 1509\n(Train) Batch 1509 Loss : 0.40531304478645325, 맞은 개수 : 111\nEpoch : 20, batch 1510\n(Train) Batch 1510 Loss : 0.40300431847572327, 맞은 개수 : 115\nEpoch : 20, batch 1511\n(Train) Batch 1511 Loss : 0.24095505475997925, 맞은 개수 : 118\nEpoch : 20, batch 1512\n(Train) Batch 1512 Loss : 0.2837729752063751, 맞은 개수 : 118\nEpoch : 20, batch 1513\n(Train) Batch 1513 Loss : 0.2151220440864563, 맞은 개수 : 120\nEpoch : 20, batch 1514\n(Train) Batch 1514 Loss : 0.2908744812011719, 맞은 개수 : 118\nEpoch : 20, batch 1515\n(Train) Batch 1515 Loss : 0.17139659821987152, 맞은 개수 : 118\nEpoch : 20, batch 1516\n(Train) Batch 1516 Loss : 0.5078837275505066, 맞은 개수 : 113\nEpoch : 20, batch 1517\n(Train) Batch 1517 Loss : 0.4584202170372009, 맞은 개수 : 109\nEpoch : 20, batch 1518\n(Train) Batch 1518 Loss : 0.26253896951675415, 맞은 개수 : 115\nEpoch : 20, batch 1519\n(Train) Batch 1519 Loss : 0.3184089958667755, 맞은 개수 : 119\nEpoch : 20, batch 1520\n(Train) Batch 1520 Loss : 0.40139997005462646, 맞은 개수 : 117\nEpoch : 20, batch 1521\n(Train) Batch 1521 Loss : 0.27973952889442444, 맞은 개수 : 116\nEpoch : 20, batch 1522\n(Train) Batch 1522 Loss : 0.39674094319343567, 맞은 개수 : 110\nEpoch : 20, batch 1523\n(Train) Batch 1523 Loss : 0.20176462829113007, 맞은 개수 : 123\nEpoch : 20, batch 1524\n(Train) Batch 1524 Loss : 0.2702115476131439, 맞은 개수 : 114\nEpoch : 20, batch 1525\n(Train) Batch 1525 Loss : 0.33544352650642395, 맞은 개수 : 116\nEpoch : 20, batch 1526\n(Train) Batch 1526 Loss : 0.2921753525733948, 맞은 개수 : 119\nEpoch : 20, batch 1527\n(Train) Batch 1527 Loss : 0.254555881023407, 맞은 개수 : 121\nEpoch : 20, batch 1528\n(Train) Batch 1528 Loss : 0.37603768706321716, 맞은 개수 : 113\nEpoch : 20, batch 1529\n(Train) Batch 1529 Loss : 0.4011354446411133, 맞은 개수 : 113\nEpoch : 20, batch 1530\n(Train) Batch 1530 Loss : 0.2540201246738434, 맞은 개수 : 117\nEpoch : 20, batch 1531\n(Train) Batch 1531 Loss : 0.2497566044330597, 맞은 개수 : 113\nEpoch : 20, batch 1532\n(Train) Batch 1532 Loss : 0.24574333429336548, 맞은 개수 : 117\nEpoch : 20, batch 1533\n(Train) Batch 1533 Loss : 0.24625472724437714, 맞은 개수 : 118\nEpoch : 20, batch 1534\n(Train) Batch 1534 Loss : 0.4014185667037964, 맞은 개수 : 114\nEpoch : 20, batch 1535\n(Train) Batch 1535 Loss : 0.28205105662345886, 맞은 개수 : 118\nEpoch : 20, batch 1536\n(Train) Batch 1536 Loss : 0.31113553047180176, 맞은 개수 : 116\nEpoch : 20, batch 1537\n(Train) Batch 1537 Loss : 0.25396832823753357, 맞은 개수 : 117\nEpoch : 20, batch 1538\n(Train) Batch 1538 Loss : 0.21986471116542816, 맞은 개수 : 121\nEpoch : 20, batch 1539\n(Train) Batch 1539 Loss : 0.43662789463996887, 맞은 개수 : 111\nEpoch : 20, batch 1540\n(Train) Batch 1540 Loss : 0.36416128277778625, 맞은 개수 : 113\nEpoch : 20, batch 1541\n(Train) Batch 1541 Loss : 0.27023598551750183, 맞은 개수 : 118\nEpoch : 20, batch 1542\n(Train) Batch 1542 Loss : 0.2904410660266876, 맞은 개수 : 116\nEpoch : 20, batch 1543\n(Train) Batch 1543 Loss : 0.3700494170188904, 맞은 개수 : 116\nEpoch : 20, batch 1544\n(Train) Batch 1544 Loss : 0.3020654320716858, 맞은 개수 : 114\nEpoch : 20, batch 1545\n(Train) Batch 1545 Loss : 0.2534724771976471, 맞은 개수 : 118\nEpoch : 20, batch 1546\n(Train) Batch 1546 Loss : 0.27926477789878845, 맞은 개수 : 119\nEpoch : 20, batch 1547\n(Train) Batch 1547 Loss : 0.4107956290245056, 맞은 개수 : 113\nEpoch : 20, batch 1548\n(Train) Batch 1548 Loss : 0.26434484124183655, 맞은 개수 : 119\nEpoch : 20, batch 1549\n(Train) Batch 1549 Loss : 0.28578564524650574, 맞은 개수 : 115\nEpoch : 20, batch 1550\n(Train) Batch 1550 Loss : 0.34235644340515137, 맞은 개수 : 110\nEpoch : 20, batch 1551\n(Train) Batch 1551 Loss : 0.411705881357193, 맞은 개수 : 108\nEpoch : 20, batch 1552\n(Train) Batch 1552 Loss : 0.22086013853549957, 맞은 개수 : 118\nEpoch : 20, batch 1553\n(Train) Batch 1553 Loss : 0.45194360613822937, 맞은 개수 : 108\nEpoch : 20, batch 1554\n(Train) Batch 1554 Loss : 0.3248669505119324, 맞은 개수 : 114\nEpoch : 20, batch 1555\n(Train) Batch 1555 Loss : 0.323635458946228, 맞은 개수 : 114\nEpoch : 20, batch 1556\n(Train) Batch 1556 Loss : 0.24585966765880585, 맞은 개수 : 115\nEpoch : 20, batch 1557\n(Train) Batch 1557 Loss : 0.337822824716568, 맞은 개수 : 117\nEpoch : 20, batch 1558\n(Train) Batch 1558 Loss : 0.16168585419654846, 맞은 개수 : 120\nEpoch : 20, batch 1559\n(Train) Batch 1559 Loss : 0.2547578811645508, 맞은 개수 : 116\nEpoch : 20, batch 1560\n(Train) Batch 1560 Loss : 0.2762591540813446, 맞은 개수 : 119\nEpoch : 20, batch 1561\n(Train) Batch 1561 Loss : 0.3519624173641205, 맞은 개수 : 113\nEpoch : 20, batch 1562\n(Train) Batch 1562 Loss : 0.21513602137565613, 맞은 개수 : 61\nepoch 20 Loss/Train :0.2950662958053771 \nepoch 20 Accuracy/Train : 0.90904\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  0   0   0   0  46   0  47   0   0   0   0   0   0   0  13 114  45   0\n   0 196   3  36  39   0   0   0   0   0  39 186   0   0], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n(VAL) Batch 0 Loss : 1.200657606124878, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  0   0   0  14  69 184 186   0   0   0   0   0  45   0   0  47   0  36\n   7   1  16   1  40   1  51  19   1   1 153   1 143   1], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n(VAL) Batch 1 Loss : 1.8189799785614014, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [174   3   1   1   1   1 132  96 197   1   1   1   1  37   1   1   1   1\n   1   1   1  10   1   1   1   1   1   1   1  40   1   1], 정답 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n(VAL) Batch 2 Loss : 1.1499407291412354, accuracy: 0.75\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  1   1   1   1  16   2   2 175   2  43 199   2  72   2  46   2   2 160\n  43  15  50   2   4  33 175   2  84 185   2 199   3   3], 정답 [1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n(VAL) Batch 3 Loss : 2.2694461345672607, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 93 199   2   2   2 178   3 198   2   2  15 199   2   2  14 196   2 178\n  35 199   3   2   2   3 196  17   5   3  32   3   5  39], 정답 [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3]\n(VAL) Batch 4 Loss : 2.6772515773773193, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  7   3   4  51 192   5  40   5   3  33 150   2   3   2   6  61  78   3\n 104   2   3  10  89   3  44   3   2  90   3   5   2  41], 정답 [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n(VAL) Batch 5 Loss : 3.3740382194519043, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  3   3 190 199   0   1  30  56 132  81  69 107   2   4 197   4  39   4\n  29   4  32 162   2 194   4 194  46 195   4   4   4   4], 정답 [3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n(VAL) Batch 6 Loss : 3.6244328022003174, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 30  46   4   4  23   2 174 196 197  33 194   2  37   4  46   4 152   4\n 108  43  78  97   4   1   4   4   3  41 153  58  24   3], 정답 [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5]\n(VAL) Batch 7 Loss : 3.847744941711426, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [136   5  44  57  12  33   1   6  51 191 107 136 164  46  52  15  34 142\n   5 118  99   5   3  35   4  48  34  34  10  51   2  39], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n(VAL) Batch 8 Loss : 5.650916576385498, accuracy: 0.09375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 99   7  52  51 196 103  41  18   6 105   5 175   3   6  85   4  78   6\n   6   6   6   6  67 196   6   6   6   6  83   6   6  52], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\n(VAL) Batch 9 Loss : 3.131448745727539, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  6  19   6   6   6  35  42   6   6   6   6   6  94   6   6 152  26   3\n   6   9   6   6 195  67  66 190   6   6  57   6   7   3], 정답 [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7]\n(VAL) Batch 10 Loss : 1.9882804155349731, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  9   7   7   7 181  16   7   9   6   6 172  19  13   7   3   8 175  41\n 156   3  13   9   7   1  54  10   7   5  94   1   7 183], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n(VAL) Batch 11 Loss : 3.8608622550964355, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  9   7 192  10   7   7  58   8 196 119   7  42 137   7   7   7   8  38\n  43   8  75   8   8   8   8   8   8  41   8   7   7   8], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n(VAL) Batch 12 Loss : 2.815492630004883, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  8   8   8   8   8  49  55  37 130   8   8  95   9   8   8  37   7   8\n   8   8   8   7   8   8  43   8   8  95   8   9  37   8], 정답 [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n(VAL) Batch 13 Loss : 1.8362613916397095, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  8   1  30  91   9   9   9   9   8   9   2   9   1   8  31 103  52 195\n   8  28   5  46  17  17  19  33   7  31  20   9   9   9], 정답 [8 8 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\n(VAL) Batch 14 Loss : 3.560138702392578, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 84  36  33  91   8 189   9  19   7   9  30  67  33   9 119   9   7  18\n 125  82  36 108  10 166  10  10  44   7  10 116 109   1], 정답 [ 9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9 10 10 10 10\n 10 10 10 10 10 10 10 10]\n(VAL) Batch 15 Loss : 3.886967897415161, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  7 177  10 104 122  20  10  36 190  41  42  41  43  40  10 192  10 152\n  43  52  16  10  44  10  33   7   7  52  39 196 156 122], 정답 [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n 10 10 10 10 10 10 10 10]\n(VAL) Batch 16 Loss : 4.515926361083984, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 41 175 139  10  10   7  11  11  87  48  11  20  11  56  43  69  78  91\n  11  11  57  11  21  11  41  11  71  20  33  11  11  11], 정답 [10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n 11 11 11 11 11 11 11 11]\n(VAL) Batch 17 Loss : 2.504483461380005, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 11  11  51  51  20 114  35  11 174  96  78  11 195  37  11  11  11  55\n  35  69  11  11 122  11  12  12  12  54  26  12  12  12], 정답 [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n 12 12 12 12 12 12 12 12]\n(VAL) Batch 18 Loss : 2.453636646270752, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 31  75 181  16  12  12  27  12  12  12  12 105  12  84  12 111  31  54\n  12  12 105  12  12  57  12 183  11  12  12  12  12  49], 정답 [12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n 12 12 12 12 12 12 12 12]\n(VAL) Batch 19 Loss : 2.2291383743286133, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 12 178  12  12  12  34   5  12 136  22  13  46  13  13   4  80  13  13\n 113 196 187 178  13  23  13  12  13  13  72  13  13 196], 정답 [12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13 13 13 13 13\n 13 13 13 13 13 13 13 13]\n(VAL) Batch 20 Loss : 2.5088603496551514, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 13  13  13  13  13  13  72  13  92  23  13 187  13  13  13  13  13  13\n  46 148  13  13  13  23  13  13 122  13  88  14  14  14], 정답 [13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13\n 13 13 13 13 14 14 14 14]\n(VAL) Batch 21 Loss : 1.1296943426132202, accuracy: 0.71875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 14  14  14 196  14   0 196  14   2  14  14  12  23 149  46 196  14   6\n  46  43 183 196  14  14  14  23 196  17  14  14  30  62], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n 14 14 14 14 14 14 14 14]\n(VAL) Batch 22 Loss : 2.5562498569488525, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [187   6  14  14  14  14  14  46  14 196 169  14 196  14  15 188 175 185\n  92  48  15   2  10  15  24 193   7   2  32 187   6  81], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15\n 15 15 15 15 15 15 15 15]\n(VAL) Batch 23 Loss : 3.5777640342712402, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 41   1 192  30  39  15  10  29   5  33  13  34  17 102  15   2  55  15\n  60  15  15  15  15   7  16   1  37  58 186   2 183  39], 정답 [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n 15 15 15 15 15 15 15 15]\n(VAL) Batch 24 Loss : 4.176605224609375, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 51  16  45 122  10  16  44 102  16   4  93  69  85  16 178  16 192   9\n  54  29  17 177 185  39 192 185  10 179  83  16  16 130], 정답 [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n 16 16 16 16 16 16 16 16]\n(VAL) Batch 25 Loss : 4.825062274932861, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  4  43 185 190  33  40   6 190   3 156  15  52  27 185  55  45   3  13\n 178  17  17  19 172   6  17  16  17  17  58  19  13  17], 정답 [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 17 17 17 17 17 17\n 17 17 17 17 17 17 17 17]\n(VAL) Batch 26 Loss : 4.385500431060791, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 15  17  17  17 182  17 191 179  16 182  17  17  17   6 174  17 110  17\n  17  17  46  17 137  17 185 196  14  27  59  46  90 185], 정답 [17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17\n 17 17 17 17 17 17 17 17]\n(VAL) Batch 27 Loss : 2.626512050628662, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [181  17 185  17 189 192   7 190  77  18 153 163 114 150 190  18 181  45\n  18  18 186  18 176 190 158  18  18  82  18  18  99 177], 정답 [17 17 17 17 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18\n 18 18 18 18 18 18 18 18]\n(VAL) Batch 28 Loss : 3.714714527130127, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 18  18  18  18  46 130 189  18  18  75 153 158 186  18 112  18   2 163\n 168  18  30   7  19  91 196  19   5  26  19  19  53  19], 정답 [18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 19 19\n 19 19 19 19 19 19 19 19]\n(VAL) Batch 29 Loss : 3.225152015686035, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  9  19   1 196   2 190  19  19  19 191  46  19  19  19   7  19  41 196\n  19  19  19 195  19 174  46  19 176 177  19  13 185 196], 정답 [19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19\n 19 19 19 19 19 19 19 19]\n(VAL) Batch 30 Loss : 3.0847156047821045, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 19 179  19  19  19  18  19 196 183  20  43  20  20  20  11  20  20  20\n  20  55  11  22  43  11  46  84  55  20 152 168  57   9], 정답 [19 19 19 19 19 19 19 19 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20\n 20 20 20 20 20 20 20 20]\n(VAL) Batch 31 Loss : 2.916404962539673, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 54 114  22  37 139  11  20  20  56  20  20  20 122  20  20  20  11  20\n  29  52   8  20  20  84 122  16 114  35  21  22 105  21], 정답 [20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20\n 20 20 21 21 21 21 21 21]\n(VAL) Batch 32 Loss : 2.6575300693511963, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [182  79  21 198 148  21  84 187  21  21  21  21  21  21 105  21 174  23\n 118  21 100  21  21  21  21  21  21  21  21  21 194  21], 정답 [21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21\n 21 21 21 21 21 21 21 21]\n(VAL) Batch 33 Loss : 2.7324111461639404, accuracy: 0.625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 11  21  20  21  31  29  21  21  21  21 100  21  12  22  11  21  20  22\n  22  22  20  22  11  22  22  22  22  22  20  22  22  22], 정답 [21 21 21 21 21 21 21 21 21 21 21 21 22 22 22 22 22 22 22 22 22 22 22 22\n 22 22 22 22 22 22 22 22]\n(VAL) Batch 34 Loss : 2.035215377807617, accuracy: 0.625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 22  96  22  22  22  75  22  21  22  27  22  40  22  22 137  22  95  22\n  22  22  22  22  22 161  22  11  23  22  22  22  23  23], 정답 [22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22\n 22 22 22 22 22 22 23 23]\n(VAL) Batch 35 Loss : 1.97456955909729, accuracy: 0.6875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 23  23  23  23  23  23  23  23  23  23  23  23  23 148 194  23  23  23\n  23  23  23  23  23  23  23  23  23  23  55  23  22 113], 정답 [23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n 23 23 23 23 23 23 23 23]\n(VAL) Batch 36 Loss : 0.46237674355506897, accuracy: 0.84375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 23  37  22  13  23  23  23  23  23  23  23  23  23  23  23  23  51  24\n  24 125  24 161  35  32  31 134  31  75 156  65 199  62], 정답 [23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 24 24 24 24 24 24 24 24\n 24 24 24 24 24 24 24 24]\n(VAL) Batch 37 Loss : 2.549345016479492, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 26  10  74  31 119  24  24  31  27  76  24  26 175 141  11  31  26  26\n  75  25  31  69  24  28 161  20 127  83  29  48  51  18], 정답 [24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24\n 24 24 24 24 24 24 24 24]\n(VAL) Batch 38 Loss : 5.157874584197998, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 11  96  25  75  25  25 156  25  25  31  26  25  26  25  25 105  25 105\n  25  25  31 192  25  25  25  25  25  25  69  34  24  25], 정답 [24 24 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25\n 25 25 25 25 25 25 25 25]\n(VAL) Batch 39 Loss : 2.089442729949951, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 25 190  25  25  12 150  26  25  15  25  25  90  24  25  25  24  25 161\n  25 139  26  34  26  27  34  26  27  89  26  47  34  27], 정답 [25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 26 26 26 26\n 26 26 26 26 26 26 26 26]\n(VAL) Batch 40 Loss : 1.9725393056869507, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [124  27  33  41  49  57  26  31  26  25  26  12  26  26  24 148  35  33\n  26  31  27  26  26  26 193  34 122  35  26  11  34 100], 정답 [26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26\n 26 26 26 26 26 26 26 26]\n(VAL) Batch 41 Loss : 3.285367965698242, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 26  27  33  26  26  28  75  35  27 123  24  27  50  27  68  28 175  27\n  27  30  60  24  55  26 122  29  33  26  27  27  27  30], 정답 [26 26 26 26 26 26 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27\n 27 27 27 27 27 27 27 27]\n(VAL) Batch 42 Loss : 3.0454485416412354, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [170  28  29 135  20  27  58 164  33  55  22  29  61  57 155  58 168  26\n 110  25  26  27  26  57  27  54  28  53  28  24  71  28], 정답 [27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27\n 28 28 28 28 28 28 28 28]\n(VAL) Batch 43 Loss : 4.795660972595215, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 27  25 105 186  20  43  47  28  60  28 111  52  28  28 118  49  28  28\n  25 114 143  28  28 100  25  28 186  79  28  57   7  28], 정답 [28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28\n 28 28 28 28 28 28 28 28]\n(VAL) Batch 44 Loss : 4.926913738250732, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 28  24  56  28  28  28  25  28  52  33  29 114  16  27  50  55  24  11\n 157  24   3 107 147 105  84  77  26  27 107  27  29  58], 정답 [28 28 28 28 28 28 28 28 28 28 29 29 29 29 29 29 29 29 29 29 29 29 29 29\n 29 29 29 29 29 29 29 29]\n(VAL) Batch 45 Loss : 3.901067018508911, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 11  29  68  55  91  28  24 100  33  27  26  22 105  29  27  57  82  32\n  31  34  54 118  29  27  27  30  34  29  32  30  26  32], 정답 [29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29\n 29 29 29 29 30 30 30 30]\n(VAL) Batch 46 Loss : 4.59079122543335, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [144  51  31  30  31  63  32  26  98  30  30  12  29  30  30 189  33  31\n  32  30 161  32 150  31  52   9 105  31 124  32  30 103], 정답 [30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30\n 30 30 30 30 30 30 30 30]\n(VAL) Batch 47 Loss : 3.1836068630218506, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 32 172   3  54  30  32 193  30  34   3  32  25  31  71 141  31  32  30\n  31   6  98  31 105  31  31 144  29  31  31  52  32  31], 정답 [30 30 30 30 30 30 30 30 30 30 30 30 30 30 31 31 31 31 31 31 31 31 31 31\n 31 31 31 31 31 31 31 31]\n(VAL) Batch 48 Loss : 3.3131513595581055, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 31  29  30  31  31  35  31  31  31  37  31 137  31  25  31  31 156  31\n  31  31 160  31  31  31 179  32  31  24 174  31   9  29], 정답 [31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31\n 31 31 31 31 31 31 31 31]\n(VAL) Batch 49 Loss : 2.4344074726104736, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 30  32  31  46 102  30  32  55  97  89  32  29  27  30  27  32  32  56\n   4  32  32   4  32  29 182  91  25  75  32  34  32   5], 정답 [32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32\n 32 32 32 32 32 32 32 32]\n(VAL) Batch 50 Loss : 3.703504800796509, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 74 174  30  32  35 125  92  39 181 112  30  73  30  22  31  32 113  33\n  99  33  33  33  53  33  33  34  51  34  33  32  52  32], 정답 [32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 33 33 33 33 33 33\n 33 33 33 33 33 33 33 33]\n(VAL) Batch 51 Loss : 3.6522152423858643, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 12  33  34  91  34  12  91  34   5  33  33  37  55  33  91  33  33  33\n  35  31  51  48  34  91  33  32  91  33  23  46  33 195], 정답 [33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33\n 33 33 33 33 33 33 33 33]\n(VAL) Batch 52 Loss : 2.665036678314209, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 56  91  33  49  34  34  31  34  51  34  34  34  34  35  33 196  26  35\n  34  91 181  34  34  34  34  34  34  34  26  34  28 161], 정답 [33 33 33 33 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34\n 34 34 34 34 34 34 34 34]\n(VAL) Batch 53 Loss : 2.586012840270996, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 34  34  34  34  34  34  34  34  33  69  57  34  34 106  35  34  34  33\n  49  34  34   3  35  35  54 147  15  35  35  35  35 169], 정답 [34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 35 35\n 35 35 35 35 35 35 35 35]\n(VAL) Batch 54 Loss : 2.0315942764282227, accuracy: 0.625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 55  34  35  35  56  50  35  35  35  35  35  35  26  11  33  58  35  35\n  35  35  57 177 107  35  35 157  35  35  35   6  33  35], 정답 [35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35\n 35 35 35 35 35 35 35 35]\n(VAL) Batch 55 Loss : 1.6113512516021729, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 35  35  58  35  25  35  54  35  36  36  36  36  36  36  36  44  36 189\n  36  38  36  39  36  61 199  36  41  37  36  36  36  36], 정답 [35 35 35 35 35 35 35 35 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n 36 36 36 36 36 36 36 36]\n(VAL) Batch 56 Loss : 1.6185576915740967, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [155  36  36  37  77  36  36 194  36  36  36  36  82  38  36  42  36  36\n  36  36  31  36  36 188  36  36 117  10   8   1  37  24], 정답 [36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n 36 36 37 37 37 37 37 37]\n(VAL) Batch 57 Loss : 2.8279905319213867, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 42 122  37  37  37  37 182  37  37  43 199  37  38  38  37 197  37 182\n  37  36   8 184   8   4  37  43  37  38  37  37  38  37], 정답 [37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37\n 37 37 37 37 37 37 37 37]\n(VAL) Batch 58 Loss : 2.87827730178833, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 37   8  31  37  37  37  37  37  37  37  37  20 183  36  38  38  38  38\n  45  41  38 181  38   9  38 199  38  38  38  38  37  38], 정답 [37 37 37 37 37 37 37 37 37 37 37 37 38 38 38 38 38 38 38 38 38 38 38 38\n 38 38 38 38 38 38 38 38]\n(VAL) Batch 59 Loss : 1.6224807500839233, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [178  38  43 122  38  38  43  38  38  38  38  38  38  38  50 186  38  38\n  38 187  68  38 187  38  38  45  39  44 187  43  39  37], 정답 [38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38\n 38 38 38 38 38 38 39 39]\n(VAL) Batch 60 Loss : 2.4160168170928955, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 43 149 149 191   1 199  39  39  37  40  38  39 178  39 199  42  39 188\n   3  40  40  36 125  39  40  39  39  42 141  42  39  11], 정답 [39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39\n 39 39 39 39 39 39 39 39]\n(VAL) Batch 61 Loss : 3.0813724994659424, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 42  42  39  42  39  40  39  39  42  37  39 175 184  39  40  43 118  77\n 175  45  40 180 149  42 199  40 137  43  40  40 152  40], 정답 [39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 40 40 40 40 40 40 40 40\n 40 40 40 40 40 40 40 40]\n(VAL) Batch 62 Loss : 3.090625047683716, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 42  88 199  40 179  40  99  40  40   5  42 140 186  40  43  42  40  39\n 192  40  41  40  10  40  40  40  39 184 199  40  42  64], 정답 [40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n 40 40 40 40 40 40 40 40]\n(VAL) Batch 63 Loss : 2.9211318492889404, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [118 189   8 159   8  41  41  31  41  41   1 115 162  41  27  41 130   7\n  15  85  41  41  37   8 187  36  41  41 158   1  41  41], 정답 [40 40 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n 41 41 41 41 41 41 41 41]\n(VAL) Batch 64 Loss : 3.4754085540771484, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 35 187  39  55  41 175 122  44 198   8  43  89 144   7  28  43  10  33\n  39   9  43  42  39   3 187 148  11  40  37 113  38  42], 정답 [41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 42 42 42 42\n 42 42 42 42 42 42 42 42]\n(VAL) Batch 65 Loss : 4.7453742027282715, accuracy: 0.09375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 42 187  42  45  42  39  10  42  33  42  64  42  42  40  16 175  52  42\n   2  42  98  42  42  42  40 144  22 184  10 152  42  37], 정답 [42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42\n 42 42 42 42 42 42 42 42]\n(VAL) Batch 66 Loss : 2.754148244857788, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 42  42   8  42  42  42  43  36  43  39 118  39 105  43  43  43  43  43\n  31  44  42  10  43  43  42  43  13 178  40 127 152  15], 정답 [42 42 42 42 42 42 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43\n 43 43 43 43 43 43 43 43]\n(VAL) Batch 67 Loss : 2.403209924697876, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 43   3  42  43  43  43 184  43  43  43  37  43  45  36  42 125  43 188\n 187  43  40  43  39   8  44  44  44 152  44 177  44  44], 정답 [43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43\n 44 44 44 44 44 44 44 44]\n(VAL) Batch 68 Loss : 2.3486874103546143, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 44  44  18  44  44  45  44  44  44  44  44  44  44  44  44  44  44  44\n  44  44  44 199  44  44  44  44  39 199  44  44  44  44], 정답 [44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44\n 44 44 44 44 44 44 44 44]\n(VAL) Batch 69 Loss : 0.9364312887191772, accuracy: 0.84375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 44  44  13  44  44  44 101  44  44 139 183  45 187  52  45  45  45 188\n  45  38  45  45  45  45  45  45  45  45  45  45   7  45], 정답 [44 44 44 44 44 44 44 44 44 44 45 45 45 45 45 45 45 45 45 45 45 45 45 45\n 45 45 45 45 45 45 45 45]\n(VAL) Batch 70 Loss : 1.1972943544387817, accuracy: 0.71875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [182  45  45  45  45  36  45  45  45  45  45  45  45  93  45  45  38  45\n  45 188  45  45  45  45  45  45  45  45  14 148 183  14], 정답 [45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45\n 45 45 45 45 46 46 46 46]\n(VAL) Batch 71 Loss : 1.1389667987823486, accuracy: 0.71875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 46  18   2  46  46  46  46  58  46 199 194  46  46 196 185 149  46 152\n  13  46  46  46  46  46  16  16  46  46  14  46  46  46], 정답 [46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46\n 46 46 46 46 46 46 46 46]\n(VAL) Batch 72 Loss : 2.0314998626708984, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [196  46  46  46  88  14   4  46 148  14  46  46   3  46  31 174 129  58\n  47  31  47  26  47  47  47  47  47  58  47  58  51  47], 정답 [46 46 46 46 46 46 46 46 46 46 46 46 46 46 47 47 47 47 47 47 47 47 47 47\n 47 47 47 47 47 47 47 47]\n(VAL) Batch 73 Loss : 2.467637300491333, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 47  41  48 134  47  47 186  47 190  47  47  47  15  31  80 181  31  24\n   3  47  50 179  47  46  32  47  47 126  47  52 130 192], 정답 [47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47\n 47 47 47 47 47 47 47 47]\n(VAL) Batch 74 Loss : 4.126985549926758, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  6 114  35  46 185  55 119 192  34  49  49  50  30  55  55  82  50  82\n 119  35  31  11  48  50  29  49 195  50  48  48 176  23], 정답 [48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48\n 48 48 48 48 48 48 48 48]\n(VAL) Batch 75 Loss : 4.9845290184021, accuracy: 0.09375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [104  53 100  48 116  48  55  48  29  32   4  35 133  48  57  33   7  48\n  21  57  66  49  50  49  66  49  33 171  22 105  52  88], 정답 [48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 49 49 49 49 49 49\n 49 49 49 49 49 49 49 49]\n(VAL) Batch 76 Loss : 4.1884565353393555, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 49  49  49  55  50  49  50  84  54  53  49  49  49 111  54  32  35  76\n  27  49  55  26  65 105 194  66 110  52 145  55  54  23], 정답 [49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49\n 49 49 49 49 49 49 49 49]\n(VAL) Batch 77 Loss : 3.457794427871704, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 27  58  65  49  50  50  48  50  50  53 196 105  50  55  50  49  50  50\n  50  50 138  50  57   6  50  49  50  54  50  50  50  57], 정답 [49 49 49 49 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n 50 50 50 50 50 50 50 50]\n(VAL) Batch 78 Loss : 2.040253162384033, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 57  50 195  50  50  50  50  50  50  50  50  50 170  50  50  50  50  50\n  50  91  50  50  51  33  35  33  34 194  33  51 196  53], 정답 [50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 51 51\n 51 51 51 51 51 51 51 51]\n(VAL) Batch 79 Loss : 1.8248459100723267, accuracy: 0.625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 12  11  24  51  51  28   4  56  51  51 195  58  48  35  53  33  51  51\n 197  56  51  96  50  51  51  20  33 161  38  52  51  51], 정답 [51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51\n 51 51 51 51 51 51 51 51]\n(VAL) Batch 80 Loss : 3.3067588806152344, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [194  20  50  33  51  51  50  91 136  52  52 192  52 182  52  52  74  52\n  52  52  52  52  33  52  52  52  52  52 113  51  52  57], 정답 [51 51 51 51 51 51 51 51 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52\n 52 52 52 52 52 52 52 52]\n(VAL) Batch 81 Loss : 2.1737794876098633, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 52 114  52  52  52  50  28  52  52  52  52  74  52  33 198  52   8  71\n  52  52 185  52  52  91  49  52  34  52  52 179  57  53], 정답 [52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52\n 52 52 53 53 53 53 53 53]\n(VAL) Batch 82 Loss : 2.722907066345215, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 53 194  53  53  55  67  29  53 158  53  80  53 102  48  53  53  71  52\n  53  42  57  53 143 163  53  57  53 132  26  57 163  53], 정답 [53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53\n 53 53 53 53 53 53 53 53]\n(VAL) Batch 83 Loss : 3.4172892570495605, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 74  53  53 107  53  53  66  53 158  35  53  31  54  54  54 169  54  57\n  54  25  54 104  56  35  54  54  54  54  54  26  54  54], 정답 [53 53 53 53 53 53 53 53 53 53 53 53 54 54 54 54 54 54 54 54 54 54 54 54\n 54 54 54 54 54 54 54 54]\n(VAL) Batch 84 Loss : 2.6344873905181885, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 54  54  47  23 171  55   2  46 148  54 141  58  54  54  50 120  54  55\n  75  56  69  25  55  54  28  50  55 150  34  54  55  55], 정답 [54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54\n 54 54 54 54 54 54 55 55]\n(VAL) Batch 85 Loss : 3.752629280090332, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 55  55  55  55 114  55  32  23  55  55  55 109  33  55  55  55  55  55\n  67  55  58  55  55  55  55   2  48  55 138  55  12 105], 정답 [55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55\n 55 55 55 55 55 55 55 55]\n(VAL) Batch 86 Loss : 1.8180793523788452, accuracy: 0.625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 23  65  54  55  27  97  55  46  55  29  28  55 110  55  55  55  34  55\n  55  56 198  48  49  56  56  56  31  56  56  52  23  56], 정답 [55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 56 56 56 56 56 56 56 56\n 56 56 56 56 56 56 56 56]\n(VAL) Batch 87 Loss : 3.2751784324645996, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 55  12  34  56  23 199  56  53  52  48  35  29  55  57  56  48  29  55\n 179  12  57  54  15 124  56 161  56  91  47  51  50  34], 정답 [56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56\n 56 56 56 56 56 56 56 56]\n(VAL) Batch 88 Loss : 4.148833751678467, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 55 102 119  48  35  50  11  57 170   6  49  57  57  56 175 199  57 128\n  57 146  57  57  49  61 140  57  57  57 175  57  51 119], 정답 [56 56 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57\n 57 57 57 57 57 57 57 57]\n(VAL) Batch 89 Loss : 2.5092296600341797, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 57  50  57  57  57  57  57  57  57 129  57  57  58  57  57  57  49  57\n 194  51 111  58  58  58  58  58  58 167  58   2  26  58], 정답 [57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 58 58 58 58\n 58 58 58 58 58 58 58 58]\n(VAL) Batch 90 Loss : 1.802581548690796, accuracy: 0.6875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 58 111  56  58  58  58  58  58  58  58  58  58  77 174  58 199  54  58\n  58  55  58  58  83  58  58  58  47  58  25  58  58  58], 정답 [58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58\n 58 58 58 58 58 58 58 58]\n(VAL) Batch 91 Loss : 1.477122187614441, accuracy: 0.6875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 58 191  58  58  58  58 146  61  59  28 188  59 145 148  59 130  92  73\n  41  59   0  59 180 140 188 116  24  59  81  59  59 186], 정답 [58 58 58 58 58 58 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59\n 59 59 59 59 59 59 59 59]\n(VAL) Batch 92 Loss : 3.253150463104248, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [171  59 113 191  74 129 135 128 140 126 167  59  82 102 125  92 109  59\n  59  65  70 144  59  93 138  60  60 150  92  60  60 168], 정답 [59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59\n 60 60 60 60 60 60 60 60]\n(VAL) Batch 93 Loss : 3.4938740730285645, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [139  60  60  49 143  54  60  60  60  60  60  60  60  60  60 143 169  60\n 100  60 155  60  60  60  60  60 106  60  27  60  60 105], 정답 [60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60\n 60 60 60 60 60 60 60 60]\n(VAL) Batch 94 Loss : 1.8420946598052979, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 63  60  60  60  60  65  60  60  60  60  61 152 126  61  97  61  61  59\n 161 172  61  61  61  61  61 128  61  61  61  93  61  89], 정답 [60 60 60 60 60 60 60 60 60 60 61 61 61 61 61 61 61 61 61 61 61 61 61 61\n 61 61 61 61 61 61 61 61]\n(VAL) Batch 95 Loss : 1.7280809879302979, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [147  61  61  65  93  89 126  61 126  61  61  61 129  61  61  67  61  68\n 135  61  61  83  58 142  61  61  61  61  90  62  62  93], 정답 [61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61\n 61 61 61 61 62 62 62 62]\n(VAL) Batch 96 Loss : 2.2811853885650635, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [134  92 149 136  90  62  64 120 130  62 149  62 155  62 134  62  89  61\n 110 169 141  90 184 151  62 185  62  62 149 158  85  90], 정답 [62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62\n 62 62 62 62 62 62 62 62]\n(VAL) Batch 97 Loss : 3.683248996734619, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 62 151  62  74  77 149 105 144 190  62  90  85 128 146 143 100 167  72\n  63 139  63  90  71 127 149 138 149 107  63  87  29  90], 정답 [62 62 62 62 62 62 62 62 62 62 62 62 62 62 63 63 63 63 63 63 63 63 63 63\n 63 63 63 63 63 63 63 63]\n(VAL) Batch 98 Loss : 4.359748363494873, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 63 138  90 127  90  21 179  40  41  92  60 154 155  63  63 138 149  60\n 127 137  63  87 103 157  94 143 105  55 133 173 193 165], 정답 [63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63\n 63 63 63 63 63 63 63 63]\n(VAL) Batch 99 Loss : 4.359135150909424, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [180 142  40 154 105 151 169 140 151  64 194 117  64  84  81  86  72  64\n  27  84 187  21   6 157  26 163  64  64 126 193  96 113], 정답 [64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n 64 64 64 64 64 64 64 64]\n(VAL) Batch 100 Loss : 4.7875847816467285, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 36  64 118  36 160  87 167 124  99 113  91 113  22  82  98  65  68  85\n  86 165  65  82 164  75 146 199  65 182 110 139 105  65], 정답 [64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 65 65 65 65 65 65\n 65 65 65 65 65 65 65 65]\n(VAL) Batch 101 Loss : 5.928689956665039, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 65 108 142 134  97 128 176 146  87 139  65  65  65 104  86 156 190 114\n  86 128  65 171  98  65 136  59 112 123 120 129  65  65], 정답 [65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65\n 65 65 65 65 65 65 65 65]\n(VAL) Batch 102 Loss : 3.9796195030212402, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [128 136 146 140 170  50  66  84  75 171  69 132 164  66 121  66 102  66\n  49  66 103  66  66  92  66  66 170 165 170  66  66  66], 정답 [65 65 65 65 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66\n 66 66 66 66 66 66 66 66]\n(VAL) Batch 103 Loss : 3.1994452476501465, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 66 154  66  50 170 103 175  66 128  66 102  66  67 165 195 170  66  66\n  66 164  66 197 132  85  61 193  67  67  34 139  37 153], 정답 [66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 67 67\n 67 67 67 67 67 67 67 67]\n(VAL) Batch 104 Loss : 3.529836654663086, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [135 120 150  67 172  59 112 123  91  61  81 160  74  67 167 190  67 109\n  98 108  89 134 140  82  65  67  48  89  80 153 167 196], 정답 [67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67\n 67 67 67 67 67 67 67 67]\n(VAL) Batch 105 Loss : 5.52620267868042, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [106 146 160 140 161 195 141 140 163  68 145  68  60  68  68 143 171 138\n 171 171  68 111  68  68  68  92  68 171  68  68  32 189], 정답 [67 67 67 67 67 67 67 67 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68\n 68 68 68 68 68 68 68 68]\n(VAL) Batch 106 Loss : 3.502323865890503, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 60 106  73  61 171  68 112 100  77  68  68  69  68 171  68  68 171  65\n  68  68 191 193  68  68 138  68 193  69  69  23  69  69], 정답 [68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68\n 68 68 69 69 69 69 69 69]\n(VAL) Batch 107 Loss : 2.5879170894622803, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 69  69 120 102  26 116  90 156  69  69 175  69 147  69 131 102 130  27\n  69 148 158  97  69  86  84 119  18 113  88  69  79  69], 정답 [69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69\n 69 69 69 69 69 69 69 69]\n(VAL) Batch 108 Loss : 3.944124937057495, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [159  69 145 153 174 169  10  69  71  79 183  88 153 194  70 111  70  70\n  70 108  70  70  84  94 107  70  70  87 153 133 133  70], 정답 [69 69 69 69 69 69 69 69 69 69 69 69 70 70 70 70 70 70 70 70 70 70 70 70\n 70 70 70 70 70 70 70 70]\n(VAL) Batch 109 Loss : 3.3848490715026855, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [153 116  70  70  94  94  70  70 133  70  70  70 153 133  94  94  71 133\n 141  94 116  78  81 128  95  82 121  70 164 121  71  71], 정답 [70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70\n 70 70 70 70 70 70 71 71]\n(VAL) Batch 110 Loss : 3.4173789024353027, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 71 101 138 137  71  71  71 173  71  71  71  71  71 158 101 154  71  71\n  71 197 137  71 124 173  71  71  71  71 173  71  71  71], 정답 [71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71\n 71 71 71 71 71 71 71 71]\n(VAL) Batch 111 Loss : 1.1636362075805664, accuracy: 0.625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 71 101 197  91  71  71 194 124  71 198  71  71  71  33  71 197  72  72\n  83 183  72  79  72  83 130 189  72 125 182 130  24  72], 정답 [71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 72 72 72 72 72 72 72 72\n 72 72 72 72 72 72 72 72]\n(VAL) Batch 112 Loss : 3.6914048194885254, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 72  86 180  62 149 135  72  72  72  72  72 128 155 124  72 155  58  72\n 167 175 171  83  90 108  72  86 186 130 110  72 123  72], 정답 [72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72\n 72 72 72 72 72 72 72 72]\n(VAL) Batch 113 Loss : 4.305342197418213, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 72  90 135  73  73  73 149  21  73  62 135 135 112 135  73  90  73 135\n  73  89  73 171 137 184 178  73 169  73 135  22  73  73], 정답 [72 72 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73\n 73 73 73 73 73 73 73 73]\n(VAL) Batch 114 Loss : 3.333693742752075, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 73 128 118  62  13 120 153  73  73  73 113 163 139 112  90  73 142  65\n   7  73  74  74 158 158  47 158  74  74  74 156 182  74], 정답 [73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 74 74 74 74\n 74 74 74 74 74 74 74 74]\n(VAL) Batch 115 Loss : 3.456477403640747, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 74  74 158  68  60 148 156  74  69 105  74  74 158  12  74  74 158  74\n  74 144 158 158  74 163 179  74  68  74  74  74 102  99], 정답 [74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74\n 74 74 74 74 74 74 74 74]\n(VAL) Batch 116 Loss : 2.4159529209136963, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 74  74  80  33  69 123  75  77  21 110 105 150  67  75  73 124 173 106\n 139 144 121 184 141 170 156 120 153 127  75  75 106  65], 정답 [74 74 74 74 74 74 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75\n 75 75 75 75 75 75 75 75]\n(VAL) Batch 117 Loss : 3.8317666053771973, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [177  47 104 141 139  73 158 147  75 184 155   5  60  26 113  21  75  64\n 109  71  55  75  67  75 178  10   6  21 114 137  32  64], 정답 [75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75\n 76 76 76 76 76 76 76 76]\n(VAL) Batch 118 Loss : 4.4146552085876465, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 76  88  91  90  78  90 139  67  51  20  53  86 172  66  54 112 127 146\n  12 129  76  16  86  63 196  95  72  55 162 159 199 103], 정답 [76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76\n 76 76 76 76 76 76 76 76]\n(VAL) Batch 119 Loss : 4.627471923828125, accuracy: 0.0625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [165  54  39  54 122 124 129  75  76 154 179  77 167  60 128  77 123 113\n 123  25 125 138  77  72  62 156  77 123  22  27  55 118], 정답 [76 76 76 76 76 76 76 76 76 76 77 77 77 77 77 77 77 77 77 77 77 77 77 77\n 77 77 77 77 77 77 77 77]\n(VAL) Batch 120 Loss : 4.7970075607299805, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [113 120  48  41  25  60 138 180  77 163 116  59 134  77  85  21  77  83\n 126 151  62 132 104 158 181 101 144  99  78  12  78  78], 정답 [77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77\n 77 77 77 77 78 78 78 78]\n(VAL) Batch 121 Loss : 5.075199127197266, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 78  78 194  78  78 128  67  78  78  78  78 117  78  78 157  78  78  78\n  78  78  78  78  78  67  78  78  78  78 146  78  78  78], 정답 [78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78\n 78 78 78 78 78 78 78 78]\n(VAL) Batch 122 Loss : 1.5673279762268066, accuracy: 0.78125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 78  78 155  79  78  78  78  86  78  78  30  78  78  78  88 104 138  83\n 167 107 138  79 100 120 127  10  43 151  79 116  72 139], 정답 [78 78 78 78 78 78 78 78 78 78 78 78 78 78 79 79 79 79 79 79 79 79 79 79\n 79 79 79 79 79 79 79 79]\n(VAL) Batch 123 Loss : 4.058304786682129, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 90 100 151   0  79  33  79 124 110 113 157  82   6 172  79  46 111 122\n  79   6  40  79 157 109 144 189 132 100  79 113  62  79], 정답 [79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79\n 79 79 79 79 79 79 79 79]\n(VAL) Batch 124 Loss : 5.636077404022217, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 93 148 128 160  72  31  96 153 122  80  80  58 184 155  73 135  75  48\n 187  67 184 128  11 175 176  80  69 157 102  92 112  23], 정답 [80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80\n 80 80 80 80 80 80 80 80]\n(VAL) Batch 125 Loss : 5.4736809730529785, accuracy: 0.09375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [102 177 184 140  52 146  85  80  36  69  72  72  76 172 130  55  87 161\n  81  98  81  81 133 156  81 153  81  92  81  81  81  81], 정답 [80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 81 81 81 81 81 81\n 81 81 81 81 81 81 81 81]\n(VAL) Batch 126 Loss : 4.480278968811035, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 81 194  81  81  81  81 110  81  81  81  81  81  81 185  81  81  81  96\n  81  81  81 121 151  81  81 170 149 100  81  81  81  81], 정답 [81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81\n 81 81 81 81 81 81 81 81]\n(VAL) Batch 127 Loss : 2.0912294387817383, accuracy: 0.71875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 81  81  81 142 189 192  82  82  82  82  82  82  73  82  82  93  82  82\n 107 151  82  82  82  91 161 146 124  65 120  82  82  82], 정답 [81 81 81 81 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82\n 82 82 82 82 82 82 82 82]\n(VAL) Batch 128 Loss : 2.3967175483703613, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 82  73 190  82  82  82  90 191  18  82  24  82  82  83  61 146  82  82\n  93  82 192  82  98  83  99  77  83  83  72 172 163  85], 정답 [82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 83 83\n 83 83 83 83 83 83 83 83]\n(VAL) Batch 129 Loss : 2.9532222747802734, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 73 100 149 140  59  83 109 112 112  83 179  83 141 180 123  99  83  83\n 186  74  17  83 104  99  83  83 161  83  83 160 182 163], 정답 [83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83\n 83 83 83 83 83 83 83 83]\n(VAL) Batch 130 Loss : 3.743255138397217, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 83 150  89 123   0  83  73  89 147  84   7 110 101  49  78  84 194  84\n 194 156 194   4  84  84 124 124  84 113 153  81 195 117], 정답 [83 83 83 83 83 83 83 83 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84\n 84 84 84 84 84 84 84 84]\n(VAL) Batch 131 Loss : 4.276984214782715, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 84  27  84  84  49 111  84 124  29  95  84   5 119 142 137 161  84 137\n  55 107  84  84 148  49  84  88 169 180  14  85  85 123], 정답 [84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84\n 84 84 85 85 85 85 85 85]\n(VAL) Batch 132 Loss : 3.747142791748047, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 85  85 149 102 113  85  85  71  85  90   6  85  85  60 134 172  85  85\n 134  62 119 169 186  77 169  85  62  85 105 134 128  85], 정답 [85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85\n 85 85 85 85 85 85 85 85]\n(VAL) Batch 133 Loss : 2.5400891304016113, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [158 119  85  85 149  85 120  63  85  85  85 195  86  92 175 138  87 128\n  62  86  86  61  86   8  86 138  86  86 106 193  86 146], 정답 [85 85 85 85 85 85 85 85 85 85 85 85 86 86 86 86 86 86 86 86 86 86 86 86\n 86 86 86 86 86 86 86 86]\n(VAL) Batch 134 Loss : 3.0814812183380127, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 87 133 128  80  86  86  86  86 167  86  60  86 128  71  86  86  78  86\n 128  65  86  86  86 128  86 110 140 128  72  86  97  67], 정답 [86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86\n 86 86 86 86 86 86 87 87]\n(VAL) Batch 135 Loss : 2.336089849472046, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 87  92  87  60  87  72 126  87  87 127  90  92  92  75  87 136 110  87\n  68  87  87 155 151 155  73  87  87 110 126  47 149 174], 정답 [87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87\n 87 87 87 87 87 87 87 87]\n(VAL) Batch 136 Loss : 2.940206289291382, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [150 171  87  87  71  89  92  87  70 160 167  92  87 172  73  87  42 148\n 168  88  88 130  88  21 143 126 174 187  47  59  88  71], 정답 [87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 88 88 88 88 88 88 88 88\n 88 88 88 88 88 88 88 88]\n(VAL) Batch 137 Loss : 4.3641886711120605, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 40  88  10  88 105 161 190 153 141 196 152 181 156  95  97 109 141  43\n  72 107  88  36 109 127  40  84 178 113  88  22 105  11], 정답 [88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88\n 88 88 88 88 88 88 88 88]\n(VAL) Batch 138 Loss : 4.494780540466309, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [156  11  67  89 178  89  89  89  98 167  89 181 151 145  78  67  62 151\n  13 193  52  89 190 149 140 110  89  83  64  89 195 192], 정답 [88 88 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89\n 89 89 89 89 89 89 89 89]\n(VAL) Batch 139 Loss : 4.245830535888672, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 89 137  70  89  89  89 167 193  89  89  80 162   4  89  86  89  89  89\n 104 161 100 184  90  90 184  90  62  90  90  90  90  90], 정답 [89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 90 90 90 90\n 90 90 90 90 90 90 90 90]\n(VAL) Batch 140 Loss : 2.532346487045288, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [144  90  90  90  90 149  73 130  90  32 105  90 149 149  90  18  78  90\n  85  90  90  90  62 137 184  90 169  90  90 168  90  90], 정답 [90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90\n 90 90 90 90 90 90 90 90]\n(VAL) Batch 141 Loss : 2.091728925704956, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [131  90 191 149  60 149  89  91 136  91   8  91 195  91  91  91  30  91\n  70  96  91  91  91 195  91  91  91  33 171  91  61  91], 정답 [90 90 90 90 90 90 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91\n 91 91 91 91 91 91 91 91]\n(VAL) Batch 142 Loss : 2.48414945602417, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 83  91  91  89 165  91  91  91  91 106  91  91  91   4  11  17  91 142\n  91 119 102  91   6  91 193 159  87 144 175 104 159  97], 정답 [91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91\n 92 92 92 92 92 92 92 92]\n(VAL) Batch 143 Loss : 2.7823784351348877, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [125 145   4  92 110  96 100 184  52 171  83 134 102 147 130  86 188 102\n 187  69  92 146  92  87  92 193 134  31 198 113  59 188], 정답 [92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92\n 92 92 92 92 92 92 92 92]\n(VAL) Batch 144 Loss : 4.381519317626953, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 99  92  83  87  92  92  92 155  92  92 196  62  93   4  93  93  82 196\n  93 140  93 115  93  93  93  93  93  93 126  93  82 171], 정답 [92 92 92 92 92 92 92 92 92 92 93 93 93 93 93 93 93 93 93 93 93 93 93 93\n 93 93 93 93 93 93 93 93]\n(VAL) Batch 145 Loss : 1.9835067987442017, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [112 150  93  93  97 164 178  93  59  93   0  82  93  93  93  74  25 191\n  61 146  93  10  82  65   7  93  90 190  70  70 174  69], 정답 [93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93\n 93 93 93 93 94 94 94 94]\n(VAL) Batch 146 Loss : 3.821719169616699, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 70 153  94  94 107  70  70  94 153  94  94  70 133  70  94 145  70  94\n  94 166 153  94  94 116  94 116 133  87 153  70  94  94], 정답 [94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94\n 94 94 94 94 94 94 94 94]\n(VAL) Batch 147 Loss : 2.4041616916656494, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [155  87  94  94  35 140  87  70 116  94  94 133  70 153 146  95  95  92\n 197 132 132  95 125  30 194 118 154 157  95 137  78  95], 정답 [94 94 94 94 94 94 94 94 94 94 94 94 94 94 95 95 95 95 95 95 95 95 95 95\n 95 95 95 95 95 95 95 95]\n(VAL) Batch 148 Loss : 3.4940085411071777, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 95 124 173  71  95 198 164 101  95  95 132  71 165 112 149  82 145 194\n 101 115 157 144  95 154  88  95  83  95  71  95  88 146], 정답 [95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95\n 95 95 95 95 95 95 95 95]\n(VAL) Batch 149 Loss : 3.573538064956665, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 69  96 108  96  96  96  96 170  96  96  96  96 198  96 115  96 198  75\n  96 197 194 194  96 154 198  96  55 145  96  31  84  51], 정답 [96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96\n 96 96 96 96 96 96 96 96]\n(VAL) Batch 150 Loss : 2.2900686264038086, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [157 167 195 197  41  72 197  96  31  96  96  96  96  94  96  87 154  96\n 113  97  68 146  87 167 185  25 115  97  97 110  97 140], 정답 [96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 97 97 97 97 97 97\n 97 97 97 97 97 97 97 97]\n(VAL) Batch 151 Loss : 3.6510608196258545, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [133 145  97  97 133  97  62  97  72  97  97  97  97 156  87  97  97  97\n  97  97 191  97  55 153  89 100  97  97  97  98  97 100], 정답 [97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97\n 97 97 97 97 97 97 97 97]\n(VAL) Batch 152 Loss : 2.533531665802002, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 97 138  97  97  98 102  79  98 147  72  98  98  98  98  97  98 128  86\n  70  98 192  67  98 167  98  80  98 142  86 146  98  98], 정답 [97 97 97 97 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98\n 98 98 98 98 98 98 98 98]\n(VAL) Batch 153 Loss : 3.053118944168091, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [113  97  65  98  98  98  98 142  98 140  97  68 126  98  97  98  98 174\n 177  98  98  98  99  99 110 102 107 181  87  99  39 125], 정답 [98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 99 99\n 99 99 99 99 99 99 99 99]\n(VAL) Batch 154 Loss : 2.888796091079712, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [147  60  99 111 169  82 144  60 113 161 149 126 175 175 159 131 146  98\n 143  57 171 124  22 171 141 119  94  39  99 113 163  72], 정답 [99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99\n 99 99 99 99 99 99 99 99]\n(VAL) Batch 155 Loss : 4.66195011138916, accuracy: 0.0625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 99  99 181 188  59 125 198  99 100 158  74  60 100  60 149 100  60  24\n 160 125 107  86 193   9  83 175 171 195  24 100 136 136], 정답 [ 99  99  99  99  99  99  99  99 100 100 100 100 100 100 100 100 100 100\n 100 100 100 100 100 100 100 100 100 100 100 100 100 100]\n(VAL) Batch 156 Loss : 4.268554210662842, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 77 100 120 122 100 138  60 140  68  75  88  72 117 120 113 172 156 100\n 174 131 100 125 187 131 113 138 157 101 157  66 101 103], 정답 [100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100\n 100 100 100 100 100 100 100 100 101 101 101 101 101 101]\n(VAL) Batch 157 Loss : 3.8864617347717285, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [132 101 139  66 101 101 101 101 101 101 124  20  95 101 101 101 115 173\n 101 101 165 101 132 139 101 101 101 101 146 101 101  90], 정답 [101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n 101 101 101 101 101 101 101 101 101 101 101 101 101 101]\n(VAL) Batch 158 Loss : 1.297642469406128, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [101  66 166  71 101 101 101 101 101 101 198 132 103  90 129  96 103 160\n 124 113 137   4 121 102  44  97 129 124 188  66 102  40], 정답 [101 101 101 101 101 101 101 101 101 101 101 101 102 102 102 102 102 102\n 102 102 102 102 102 102 102 102 102 102 102 102 102 102]\n(VAL) Batch 159 Loss : 4.246280193328857, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 13 102 102 102  55 108  66 124  83  83 102 102  96 109  76 124  57 119\n 190 102   6 102 124 105  61 102 124  84  90  75 103 103], 정답 [102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102\n 102 102 102 102 102 102 102 102 102 102 102 102 103 103]\n(VAL) Batch 160 Loss : 3.3922979831695557, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 23 103 103 103 103 103 153 103 103 103 197 103 103 103 103 103 103  67\n  87 116 115 103 103 103 103   4 103 103 103  98 103 103], 정답 [103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103\n 103 103 103 103 103 103 103 103 103 103 103 103 103 103]\n(VAL) Batch 161 Loss : 1.3682430982589722, accuracy: 0.71875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [103 103 103 116 103 103 103 103 195 165 103 173  84 103 103 103 104 191\n 174 144 174 190 174 192 193 187 177  95 125  98 192 192], 정답 [103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 104 104\n 104 104 104 104 104 104 104 104 104 104 104 104 104 104]\n(VAL) Batch 162 Loss : 2.9524788856506348, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 48  82 174 104 174 136 123 104 179 104 104 155 178 104 191  77 190 186\n 191 104  24 141 104 179 182 174 182 104 192 192 104 192], 정답 [104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104\n 104 104 104 104 104 104 104 104 104 104 104 104 104 104]\n(VAL) Batch 163 Loss : 3.705514907836914, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [186  90 105 119 123   6 105 105 105 105 169 105  55  89 140 106 105 105\n  82 100  60 112 125 105 131 105 123 144 105 120  74 136], 정답 [104 104 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n 105 105 105 105 105 105 105 105 105 105 105 105 105 105]\n(VAL) Batch 164 Loss : 3.634343385696411, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [123 128 105 151  60  82 111 120  73 163 169 105  49 105  53  12 105  73\n 111 105  92  37 123 106  73 143  77 106  68  63 112  71], 정답 [105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n 105 105 106 106 106 106 106 106 106 106 106 106 106 106]\n(VAL) Batch 165 Loss : 3.6422533988952637, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [106  63 143  12  75 106 125  30 122 156 106 112 123 140 120   5  82 144\n 106  37 106 112 105 106 180  60 181  68 104 156 160  49], 정답 [106 106 106 106 106 106 106 106 106 106 106 106 106 106 106 106 106 106\n 106 106 106 106 106 106 106 106 106 106 106 106 106 106]\n(VAL) Batch 166 Loss : 3.974590539932251, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 99 109 180 119  69  13 107 107  43 107 107 107 164  97 107 107 107 107\n 107 178  97 107 107 107  20 153 114 107 194 107 107  24], 정답 [106 106 106 106 106 106 107 107 107 107 107 107 107 107 107 107 107 107\n 107 107 107 107 107 107 107 107 107 107 107 107 107 107]\n(VAL) Batch 167 Loss : 2.7695024013519287, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [107 107 107 107 107 107 107 107 107 107 107  97  87 107  70 153 107 107\n 153 107 111 107 107  70 108 165 108 108 108  66   8 153], 정답 [107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107\n 107 107 107 107 107 107 108 108 108 108 108 108 108 108]\n(VAL) Batch 168 Loss : 1.1357083320617676, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [108 108 108 108 147 108 108 108 167 108 116 197 126 108 135 108 108 108\n  65 108   5 108 108 108 197 108 108 138 108 108 108 166], 정답 [108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108\n 108 108 108 108 108 108 108 108 108 108 108 108 108 108]\n(VAL) Batch 169 Loss : 1.4419783353805542, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [108 108 145 126 108 108 197 103 108 157 109 109  63 132   8 113  64 114\n 139  18  99 163  99  99 127 106  63  95 109 120 130 180], 정답 [108 108 108 108 108 108 108 108 108 108 109 109 109 109 109 109 109 109\n 109 109 109 109 109 109 109 109 109 109 109 109 109 109]\n(VAL) Batch 170 Loss : 3.8673019409179688, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [188 105 189 135 109  40 161  60 109  90 109 111 109  68  45 129 109 109\n  68 122 157  73  60 109 181 105 171  24 144 110  95  87], 정답 [109 109 109 109 109 109 109 109 109 109 109 109 109 109 109 109 109 109\n 109 109 109 109 109 109 109 109 109 109 110 110 110 110]\n(VAL) Batch 171 Loss : 3.662306785583496, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [130  86 155  86 110 174  99  97 110 110 110 104 142 110   0  28 130  92\n 141 140  86  22  32 110 123  97 127 110 110 178 110  72], 정답 [110 110 110 110 110 110 110 110 110 110 110 110 110 110 110 110 110 110\n 110 110 110 110 110 110 110 110 110 110 110 110 110 110]\n(VAL) Batch 172 Loss : 3.969578504562378, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [151 141  62 139 110 110 110 113 110  45 110  80  74 110 148  57 111 111\n 111 111 111 111 111 111  82 111 111 107  97 120 165  73], 정답 [110 110 110 110 110 110 110 110 110 110 110 110 110 110 111 111 111 111\n 111 111 111 111 111 111 111 111 111 111 111 111 111 111]\n(VAL) Batch 173 Loss : 3.164928674697876, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [116  65 111 112 111 111 111  67 111 111 111 111 111 167 116 111 111 111\n  65 111 111 111 111  84 139 111 129 119  11 111 111 111], 정답 [111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111\n 111 111 111 111 111 111 111 111 111 111 111 111 111 111]\n(VAL) Batch 174 Loss : 1.6697945594787598, accuracy: 0.625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [112 112 112 112  93 149 114 146  83  68 112 112 169  62 105 112  60 199\n 161   9 112 112 120  60 112  60 120 112 112 196 105 123], 정답 [112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112\n 112 112 112 112 112 112 112 112 112 112 112 112 112 112]\n(VAL) Batch 175 Loss : 3.663839817047119, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 68 163 176 169  18 112 156 119  82 112 180  60 169  60 167 130  63 134\n  97 186  71 172 144 168 113  83  87 113  40 138  97 169], 정답 [112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112\n 113 113 113 113 113 113 113 113 113 113 113 113 113 113]\n(VAL) Batch 176 Loss : 3.841553211212158, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 98 113  10 193 113 135 113 113 113  21 169 113 142  55  97 126 151 113\n 155 113  86 113 113 187 193 113 113 113  45 113   6  83], 정답 [113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113\n 113 113 113 113 113 113 113 113 113 113 113 113 113 113]\n(VAL) Batch 177 Loss : 2.849790096282959, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [186  31  61 124 118 114 153  82  52 114 114 114 164 114  90 114 114  73\n  75 114 114  45 114 114 185 171 153 164  94 114  59 114], 정답 [113 113 113 113 114 114 114 114 114 114 114 114 114 114 114 114 114 114\n 114 114 114 114 114 114 114 114 114 114 114 114 114 114]\n(VAL) Batch 178 Loss : 3.2509796619415283, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [114 107 118 111 120 114 164 114  64  59 114 114 146 163  42 114 114 197\n 127 114  92 107 115 115 195 115 115 115 115 115 115  32], 정답 [114 114 114 114 114 114 114 114 114 114 114 114 114 114 114 114 114 114\n 114 114 114 114 115 115 115 115 115 115 115 115 115 115]\n(VAL) Batch 179 Loss : 3.1003189086914062, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [115 115 115 115 153 115 115 115 115  36 115 115 115 115 115 103 115 115\n 145  95 115 115 107 148 115 198 145 115 108 115 115 115], 정답 [115 115 115 115 115 115 115 115 115 115 115 115 115 115 115 115 115 115\n 115 115 115 115 115 115 115 115 115 115 115 115 115 115]\n(VAL) Batch 180 Loss : 1.7673989534378052, accuracy: 0.6875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [115  18 115 115 166 115 115 115 116 116 110 116 116 107 116 163  87 110\n 197 116 116 118 116 166 116 150 121 116 150  65 192 154], 정답 [115 115 115 115 115 115 115 115 116 116 116 116 116 116 116 116 116 116\n 116 116 116 116 116 116 116 116 116 116 116 116 116 116]\n(VAL) Batch 181 Loss : 2.131136894226074, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 93 116 116  70  70  91 133  82 116 116 116 116 133 116 153  81 116 116\n 143 116 145 166 116 153 116  70  45 117 155  53 179 117], 정답 [116 116 116 116 116 116 116 116 116 116 116 116 116 116 116 116 116 116\n 116 116 116 116 116 116 116 116 117 117 117 117 117 117]\n(VAL) Batch 182 Loss : 2.490044116973877, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  5  69 178 117 117 117  72 117 174 149 117 117 117 117 155 117 172 139\n 117 117 117  15 167 117  85  90  98 117 117 155  88 159], 정답 [117 117 117 117 117 117 117 117 117 117 117 117 117 117 117 117 117 117\n 117 117 117 117 117 117 117 117 117 117 117 117 117 117]\n(VAL) Batch 183 Loss : 3.6187386512756348, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [132 176 155 117  21 117   6 181 141 155  20 155 101 170 118  84 118 118\n 118 118 118 118 132 118 118 118 114  50 118 196 118 172], 정답 [117 117 117 117 117 117 117 117 117 117 117 117 118 118 118 118 118 118\n 118 118 118 118 118 118 118 118 118 118 118 118 118 118]\n(VAL) Batch 184 Loss : 2.919903039932251, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [118 132 118 118  62  52 118  65 146 118 118 118 118 118 118 118 118 118\n 118 118 118 118 118 118 118  95 197 118 118 118 119 119], 정답 [118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118\n 118 118 118 118 118 118 118 118 118 118 118 118 119 119]\n(VAL) Batch 185 Loss : 1.304059386253357, accuracy: 0.78125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [119 119 119 105  74 119  77 108  92 139 140 148 119 165 105  75  91 119\n  29 119 156 119  60 105 119  60 113 105 167 128 123 119], 정답 [119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119\n 119 119 119 119 119 119 119 119 119 119 119 119 119 119]\n(VAL) Batch 186 Loss : 3.0712087154388428, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [112  60  57 119  61 133 119 119  68 199 163  84  60 124  19  84 111  97\n 142 118 120 120  58 120 120 120 120 131 120  74 138  62], 정답 [119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 120 120\n 120 120 120 120 120 120 120 120 120 120 120 120 120 120]\n(VAL) Batch 187 Loss : 4.662998199462891, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [159 144  85  77 120 112 131 120 120  75 149 120 120 125 178  80 175 120\n 120  92 146 120  27 120 123  86 141  60 109 120  68  24], 정답 [120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120\n 120 120 120 120 120 120 120 120 120 120 120 120 120 120]\n(VAL) Batch 188 Loss : 3.4808578491210938, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [113 134 123 153 133 121 121  70 121  70  96 121  94 121 145 121 121 166\n 140 121 121 128 140 171 121 133 166 121  70 103  97  89], 정답 [120 120 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n 121 121 121 121 121 121 121 121 121 121 121 121 121 121]\n(VAL) Batch 189 Loss : 2.5390915870666504, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 70 121 116 121 121  70 145  97  81 166 121 103 165 121 121 121 121 194\n 133 121  76   6 111 122 125  14  45 121 110  31  91  73], 정답 [121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n 121 121 122 122 122 122 122 122 122 122 122 122 122 122]\n(VAL) Batch 190 Loss : 3.5297505855560303, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [149 198 194   8  23  88 132 122  67  58  92  99 122  43  58 109 175  30\n 144 176 104   6  69  10 122  91  26 151 122 122 125  92], 정답 [122 122 122 122 122 122 122 122 122 122 122 122 122 122 122 122 122 122\n 122 122 122 122 122 122 122 122 122 122 122 122 122 122]\n(VAL) Batch 191 Loss : 4.10828161239624, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [131  29  14  30 170  72 123 123  90 171 138  74 143 156 100  60  12  73\n 123  23  77 100 180 156 106  45  60  59  60 163  77 123], 정답 [122 122 122 122 122 122 123 123 123 123 123 123 123 123 123 123 123 123\n 123 123 123 123 123 123 123 123 123 123 123 123 123 123]\n(VAL) Batch 192 Loss : 4.9896016120910645, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [116  57 123 112  77  60 120 180 100  80  60 123 123 123  85  22 123 123\n  33 143 105 123 123 148 124  79 124 124 124 124 124 105], 정답 [123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123\n 123 123 123 123 123 123 124 124 124 124 124 124 124 124]\n(VAL) Batch 193 Loss : 3.0228614807128906, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 79 124 124 124 124 124 124 124 124 124 132  79 124 124 151 124 124 124\n 124 197 124 124 124 154 124 124 124 124 124 124 173 124], 정답 [124 124 124 124 124 124 124 124 124 124 124 124 124 124 124 124 124 124\n 124 124 124 124 124 124 124 124 124 124 124 124 124 124]\n(VAL) Batch 194 Loss : 1.277195692062378, accuracy: 0.78125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [124 124 124 124 124 124 124 124 122 124 144 123   3  43 120 125 123 150\n  19 112 146 125  88 125 138 156 188  60 123  67  83  99], 정답 [124 124 124 124 124 124 124 124 124 124 125 125 125 125 125 125 125 125\n 125 125 125 125 125 125 125 125 125 125 125 125 125 125]\n(VAL) Batch 195 Loss : 2.682175636291504, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [131  10 159  95 144 152  61 125  88  99  23 188 140  61  88 125  88  60\n 163 125  60 125 151 125  77 180 123 138 126  92 109  61], 정답 [125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125\n 125 125 125 125 125 125 125 125 125 125 126 126 126 126]\n(VAL) Batch 196 Loss : 3.5094408988952637, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [126 126 126 126 126 126 126 162 126  17  54 126 126 126 126  73 126 129\n  61  61 126  61 126 123  98 126  79  52 126 113  59 126], 정답 [126 126 126 126 126 126 126 126 126 126 126 126 126 126 126 126 126 126\n 126 126 126 126 126 126 126 126 126 126 126 126 126 126]\n(VAL) Batch 197 Loss : 2.7492737770080566, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 59  77 126 126 172  61 126 102 126 126 169 129 126 126  32 172  92  93\n 127  64 127  86  78  15 127 127 127 127 127 148 127  98], 정답 [126 126 126 126 126 126 126 126 126 126 126 126 126 126 127 127 127 127\n 127 127 127 127 127 127 127 127 127 127 127 127 127 127]\n(VAL) Batch 198 Loss : 3.025578022003174, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [127 167  60 187 137 139 102  71 105 127 127  87  68 127  87  73 121 102\n  75 104 110 127  86 140 154  86  83 128 168 140 141 104], 정답 [127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127\n 127 127 127 127 127 127 127 127 127 127 127 127 127 127]\n(VAL) Batch 199 Loss : 4.551322937011719, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [128  60 155 145 188 128 128 128 167 128 128 169 128 138 128 118  60 128\n 128 154 128  75  86  86  86  67 164  55  60 128 105 130], 정답 [128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n 128 128 128 128 128 128 128 128 128 128 128 128 128 128]\n(VAL) Batch 200 Loss : 3.5277464389801025, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [128 110 138 128 128 128 128 128 167 128 128  86 171  86 128 140 128 128\n 102  44 129 129 183  55 108 164 129 129 129 129 129 129], 정답 [128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n 129 129 129 129 129 129 129 129 129 129 129 129 129 129]\n(VAL) Batch 201 Loss : 1.6182717084884644, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [112  60 167  92 129 129 198 129 152 129 169 172  72 170 129  65 129 129\n 129 134  71 127 105 146 126 129  98 146 129 129 129  11], 정답 [129 129 129 129 129 129 129 129 129 129 129 129 129 129 129 129 129 129\n 129 129 129 129 129 129 129 129 129 129 129 129 129 129]\n(VAL) Batch 202 Loss : 3.4652462005615234, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [146 129 129 129 100  90 130  83 130 130 149 171 130 130 128 130  73 130\n  23 193 130 189 130  62  21  17 184 130 130 193  69  80], 정답 [129 129 129 129 130 130 130 130 130 130 130 130 130 130 130 130 130 130\n 130 130 130 130 130 130 130 130 130 130 130 130 130 130]\n(VAL) Batch 203 Loss : 3.1551272869110107, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 99 140 130 130  65 130 186 140 130 107 113 161 147  97 130 186 130 130\n  72  84 130 193 138  74 144 139 183  62 110  97 169 171], 정답 [130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130\n 130 130 130 130 131 131 131 131 131 131 131 131 131 131]\n(VAL) Batch 204 Loss : 4.247857570648193, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 77 123  60 152 131 130  69 135 124 125 185  12 130 135 138 131  26  83\n  99 183 131 140  17 152 116 132 131  98 131 140  83 125], 정답 [131 131 131 131 131 131 131 131 131 131 131 131 131 131 131 131 131 131\n 131 131 131 131 131 131 131 131 131 131 131 131 131 131]\n(VAL) Batch 205 Loss : 5.439886093139648, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 75 113 167  22  99 100 167  89  19 101 132 125 101  65 132 197 124  92\n 127 109 158 101 132  86  71 154 124  71 124  68 122 132], 정답 [131 131 131 131 131 131 131 131 132 132 132 132 132 132 132 132 132 132\n 132 132 132 132 132 132 132 132 132 132 132 132 132 132]\n(VAL) Batch 206 Loss : 5.223879337310791, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [132  60 122  95 109  62 124 101 129 131 132 171   7 163 124  98  94 180\n 138 124 101 172 185 141 129 180 133 133 116 133 133 133], 정답 [132 132 132 132 132 132 132 132 132 132 132 132 132 132 132 132 132 132\n 132 132 132 132 132 132 132 132 133 133 133 133 133 133]\n(VAL) Batch 207 Loss : 4.580377578735352, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [133 133 133 133 197 133 133  87 133 133 133 116 133 166 133 133 133 153\n 133 133  60 133 133 133 133 133 116 133 153  94 133 133], 정답 [133 133 133 133 133 133 133 133 133 133 133 133 133 133 133 133 133 133\n 133 133 133 133 133 133 133 133 133 133 133 133 133 133]\n(VAL) Batch 208 Loss : 1.2634525299072266, accuracy: 0.71875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [133 133 133 133 121 133 133 133 133 116 116  94  85 149  82  99 134 173\n 132 134 118  16 155 112 171  59  90 134  85 163 134 134], 정답 [133 133 133 133 133 133 133 133 133 133 133 133 134 134 134 134 134 134\n 134 134 134 134 134 134 134 134 134 134 134 134 134 134]\n(VAL) Batch 209 Loss : 2.894498825073242, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [134 134 169 149 134 134 129  46  90 134 134  85 119 114 149 134  85 134\n 113 134 107 105  78 119  85  80  14  90  88  45 182 138], 정답 [134 134 134 134 134 134 134 134 134 134 134 134 134 134 134 134 134 134\n 134 134 134 134 134 134 134 134 134 134 134 134 135 135]\n(VAL) Batch 210 Loss : 3.588230609893799, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [172 165 108 193  67  85  73 187 135 189 155 186 135 110 149 138 156 149\n  73  90  89 128  61 118 176 193 103 163 128 163  72  90], 정답 [135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135\n 135 135 135 135 135 135 135 135 135 135 135 135 135 135]\n(VAL) Batch 211 Loss : 6.240584373474121, accuracy: 0.0625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 73  90 135 155  26  71  93  97  73  31  73  85 172 175 130 141  12 144\n  18 136 140 171 171 136  75 136  24  83  54  92  27  40], 정답 [135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 136 136\n 136 136 136 136 136 136 136 136 136 136 136 136 136 136]\n(VAL) Batch 212 Loss : 4.459437847137451, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [175   3 130  86  86  92 141  97 172  97 136 120 136  99  28 136 136  68\n 173 178 104 136 104  65 144 139 161  97 136  83 161 142], 정답 [136 136 136 136 136 136 136 136 136 136 136 136 136 136 136 136 136 136\n 136 136 136 136 136 136 136 136 136 136 136 136 136 136]\n(VAL) Batch 213 Loss : 4.543783664703369, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [136 136 197 153 137 113  55 137 105 124  75 137 107 124 116   0 137 137\n 153 136 137  70  84  92  79 137 194  85  99 118 101  84], 정답 [136 136 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137\n 137 137 137 137 137 137 137 137 137 137 137 137 137 137]\n(VAL) Batch 214 Loss : 4.05244779586792, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [128  36  71  21 146  61 119 124  23 124 116 140 113 137 172  71 132  22\n 137 122  90 172 138 138 112 163 138 138 138 138 172 151], 정답 [137 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137\n 137 137 138 138 138 138 138 138 138 138 138 138 138 138]\n(VAL) Batch 215 Loss : 3.964703321456909, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [100 156 113  86 131 116 138  24 114  73 140 175 138 186 138  90 159 110\n 149 138  73   6 100  32 138  90  86  90  21  72 113 113], 정답 [138 138 138 138 138 138 138 138 138 138 138 138 138 138 138 138 138 138\n 138 138 138 138 138 138 138 138 138 138 138 138 138 138]\n(VAL) Batch 216 Loss : 5.0163445472717285, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [128 132  86 138 158 111 198 194 115 197  92 121  12 117 123  75 139 157\n 141 140 189 116  22 198 139 158 166  88 139 143 178  95], 정답 [138 138 138 138 138 138 139 139 139 139 139 139 139 139 139 139 139 139\n 139 139 139 139 139 139 139 139 139 139 139 139 139 139]\n(VAL) Batch 217 Loss : 5.215328216552734, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [111 185 139  21 139 139  68  40 139 164  73 140 145  32 105 139 132 139\n  30  99  75 112 139   7  97 140 140 171  93  89 140  86], 정답 [139 139 139 139 139 139 139 139 139 139 139 139 139 139 139 139 139 139\n 139 139 139 139 139 139 140 140 140 140 140 140 140 140]\n(VAL) Batch 218 Loss : 4.438968658447266, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [140 140  86 140 176 140 113  31 140 140 140  32 140 135 128  31 102  14\n 140 169 151 148 113 138 174 140 140 140  86 140 110 138], 정답 [140 140 140 140 140 140 140 140 140 140 140 140 140 140 140 140 140 140\n 140 140 140 140 140 140 140 140 140 140 140 140 140 140]\n(VAL) Batch 219 Loss : 3.4667704105377197, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [139 128 140 174 110 140  86 140 140  17 125  81 141 151 141 125 193 141\n 141  59  65 103 130 141 155  73  59 127  61 128  86 141], 정답 [140 140 140 140 140 140 140 140 140 140 141 141 141 141 141 141 141 141\n 141 141 141 141 141 141 141 141 141 141 141 141 141 141]\n(VAL) Batch 220 Loss : 3.6642045974731445, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 23  77 103 141 149 149 141 140 141 130 141 110  27 160 128  36  65 160\n  97 110 110 151 155 193 155 141 151 102  27 142  53 142], 정답 [141 141 141 141 141 141 141 141 141 141 141 141 141 141 141 141 141 141\n 141 141 141 141 141 141 141 141 141 141 142 142 142 142]\n(VAL) Batch 221 Loss : 4.6140522956848145, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [142  86 142 142 100 157 142  59 125 111 142 111 142 167  66 142  40 142\n 104 128 142 127  64 171 167 142  52  64 128 142 107 124], 정답 [142 142 142 142 142 142 142 142 142 142 142 142 142 142 142 142 142 142\n 142 142 142 142 142 142 142 142 142 142 142 142 142 142]\n(VAL) Batch 222 Loss : 3.7007596492767334, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [126  32  98  72 131 115  61 154  82 122 136 151  69 117 143 143 143 143\n  90 143 187 143 143 143 143 143 120 143 143 143 143 143], 정답 [142 142 142 142 142 142 142 142 142 142 142 142 142 142 143 143 143 143\n 143 143 143 143 143 143 143 143 143 143 143 143 143 143]\n(VAL) Batch 223 Loss : 2.7026827335357666, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [143 143 143 143 143 143  71 143 143 143 143 143 157 143  11 158 143 143\n 143 143 143 143 143 143 198 143 143 163 143 123 143 123], 정답 [143 143 143 143 143 143 143 143 143 143 143 143 143 143 143 143 143 143\n 143 143 143 143 143 143 143 143 143 143 143 143 143 143]\n(VAL) Batch 224 Loss : 2.1245059967041016, accuracy: 0.75\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [144 144 139 158 130 119  89 131 144 183  85 179 112 144  47  83 116 138\n 156 144 175 193 144 122 144 104 172 144 156  79 122  30], 정답 [144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144\n 144 144 144 144 144 144 144 144 144 144 144 144 144 144]\n(VAL) Batch 225 Loss : 4.815089225769043, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [172  21  32 107 167  89 158 144 160 144 144 144 144 110  21  90 179 175\n 145 145  94 145 145 145 145 145 145 145 145 157  66 145], 정답 [144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144\n 145 145 145 145 145 145 145 145 145 145 145 145 145 145]\n(VAL) Batch 226 Loss : 3.4266562461853027, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [145 145 145 145 145 145 145 145 145 101 128 145 145 145 145 145 145 145\n 145 145 145 145 145 145 145 145 145 107 139 145 145 145], 정답 [145 145 145 145 145 145 145 145 145 145 145 145 145 145 145 145 145 145\n 145 145 145 145 145 145 145 145 145 145 145 145 145 145]\n(VAL) Batch 227 Loss : 0.5969777703285217, accuracy: 0.875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [145 145 145 145  86 145 107 146 146 146 146 146 176 146 146 146 146 146\n 146 146 146 146 146 146 128  96 146 146 128 146 171 116], 정답 [145 145 145 145 146 146 146 146 146 146 146 146 146 146 146 146 146 146\n 146 146 146 146 146 146 146 146 146 146 146 146 146 146]\n(VAL) Batch 228 Loss : 0.8958002924919128, accuracy: 0.71875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 68 115 146  72 146 146 146 155 146  96 128 128  59 121 146 146  98  24\n 184 126  86  97 184 187 123 147 147  54 153  65  73  82], 정답 [146 146 146 146 146 146 146 146 146 146 146 146 146 146 146 146 146 146\n 146 146 146 146 147 147 147 147 147 147 147 147 147 147]\n(VAL) Batch 229 Loss : 4.15754508972168, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [163 147 140  98 174  97 121 147 153 163 176 147 107 172  99 140 140  87\n  87 147  98 147 147  87 147 147  89 147  97 153 158 174], 정답 [147 147 147 147 147 147 147 147 147 147 147 147 147 147 147 147 147 147\n 147 147 147 147 147 147 147 147 147 147 147 147 147 147]\n(VAL) Batch 230 Loss : 3.655651330947876, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 97  73  97  97 193 160 147  62 158  76 148 137 120 148 148  74 148  69\n 115  80 148 148 196  60 148 148 122  23 148 148 143 158], 정답 [147 147 147 147 147 147 147 147 148 148 148 148 148 148 148 148 148 148\n 148 148 148 148 148 148 148 148 148 148 148 148 148 148]\n(VAL) Batch 231 Loss : 4.163516521453857, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [148 148  23 148  23  11 125 128  59  23 148 148 196 148  24  17  23 179\n  31 139 130  69 148  23 148 148 149 149 149 107  75 149], 정답 [148 148 148 148 148 148 148 148 148 148 148 148 148 148 148 148 148 148\n 148 148 148 148 148 148 148 148 149 149 149 149 149 149]\n(VAL) Batch 232 Loss : 3.5629770755767822, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 18  90  90 149 149  74  90 175  85 178 172 160  31 149  90 149  73 189\n 149  86 149 149 149 149  85  93  62 149 149 128  66 135], 정답 [149 149 149 149 149 149 149 149 149 149 149 149 149 149 149 149 149 149\n 149 149 149 149 149 149 149 149 149 149 149 149 149 149]\n(VAL) Batch 233 Loss : 3.694681167602539, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [161  93 114 139 149 193 149  77 149 106 158  91 114  84 149  77 112 123\n 123 191 113 119 139 140 153  86 150 150  91 150 150 150], 정답 [149 149 149 149 149 149 149 149 149 149 149 149 150 150 150 150 150 150\n 150 150 150 150 150 150 150 150 150 150 150 150 150 150]\n(VAL) Batch 234 Loss : 4.736550807952881, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [123 150 127  97 193  25 169  24 150 163 150 161  58 150 116 172  60 112\n 185 169 105  89 136 150 150 150 156 172 134 123 110 137], 정답 [150 150 150 150 150 150 150 150 150 150 150 150 150 150 150 150 150 150\n 150 150 150 150 150 150 150 150 150 150 150 150 151 151]\n(VAL) Batch 235 Loss : 4.525950908660889, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 30 149  62 151  69 151 151 110 173  82 180  87 151  59  83  97  80 123\n 130 141 151 173 151 167 151  97 169 157  77 126 140 172], 정답 [151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151\n 151 151 151 151 151 151 151 151 151 151 151 151 151 151]\n(VAL) Batch 236 Loss : 3.732290267944336, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [130 172 150 104 151  99 151 140 151 129 151  95 151 151 154  94  14 152\n 196 196 152 152 152 152  46   2 152  76 152  37 172  78], 정답 [151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 152 152\n 152 152 152 152 152 152 152 152 152 152 152 152 152 152]\n(VAL) Batch 237 Loss : 2.9425599575042725, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 78 195 152 152   4  62  35 152  89  64 152 152 152 152 187  89   2  39\n  78 152 152   8 152   8  14 152 152 196 162 152  76 152], 정답 [152 152 152 152 152 152 152 152 152 152 152 152 152 152 152 152 152 152\n 152 152 152 152 152 152 152 152 152 152 152 152 152 152]\n(VAL) Batch 238 Loss : 2.5960865020751953, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 32 152 153 153 116 102 153 153 153 153  70 153 153 151 153  94 153 171\n 116 153  87 116  70  93 133 145 116 153 153  70 153 153], 정답 [152 152 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153\n 153 153 153 153 153 153 153 153 153 153 153 153 153 153]\n(VAL) Batch 239 Loss : 2.0145034790039062, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [153 153  87 145 153 145 153 153  94 153  70 153 153 153 115  94  70 153\n 153 153 154  87 124  70 152 128 157 164 154 165 124 154], 정답 [153 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153\n 153 153 154 154 154 154 154 154 154 154 154 154 154 154]\n(VAL) Batch 240 Loss : 3.2662665843963623, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [153 152  78 154 157 103 154 154 157  84 170 100 154 132 154  87 139 154\n  59  71  12 198 154 154 129 108 154 154 154 166 167 154], 정답 [154 154 154 154 154 154 154 154 154 154 154 154 154 154 154 154 154 154\n 154 154 154 154 154 154 154 154 154 154 154 154 154 154]\n(VAL) Batch 241 Loss : 4.180985927581787, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [154  95 154 154 154 154 155 155 155 117 110 155  13 155  87 123 149 132\n 155 123 155 148 155 110  97 155 155 141 171 188  83 106], 정답 [154 154 154 154 154 154 155 155 155 155 155 155 155 155 155 155 155 155\n 155 155 155 155 155 155 155 155 155 155 155 155 155 155]\n(VAL) Batch 242 Loss : 3.333002805709839, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [155 117 155 117 155 155 193 155 139 155 184 141 155  88  63  90 155 155\n 124 149 155 155 155  69 138 123 156 112 125  77 179 193], 정답 [155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155\n 155 155 155 155 155 155 156 156 156 156 156 156 156 156]\n(VAL) Batch 243 Loss : 3.2793869972229004, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  3 136 156  83 119 156  60 127 156 156 125 156 156  79 156 156   4  83\n 123 183 156  62  77 174 168 144 187 156 156  97 156 156], 정답 [156 156 156 156 156 156 156 156 156 156 156 156 156 156 156 156 156 156\n 156 156 156 156 156 156 156 156 156 156 156 156 156 156]\n(VAL) Batch 244 Loss : 3.6097543239593506, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [156  87 156  22 156 156 156   3 156 156 195 157 166 157 157  84 157 118\n 157 124 132  71 157 157 145   4 102  61 195 124 157 198], 정답 [156 156 156 156 156 156 156 156 156 156 157 157 157 157 157 157 157 157\n 157 157 157 157 157 157 157 157 157 157 157 157 157 157]\n(VAL) Batch 245 Loss : 2.9031155109405518, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [102  95 120 157 197   4 162 118 129 157 154  64 157 124 163 157 157  72\n  64  96 197  78 157  22 157 157 124  64 158 158 158 136], 정답 [157 157 157 157 157 157 157 157 157 157 157 157 157 157 157 157 157 157\n 157 157 157 157 157 157 157 157 157 157 158 158 158 158]\n(VAL) Batch 246 Loss : 2.653062343597412, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 62 158 146 158 139  58 149 181  60 171 149 158 102 158 158 168 158 171\n 131 171 120 112 158 118 134  80 168  51 158 158 149  97], 정답 [158 158 158 158 158 158 158 158 158 158 158 158 158 158 158 158 158 158\n 158 158 158 158 158 158 158 158 158 158 158 158 158 158]\n(VAL) Batch 247 Loss : 3.9519758224487305, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [156 158 158 158 196 120   7 158 125 158  74 117  99 158  13  21 186  88\n 180 159 189 119 132  40 149   3  92 184 156  61 122 125], 정답 [158 158 158 158 158 158 158 158 158 158 158 158 158 158 159 159 159 159\n 159 159 159 159 159 159 159 159 159 159 159 159 159 159]\n(VAL) Batch 248 Loss : 4.30685567855835, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 77 141   4  30  59  42 122 131  36 161 124 120  72 109 156 113 199  87\n 175 149 125 189 159 119 148 140 167 131   5  83 106  32], 정답 [159 159 159 159 159 159 159 159 159 159 159 159 159 159 159 159 159 159\n 159 159 159 159 159 159 159 159 159 159 159 159 159 159]\n(VAL) Batch 249 Loss : 5.362838268280029, accuracy: 0.03125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [114  71 183 188  80 112   3  67 184  69 167 160   7  36 178 160  36  72\n  69 172 160 141 146  98 160   2 117 181 179   6 177 110], 정답 [160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160\n 160 160 160 160 160 160 160 160 160 160 160 160 160 160]\n(VAL) Batch 250 Loss : 4.729058265686035, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [160  10  80 114 119 160 172  42  75 161 160 172 172  85 106 160 192  17\n  34 161  32  83 189  51  45 189 161  65  73 139  54  32], 정답 [160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160\n 161 161 161 161 161 161 161 161 161 161 161 161 161 161]\n(VAL) Batch 251 Loss : 4.476486682891846, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [113 161  36  82 161 161  52 190  48  34 100  83 161  47  90 161  27  31\n 131  31 134 161  26  47  27  31  22 161 171 161  58 161], 정답 [161 161 161 161 161 161 161 161 161 161 161 161 161 161 161 161 161 161\n 161 161 161 161 161 161 161 161 161 161 161 161 161 161]\n(VAL) Batch 252 Loss : 3.5638060569763184, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 24 158 149  24 198 162 162 162  66 174  84  53 162 162 162 162 162 162\n 162  91 195  61 103 162 162 113 114 162 152 162 162  14], 정답 [161 161 161 161 162 162 162 162 162 162 162 162 162 162 162 162 162 162\n 162 162 162 162 162 162 162 162 162 162 162 162 162 162]\n(VAL) Batch 253 Loss : 2.610685348510742, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [137 162 152  66  94 195 195   4  14  93 162 197 162 162 162 162 162 162\n  66 165  12  61 173  60 177 189 163 116 163  97 163 112], 정답 [162 162 162 162 162 162 162 162 162 162 162 162 162 162 162 162 162 162\n 162 162 162 162 163 163 163 163 163 163 163 163 163 163]\n(VAL) Batch 254 Loss : 3.03894305229187, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [163 163 163 163  65 163  60 163 163 105 163  60 163 163 163 163  13 163\n  13 163 125  83 163 163 163 163 112 104 163 163 171 163], 정답 [163 163 163 163 163 163 163 163 163 163 163 163 163 163 163 163 163 163\n 163 163 163 163 163 163 163 163 163 163 163 163 163 163]\n(VAL) Batch 255 Loss : 2.220327854156494, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [163 163  60  18 101 118 163 163 164 164 164 114 115 164 164 133 115 114\n  66 164  70 137 164 114 146  49 114 164 164 197 116 164], 정답 [163 163 163 163 163 163 163 163 164 164 164 164 164 164 164 164 164 164\n 164 164 164 164 164 164 164 164 164 164 164 164 164 164]\n(VAL) Batch 256 Loss : 2.5451345443725586, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [133 164 164 164  95 164 164  82 119 107  55 102 107 128  60 164 143 111\n 196  98 146 111 108 164 111 118 165 165 165 168 121 165], 정답 [164 164 164 164 164 164 164 164 164 164 164 164 164 164 164 164 164 164\n 164 164 164 164 164 164 164 164 165 165 165 165 165 165]\n(VAL) Batch 257 Loss : 3.415400743484497, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [102 165 165 165 165  72 165 121 165 102 165 165 165 165 124 165  67 165\n 165 165 165  96  91 165 128 165 165 165 165 165 165 165], 정답 [165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165\n 165 165 165 165 165 165 165 165 165 165 165 165 165 165]\n(VAL) Batch 258 Loss : 1.6221109628677368, accuracy: 0.71875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [165 165 165 165 195 165  66  71 193 165 165 119 166 166 167 166 145 166\n 166 133 166 166 166 146 166 166 166 166 166 154 166 166], 정답 [165 165 165 165 165 165 165 165 165 165 165 165 166 166 166 166 166 166\n 166 166 166 166 166 166 166 166 166 166 166 166 166 166]\n(VAL) Batch 259 Loss : 1.4627089500427246, accuracy: 0.6875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [162  71 166 166 166 166 166 166 166 121 166 146 145 166 166 166 166 166\n 166 166 197 166 166 166 166 166 115 166 166 167  74 167], 정답 [166 166 166 166 166 166 166 166 166 166 166 166 166 166 166 166 166 166\n 166 166 166 166 166 166 166 166 166 166 166 166 167 167]\n(VAL) Batch 260 Loss : 1.3974449634552002, accuracy: 0.71875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [128  68 151 167  49 167 132 123 128 167 140 128 167  86 106 146 167 127\n 128 167 128 167 167 170 167 167 163 167 128 111  61 171], 정답 [167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 167\n 167 167 167 167 167 167 167 167 167 167 167 167 167 167]\n(VAL) Batch 261 Loss : 3.0429840087890625, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [167 167 167  86 167 139 167 132 167 167 167 163 167  86  85 167  24  59\n 111  60 148 116  65 170 164 154 171 156 137 153  64  69], 정답 [167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 168 168\n 168 168 168 168 168 168 168 168 168 168 168 168 168 168]\n(VAL) Batch 262 Loss : 4.578324317932129, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [180  59 116  84 180 168  82 187 149  97  66 131 111 143 169 141 168 119\n 112  70 163  73 168 150 122 172 114 113  27 136  80 153], 정답 [168 168 168 168 168 168 168 168 168 168 168 168 168 168 168 168 168 168\n 168 168 168 168 168 168 168 168 168 168 168 168 168 168]\n(VAL) Batch 263 Loss : 5.542876243591309, accuracy: 0.09375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [153  93 145 126 188  85 169 163 129 169 169 169  57  19  80 169  73 169\n 169 124  89  90 107 169  61 169 169  93  62 169 180  62], 정답 [168 168 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169\n 169 169 169 169 169 169 169 169 169 169 169 169 169 169]\n(VAL) Batch 264 Loss : 4.076252460479736, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 88 128 128  62  85  62 169  62 169  60 169  86 169 163 169  62 169 171\n  61 169 129 163 170 165 137 170 170   4 170 170 170 170], 정답 [169 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169\n 169 169 170 170 170 170 170 170 170 170 170 170 170 170]\n(VAL) Batch 265 Loss : 3.1060705184936523, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [197 162  75 103  67 194 170 162 170 170 170 118  96 170 170 170 170  69\n 170 194 194 197 170 170 170 170 170 165 170  91 170  96], 정답 [170 170 170 170 170 170 170 170 170 170 170 170 170 170 170 170 170 170\n 170 170 170 170 170 170 170 170 170 170 170 170 170 170]\n(VAL) Batch 266 Loss : 1.9266479015350342, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [170 105 154 170 170  70 171 171 171 171 175 171 113 171 171 167 171 171\n  32 139 171 171  74 130 172 171  91  68 171 171 171  77], 정답 [170 170 170 170 170 170 171 171 171 171 171 171 171 171 171 171 171 171\n 171 171 171 171 171 171 171 171 171 171 171 171 171 171]\n(VAL) Batch 267 Loss : 2.686779737472534, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 95  68  65 171 118 163 171 171 105 171  28 171 171  73 118 171 171 120\n 171 181  28  68 171 171 111  72 130 167 172 172 106 167], 정답 [171 171 171 171 171 171 171 171 171 171 171 171 171 171 171 171 171 171\n 171 171 171 171 171 171 172 172 172 172 172 172 172 172]\n(VAL) Batch 268 Loss : 2.933818817138672, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 90 100  62 140 138  15 172 173 167 172 172  62 169  67 172  62  90 144\n 188 172 187 138 160 172 109 183  65 149 131 111  31 187], 정답 [172 172 172 172 172 172 172 172 172 172 172 172 172 172 172 172 172 172\n 172 172 172 172 172 172 172 172 172 172 172 172 172 172]\n(VAL) Batch 269 Loss : 3.527736186981201, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  0  72 172 172 194 177  67  90  52  80 109 173 173 173 129  71 101 173\n 173 173 137 173  11 101 194 123 173 173 173 102  71 173], 정답 [172 172 172 172 172 172 172 172 172 172 173 173 173 173 173 173 173 173\n 173 173 173 173 173 173 173 173 173 173 173 173 173 173]\n(VAL) Batch 270 Loss : 2.7161922454833984, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [173  71 173   0 173 103 101  71 165 154 170 173 173  71 173  71  61 124\n 173 173 173 173 173 173 184 173 173 173  92 107 104 104], 정답 [173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173\n 173 173 173 173 173 173 173 173 173 173 174 174 174 174]\n(VAL) Batch 271 Loss : 2.770045280456543, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [174 174 182 174  25 183 178 147 190 192 174 174 190  17  69 191 174 178\n 178  46 174 174 174 174 174 104 174 174 174 161 104 104], 정답 [174 174 174 174 174 174 174 174 174 174 174 174 174 174 174 174 174 174\n 174 174 174 174 174 174 174 174 174 174 174 174 174 174]\n(VAL) Batch 272 Loss : 2.7757325172424316, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [177  98 178 174 154 147  32 175 193 174 174 104  38  63  99 193 186  26\n 107  27  98 188 175 182 113 113 175 190 191  88 113 113], 정답 [174 174 174 174 174 174 174 174 174 174 174 174 174 174 175 175 175 175\n 175 175 175 175 175 175 175 175 175 175 175 175 175 175]\n(VAL) Batch 273 Loss : 4.456295490264893, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [110  34 181 155 163 163 119 174 109 124 151  22 128 175 156 179  60 191\n  69 175 138  97 122 138 175 158 104 175  15 139   0 175], 정답 [175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175\n 175 175 175 175 175 175 175 175 175 175 175 175 175 175]\n(VAL) Batch 274 Loss : 5.147111892700195, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [176 176  93  93  93 176 176  90  88  93  89 123 147 176 176 108 100 176\n 181 176 176  62  62 176 181  38 115 176 140 176 176 176], 정답 [176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176\n 176 176 176 176 176 176 176 176 176 176 176 176 176 176]\n(VAL) Batch 275 Loss : 2.433257818222046, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [176 176  62 176  17 176 176  82 176 164 146  90  30 196 176 169 149  98\n  80 178 177 192  41 177  41 177 136 182 199 177 192 192], 정답 [176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176\n 177 177 177 177 177 177 177 177 177 177 177 177 177 177]\n(VAL) Batch 276 Loss : 2.6586921215057373, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [177 190 181 177   5 177  62 177  25 177 177 193 181 177 177 177 174 192\n 177 178 177 178 190 177 190 193 178 141 178 177 190 191], 정답 [177 177 177 177 177 177 177 177 177 177 177 177 177 177 177 177 177 177\n 177 177 177 177 177 177 177 177 177 177 177 177 177 177]\n(VAL) Batch 277 Loss : 2.909672737121582, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [178 177 177  99   2 185 178 177 178 178 178 178 178 178 183 178 178  18\n 178 178 178 191 178 178 182  75 178 178 181 178 190  44], 정답 [177 177 177 177 178 178 178 178 178 178 178 178 178 178 178 178 178 178\n 178 178 178 178 178 178 178 178 178 178 178 178 178 178]\n(VAL) Batch 278 Loss : 1.9496691226959229, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  2 189 192 177 178 178 178 182 178 178  44 178 178 183 178 178 178 178\n 175 102 178 178 161 178 182 179 138 130 193 178  14 156], 정답 [178 178 178 178 178 178 178 178 178 178 178 178 178 178 178 178 178 178\n 178 178 178 178 179 179 179 179 179 179 179 179 179 179]\n(VAL) Batch 279 Loss : 2.5800836086273193, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [192 179 189 189 177  83 141 179 193 182 177 182 189  41 153 161 179  86\n 177 125 177 192 179 182 179 179 179 179 123 189  24  24], 정답 [179 179 179 179 179 179 179 179 179 179 179 179 179 179 179 179 179 179\n 179 179 179 179 179 179 179 179 179 179 179 179 179 179]\n(VAL) Batch 280 Loss : 4.014411449432373, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [193 181  49  32 179 131 182 182 180 141  74 149  60 180 180 161 123 123\n 149 125  60 180  99  13   0 175  68  18 138  73  33 136], 정답 [179 179 179 179 179 179 179 179 180 180 180 180 180 180 180 180 180 180\n 180 180 180 180 180 180 180 180 180 180 180 180 180 180]\n(VAL) Batch 281 Loss : 5.4181342124938965, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 60 163  24 158 180 180 113 180 184 180  60 134 168  74 148  79 114 150\n  85 192 175 144 180 180 134 130  82 181  74 181 181 181], 정답 [180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180\n 180 180 180 180 180 180 180 180 181 181 181 181 181 181]\n(VAL) Batch 282 Loss : 4.138798713684082, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [181  44 181 192  18 181 192 130 181 175 181 194 177 177 191 193 180 181\n   3 181 151 182 178 192  74 104  15  73   4 181 181 181], 정답 [181 181 181 181 181 181 181 181 181 181 181 181 181 181 181 181 181 181\n 181 181 181 181 181 181 181 181 181 181 181 181 181 181]\n(VAL) Batch 283 Loss : 4.752835750579834, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [181 179 181 181 181 182 114 199 181 181 181 181 182 182 190 178 186 182\n 178 182 182 178 177 182 192 182 177  27 182 190 190 191], 정답 [181 181 181 181 181 181 181 181 181 181 181 181 182 182 182 182 182 182\n 182 182 182 182 182 182 182 182 182 182 182 182 182 182]\n(VAL) Batch 284 Loss : 2.269087791442871, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [192  68 193 182 182 182 192 182 187 182 182   0 182 199 192 182  42 191\n 192 186  64  36  33 192 190 182 192 182 182 181 183  47], 정답 [182 182 182 182 182 182 182 182 182 182 182 182 182 182 182 182 182 182\n 182 182 182 182 182 182 182 182 182 182 182 182 183 183]\n(VAL) Batch 285 Loss : 3.730541229248047, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 17 179 182 183 177 182 183 183 183 196 183  45 183 183 183  38 155 183\n 183  40 182 179 182  45 183 183  40 183 183 183 199 183], 정답 [183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 183\n 183 183 183 183 183 183 183 183 183 183 183 183 183 183]\n(VAL) Batch 286 Loss : 2.6288557052612305, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [188 183 181 179 183 183 186  85 184 183 196 182 182 178 196 199  43 199\n 184 184 184 184  42 184 187 187  88 184 184 184 179  45], 정답 [183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 184 184\n 184 184 184 184 184 184 184 184 184 184 184 184 184 184]\n(VAL) Batch 287 Loss : 2.5364465713500977, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [184 184 186 187  47 184  45 184 184 184 184 184 184 184 184 184 186 159\n 184 184 138 115 184 186 184 184 184 184 184 184 184 199], 정답 [184 184 184 184 184 184 184 184 184 184 184 184 184 184 184 184 184 184\n 184 184 184 184 184 184 184 184 184 184 184 184 184 184]\n(VAL) Batch 288 Loss : 1.4750231504440308, accuracy: 0.6875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [184 187  89 136 124 185  15 185  91 185  33  58  85 185 162 185  12  46\n 185  15 185 149 173  58  93 185 185 199   6 185 185 185], 정답 [184 184 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185\n 185 185 185 185 185 185 185 185 185 185 185 185 185 185]\n(VAL) Batch 289 Loss : 3.5024523735046387, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [114 185 185 199 185 185   0 185 185 185 185 185 185 185 162  83 185   3\n   0  58 186 153 186 186 187 186  45 188   0 186 186 187], 정답 [185 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185\n 185 185 186 186 186 186 186 186 186 186 186 186 186 186]\n(VAL) Batch 290 Loss : 2.4541096687316895, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [186 186 186 186 186 187 186 180 186 149 186 186 186 184 186 187 187 190\n 186 186 186 153 187 186 186 191 187 188 186 187 186 186], 정답 [186 186 186 186 186 186 186 186 186 186 186 186 186 186 186 186 186 186\n 186 186 186 186 186 186 186 186 186 186 186 186 186 186]\n(VAL) Batch 291 Loss : 2.04776668548584, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 22 186 188 186 186 187 188 172 187 187 187  45 187 187 187 187 187  45\n 158 187 187 187 187 187 187 187 187 161 187 153 187 188], 정답 [186 186 186 186 186 186 187 187 187 187 187 187 187 187 187 187 187 187\n 187 187 187 187 187 187 187 187 187 187 187 187 187 187]\n(VAL) Batch 292 Loss : 1.4424245357513428, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [187 187 187 187 187 187 187 199 186 102 187 187 187 187 192 187 187 187\n 187 187 187 104 187 193 188 171 186 180 178 188  92 188], 정답 [187 187 187 187 187 187 187 187 187 187 187 187 187 187 187 187 187 187\n 187 187 187 187 187 187 188 188 188 188 188 188 188 188]\n(VAL) Batch 293 Loss : 1.6746420860290527, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 37 188 188 192 186 187  18 188 181 188 188 186  39 187 199 188  38 187\n 139  99 179 188 180  99 184 188 188 192 188 187 181 188], 정답 [188 188 188 188 188 188 188 188 188 188 188 188 188 188 188 188 188 188\n 188 188 188 188 188 188 188 188 188 188 188 188 188 188]\n(VAL) Batch 294 Loss : 3.6842703819274902, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [184  82 188 188 188 187  44 188 188 182 189 189  90 189 189 189 189 186\n 185 189 177 189 189 149 189  90 189  59  46 189 184 189], 정답 [188 188 188 188 188 188 188 188 188 188 189 189 189 189 189 189 189 189\n 189 189 189 189 189 189 189 189 189 189 189 189 189 189]\n(VAL) Batch 295 Loss : 1.9007290601730347, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [184 189 187 199 189 189 189  52 189 141 189 189 192 184 184 189  39 189\n 189 189 117 186 189 184 185  36 189  37 190 190 190 190], 정답 [189 189 189 189 189 189 189 189 189 189 189 189 189 189 189 189 189 189\n 189 189 189 189 189 189 189 189 189 189 190 190 190 190]\n(VAL) Batch 296 Loss : 2.192298412322998, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [190 190  18 190 177 190 189 177  89 190 182 190  41 190 190 190 177 191\n 190 104  41 178 190 192 105 174   3 177 191 182 190 179], 정답 [190 190 190 190 190 190 190 190 190 190 190 190 190 190 190 190 190 190\n 190 190 190 190 190 190 190 190 190 190 190 190 190 190]\n(VAL) Batch 297 Loss : 2.8526346683502197, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [190 190  47 190 191 190 104 190 192 190 187 177 192 136 178 192 192 104\n 191  19 191 191 190 191 191 191 191 190 190 190 191 191], 정답 [190 190 190 190 190 190 190 190 190 190 190 190 190 190 191 191 191 191\n 191 191 191 191 191 191 191 191 191 191 191 191 191 191]\n(VAL) Batch 298 Loss : 2.3840112686157227, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [191 191 177 191 192 145 192 191 191 191 191  18 191 191 191 191 191 191\n 191 191 191 190 191 190 181 191 191 191  82 191 191  41], 정답 [191 191 191 191 191 191 191 191 191 191 191 191 191 191 191 191 191 191\n 191 191 191 191 191 191 191 191 191 191 191 191 191 191]\n(VAL) Batch 299 Loss : 1.168908953666687, accuracy: 0.6875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [187 192 187  25 192 192  89 192 192 192 192 186 181 178 188 192 192 192\n 182 190 192 192 192 182 182 192 190 192 192 192 193 192], 정답 [192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192\n 192 192 192 192 192 192 192 192 192 192 192 192 192 192]\n(VAL) Batch 300 Loss : 1.8595662117004395, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [192 192  83 181 192 192 191 182 192 192 192 192 183  26 192 192 192 187\n 193 193 193 193 193 181 193 193 193 193 193 193 193 150], 정답 [192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192\n 193 193 193 193 193 193 193 193 193 193 193 193 193 193]\n(VAL) Batch 301 Loss : 1.4230058193206787, accuracy: 0.71875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [193  69 181 193 193 193 193 193 193 193  93  69 193 193 193 151  99 193\n 174 193  98 113 193 193 193 193 174 193 193 193 175 193], 정답 [193 193 193 193 193 193 193 193 193 193 193 193 193 193 193 193 193 193\n 193 193 193 193 193 193 193 193 193 193 193 193 193 193]\n(VAL) Batch 302 Loss : 2.2933778762817383, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [193 113 193 193 148 194  32 194 197 194  62 194 194 194 194 194 194 165\n 194 194 194  43 194  71 194 114 194 194 194  71 194  14], 정답 [193 193 193 193 194 194 194 194 194 194 194 194 194 194 194 194 194 194\n 194 194 194 194 194 194 194 194 194 194 194 194 194 194]\n(VAL) Batch 303 Loss : 1.4835773706436157, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 84 194 194  11 194 194 118  66 117 194 194 129 183 118 194  96  84 194\n 119 127  22 194   8 195  63 198 190 195 197  56 195  96], 정답 [194 194 194 194 194 194 194 194 194 194 194 194 194 194 194 194 194 194\n 194 194 194 194 195 195 195 195 195 195 195 195 195 195]\n(VAL) Batch 304 Loss : 2.5078492164611816, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 89 105 195 195  88   4  66 194 194 195 101 195 194 197  57 195 197  91\n 195 101  55 198 195 195 194 198  55 198 195  85 154 190], 정답 [195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195\n 195 195 195 195 195 195 195 195 195 195 195 195 195 195]\n(VAL) Batch 305 Loss : 3.425422191619873, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 96 197 194 195  91 195 195 195 194  23 196 196 196 196  14  46  17 196\n 196  13 196  14 196 183 196 196  14 196  23 196 196 196], 정답 [195 195 195 195 195 195 195 195 196 196 196 196 196 196 196 196 196 196\n 196 196 196 196 196 196 196 196 196 196 196 196 196 196]\n(VAL) Batch 306 Loss : 1.5623489618301392, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [196 183  33 196  14 196  17 198  14 196 148 196   2  14 196 196 196 135\n 196 196  46 196 196  10  19  14 194  84 197 107 194 198], 정답 [196 196 196 196 196 196 196 196 196 196 196 196 196 196 196 196 196 196\n 196 196 196 196 196 196 196 196 197 197 197 197 197 197]\n(VAL) Batch 307 Loss : 2.2754533290863037, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [197  23 194 146 162 198 165 101  96  23 157 197 197 158 154 197 197 146\n 170 197 198 197 198 196 101 197  45  71  51 108 198 154], 정답 [197 197 197 197 197 197 197 197 197 197 197 197 197 197 197 197 197 197\n 197 197 197 197 197 197 197 197 197 197 197 197 197 197]\n(VAL) Batch 308 Loss : 3.5357091426849365, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 91  29  52 123 195 139  43 197 173 102  71 148 102 198 198 158 197 154\n 198 197 198 187 198 198 198 195  69  96 195 198 154 197], 정답 [197 197 197 197 197 197 197 197 197 197 197 197 198 198 198 198 198 198\n 198 198 198 198 198 198 198 198 198 198 198 198 198 198]\n(VAL) Batch 309 Loss : 3.277261734008789, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [198 197 194 198 198 198 198 198 198  66   0 116 122 198 198 124 198 198\n 124 198 198   4 198 158 195 198 198 198 198 197 199 182], 정답 [198 198 198 198 198 198 198 198 198 198 198 198 198 198 198 198 198 198\n 198 198 198 198 198 198 198 198 198 198 198 198 199 199]\n(VAL) Batch 310 Loss : 2.892153739929199, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 36 199 199  15 199 199 199   1 199 187 199 179 148   1 199  69  35 177\n 185   0   6 199 180  38  34 184 199  36 199 199  84 199], 정답 [199 199 199 199 199 199 199 199 199 199 199 199 199 199 199 199 199 199\n 199 199 199 199 199 199 199 199 199 199 199 199 199 199]\n(VAL) Batch 311 Loss : 3.3888978958129883, accuracy: 0.40625\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [  6 187  13 199  42  20 185 114 199  77 199  73 199 119  43  36], 정답 [199 199 199 199 199 199 199 199 199 199 199 199 199 199 199 199]\n(VAL) Batch 312 Loss : 4.8436174392700195, accuracy: 0.25\nepoch 20 Loss/Validate :3.171726841991321 \nepoch 20 Accuracy/Validate : 0.3983\nEpoch : 21, batch 0\n(Train) Batch 0 Loss : 0.29213619232177734, 맞은 개수 : 114\nEpoch : 21, batch 1\n(Train) Batch 1 Loss : 0.21181610226631165, 맞은 개수 : 123\nEpoch : 21, batch 2\n(Train) Batch 2 Loss : 0.17533496022224426, 맞은 개수 : 120\nEpoch : 21, batch 3\n(Train) Batch 3 Loss : 0.2284204214811325, 맞은 개수 : 118\nEpoch : 21, batch 4\n(Train) Batch 4 Loss : 0.280432790517807, 맞은 개수 : 117\nEpoch : 21, batch 5\n(Train) Batch 5 Loss : 0.2834923565387726, 맞은 개수 : 118\nEpoch : 21, batch 6\n(Train) Batch 6 Loss : 0.3950673043727875, 맞은 개수 : 113\nEpoch : 21, batch 7\n(Train) Batch 7 Loss : 0.10819987952709198, 맞은 개수 : 124\nEpoch : 21, batch 8\n(Train) Batch 8 Loss : 0.24505092203617096, 맞은 개수 : 119\nEpoch : 21, batch 9\n(Train) Batch 9 Loss : 0.2895514965057373, 맞은 개수 : 113\nEpoch : 21, batch 10\n(Train) Batch 10 Loss : 0.1894332319498062, 맞은 개수 : 122\nEpoch : 21, batch 11\n(Train) Batch 11 Loss : 0.21056291460990906, 맞은 개수 : 116\nEpoch : 21, batch 12\n(Train) Batch 12 Loss : 0.35530775785446167, 맞은 개수 : 119\nEpoch : 21, batch 13\n(Train) Batch 13 Loss : 0.17551347613334656, 맞은 개수 : 123\nEpoch : 21, batch 14\n(Train) Batch 14 Loss : 0.31560108065605164, 맞은 개수 : 118\nEpoch : 21, batch 15\n(Train) Batch 15 Loss : 0.20417547225952148, 맞은 개수 : 118\nEpoch : 21, batch 16\n(Train) Batch 16 Loss : 0.2761184275150299, 맞은 개수 : 117\nEpoch : 21, batch 17\n(Train) Batch 17 Loss : 0.372566819190979, 맞은 개수 : 113\nEpoch : 21, batch 18\n(Train) Batch 18 Loss : 0.17296622693538666, 맞은 개수 : 120\nEpoch : 21, batch 19\n(Train) Batch 19 Loss : 0.21418538689613342, 맞은 개수 : 119\nEpoch : 21, batch 20\n(Train) Batch 20 Loss : 0.2639513313770294, 맞은 개수 : 118\nEpoch : 21, batch 21\n(Train) Batch 21 Loss : 0.263668030500412, 맞은 개수 : 115\nEpoch : 21, batch 22\n(Train) Batch 22 Loss : 0.30352669954299927, 맞은 개수 : 113\nEpoch : 21, batch 23\n(Train) Batch 23 Loss : 0.29128432273864746, 맞은 개수 : 117\nEpoch : 21, batch 24\n(Train) Batch 24 Loss : 0.24039387702941895, 맞은 개수 : 117\nEpoch : 21, batch 25\n(Train) Batch 25 Loss : 0.3655317425727844, 맞은 개수 : 114\nEpoch : 21, batch 26\n(Train) Batch 26 Loss : 0.2756717801094055, 맞은 개수 : 119\nEpoch : 21, batch 27\n(Train) Batch 27 Loss : 0.22262297570705414, 맞은 개수 : 119\nEpoch : 21, batch 28\n(Train) Batch 28 Loss : 0.16541688144207, 맞은 개수 : 122\nEpoch : 21, batch 29\n(Train) Batch 29 Loss : 0.21026304364204407, 맞은 개수 : 117\nEpoch : 21, batch 30\n(Train) Batch 30 Loss : 0.2803862690925598, 맞은 개수 : 117\nEpoch : 21, batch 31\n(Train) Batch 31 Loss : 0.0921289250254631, 맞은 개수 : 124\nEpoch : 21, batch 32\n(Train) Batch 32 Loss : 0.20418229699134827, 맞은 개수 : 119\nEpoch : 21, batch 33\n(Train) Batch 33 Loss : 0.2337903529405594, 맞은 개수 : 118\nEpoch : 21, batch 34\n(Train) Batch 34 Loss : 0.27921319007873535, 맞은 개수 : 118\nEpoch : 21, batch 35\n(Train) Batch 35 Loss : 0.28746548295021057, 맞은 개수 : 116\nEpoch : 21, batch 36\n(Train) Batch 36 Loss : 0.22551335394382477, 맞은 개수 : 117\nEpoch : 21, batch 37\n(Train) Batch 37 Loss : 0.25606250762939453, 맞은 개수 : 115\nEpoch : 21, batch 38\n(Train) Batch 38 Loss : 0.29448580741882324, 맞은 개수 : 118\nEpoch : 21, batch 39\n(Train) Batch 39 Loss : 0.1876504272222519, 맞은 개수 : 121\nEpoch : 21, batch 40\n(Train) Batch 40 Loss : 0.2207176238298416, 맞은 개수 : 118\nEpoch : 21, batch 41\n(Train) Batch 41 Loss : 0.24381811916828156, 맞은 개수 : 117\nEpoch : 21, batch 42\n(Train) Batch 42 Loss : 0.288476824760437, 맞은 개수 : 117\nEpoch : 21, batch 43\n(Train) Batch 43 Loss : 0.20174194872379303, 맞은 개수 : 123\nEpoch : 21, batch 44\n(Train) Batch 44 Loss : 0.3400524854660034, 맞은 개수 : 117\nEpoch : 21, batch 45\n(Train) Batch 45 Loss : 0.2847289443016052, 맞은 개수 : 116\nEpoch : 21, batch 46\n(Train) Batch 46 Loss : 0.17981474101543427, 맞은 개수 : 119\nEpoch : 21, batch 47\n(Train) Batch 47 Loss : 0.24194049835205078, 맞은 개수 : 117\nEpoch : 21, batch 48\n(Train) Batch 48 Loss : 0.20126627385616302, 맞은 개수 : 120\nEpoch : 21, batch 49\n(Train) Batch 49 Loss : 0.1880691647529602, 맞은 개수 : 120\nEpoch : 21, batch 50\n(Train) Batch 50 Loss : 0.25855177640914917, 맞은 개수 : 117\nEpoch : 21, batch 51\n(Train) Batch 51 Loss : 0.4327560067176819, 맞은 개수 : 113\nEpoch : 21, batch 52\n(Train) Batch 52 Loss : 0.1861703097820282, 맞은 개수 : 120\nEpoch : 21, batch 53\n(Train) Batch 53 Loss : 0.1760275810956955, 맞은 개수 : 120\nEpoch : 21, batch 54\n(Train) Batch 54 Loss : 0.25878915190696716, 맞은 개수 : 116\nEpoch : 21, batch 55\n(Train) Batch 55 Loss : 0.19489344954490662, 맞은 개수 : 120\nEpoch : 21, batch 56\n(Train) Batch 56 Loss : 0.1814497709274292, 맞은 개수 : 118\nEpoch : 21, batch 57\n(Train) Batch 57 Loss : 0.2842254638671875, 맞은 개수 : 119\nEpoch : 21, batch 58\n(Train) Batch 58 Loss : 0.38790908455848694, 맞은 개수 : 113\nEpoch : 21, batch 59\n(Train) Batch 59 Loss : 0.23671188950538635, 맞은 개수 : 118\nEpoch : 21, batch 60\n(Train) Batch 60 Loss : 0.4497002959251404, 맞은 개수 : 112\nEpoch : 21, batch 61\n(Train) Batch 61 Loss : 0.23382814228534698, 맞은 개수 : 119\nEpoch : 21, batch 62\n(Train) Batch 62 Loss : 0.29618901014328003, 맞은 개수 : 116\nEpoch : 21, batch 63\n(Train) Batch 63 Loss : 0.23823252320289612, 맞은 개수 : 120\nEpoch : 21, batch 64\n(Train) Batch 64 Loss : 0.13655182719230652, 맞은 개수 : 123\nEpoch : 21, batch 65\n(Train) Batch 65 Loss : 0.10894869267940521, 맞은 개수 : 124\nEpoch : 21, batch 66\n(Train) Batch 66 Loss : 0.26016804575920105, 맞은 개수 : 117\nEpoch : 21, batch 67\n(Train) Batch 67 Loss : 0.23343868553638458, 맞은 개수 : 114\nEpoch : 21, batch 68\n(Train) Batch 68 Loss : 0.32625141739845276, 맞은 개수 : 114\nEpoch : 21, batch 69\n(Train) Batch 69 Loss : 0.16336803138256073, 맞은 개수 : 119\nEpoch : 21, batch 70\n(Train) Batch 70 Loss : 0.2015237808227539, 맞은 개수 : 120\nEpoch : 21, batch 71\n(Train) Batch 71 Loss : 0.2317662388086319, 맞은 개수 : 119\nEpoch : 21, batch 72\n(Train) Batch 72 Loss : 0.17043334245681763, 맞은 개수 : 120\nEpoch : 21, batch 73\n(Train) Batch 73 Loss : 0.24315251410007477, 맞은 개수 : 117\nEpoch : 21, batch 74\n(Train) Batch 74 Loss : 0.2690977454185486, 맞은 개수 : 116\nEpoch : 21, batch 75\n(Train) Batch 75 Loss : 0.22644558548927307, 맞은 개수 : 119\nEpoch : 21, batch 76\n(Train) Batch 76 Loss : 0.33878180384635925, 맞은 개수 : 116\nEpoch : 21, batch 77\n(Train) Batch 77 Loss : 0.2621355950832367, 맞은 개수 : 120\nEpoch : 21, batch 78\n(Train) Batch 78 Loss : 0.35982421040534973, 맞은 개수 : 114\nEpoch : 21, batch 79\n(Train) Batch 79 Loss : 0.21469548344612122, 맞은 개수 : 120\nEpoch : 21, batch 80\n(Train) Batch 80 Loss : 0.2736804187297821, 맞은 개수 : 117\nEpoch : 21, batch 81\n(Train) Batch 81 Loss : 0.1178780272603035, 맞은 개수 : 123\nEpoch : 21, batch 82\n(Train) Batch 82 Loss : 0.19435997307300568, 맞은 개수 : 120\nEpoch : 21, batch 83\n(Train) Batch 83 Loss : 0.2595204710960388, 맞은 개수 : 118\nEpoch : 21, batch 84\n(Train) Batch 84 Loss : 0.2512683868408203, 맞은 개수 : 117\nEpoch : 21, batch 85\n(Train) Batch 85 Loss : 0.2044883668422699, 맞은 개수 : 120\nEpoch : 21, batch 86\n(Train) Batch 86 Loss : 0.18779754638671875, 맞은 개수 : 121\nEpoch : 21, batch 87\n(Train) Batch 87 Loss : 0.3777240514755249, 맞은 개수 : 115\nEpoch : 21, batch 88\n(Train) Batch 88 Loss : 0.15378232300281525, 맞은 개수 : 123\nEpoch : 21, batch 89\n(Train) Batch 89 Loss : 0.1291184425354004, 맞은 개수 : 122\nEpoch : 21, batch 90\n(Train) Batch 90 Loss : 0.22259043157100677, 맞은 개수 : 115\nEpoch : 21, batch 91\n(Train) Batch 91 Loss : 0.2150273621082306, 맞은 개수 : 122\nEpoch : 21, batch 92\n(Train) Batch 92 Loss : 0.3595559597015381, 맞은 개수 : 116\nEpoch : 21, batch 93\n(Train) Batch 93 Loss : 0.18876045942306519, 맞은 개수 : 123\nEpoch : 21, batch 94\n(Train) Batch 94 Loss : 0.24558869004249573, 맞은 개수 : 119\nEpoch : 21, batch 95\n(Train) Batch 95 Loss : 0.26635050773620605, 맞은 개수 : 119\nEpoch : 21, batch 96\n(Train) Batch 96 Loss : 0.24694737792015076, 맞은 개수 : 118\nEpoch : 21, batch 97\n(Train) Batch 97 Loss : 0.2414923757314682, 맞은 개수 : 119\nEpoch : 21, batch 98\n(Train) Batch 98 Loss : 0.15251851081848145, 맞은 개수 : 121\nEpoch : 21, batch 99\n(Train) Batch 99 Loss : 0.2082929015159607, 맞은 개수 : 119\nEpoch : 21, batch 100\n(Train) Batch 100 Loss : 0.258085161447525, 맞은 개수 : 118\nEpoch : 21, batch 101\n(Train) Batch 101 Loss : 0.2520742416381836, 맞은 개수 : 118\nEpoch : 21, batch 102\n(Train) Batch 102 Loss : 0.2290029376745224, 맞은 개수 : 117\nEpoch : 21, batch 103\n(Train) Batch 103 Loss : 0.3134523928165436, 맞은 개수 : 116\nEpoch : 21, batch 104\n(Train) Batch 104 Loss : 0.14622275531291962, 맞은 개수 : 120\nEpoch : 21, batch 105\n(Train) Batch 105 Loss : 0.14550469815731049, 맞은 개수 : 122\nEpoch : 21, batch 106\n(Train) Batch 106 Loss : 0.2733602225780487, 맞은 개수 : 116\nEpoch : 21, batch 107\n(Train) Batch 107 Loss : 0.25619882345199585, 맞은 개수 : 118\nEpoch : 21, batch 108\n(Train) Batch 108 Loss : 0.21371515095233917, 맞은 개수 : 120\nEpoch : 21, batch 109\n(Train) Batch 109 Loss : 0.2729054093360901, 맞은 개수 : 117\nEpoch : 21, batch 110\n(Train) Batch 110 Loss : 0.24083134531974792, 맞은 개수 : 123\nEpoch : 21, batch 111\n(Train) Batch 111 Loss : 0.17203351855278015, 맞은 개수 : 123\nEpoch : 21, batch 112\n(Train) Batch 112 Loss : 0.20715263485908508, 맞은 개수 : 121\nEpoch : 21, batch 113\n(Train) Batch 113 Loss : 0.3139602839946747, 맞은 개수 : 115\nEpoch : 21, batch 114\n(Train) Batch 114 Loss : 0.20085598528385162, 맞은 개수 : 118\nEpoch : 21, batch 115\n(Train) Batch 115 Loss : 0.21923212707042694, 맞은 개수 : 118\nEpoch : 21, batch 116\n(Train) Batch 116 Loss : 0.1879589557647705, 맞은 개수 : 119\nEpoch : 21, batch 117\n(Train) Batch 117 Loss : 0.17273998260498047, 맞은 개수 : 123\nEpoch : 21, batch 118\n(Train) Batch 118 Loss : 0.27264538407325745, 맞은 개수 : 115\nEpoch : 21, batch 119\n(Train) Batch 119 Loss : 0.19758816063404083, 맞은 개수 : 119\nEpoch : 21, batch 120\n(Train) Batch 120 Loss : 0.17416462302207947, 맞은 개수 : 124\nEpoch : 21, batch 121\n(Train) Batch 121 Loss : 0.19926442205905914, 맞은 개수 : 123\nEpoch : 21, batch 122\n(Train) Batch 122 Loss : 0.1795298308134079, 맞은 개수 : 119\nEpoch : 21, batch 123\n(Train) Batch 123 Loss : 0.2535502016544342, 맞은 개수 : 119\nEpoch : 21, batch 124\n(Train) Batch 124 Loss : 0.23906125128269196, 맞은 개수 : 119\nEpoch : 21, batch 125\n(Train) Batch 125 Loss : 0.1381554752588272, 맞은 개수 : 122\nEpoch : 21, batch 126\n(Train) Batch 126 Loss : 0.17252585291862488, 맞은 개수 : 122\nEpoch : 21, batch 127\n(Train) Batch 127 Loss : 0.3939960300922394, 맞은 개수 : 111\nEpoch : 21, batch 128\n(Train) Batch 128 Loss : 0.21194058656692505, 맞은 개수 : 118\nEpoch : 21, batch 129\n(Train) Batch 129 Loss : 0.28978782892227173, 맞은 개수 : 113\nEpoch : 21, batch 130\n(Train) Batch 130 Loss : 0.23546108603477478, 맞은 개수 : 119\nEpoch : 21, batch 131\n(Train) Batch 131 Loss : 0.29083681106567383, 맞은 개수 : 115\nEpoch : 21, batch 132\n(Train) Batch 132 Loss : 0.14917412400245667, 맞은 개수 : 121\nEpoch : 21, batch 133\n(Train) Batch 133 Loss : 0.3185955882072449, 맞은 개수 : 119\nEpoch : 21, batch 134\n(Train) Batch 134 Loss : 0.1323348879814148, 맞은 개수 : 125\nEpoch : 21, batch 135\n(Train) Batch 135 Loss : 0.2629552185535431, 맞은 개수 : 119\nEpoch : 21, batch 136\n(Train) Batch 136 Loss : 0.2669042944908142, 맞은 개수 : 118\nEpoch : 21, batch 137\n(Train) Batch 137 Loss : 0.2130359262228012, 맞은 개수 : 120\nEpoch : 21, batch 138\n(Train) Batch 138 Loss : 0.285342276096344, 맞은 개수 : 117\nEpoch : 21, batch 139\n(Train) Batch 139 Loss : 0.19553272426128387, 맞은 개수 : 120\nEpoch : 21, batch 140\n(Train) Batch 140 Loss : 0.3313717842102051, 맞은 개수 : 114\nEpoch : 21, batch 141\n(Train) Batch 141 Loss : 0.24053238332271576, 맞은 개수 : 119\nEpoch : 21, batch 142\n(Train) Batch 142 Loss : 0.3137722909450531, 맞은 개수 : 116\nEpoch : 21, batch 143\n(Train) Batch 143 Loss : 0.2769501805305481, 맞은 개수 : 120\nEpoch : 21, batch 144\n(Train) Batch 144 Loss : 0.4054381847381592, 맞은 개수 : 115\nEpoch : 21, batch 145\n(Train) Batch 145 Loss : 0.2949744164943695, 맞은 개수 : 119\nEpoch : 21, batch 146\n(Train) Batch 146 Loss : 0.22826001048088074, 맞은 개수 : 118\nEpoch : 21, batch 147\n(Train) Batch 147 Loss : 0.3271201252937317, 맞은 개수 : 114\nEpoch : 21, batch 148\n(Train) Batch 148 Loss : 0.1247742772102356, 맞은 개수 : 124\nEpoch : 21, batch 149\n(Train) Batch 149 Loss : 0.3882080912590027, 맞은 개수 : 114\nEpoch : 21, batch 150\n(Train) Batch 150 Loss : 0.26493117213249207, 맞은 개수 : 112\nEpoch : 21, batch 151\n(Train) Batch 151 Loss : 0.35513177514076233, 맞은 개수 : 117\nEpoch : 21, batch 152\n(Train) Batch 152 Loss : 0.2921013832092285, 맞은 개수 : 115\nEpoch : 21, batch 153\n(Train) Batch 153 Loss : 0.2713777422904968, 맞은 개수 : 115\nEpoch : 21, batch 154\n(Train) Batch 154 Loss : 0.3645187020301819, 맞은 개수 : 114\nEpoch : 21, batch 155\n(Train) Batch 155 Loss : 0.30145618319511414, 맞은 개수 : 120\nEpoch : 21, batch 156\n(Train) Batch 156 Loss : 0.30424782633781433, 맞은 개수 : 115\nEpoch : 21, batch 157\n(Train) Batch 157 Loss : 0.21381667256355286, 맞은 개수 : 116\nEpoch : 21, batch 158\n(Train) Batch 158 Loss : 0.23587507009506226, 맞은 개수 : 118\nEpoch : 21, batch 159\n(Train) Batch 159 Loss : 0.13500593602657318, 맞은 개수 : 120\nEpoch : 21, batch 160\n(Train) Batch 160 Loss : 0.16842934489250183, 맞은 개수 : 119\nEpoch : 21, batch 161\n(Train) Batch 161 Loss : 0.24958699941635132, 맞은 개수 : 119\nEpoch : 21, batch 162\n(Train) Batch 162 Loss : 0.2733253836631775, 맞은 개수 : 118\nEpoch : 21, batch 163\n(Train) Batch 163 Loss : 0.2572415769100189, 맞은 개수 : 119\nEpoch : 21, batch 164\n(Train) Batch 164 Loss : 0.26975134015083313, 맞은 개수 : 118\nEpoch : 21, batch 165\n(Train) Batch 165 Loss : 0.20894968509674072, 맞은 개수 : 121\nEpoch : 21, batch 166\n(Train) Batch 166 Loss : 0.18596991896629333, 맞은 개수 : 119\nEpoch : 21, batch 167\n(Train) Batch 167 Loss : 0.40462300181388855, 맞은 개수 : 112\nEpoch : 21, batch 168\n(Train) Batch 168 Loss : 0.3356431722640991, 맞은 개수 : 119\nEpoch : 21, batch 169\n(Train) Batch 169 Loss : 0.11915034055709839, 맞은 개수 : 123\nEpoch : 21, batch 170\n(Train) Batch 170 Loss : 0.20337437093257904, 맞은 개수 : 119\nEpoch : 21, batch 171\n(Train) Batch 171 Loss : 0.2997075319290161, 맞은 개수 : 117\nEpoch : 21, batch 172\n(Train) Batch 172 Loss : 0.17734955251216888, 맞은 개수 : 124\nEpoch : 21, batch 173\n(Train) Batch 173 Loss : 0.1876167505979538, 맞은 개수 : 122\nEpoch : 21, batch 174\n(Train) Batch 174 Loss : 0.25110918283462524, 맞은 개수 : 119\nEpoch : 21, batch 175\n(Train) Batch 175 Loss : 0.256858766078949, 맞은 개수 : 119\nEpoch : 21, batch 176\n(Train) Batch 176 Loss : 0.12168817222118378, 맞은 개수 : 123\nEpoch : 21, batch 177\n(Train) Batch 177 Loss : 0.3111962676048279, 맞은 개수 : 114\nEpoch : 21, batch 178\n(Train) Batch 178 Loss : 0.22155870497226715, 맞은 개수 : 120\nEpoch : 21, batch 179\n(Train) Batch 179 Loss : 0.26985955238342285, 맞은 개수 : 117\nEpoch : 21, batch 180\n(Train) Batch 180 Loss : 0.28126809000968933, 맞은 개수 : 113\nEpoch : 21, batch 181\n(Train) Batch 181 Loss : 0.1825721263885498, 맞은 개수 : 120\nEpoch : 21, batch 182\n(Train) Batch 182 Loss : 0.23069404065608978, 맞은 개수 : 119\nEpoch : 21, batch 183\n(Train) Batch 183 Loss : 0.17794650793075562, 맞은 개수 : 124\nEpoch : 21, batch 184\n(Train) Batch 184 Loss : 0.24131526052951813, 맞은 개수 : 117\nEpoch : 21, batch 185\n(Train) Batch 185 Loss : 0.3258228302001953, 맞은 개수 : 114\nEpoch : 21, batch 186\n(Train) Batch 186 Loss : 0.2845894992351532, 맞은 개수 : 120\nEpoch : 21, batch 187\n(Train) Batch 187 Loss : 0.22211796045303345, 맞은 개수 : 122\nEpoch : 21, batch 188\n(Train) Batch 188 Loss : 0.10971427708864212, 맞은 개수 : 122\nEpoch : 21, batch 189\n(Train) Batch 189 Loss : 0.23472003638744354, 맞은 개수 : 121\nEpoch : 21, batch 190\n(Train) Batch 190 Loss : 0.22308559715747833, 맞은 개수 : 118\nEpoch : 21, batch 191\n(Train) Batch 191 Loss : 0.3094857335090637, 맞은 개수 : 114\nEpoch : 21, batch 192\n(Train) Batch 192 Loss : 0.26778900623321533, 맞은 개수 : 120\nEpoch : 21, batch 193\n(Train) Batch 193 Loss : 0.2718726098537445, 맞은 개수 : 119\nEpoch : 21, batch 194\n(Train) Batch 194 Loss : 0.27217525243759155, 맞은 개수 : 114\nEpoch : 21, batch 195\n(Train) Batch 195 Loss : 0.3321421444416046, 맞은 개수 : 117\nEpoch : 21, batch 196\n(Train) Batch 196 Loss : 0.36746352910995483, 맞은 개수 : 114\nEpoch : 21, batch 197\n(Train) Batch 197 Loss : 0.3632699251174927, 맞은 개수 : 118\nEpoch : 21, batch 198\n(Train) Batch 198 Loss : 0.37829017639160156, 맞은 개수 : 115\nEpoch : 21, batch 199\n(Train) Batch 199 Loss : 0.21676763892173767, 맞은 개수 : 118\nEpoch : 21, batch 200\n(Train) Batch 200 Loss : 0.3149029314517975, 맞은 개수 : 119\nEpoch : 21, batch 201\n(Train) Batch 201 Loss : 0.28383514285087585, 맞은 개수 : 118\nEpoch : 21, batch 202\n(Train) Batch 202 Loss : 0.25032639503479004, 맞은 개수 : 118\nEpoch : 21, batch 203\n(Train) Batch 203 Loss : 0.17441360652446747, 맞은 개수 : 120\nEpoch : 21, batch 204\n(Train) Batch 204 Loss : 0.24962909519672394, 맞은 개수 : 119\nEpoch : 21, batch 205\n(Train) Batch 205 Loss : 0.34182584285736084, 맞은 개수 : 109\nEpoch : 21, batch 206\n(Train) Batch 206 Loss : 0.3003489375114441, 맞은 개수 : 119\nEpoch : 21, batch 207\n(Train) Batch 207 Loss : 0.39192983508110046, 맞은 개수 : 117\nEpoch : 21, batch 208\n(Train) Batch 208 Loss : 0.2168465107679367, 맞은 개수 : 116\nEpoch : 21, batch 209\n(Train) Batch 209 Loss : 0.2763062119483948, 맞은 개수 : 117\nEpoch : 21, batch 210\n(Train) Batch 210 Loss : 0.33614107966423035, 맞은 개수 : 114\nEpoch : 21, batch 211\n(Train) Batch 211 Loss : 0.19709378480911255, 맞은 개수 : 119\nEpoch : 21, batch 212\n(Train) Batch 212 Loss : 0.25975340604782104, 맞은 개수 : 117\nEpoch : 21, batch 213\n(Train) Batch 213 Loss : 0.30128973722457886, 맞은 개수 : 116\nEpoch : 21, batch 214\n(Train) Batch 214 Loss : 0.2820959687232971, 맞은 개수 : 115\nEpoch : 21, batch 215\n(Train) Batch 215 Loss : 0.3123740553855896, 맞은 개수 : 115\nEpoch : 21, batch 216\n(Train) Batch 216 Loss : 0.24839062988758087, 맞은 개수 : 114\nEpoch : 21, batch 217\n(Train) Batch 217 Loss : 0.2673985958099365, 맞은 개수 : 117\nEpoch : 21, batch 218\n(Train) Batch 218 Loss : 0.20911474525928497, 맞은 개수 : 119\nEpoch : 21, batch 219\n(Train) Batch 219 Loss : 0.25648024678230286, 맞은 개수 : 115\nEpoch : 21, batch 220\n(Train) Batch 220 Loss : 0.26931798458099365, 맞은 개수 : 116\nEpoch : 21, batch 221\n(Train) Batch 221 Loss : 0.22366268932819366, 맞은 개수 : 115\nEpoch : 21, batch 222\n(Train) Batch 222 Loss : 0.24326321482658386, 맞은 개수 : 117\nEpoch : 21, batch 223\n(Train) Batch 223 Loss : 0.36457115411758423, 맞은 개수 : 115\nEpoch : 21, batch 224\n(Train) Batch 224 Loss : 0.1535097062587738, 맞은 개수 : 123\nEpoch : 21, batch 225\n(Train) Batch 225 Loss : 0.23062624037265778, 맞은 개수 : 122\nEpoch : 21, batch 226\n(Train) Batch 226 Loss : 0.154601588845253, 맞은 개수 : 120\nEpoch : 21, batch 227\n(Train) Batch 227 Loss : 0.16494889557361603, 맞은 개수 : 120\nEpoch : 21, batch 228\n(Train) Batch 228 Loss : 0.22075487673282623, 맞은 개수 : 119\nEpoch : 21, batch 229\n(Train) Batch 229 Loss : 0.3233078718185425, 맞은 개수 : 117\nEpoch : 21, batch 230\n(Train) Batch 230 Loss : 0.23268498480319977, 맞은 개수 : 120\nEpoch : 21, batch 231\n(Train) Batch 231 Loss : 0.15866363048553467, 맞은 개수 : 122\nEpoch : 21, batch 232\n(Train) Batch 232 Loss : 0.34752845764160156, 맞은 개수 : 116\nEpoch : 21, batch 233\n(Train) Batch 233 Loss : 0.22622118890285492, 맞은 개수 : 121\nEpoch : 21, batch 234\n(Train) Batch 234 Loss : 0.3998735547065735, 맞은 개수 : 115\nEpoch : 21, batch 235\n(Train) Batch 235 Loss : 0.11949435621500015, 맞은 개수 : 124\nEpoch : 21, batch 236\n(Train) Batch 236 Loss : 0.3403036296367645, 맞은 개수 : 117\nEpoch : 21, batch 237\n(Train) Batch 237 Loss : 0.33527737855911255, 맞은 개수 : 115\nEpoch : 21, batch 238\n(Train) Batch 238 Loss : 0.319722056388855, 맞은 개수 : 117\nEpoch : 21, batch 239\n(Train) Batch 239 Loss : 0.18467818200588226, 맞은 개수 : 118\nEpoch : 21, batch 240\n(Train) Batch 240 Loss : 0.2198978215456009, 맞은 개수 : 119\nEpoch : 21, batch 241\n(Train) Batch 241 Loss : 0.19541166722774506, 맞은 개수 : 121\nEpoch : 21, batch 242\n(Train) Batch 242 Loss : 0.18391861021518707, 맞은 개수 : 122\nEpoch : 21, batch 243\n(Train) Batch 243 Loss : 0.19738633930683136, 맞은 개수 : 120\nEpoch : 21, batch 244\n(Train) Batch 244 Loss : 0.15332676470279694, 맞은 개수 : 123\nEpoch : 21, batch 245\n(Train) Batch 245 Loss : 0.22087587416172028, 맞은 개수 : 119\nEpoch : 21, batch 246\n(Train) Batch 246 Loss : 0.21606522798538208, 맞은 개수 : 120\nEpoch : 21, batch 247\n(Train) Batch 247 Loss : 0.1355878710746765, 맞은 개수 : 123\nEpoch : 21, batch 248\n(Train) Batch 248 Loss : 0.2833142578601837, 맞은 개수 : 117\nEpoch : 21, batch 249\n(Train) Batch 249 Loss : 0.2570312023162842, 맞은 개수 : 119\nEpoch : 21, batch 250\n(Train) Batch 250 Loss : 0.29457977414131165, 맞은 개수 : 119\nEpoch : 21, batch 251\n(Train) Batch 251 Loss : 0.2201041430234909, 맞은 개수 : 119\nEpoch : 21, batch 252\n(Train) Batch 252 Loss : 0.19354990124702454, 맞은 개수 : 120\nEpoch : 21, batch 253\n(Train) Batch 253 Loss : 0.31835439801216125, 맞은 개수 : 117\nEpoch : 21, batch 254\n(Train) Batch 254 Loss : 0.3284081816673279, 맞은 개수 : 115\nEpoch : 21, batch 255\n(Train) Batch 255 Loss : 0.26486673951148987, 맞은 개수 : 115\nEpoch : 21, batch 256\n(Train) Batch 256 Loss : 0.25543510913848877, 맞은 개수 : 116\nEpoch : 21, batch 257\n(Train) Batch 257 Loss : 0.24323031306266785, 맞은 개수 : 119\nEpoch : 21, batch 258\n(Train) Batch 258 Loss : 0.24908597767353058, 맞은 개수 : 118\nEpoch : 21, batch 259\n(Train) Batch 259 Loss : 0.23418323695659637, 맞은 개수 : 118\nEpoch : 21, batch 260\n(Train) Batch 260 Loss : 0.272011935710907, 맞은 개수 : 116\nEpoch : 21, batch 261\n(Train) Batch 261 Loss : 0.1962607502937317, 맞은 개수 : 121\nEpoch : 21, batch 262\n(Train) Batch 262 Loss : 0.22997333109378815, 맞은 개수 : 121\nEpoch : 21, batch 263\n(Train) Batch 263 Loss : 0.29870712757110596, 맞은 개수 : 115\nEpoch : 21, batch 264\n(Train) Batch 264 Loss : 0.23574070632457733, 맞은 개수 : 120\nEpoch : 21, batch 265\n(Train) Batch 265 Loss : 0.3287399411201477, 맞은 개수 : 115\nEpoch : 21, batch 266\n(Train) Batch 266 Loss : 0.4511825442314148, 맞은 개수 : 111\nEpoch : 21, batch 267\n(Train) Batch 267 Loss : 0.15263910591602325, 맞은 개수 : 124\nEpoch : 21, batch 268\n(Train) Batch 268 Loss : 0.3234790861606598, 맞은 개수 : 112\nEpoch : 21, batch 269\n(Train) Batch 269 Loss : 0.22187215089797974, 맞은 개수 : 118\nEpoch : 21, batch 270\n(Train) Batch 270 Loss : 0.3015919625759125, 맞은 개수 : 119\nEpoch : 21, batch 271\n(Train) Batch 271 Loss : 0.3167322874069214, 맞은 개수 : 121\nEpoch : 21, batch 272\n(Train) Batch 272 Loss : 0.2733009159564972, 맞은 개수 : 116\nEpoch : 21, batch 273\n(Train) Batch 273 Loss : 0.2424430251121521, 맞은 개수 : 114\nEpoch : 21, batch 274\n(Train) Batch 274 Loss : 0.420663058757782, 맞은 개수 : 110\nEpoch : 21, batch 275\n(Train) Batch 275 Loss : 0.19351615011692047, 맞은 개수 : 122\nEpoch : 21, batch 276\n(Train) Batch 276 Loss : 0.15129250288009644, 맞은 개수 : 120\nEpoch : 21, batch 277\n(Train) Batch 277 Loss : 0.22743365168571472, 맞은 개수 : 116\nEpoch : 21, batch 278\n(Train) Batch 278 Loss : 0.1756850779056549, 맞은 개수 : 119\nEpoch : 21, batch 279\n(Train) Batch 279 Loss : 0.15726445615291595, 맞은 개수 : 122\nEpoch : 21, batch 280\n(Train) Batch 280 Loss : 0.36910489201545715, 맞은 개수 : 116\nEpoch : 21, batch 281\n(Train) Batch 281 Loss : 0.22162123024463654, 맞은 개수 : 119\nEpoch : 21, batch 282\n(Train) Batch 282 Loss : 0.16788430511951447, 맞은 개수 : 122\nEpoch : 21, batch 283\n(Train) Batch 283 Loss : 0.14659813046455383, 맞은 개수 : 121\nEpoch : 21, batch 284\n(Train) Batch 284 Loss : 0.26658979058265686, 맞은 개수 : 117\nEpoch : 21, batch 285\n(Train) Batch 285 Loss : 0.23709318041801453, 맞은 개수 : 117\nEpoch : 21, batch 286\n(Train) Batch 286 Loss : 0.2505946159362793, 맞은 개수 : 117\nEpoch : 21, batch 287\n(Train) Batch 287 Loss : 0.2963780164718628, 맞은 개수 : 115\nEpoch : 21, batch 288\n(Train) Batch 288 Loss : 0.18843050301074982, 맞은 개수 : 121\nEpoch : 21, batch 289\n(Train) Batch 289 Loss : 0.2821797728538513, 맞은 개수 : 116\nEpoch : 21, batch 290\n(Train) Batch 290 Loss : 0.1582045555114746, 맞은 개수 : 121\nEpoch : 21, batch 291\n(Train) Batch 291 Loss : 0.2504880726337433, 맞은 개수 : 120\nEpoch : 21, batch 292\n(Train) Batch 292 Loss : 0.3425515592098236, 맞은 개수 : 116\nEpoch : 21, batch 293\n(Train) Batch 293 Loss : 0.2053525596857071, 맞은 개수 : 121\nEpoch : 21, batch 294\n(Train) Batch 294 Loss : 0.17738205194473267, 맞은 개수 : 121\nEpoch : 21, batch 295\n(Train) Batch 295 Loss : 0.20337910950183868, 맞은 개수 : 121\nEpoch : 21, batch 296\n(Train) Batch 296 Loss : 0.23912793397903442, 맞은 개수 : 116\nEpoch : 21, batch 297\n(Train) Batch 297 Loss : 0.30811530351638794, 맞은 개수 : 116\nEpoch : 21, batch 298\n(Train) Batch 298 Loss : 0.22007498145103455, 맞은 개수 : 116\nEpoch : 21, batch 299\n(Train) Batch 299 Loss : 0.28472551703453064, 맞은 개수 : 115\nEpoch : 21, batch 300\n(Train) Batch 300 Loss : 0.28546378016471863, 맞은 개수 : 116\nEpoch : 21, batch 301\n(Train) Batch 301 Loss : 0.24229273200035095, 맞은 개수 : 116\nEpoch : 21, batch 302\n(Train) Batch 302 Loss : 0.39095446467399597, 맞은 개수 : 109\nEpoch : 21, batch 303\n(Train) Batch 303 Loss : 0.3048413097858429, 맞은 개수 : 113\nEpoch : 21, batch 304\n(Train) Batch 304 Loss : 0.1837671399116516, 맞은 개수 : 117\nEpoch : 21, batch 305\n(Train) Batch 305 Loss : 0.249955415725708, 맞은 개수 : 115\nEpoch : 21, batch 306\n(Train) Batch 306 Loss : 0.39807823300361633, 맞은 개수 : 113\nEpoch : 21, batch 307\n(Train) Batch 307 Loss : 0.24632573127746582, 맞은 개수 : 113\nEpoch : 21, batch 308\n(Train) Batch 308 Loss : 0.25883662700653076, 맞은 개수 : 120\nEpoch : 21, batch 309\n(Train) Batch 309 Loss : 0.24189075827598572, 맞은 개수 : 118\nEpoch : 21, batch 310\n(Train) Batch 310 Loss : 0.38191959261894226, 맞은 개수 : 113\nEpoch : 21, batch 311\n(Train) Batch 311 Loss : 0.48618748784065247, 맞은 개수 : 110\nEpoch : 21, batch 312\n(Train) Batch 312 Loss : 0.465725302696228, 맞은 개수 : 112\nEpoch : 21, batch 313\n(Train) Batch 313 Loss : 0.32420527935028076, 맞은 개수 : 116\nEpoch : 21, batch 314\n(Train) Batch 314 Loss : 0.2535005807876587, 맞은 개수 : 120\nEpoch : 21, batch 315\n(Train) Batch 315 Loss : 0.25306472182273865, 맞은 개수 : 115\nEpoch : 21, batch 316\n(Train) Batch 316 Loss : 0.23957166075706482, 맞은 개수 : 117\nEpoch : 21, batch 317\n(Train) Batch 317 Loss : 0.3745681345462799, 맞은 개수 : 116\nEpoch : 21, batch 318\n(Train) Batch 318 Loss : 0.291869580745697, 맞은 개수 : 115\nEpoch : 21, batch 319\n(Train) Batch 319 Loss : 0.28678184747695923, 맞은 개수 : 118\nEpoch : 21, batch 320\n(Train) Batch 320 Loss : 0.18100731074810028, 맞은 개수 : 121\nEpoch : 21, batch 321\n(Train) Batch 321 Loss : 0.30966901779174805, 맞은 개수 : 117\nEpoch : 21, batch 322\n(Train) Batch 322 Loss : 0.25907471776008606, 맞은 개수 : 114\nEpoch : 21, batch 323\n(Train) Batch 323 Loss : 0.12395749986171722, 맞은 개수 : 123\nEpoch : 21, batch 324\n(Train) Batch 324 Loss : 0.2437172681093216, 맞은 개수 : 118\nEpoch : 21, batch 325\n(Train) Batch 325 Loss : 0.3255687654018402, 맞은 개수 : 117\nEpoch : 21, batch 326\n(Train) Batch 326 Loss : 0.21152640879154205, 맞은 개수 : 118\nEpoch : 21, batch 327\n(Train) Batch 327 Loss : 0.3939918279647827, 맞은 개수 : 113\nEpoch : 21, batch 328\n(Train) Batch 328 Loss : 0.1536155790090561, 맞은 개수 : 122\nEpoch : 21, batch 329\n(Train) Batch 329 Loss : 0.22483184933662415, 맞은 개수 : 117\nEpoch : 21, batch 330\n(Train) Batch 330 Loss : 0.1777009516954422, 맞은 개수 : 122\nEpoch : 21, batch 331\n(Train) Batch 331 Loss : 0.21806983649730682, 맞은 개수 : 121\nEpoch : 21, batch 332\n(Train) Batch 332 Loss : 0.24861252307891846, 맞은 개수 : 118\nEpoch : 21, batch 333\n(Train) Batch 333 Loss : 0.3233224153518677, 맞은 개수 : 116\nEpoch : 21, batch 334\n(Train) Batch 334 Loss : 0.3010224401950836, 맞은 개수 : 116\nEpoch : 21, batch 335\n(Train) Batch 335 Loss : 0.1799696385860443, 맞은 개수 : 120\nEpoch : 21, batch 336\n(Train) Batch 336 Loss : 0.1971428394317627, 맞은 개수 : 121\nEpoch : 21, batch 337\n(Train) Batch 337 Loss : 0.3374227285385132, 맞은 개수 : 113\nEpoch : 21, batch 338\n(Train) Batch 338 Loss : 0.302567720413208, 맞은 개수 : 111\nEpoch : 21, batch 339\n(Train) Batch 339 Loss : 0.2044868767261505, 맞은 개수 : 117\nEpoch : 21, batch 340\n(Train) Batch 340 Loss : 0.13053077459335327, 맞은 개수 : 121\nEpoch : 21, batch 341\n(Train) Batch 341 Loss : 0.30244317650794983, 맞은 개수 : 115\nEpoch : 21, batch 342\n(Train) Batch 342 Loss : 0.2807970941066742, 맞은 개수 : 116\nEpoch : 21, batch 343\n(Train) Batch 343 Loss : 0.15067856013774872, 맞은 개수 : 122\nEpoch : 21, batch 344\n(Train) Batch 344 Loss : 0.3191620707511902, 맞은 개수 : 118\nEpoch : 21, batch 345\n(Train) Batch 345 Loss : 0.2842322587966919, 맞은 개수 : 121\nEpoch : 21, batch 346\n(Train) Batch 346 Loss : 0.30303308367729187, 맞은 개수 : 116\nEpoch : 21, batch 347\n(Train) Batch 347 Loss : 0.270083487033844, 맞은 개수 : 116\nEpoch : 21, batch 348\n(Train) Batch 348 Loss : 0.31354284286499023, 맞은 개수 : 115\nEpoch : 21, batch 349\n(Train) Batch 349 Loss : 0.15652933716773987, 맞은 개수 : 121\nEpoch : 21, batch 350\n(Train) Batch 350 Loss : 0.2768020033836365, 맞은 개수 : 120\nEpoch : 21, batch 351\n(Train) Batch 351 Loss : 0.2945738434791565, 맞은 개수 : 119\nEpoch : 21, batch 352\n(Train) Batch 352 Loss : 0.2648925185203552, 맞은 개수 : 118\nEpoch : 21, batch 353\n(Train) Batch 353 Loss : 0.2437639683485031, 맞은 개수 : 120\nEpoch : 21, batch 354\n(Train) Batch 354 Loss : 0.174769327044487, 맞은 개수 : 120\nEpoch : 21, batch 355\n(Train) Batch 355 Loss : 0.36316028237342834, 맞은 개수 : 112\nEpoch : 21, batch 356\n(Train) Batch 356 Loss : 0.1567608118057251, 맞은 개수 : 123\nEpoch : 21, batch 357\n(Train) Batch 357 Loss : 0.11717557907104492, 맞은 개수 : 123\nEpoch : 21, batch 358\n(Train) Batch 358 Loss : 0.2902793288230896, 맞은 개수 : 117\nEpoch : 21, batch 359\n(Train) Batch 359 Loss : 0.3671327829360962, 맞은 개수 : 116\nEpoch : 21, batch 360\n(Train) Batch 360 Loss : 0.3183895945549011, 맞은 개수 : 114\nEpoch : 21, batch 361\n(Train) Batch 361 Loss : 0.25789594650268555, 맞은 개수 : 118\nEpoch : 21, batch 362\n(Train) Batch 362 Loss : 0.18184532225131989, 맞은 개수 : 119\nEpoch : 21, batch 363\n(Train) Batch 363 Loss : 0.30019521713256836, 맞은 개수 : 116\nEpoch : 21, batch 364\n(Train) Batch 364 Loss : 0.17147931456565857, 맞은 개수 : 120\nEpoch : 21, batch 365\n(Train) Batch 365 Loss : 0.2700783610343933, 맞은 개수 : 116\nEpoch : 21, batch 366\n(Train) Batch 366 Loss : 0.15297725796699524, 맞은 개수 : 123\nEpoch : 21, batch 367\n(Train) Batch 367 Loss : 0.23454631865024567, 맞은 개수 : 116\nEpoch : 21, batch 368\n(Train) Batch 368 Loss : 0.34446224570274353, 맞은 개수 : 116\nEpoch : 21, batch 369\n(Train) Batch 369 Loss : 0.3188924193382263, 맞은 개수 : 118\nEpoch : 21, batch 370\n(Train) Batch 370 Loss : 0.1679074913263321, 맞은 개수 : 121\nEpoch : 21, batch 371\n(Train) Batch 371 Loss : 0.3285423815250397, 맞은 개수 : 118\nEpoch : 21, batch 372\n(Train) Batch 372 Loss : 0.3822895586490631, 맞은 개수 : 111\nEpoch : 21, batch 373\n(Train) Batch 373 Loss : 0.3652820587158203, 맞은 개수 : 115\nEpoch : 21, batch 374\n(Train) Batch 374 Loss : 0.24305889010429382, 맞은 개수 : 119\nEpoch : 21, batch 375\n(Train) Batch 375 Loss : 0.23025088012218475, 맞은 개수 : 118\nEpoch : 21, batch 376\n(Train) Batch 376 Loss : 0.23345279693603516, 맞은 개수 : 118\nEpoch : 21, batch 377\n(Train) Batch 377 Loss : 0.1991117000579834, 맞은 개수 : 121\nEpoch : 21, batch 378\n(Train) Batch 378 Loss : 0.3351626694202423, 맞은 개수 : 116\nEpoch : 21, batch 379\n(Train) Batch 379 Loss : 0.2691917419433594, 맞은 개수 : 115\nEpoch : 21, batch 380\n(Train) Batch 380 Loss : 0.29292476177215576, 맞은 개수 : 116\nEpoch : 21, batch 381\n(Train) Batch 381 Loss : 0.3518249988555908, 맞은 개수 : 117\nEpoch : 21, batch 382\n(Train) Batch 382 Loss : 0.13260947167873383, 맞은 개수 : 124\nEpoch : 21, batch 383\n(Train) Batch 383 Loss : 0.28495606780052185, 맞은 개수 : 115\nEpoch : 21, batch 384\n(Train) Batch 384 Loss : 0.2073153853416443, 맞은 개수 : 119\nEpoch : 21, batch 385\n(Train) Batch 385 Loss : 0.1288747936487198, 맞은 개수 : 123\nEpoch : 21, batch 386\n(Train) Batch 386 Loss : 0.2201954573392868, 맞은 개수 : 118\nEpoch : 21, batch 387\n(Train) Batch 387 Loss : 0.2624738812446594, 맞은 개수 : 118\nEpoch : 21, batch 388\n(Train) Batch 388 Loss : 0.23670470714569092, 맞은 개수 : 119\nEpoch : 21, batch 389\n(Train) Batch 389 Loss : 0.28277719020843506, 맞은 개수 : 117\nEpoch : 21, batch 390\n(Train) Batch 390 Loss : 0.16801969707012177, 맞은 개수 : 121\nEpoch : 21, batch 391\n(Train) Batch 391 Loss : 0.16742365062236786, 맞은 개수 : 120\nEpoch : 21, batch 392\n(Train) Batch 392 Loss : 0.19565615057945251, 맞은 개수 : 121\nEpoch : 21, batch 393\n(Train) Batch 393 Loss : 0.1719026267528534, 맞은 개수 : 124\nEpoch : 21, batch 394\n(Train) Batch 394 Loss : 0.32901257276535034, 맞은 개수 : 116\nEpoch : 21, batch 395\n(Train) Batch 395 Loss : 0.26054468750953674, 맞은 개수 : 115\nEpoch : 21, batch 396\n(Train) Batch 396 Loss : 0.27772021293640137, 맞은 개수 : 119\nEpoch : 21, batch 397\n(Train) Batch 397 Loss : 0.2617384195327759, 맞은 개수 : 123\nEpoch : 21, batch 398\n(Train) Batch 398 Loss : 0.18438518047332764, 맞은 개수 : 120\nEpoch : 21, batch 399\n(Train) Batch 399 Loss : 0.46210142970085144, 맞은 개수 : 113\nEpoch : 21, batch 400\n(Train) Batch 400 Loss : 0.3338637948036194, 맞은 개수 : 116\nEpoch : 21, batch 401\n(Train) Batch 401 Loss : 0.20809541642665863, 맞은 개수 : 121\nEpoch : 21, batch 402\n(Train) Batch 402 Loss : 0.2361593097448349, 맞은 개수 : 115\nEpoch : 21, batch 403\n(Train) Batch 403 Loss : 0.2321658730506897, 맞은 개수 : 120\nEpoch : 21, batch 404\n(Train) Batch 404 Loss : 0.29283052682876587, 맞은 개수 : 116\nEpoch : 21, batch 405\n(Train) Batch 405 Loss : 0.25094226002693176, 맞은 개수 : 117\nEpoch : 21, batch 406\n(Train) Batch 406 Loss : 0.20926345884799957, 맞은 개수 : 117\nEpoch : 21, batch 407\n(Train) Batch 407 Loss : 0.24471083283424377, 맞은 개수 : 119\nEpoch : 21, batch 408\n(Train) Batch 408 Loss : 0.32986417412757874, 맞은 개수 : 111\nEpoch : 21, batch 409\n(Train) Batch 409 Loss : 0.26703473925590515, 맞은 개수 : 113\nEpoch : 21, batch 410\n(Train) Batch 410 Loss : 0.2638739049434662, 맞은 개수 : 117\nEpoch : 21, batch 411\n(Train) Batch 411 Loss : 0.28949064016342163, 맞은 개수 : 117\nEpoch : 21, batch 412\n(Train) Batch 412 Loss : 0.12530404329299927, 맞은 개수 : 123\nEpoch : 21, batch 413\n(Train) Batch 413 Loss : 0.19615061581134796, 맞은 개수 : 121\nEpoch : 21, batch 414\n(Train) Batch 414 Loss : 0.28906890749931335, 맞은 개수 : 116\nEpoch : 21, batch 415\n(Train) Batch 415 Loss : 0.24704571068286896, 맞은 개수 : 119\nEpoch : 21, batch 416\n(Train) Batch 416 Loss : 0.2491455078125, 맞은 개수 : 118\nEpoch : 21, batch 417\n(Train) Batch 417 Loss : 0.3366318941116333, 맞은 개수 : 115\nEpoch : 21, batch 418\n(Train) Batch 418 Loss : 0.27056819200515747, 맞은 개수 : 116\nEpoch : 21, batch 419\n(Train) Batch 419 Loss : 0.419423371553421, 맞은 개수 : 115\nEpoch : 21, batch 420\n(Train) Batch 420 Loss : 0.12494993954896927, 맞은 개수 : 122\nEpoch : 21, batch 421\n(Train) Batch 421 Loss : 0.19239689409732819, 맞은 개수 : 123\nEpoch : 21, batch 422\n(Train) Batch 422 Loss : 0.21795758605003357, 맞은 개수 : 122\nEpoch : 21, batch 423\n(Train) Batch 423 Loss : 0.22113582491874695, 맞은 개수 : 118\nEpoch : 21, batch 424\n(Train) Batch 424 Loss : 0.22158518433570862, 맞은 개수 : 121\nEpoch : 21, batch 425\n(Train) Batch 425 Loss : 0.1866115927696228, 맞은 개수 : 120\nEpoch : 21, batch 426\n(Train) Batch 426 Loss : 0.2976592481136322, 맞은 개수 : 115\nEpoch : 21, batch 427\n(Train) Batch 427 Loss : 0.15375398099422455, 맞은 개수 : 121\nEpoch : 21, batch 428\n(Train) Batch 428 Loss : 0.43407344818115234, 맞은 개수 : 111\nEpoch : 21, batch 429\n(Train) Batch 429 Loss : 0.2850715219974518, 맞은 개수 : 115\nEpoch : 21, batch 430\n(Train) Batch 430 Loss : 0.2713072896003723, 맞은 개수 : 117\nEpoch : 21, batch 431\n(Train) Batch 431 Loss : 0.2927546501159668, 맞은 개수 : 116\nEpoch : 21, batch 432\n(Train) Batch 432 Loss : 0.3544408082962036, 맞은 개수 : 113\nEpoch : 21, batch 433\n(Train) Batch 433 Loss : 0.2892734110355377, 맞은 개수 : 119\nEpoch : 21, batch 434\n(Train) Batch 434 Loss : 0.21995270252227783, 맞은 개수 : 115\nEpoch : 21, batch 435\n(Train) Batch 435 Loss : 0.1236603632569313, 맞은 개수 : 123\nEpoch : 21, batch 436\n(Train) Batch 436 Loss : 0.2378942370414734, 맞은 개수 : 119\nEpoch : 21, batch 437\n(Train) Batch 437 Loss : 0.18403568863868713, 맞은 개수 : 119\nEpoch : 21, batch 438\n(Train) Batch 438 Loss : 0.2403983771800995, 맞은 개수 : 115\nEpoch : 21, batch 439\n(Train) Batch 439 Loss : 0.3169907033443451, 맞은 개수 : 114\nEpoch : 21, batch 440\n(Train) Batch 440 Loss : 0.22753582894802094, 맞은 개수 : 120\nEpoch : 21, batch 441\n(Train) Batch 441 Loss : 0.3202231228351593, 맞은 개수 : 115\nEpoch : 21, batch 442\n(Train) Batch 442 Loss : 0.19674476981163025, 맞은 개수 : 119\nEpoch : 21, batch 443\n(Train) Batch 443 Loss : 0.33767303824424744, 맞은 개수 : 114\nEpoch : 21, batch 444\n(Train) Batch 444 Loss : 0.2523845136165619, 맞은 개수 : 118\nEpoch : 21, batch 445\n(Train) Batch 445 Loss : 0.196555957198143, 맞은 개수 : 121\nEpoch : 21, batch 446\n(Train) Batch 446 Loss : 0.4385931193828583, 맞은 개수 : 110\nEpoch : 21, batch 447\n(Train) Batch 447 Loss : 0.2765532433986664, 맞은 개수 : 121\nEpoch : 21, batch 448\n(Train) Batch 448 Loss : 0.20178315043449402, 맞은 개수 : 122\nEpoch : 21, batch 449\n(Train) Batch 449 Loss : 0.31016805768013, 맞은 개수 : 115\nEpoch : 21, batch 450\n(Train) Batch 450 Loss : 0.2227085530757904, 맞은 개수 : 122\nEpoch : 21, batch 451\n(Train) Batch 451 Loss : 0.16495971381664276, 맞은 개수 : 119\nEpoch : 21, batch 452\n(Train) Batch 452 Loss : 0.252867192029953, 맞은 개수 : 117\nEpoch : 21, batch 453\n(Train) Batch 453 Loss : 0.156844362616539, 맞은 개수 : 121\nEpoch : 21, batch 454\n(Train) Batch 454 Loss : 0.13017012178897858, 맞은 개수 : 122\nEpoch : 21, batch 455\n(Train) Batch 455 Loss : 0.25798356533050537, 맞은 개수 : 116\nEpoch : 21, batch 456\n(Train) Batch 456 Loss : 0.3046966791152954, 맞은 개수 : 111\nEpoch : 21, batch 457\n(Train) Batch 457 Loss : 0.2519931495189667, 맞은 개수 : 116\nEpoch : 21, batch 458\n(Train) Batch 458 Loss : 0.18542519211769104, 맞은 개수 : 119\nEpoch : 21, batch 459\n(Train) Batch 459 Loss : 0.2411366105079651, 맞은 개수 : 116\nEpoch : 21, batch 460\n(Train) Batch 460 Loss : 0.29446089267730713, 맞은 개수 : 122\nEpoch : 21, batch 461\n(Train) Batch 461 Loss : 0.30648598074913025, 맞은 개수 : 114\nEpoch : 21, batch 462\n(Train) Batch 462 Loss : 0.18088501691818237, 맞은 개수 : 118\nEpoch : 21, batch 463\n(Train) Batch 463 Loss : 0.24227996170520782, 맞은 개수 : 116\nEpoch : 21, batch 464\n(Train) Batch 464 Loss : 0.14780108630657196, 맞은 개수 : 124\nEpoch : 21, batch 465\n(Train) Batch 465 Loss : 0.18573097884655, 맞은 개수 : 122\nEpoch : 21, batch 466\n(Train) Batch 466 Loss : 0.23940084874629974, 맞은 개수 : 116\nEpoch : 21, batch 467\n(Train) Batch 467 Loss : 0.2501477599143982, 맞은 개수 : 122\nEpoch : 21, batch 468\n(Train) Batch 468 Loss : 0.3202616572380066, 맞은 개수 : 112\nEpoch : 21, batch 469\n(Train) Batch 469 Loss : 0.14810554683208466, 맞은 개수 : 121\nEpoch : 21, batch 470\n(Train) Batch 470 Loss : 0.28114357590675354, 맞은 개수 : 116\nEpoch : 21, batch 471\n(Train) Batch 471 Loss : 0.20350562036037445, 맞은 개수 : 122\nEpoch : 21, batch 472\n(Train) Batch 472 Loss : 0.16913774609565735, 맞은 개수 : 119\nEpoch : 21, batch 473\n(Train) Batch 473 Loss : 0.40310606360435486, 맞은 개수 : 114\nEpoch : 21, batch 474\n(Train) Batch 474 Loss : 0.3589935004711151, 맞은 개수 : 115\nEpoch : 21, batch 475\n(Train) Batch 475 Loss : 0.18436269462108612, 맞은 개수 : 120\nEpoch : 21, batch 476\n(Train) Batch 476 Loss : 0.14165668189525604, 맞은 개수 : 122\nEpoch : 21, batch 477\n(Train) Batch 477 Loss : 0.27649641036987305, 맞은 개수 : 116\nEpoch : 21, batch 478\n(Train) Batch 478 Loss : 0.29599642753601074, 맞은 개수 : 115\nEpoch : 21, batch 479\n(Train) Batch 479 Loss : 0.30849266052246094, 맞은 개수 : 118\nEpoch : 21, batch 480\n(Train) Batch 480 Loss : 0.20041118562221527, 맞은 개수 : 121\nEpoch : 21, batch 481\n(Train) Batch 481 Loss : 0.14272314310073853, 맞은 개수 : 121\nEpoch : 21, batch 482\n(Train) Batch 482 Loss : 0.23515933752059937, 맞은 개수 : 119\nEpoch : 21, batch 483\n(Train) Batch 483 Loss : 0.11691167205572128, 맞은 개수 : 124\nEpoch : 21, batch 484\n(Train) Batch 484 Loss : 0.1836645007133484, 맞은 개수 : 120\nEpoch : 21, batch 485\n(Train) Batch 485 Loss : 0.2632589042186737, 맞은 개수 : 117\nEpoch : 21, batch 486\n(Train) Batch 486 Loss : 0.36598333716392517, 맞은 개수 : 117\nEpoch : 21, batch 487\n(Train) Batch 487 Loss : 0.18130002915859222, 맞은 개수 : 123\nEpoch : 21, batch 488\n(Train) Batch 488 Loss : 0.21880261600017548, 맞은 개수 : 120\nEpoch : 21, batch 489\n(Train) Batch 489 Loss : 0.351727694272995, 맞은 개수 : 116\nEpoch : 21, batch 490\n(Train) Batch 490 Loss : 0.26314446330070496, 맞은 개수 : 119\nEpoch : 21, batch 491\n(Train) Batch 491 Loss : 0.32001394033432007, 맞은 개수 : 117\nEpoch : 21, batch 492\n(Train) Batch 492 Loss : 0.23320835828781128, 맞은 개수 : 118\nEpoch : 21, batch 493\n(Train) Batch 493 Loss : 0.2775513529777527, 맞은 개수 : 115\nEpoch : 21, batch 494\n(Train) Batch 494 Loss : 0.22375226020812988, 맞은 개수 : 120\nEpoch : 21, batch 495\n(Train) Batch 495 Loss : 0.31786012649536133, 맞은 개수 : 114\nEpoch : 21, batch 496\n(Train) Batch 496 Loss : 0.4718596637248993, 맞은 개수 : 108\nEpoch : 21, batch 497\n(Train) Batch 497 Loss : 0.34863001108169556, 맞은 개수 : 113\nEpoch : 21, batch 498\n(Train) Batch 498 Loss : 0.217742919921875, 맞은 개수 : 117\nEpoch : 21, batch 499\n(Train) Batch 499 Loss : 0.2214445322751999, 맞은 개수 : 121\nEpoch : 21, batch 500\n(Train) Batch 500 Loss : 0.2628137767314911, 맞은 개수 : 116\nEpoch : 21, batch 501\n(Train) Batch 501 Loss : 0.16742977499961853, 맞은 개수 : 121\nEpoch : 21, batch 502\n(Train) Batch 502 Loss : 0.17561306059360504, 맞은 개수 : 119\nEpoch : 21, batch 503\n(Train) Batch 503 Loss : 0.2626951336860657, 맞은 개수 : 117\nEpoch : 21, batch 504\n(Train) Batch 504 Loss : 0.19826990365982056, 맞은 개수 : 120\nEpoch : 21, batch 505\n(Train) Batch 505 Loss : 0.15657123923301697, 맞은 개수 : 121\nEpoch : 21, batch 506\n(Train) Batch 506 Loss : 0.33277627825737, 맞은 개수 : 114\nEpoch : 21, batch 507\n(Train) Batch 507 Loss : 0.2112421840429306, 맞은 개수 : 118\nEpoch : 21, batch 508\n(Train) Batch 508 Loss : 0.22868546843528748, 맞은 개수 : 119\nEpoch : 21, batch 509\n(Train) Batch 509 Loss : 0.37085944414138794, 맞은 개수 : 115\nEpoch : 21, batch 510\n(Train) Batch 510 Loss : 0.1123741865158081, 맞은 개수 : 122\nEpoch : 21, batch 511\n(Train) Batch 511 Loss : 0.3013918399810791, 맞은 개수 : 116\nEpoch : 21, batch 512\n(Train) Batch 512 Loss : 0.2581269443035126, 맞은 개수 : 120\nEpoch : 21, batch 513\n(Train) Batch 513 Loss : 0.1671363264322281, 맞은 개수 : 122\nEpoch : 21, batch 514\n(Train) Batch 514 Loss : 0.17753350734710693, 맞은 개수 : 119\nEpoch : 21, batch 515\n(Train) Batch 515 Loss : 0.40835171937942505, 맞은 개수 : 112\nEpoch : 21, batch 516\n(Train) Batch 516 Loss : 0.15853361785411835, 맞은 개수 : 121\nEpoch : 21, batch 517\n(Train) Batch 517 Loss : 0.22611314058303833, 맞은 개수 : 119\nEpoch : 21, batch 518\n(Train) Batch 518 Loss : 0.4892425239086151, 맞은 개수 : 114\nEpoch : 21, batch 519\n(Train) Batch 519 Loss : 0.2643703818321228, 맞은 개수 : 120\nEpoch : 21, batch 520\n(Train) Batch 520 Loss : 0.18494237959384918, 맞은 개수 : 120\nEpoch : 21, batch 521\n(Train) Batch 521 Loss : 0.31537505984306335, 맞은 개수 : 116\nEpoch : 21, batch 522\n(Train) Batch 522 Loss : 0.247637540102005, 맞은 개수 : 116\nEpoch : 21, batch 523\n(Train) Batch 523 Loss : 0.3517386317253113, 맞은 개수 : 119\nEpoch : 21, batch 524\n(Train) Batch 524 Loss : 0.24219977855682373, 맞은 개수 : 121\nEpoch : 21, batch 525\n(Train) Batch 525 Loss : 0.21920450031757355, 맞은 개수 : 120\nEpoch : 21, batch 526\n(Train) Batch 526 Loss : 0.2779902219772339, 맞은 개수 : 117\nEpoch : 21, batch 527\n(Train) Batch 527 Loss : 0.22105872631072998, 맞은 개수 : 121\nEpoch : 21, batch 528\n(Train) Batch 528 Loss : 0.2266143411397934, 맞은 개수 : 119\nEpoch : 21, batch 529\n(Train) Batch 529 Loss : 0.16589272022247314, 맞은 개수 : 124\nEpoch : 21, batch 530\n(Train) Batch 530 Loss : 0.24516409635543823, 맞은 개수 : 117\nEpoch : 21, batch 531\n(Train) Batch 531 Loss : 0.3256984055042267, 맞은 개수 : 118\nEpoch : 21, batch 532\n(Train) Batch 532 Loss : 0.31903979182243347, 맞은 개수 : 119\nEpoch : 21, batch 533\n(Train) Batch 533 Loss : 0.21707232296466827, 맞은 개수 : 119\nEpoch : 21, batch 534\n(Train) Batch 534 Loss : 0.31452110409736633, 맞은 개수 : 116\nEpoch : 21, batch 535\n(Train) Batch 535 Loss : 0.207877978682518, 맞은 개수 : 121\nEpoch : 21, batch 536\n(Train) Batch 536 Loss : 0.47096532583236694, 맞은 개수 : 109\nEpoch : 21, batch 537\n(Train) Batch 537 Loss : 0.31386125087738037, 맞은 개수 : 112\nEpoch : 21, batch 538\n(Train) Batch 538 Loss : 0.15608790516853333, 맞은 개수 : 122\nEpoch : 21, batch 539\n(Train) Batch 539 Loss : 0.38904547691345215, 맞은 개수 : 114\nEpoch : 21, batch 540\n(Train) Batch 540 Loss : 0.27232125401496887, 맞은 개수 : 117\nEpoch : 21, batch 541\n(Train) Batch 541 Loss : 0.28944215178489685, 맞은 개수 : 116\nEpoch : 21, batch 542\n(Train) Batch 542 Loss : 0.30663710832595825, 맞은 개수 : 115\nEpoch : 21, batch 543\n(Train) Batch 543 Loss : 0.27242863178253174, 맞은 개수 : 118\nEpoch : 21, batch 544\n(Train) Batch 544 Loss : 0.24268478155136108, 맞은 개수 : 116\nEpoch : 21, batch 545\n(Train) Batch 545 Loss : 0.34064579010009766, 맞은 개수 : 114\nEpoch : 21, batch 546\n(Train) Batch 546 Loss : 0.2531323730945587, 맞은 개수 : 119\nEpoch : 21, batch 547\n(Train) Batch 547 Loss : 0.3587169051170349, 맞은 개수 : 113\nEpoch : 21, batch 548\n(Train) Batch 548 Loss : 0.35009101033210754, 맞은 개수 : 112\nEpoch : 21, batch 549\n(Train) Batch 549 Loss : 0.23997843265533447, 맞은 개수 : 120\nEpoch : 21, batch 550\n(Train) Batch 550 Loss : 0.3842138648033142, 맞은 개수 : 114\nEpoch : 21, batch 551\n(Train) Batch 551 Loss : 0.18523508310317993, 맞은 개수 : 118\nEpoch : 21, batch 552\n(Train) Batch 552 Loss : 0.39628368616104126, 맞은 개수 : 111\nEpoch : 21, batch 553\n(Train) Batch 553 Loss : 0.23325930535793304, 맞은 개수 : 119\nEpoch : 21, batch 554\n(Train) Batch 554 Loss : 0.2517901360988617, 맞은 개수 : 122\nEpoch : 21, batch 555\n(Train) Batch 555 Loss : 0.18796421587467194, 맞은 개수 : 119\nEpoch : 21, batch 556\n(Train) Batch 556 Loss : 0.29498225450515747, 맞은 개수 : 116\nEpoch : 21, batch 557\n(Train) Batch 557 Loss : 0.1912112832069397, 맞은 개수 : 119\nEpoch : 21, batch 558\n(Train) Batch 558 Loss : 0.2627483606338501, 맞은 개수 : 119\nEpoch : 21, batch 559\n(Train) Batch 559 Loss : 0.19717052578926086, 맞은 개수 : 118\nEpoch : 21, batch 560\n(Train) Batch 560 Loss : 0.2344551533460617, 맞은 개수 : 121\nEpoch : 21, batch 561\n(Train) Batch 561 Loss : 0.28635334968566895, 맞은 개수 : 117\nEpoch : 21, batch 562\n(Train) Batch 562 Loss : 0.2524903416633606, 맞은 개수 : 116\nEpoch : 21, batch 563\n(Train) Batch 563 Loss : 0.28370025753974915, 맞은 개수 : 117\nEpoch : 21, batch 564\n(Train) Batch 564 Loss : 0.21224744617938995, 맞은 개수 : 116\nEpoch : 21, batch 565\n(Train) Batch 565 Loss : 0.2124338299036026, 맞은 개수 : 119\nEpoch : 21, batch 566\n(Train) Batch 566 Loss : 0.35709190368652344, 맞은 개수 : 115\nEpoch : 21, batch 567\n(Train) Batch 567 Loss : 0.27409616112709045, 맞은 개수 : 117\nEpoch : 21, batch 568\n(Train) Batch 568 Loss : 0.3315770626068115, 맞은 개수 : 111\nEpoch : 21, batch 569\n(Train) Batch 569 Loss : 0.2709997892379761, 맞은 개수 : 119\nEpoch : 21, batch 570\n(Train) Batch 570 Loss : 0.4797503650188446, 맞은 개수 : 116\nEpoch : 21, batch 571\n(Train) Batch 571 Loss : 0.61402428150177, 맞은 개수 : 107\nEpoch : 21, batch 572\n(Train) Batch 572 Loss : 0.29453545808792114, 맞은 개수 : 116\nEpoch : 21, batch 573\n(Train) Batch 573 Loss : 0.21838606894016266, 맞은 개수 : 118\nEpoch : 21, batch 574\n(Train) Batch 574 Loss : 0.24097780883312225, 맞은 개수 : 119\nEpoch : 21, batch 575\n(Train) Batch 575 Loss : 0.31693175435066223, 맞은 개수 : 119\nEpoch : 21, batch 576\n(Train) Batch 576 Loss : 0.3988174796104431, 맞은 개수 : 115\nEpoch : 21, batch 577\n(Train) Batch 577 Loss : 0.19194452464580536, 맞은 개수 : 119\nEpoch : 21, batch 578\n(Train) Batch 578 Loss : 0.5090466141700745, 맞은 개수 : 109\nEpoch : 21, batch 579\n(Train) Batch 579 Loss : 0.30681464076042175, 맞은 개수 : 116\nEpoch : 21, batch 580\n(Train) Batch 580 Loss : 0.2116677314043045, 맞은 개수 : 119\nEpoch : 21, batch 581\n(Train) Batch 581 Loss : 0.17991763353347778, 맞은 개수 : 122\nEpoch : 21, batch 582\n(Train) Batch 582 Loss : 0.25765517354011536, 맞은 개수 : 118\nEpoch : 21, batch 583\n(Train) Batch 583 Loss : 0.29042595624923706, 맞은 개수 : 117\nEpoch : 21, batch 584\n(Train) Batch 584 Loss : 0.2688516080379486, 맞은 개수 : 114\nEpoch : 21, batch 585\n(Train) Batch 585 Loss : 0.29572293162345886, 맞은 개수 : 119\nEpoch : 21, batch 586\n(Train) Batch 586 Loss : 0.2439393401145935, 맞은 개수 : 119\nEpoch : 21, batch 587\n(Train) Batch 587 Loss : 0.27473005652427673, 맞은 개수 : 116\nEpoch : 21, batch 588\n(Train) Batch 588 Loss : 0.34848442673683167, 맞은 개수 : 117\nEpoch : 21, batch 589\n(Train) Batch 589 Loss : 0.2958553433418274, 맞은 개수 : 115\nEpoch : 21, batch 590\n(Train) Batch 590 Loss : 0.2381921410560608, 맞은 개수 : 116\nEpoch : 21, batch 591\n(Train) Batch 591 Loss : 0.35588589310646057, 맞은 개수 : 116\nEpoch : 21, batch 592\n(Train) Batch 592 Loss : 0.1821967512369156, 맞은 개수 : 124\nEpoch : 21, batch 593\n(Train) Batch 593 Loss : 0.3332662880420685, 맞은 개수 : 111\nEpoch : 21, batch 594\n(Train) Batch 594 Loss : 0.29473721981048584, 맞은 개수 : 117\nEpoch : 21, batch 595\n(Train) Batch 595 Loss : 0.25404661893844604, 맞은 개수 : 115\nEpoch : 21, batch 596\n(Train) Batch 596 Loss : 0.2586445212364197, 맞은 개수 : 120\nEpoch : 21, batch 597\n(Train) Batch 597 Loss : 0.37664860486984253, 맞은 개수 : 116\nEpoch : 21, batch 598\n(Train) Batch 598 Loss : 0.2113802134990692, 맞은 개수 : 117\nEpoch : 21, batch 599\n(Train) Batch 599 Loss : 0.27799370884895325, 맞은 개수 : 120\nEpoch : 21, batch 600\n(Train) Batch 600 Loss : 0.5075401663780212, 맞은 개수 : 112\nEpoch : 21, batch 601\n(Train) Batch 601 Loss : 0.32893335819244385, 맞은 개수 : 116\nEpoch : 21, batch 602\n(Train) Batch 602 Loss : 0.4527668058872223, 맞은 개수 : 115\nEpoch : 21, batch 603\n(Train) Batch 603 Loss : 0.17874091863632202, 맞은 개수 : 118\nEpoch : 21, batch 604\n(Train) Batch 604 Loss : 0.1604723185300827, 맞은 개수 : 121\nEpoch : 21, batch 605\n(Train) Batch 605 Loss : 0.22936028242111206, 맞은 개수 : 117\nEpoch : 21, batch 606\n(Train) Batch 606 Loss : 0.19657231867313385, 맞은 개수 : 121\nEpoch : 21, batch 607\n(Train) Batch 607 Loss : 0.1871929168701172, 맞은 개수 : 120\nEpoch : 21, batch 608\n(Train) Batch 608 Loss : 0.44096124172210693, 맞은 개수 : 114\nEpoch : 21, batch 609\n(Train) Batch 609 Loss : 0.23097634315490723, 맞은 개수 : 120\nEpoch : 21, batch 610\n(Train) Batch 610 Loss : 0.27436700463294983, 맞은 개수 : 120\nEpoch : 21, batch 611\n(Train) Batch 611 Loss : 0.3212127685546875, 맞은 개수 : 116\nEpoch : 21, batch 612\n(Train) Batch 612 Loss : 0.35104840993881226, 맞은 개수 : 116\nEpoch : 21, batch 613\n(Train) Batch 613 Loss : 0.2235718071460724, 맞은 개수 : 120\nEpoch : 21, batch 614\n(Train) Batch 614 Loss : 0.24343886971473694, 맞은 개수 : 118\nEpoch : 21, batch 615\n(Train) Batch 615 Loss : 0.3349064290523529, 맞은 개수 : 117\nEpoch : 21, batch 616\n(Train) Batch 616 Loss : 0.2778657078742981, 맞은 개수 : 114\nEpoch : 21, batch 617\n(Train) Batch 617 Loss : 0.2723658084869385, 맞은 개수 : 118\nEpoch : 21, batch 618\n(Train) Batch 618 Loss : 0.2599925696849823, 맞은 개수 : 118\nEpoch : 21, batch 619\n(Train) Batch 619 Loss : 0.22213079035282135, 맞은 개수 : 119\nEpoch : 21, batch 620\n(Train) Batch 620 Loss : 0.28383195400238037, 맞은 개수 : 115\nEpoch : 21, batch 621\n(Train) Batch 621 Loss : 0.22706353664398193, 맞은 개수 : 119\nEpoch : 21, batch 622\n(Train) Batch 622 Loss : 0.2095465511083603, 맞은 개수 : 120\nEpoch : 21, batch 623\n(Train) Batch 623 Loss : 0.25308600068092346, 맞은 개수 : 121\nEpoch : 21, batch 624\n(Train) Batch 624 Loss : 0.4619792103767395, 맞은 개수 : 112\nEpoch : 21, batch 625\n(Train) Batch 625 Loss : 0.1599145382642746, 맞은 개수 : 122\nEpoch : 21, batch 626\n(Train) Batch 626 Loss : 0.24743223190307617, 맞은 개수 : 116\nEpoch : 21, batch 627\n(Train) Batch 627 Loss : 0.2472371608018875, 맞은 개수 : 116\nEpoch : 21, batch 628\n(Train) Batch 628 Loss : 0.321999728679657, 맞은 개수 : 115\nEpoch : 21, batch 629\n(Train) Batch 629 Loss : 0.2989746332168579, 맞은 개수 : 117\nEpoch : 21, batch 630\n(Train) Batch 630 Loss : 0.17603935301303864, 맞은 개수 : 118\nEpoch : 21, batch 631\n(Train) Batch 631 Loss : 0.3043697774410248, 맞은 개수 : 114\nEpoch : 21, batch 632\n(Train) Batch 632 Loss : 0.21069365739822388, 맞은 개수 : 122\nEpoch : 21, batch 633\n(Train) Batch 633 Loss : 0.27663159370422363, 맞은 개수 : 113\nEpoch : 21, batch 634\n(Train) Batch 634 Loss : 0.2953617572784424, 맞은 개수 : 116\nEpoch : 21, batch 635\n(Train) Batch 635 Loss : 0.35662221908569336, 맞은 개수 : 113\nEpoch : 21, batch 636\n(Train) Batch 636 Loss : 0.2594009041786194, 맞은 개수 : 117\nEpoch : 21, batch 637\n(Train) Batch 637 Loss : 0.2772093713283539, 맞은 개수 : 116\nEpoch : 21, batch 638\n(Train) Batch 638 Loss : 0.3791707456111908, 맞은 개수 : 116\nEpoch : 21, batch 639\n(Train) Batch 639 Loss : 0.18843264877796173, 맞은 개수 : 122\nEpoch : 21, batch 640\n(Train) Batch 640 Loss : 0.15056918561458588, 맞은 개수 : 120\nEpoch : 21, batch 641\n(Train) Batch 641 Loss : 0.3706778287887573, 맞은 개수 : 115\nEpoch : 21, batch 642\n(Train) Batch 642 Loss : 0.1615828424692154, 맞은 개수 : 122\nEpoch : 21, batch 643\n(Train) Batch 643 Loss : 0.3200145363807678, 맞은 개수 : 116\nEpoch : 21, batch 644\n(Train) Batch 644 Loss : 0.2897287905216217, 맞은 개수 : 114\nEpoch : 21, batch 645\n(Train) Batch 645 Loss : 0.21756477653980255, 맞은 개수 : 120\nEpoch : 21, batch 646\n(Train) Batch 646 Loss : 0.23959697782993317, 맞은 개수 : 116\nEpoch : 21, batch 647\n(Train) Batch 647 Loss : 0.20243202149868011, 맞은 개수 : 122\nEpoch : 21, batch 648\n(Train) Batch 648 Loss : 0.20185939967632294, 맞은 개수 : 120\nEpoch : 21, batch 649\n(Train) Batch 649 Loss : 0.3190936744213104, 맞은 개수 : 114\nEpoch : 21, batch 650\n(Train) Batch 650 Loss : 0.17334766685962677, 맞은 개수 : 122\nEpoch : 21, batch 651\n(Train) Batch 651 Loss : 0.25009191036224365, 맞은 개수 : 118\nEpoch : 21, batch 652\n(Train) Batch 652 Loss : 0.46889400482177734, 맞은 개수 : 114\nEpoch : 21, batch 653\n(Train) Batch 653 Loss : 0.2418060153722763, 맞은 개수 : 116\nEpoch : 21, batch 654\n(Train) Batch 654 Loss : 0.35414713621139526, 맞은 개수 : 114\nEpoch : 21, batch 655\n(Train) Batch 655 Loss : 0.2347903549671173, 맞은 개수 : 118\nEpoch : 21, batch 656\n(Train) Batch 656 Loss : 0.18673871457576752, 맞은 개수 : 119\nEpoch : 21, batch 657\n(Train) Batch 657 Loss : 0.285090833902359, 맞은 개수 : 121\nEpoch : 21, batch 658\n(Train) Batch 658 Loss : 0.29366937279701233, 맞은 개수 : 114\nEpoch : 21, batch 659\n(Train) Batch 659 Loss : 0.2713964879512787, 맞은 개수 : 119\nEpoch : 21, batch 660\n(Train) Batch 660 Loss : 0.18533101677894592, 맞은 개수 : 122\nEpoch : 21, batch 661\n(Train) Batch 661 Loss : 0.15792711079120636, 맞은 개수 : 119\nEpoch : 21, batch 662\n(Train) Batch 662 Loss : 0.359144926071167, 맞은 개수 : 115\nEpoch : 21, batch 663\n(Train) Batch 663 Loss : 0.28068268299102783, 맞은 개수 : 116\nEpoch : 21, batch 664\n(Train) Batch 664 Loss : 0.19237641990184784, 맞은 개수 : 122\nEpoch : 21, batch 665\n(Train) Batch 665 Loss : 0.24027083814144135, 맞은 개수 : 119\nEpoch : 21, batch 666\n(Train) Batch 666 Loss : 0.276493638753891, 맞은 개수 : 119\nEpoch : 21, batch 667\n(Train) Batch 667 Loss : 0.10784675925970078, 맞은 개수 : 124\nEpoch : 21, batch 668\n(Train) Batch 668 Loss : 0.1658799648284912, 맞은 개수 : 120\nEpoch : 21, batch 669\n(Train) Batch 669 Loss : 0.20320039987564087, 맞은 개수 : 120\nEpoch : 21, batch 670\n(Train) Batch 670 Loss : 0.24480406939983368, 맞은 개수 : 117\nEpoch : 21, batch 671\n(Train) Batch 671 Loss : 0.1139628142118454, 맞은 개수 : 124\nEpoch : 21, batch 672\n(Train) Batch 672 Loss : 0.20270128548145294, 맞은 개수 : 117\nEpoch : 21, batch 673\n(Train) Batch 673 Loss : 0.2521211802959442, 맞은 개수 : 119\nEpoch : 21, batch 674\n(Train) Batch 674 Loss : 0.323647677898407, 맞은 개수 : 113\nEpoch : 21, batch 675\n(Train) Batch 675 Loss : 0.16602514684200287, 맞은 개수 : 120\nEpoch : 21, batch 676\n(Train) Batch 676 Loss : 0.5276911854743958, 맞은 개수 : 115\nEpoch : 21, batch 677\n(Train) Batch 677 Loss : 0.1620950549840927, 맞은 개수 : 119\nEpoch : 21, batch 678\n(Train) Batch 678 Loss : 0.15800544619560242, 맞은 개수 : 122\nEpoch : 21, batch 679\n(Train) Batch 679 Loss : 0.2514132857322693, 맞은 개수 : 118\nEpoch : 21, batch 680\n(Train) Batch 680 Loss : 0.24616394937038422, 맞은 개수 : 116\nEpoch : 21, batch 681\n(Train) Batch 681 Loss : 0.4698931872844696, 맞은 개수 : 112\nEpoch : 21, batch 682\n(Train) Batch 682 Loss : 0.1185820922255516, 맞은 개수 : 124\nEpoch : 21, batch 683\n(Train) Batch 683 Loss : 0.18474014103412628, 맞은 개수 : 123\nEpoch : 21, batch 684\n(Train) Batch 684 Loss : 0.15169236063957214, 맞은 개수 : 121\nEpoch : 21, batch 685\n(Train) Batch 685 Loss : 0.29424747824668884, 맞은 개수 : 115\nEpoch : 21, batch 686\n(Train) Batch 686 Loss : 0.33805522322654724, 맞은 개수 : 115\nEpoch : 21, batch 687\n(Train) Batch 687 Loss : 0.35860055685043335, 맞은 개수 : 114\nEpoch : 21, batch 688\n(Train) Batch 688 Loss : 0.49209117889404297, 맞은 개수 : 110\nEpoch : 21, batch 689\n(Train) Batch 689 Loss : 0.2446686327457428, 맞은 개수 : 119\nEpoch : 21, batch 690\n(Train) Batch 690 Loss : 0.31045037508010864, 맞은 개수 : 119\nEpoch : 21, batch 691\n(Train) Batch 691 Loss : 0.24682961404323578, 맞은 개수 : 118\nEpoch : 21, batch 692\n(Train) Batch 692 Loss : 0.16216178238391876, 맞은 개수 : 121\nEpoch : 21, batch 693\n(Train) Batch 693 Loss : 0.2743881344795227, 맞은 개수 : 116\nEpoch : 21, batch 694\n(Train) Batch 694 Loss : 0.20343546569347382, 맞은 개수 : 121\nEpoch : 21, batch 695\n(Train) Batch 695 Loss : 0.15145577490329742, 맞은 개수 : 122\nEpoch : 21, batch 696\n(Train) Batch 696 Loss : 0.1957285851240158, 맞은 개수 : 120\nEpoch : 21, batch 697\n(Train) Batch 697 Loss : 0.25397443771362305, 맞은 개수 : 121\nEpoch : 21, batch 698\n(Train) Batch 698 Loss : 0.3186245560646057, 맞은 개수 : 119\nEpoch : 21, batch 699\n(Train) Batch 699 Loss : 0.26600921154022217, 맞은 개수 : 115\nEpoch : 21, batch 700\n(Train) Batch 700 Loss : 0.201890766620636, 맞은 개수 : 119\nEpoch : 21, batch 701\n(Train) Batch 701 Loss : 0.27342715859413147, 맞은 개수 : 121\nEpoch : 21, batch 702\n(Train) Batch 702 Loss : 0.21737264096736908, 맞은 개수 : 118\nEpoch : 21, batch 703\n(Train) Batch 703 Loss : 0.2920133173465729, 맞은 개수 : 117\nEpoch : 21, batch 704\n(Train) Batch 704 Loss : 0.37852200865745544, 맞은 개수 : 115\nEpoch : 21, batch 705\n(Train) Batch 705 Loss : 0.2798243761062622, 맞은 개수 : 118\nEpoch : 21, batch 706\n(Train) Batch 706 Loss : 0.21843639016151428, 맞은 개수 : 121\nEpoch : 21, batch 707\n(Train) Batch 707 Loss : 0.2900439500808716, 맞은 개수 : 119\nEpoch : 21, batch 708\n(Train) Batch 708 Loss : 0.3796156644821167, 맞은 개수 : 117\nEpoch : 21, batch 709\n(Train) Batch 709 Loss : 0.3410855829715729, 맞은 개수 : 113\nEpoch : 21, batch 710\n(Train) Batch 710 Loss : 0.2937259376049042, 맞은 개수 : 117\nEpoch : 21, batch 711\n(Train) Batch 711 Loss : 0.14020675420761108, 맞은 개수 : 122\nEpoch : 21, batch 712\n(Train) Batch 712 Loss : 0.3994247615337372, 맞은 개수 : 111\nEpoch : 21, batch 713\n(Train) Batch 713 Loss : 0.3121171295642853, 맞은 개수 : 115\nEpoch : 21, batch 714\n(Train) Batch 714 Loss : 0.2881297767162323, 맞은 개수 : 117\nEpoch : 21, batch 715\n(Train) Batch 715 Loss : 0.2813258469104767, 맞은 개수 : 116\nEpoch : 21, batch 716\n(Train) Batch 716 Loss : 0.2172800451517105, 맞은 개수 : 115\nEpoch : 21, batch 717\n(Train) Batch 717 Loss : 0.31021350622177124, 맞은 개수 : 116\nEpoch : 21, batch 718\n(Train) Batch 718 Loss : 0.35185378789901733, 맞은 개수 : 114\nEpoch : 21, batch 719\n(Train) Batch 719 Loss : 0.2021746039390564, 맞은 개수 : 118\nEpoch : 21, batch 720\n(Train) Batch 720 Loss : 0.35954800248146057, 맞은 개수 : 115\nEpoch : 21, batch 721\n(Train) Batch 721 Loss : 0.27591222524642944, 맞은 개수 : 120\nEpoch : 21, batch 722\n(Train) Batch 722 Loss : 0.3060701787471771, 맞은 개수 : 116\nEpoch : 21, batch 723\n(Train) Batch 723 Loss : 0.2898474931716919, 맞은 개수 : 117\nEpoch : 21, batch 724\n(Train) Batch 724 Loss : 0.22565582394599915, 맞은 개수 : 119\nEpoch : 21, batch 725\n(Train) Batch 725 Loss : 0.3175632655620575, 맞은 개수 : 115\nEpoch : 21, batch 726\n(Train) Batch 726 Loss : 0.2668811082839966, 맞은 개수 : 118\nEpoch : 21, batch 727\n(Train) Batch 727 Loss : 0.29140505194664, 맞은 개수 : 116\nEpoch : 21, batch 728\n(Train) Batch 728 Loss : 0.3288416266441345, 맞은 개수 : 118\nEpoch : 21, batch 729\n(Train) Batch 729 Loss : 0.3506659269332886, 맞은 개수 : 112\nEpoch : 21, batch 730\n(Train) Batch 730 Loss : 0.21693257987499237, 맞은 개수 : 118\nEpoch : 21, batch 731\n(Train) Batch 731 Loss : 0.20784081518650055, 맞은 개수 : 119\nEpoch : 21, batch 732\n(Train) Batch 732 Loss : 0.3694677948951721, 맞은 개수 : 114\nEpoch : 21, batch 733\n(Train) Batch 733 Loss : 0.3769209086894989, 맞은 개수 : 116\nEpoch : 21, batch 734\n(Train) Batch 734 Loss : 0.2196495085954666, 맞은 개수 : 120\nEpoch : 21, batch 735\n(Train) Batch 735 Loss : 0.24876520037651062, 맞은 개수 : 117\nEpoch : 21, batch 736\n(Train) Batch 736 Loss : 0.24634510278701782, 맞은 개수 : 116\nEpoch : 21, batch 737\n(Train) Batch 737 Loss : 0.3206101655960083, 맞은 개수 : 117\nEpoch : 21, batch 738\n(Train) Batch 738 Loss : 0.28454527258872986, 맞은 개수 : 118\nEpoch : 21, batch 739\n(Train) Batch 739 Loss : 0.25065985321998596, 맞은 개수 : 119\nEpoch : 21, batch 740\n(Train) Batch 740 Loss : 0.2913256585597992, 맞은 개수 : 114\nEpoch : 21, batch 741\n(Train) Batch 741 Loss : 0.2969342768192291, 맞은 개수 : 116\nEpoch : 21, batch 742\n(Train) Batch 742 Loss : 0.21819037199020386, 맞은 개수 : 119\nEpoch : 21, batch 743\n(Train) Batch 743 Loss : 0.3085867762565613, 맞은 개수 : 115\nEpoch : 21, batch 744\n(Train) Batch 744 Loss : 0.2566184103488922, 맞은 개수 : 115\nEpoch : 21, batch 745\n(Train) Batch 745 Loss : 0.2902897596359253, 맞은 개수 : 115\nEpoch : 21, batch 746\n(Train) Batch 746 Loss : 0.2927758991718292, 맞은 개수 : 119\nEpoch : 21, batch 747\n(Train) Batch 747 Loss : 0.12202705442905426, 맞은 개수 : 123\nEpoch : 21, batch 748\n(Train) Batch 748 Loss : 0.3264249861240387, 맞은 개수 : 115\nEpoch : 21, batch 749\n(Train) Batch 749 Loss : 0.31822749972343445, 맞은 개수 : 119\nEpoch : 21, batch 750\n(Train) Batch 750 Loss : 0.29447391629219055, 맞은 개수 : 113\nEpoch : 21, batch 751\n(Train) Batch 751 Loss : 0.3120054006576538, 맞은 개수 : 117\nEpoch : 21, batch 752\n(Train) Batch 752 Loss : 0.2034236043691635, 맞은 개수 : 121\nEpoch : 21, batch 753\n(Train) Batch 753 Loss : 0.3967658579349518, 맞은 개수 : 110\nEpoch : 21, batch 754\n(Train) Batch 754 Loss : 0.23652848601341248, 맞은 개수 : 117\nEpoch : 21, batch 755\n(Train) Batch 755 Loss : 0.2819869816303253, 맞은 개수 : 117\nEpoch : 21, batch 756\n(Train) Batch 756 Loss : 0.2731230854988098, 맞은 개수 : 118\nEpoch : 21, batch 757\n(Train) Batch 757 Loss : 0.2771534025669098, 맞은 개수 : 118\nEpoch : 21, batch 758\n(Train) Batch 758 Loss : 0.18820062279701233, 맞은 개수 : 118\nEpoch : 21, batch 759\n(Train) Batch 759 Loss : 0.33211371302604675, 맞은 개수 : 114\nEpoch : 21, batch 760\n(Train) Batch 760 Loss : 0.29973727464675903, 맞은 개수 : 117\nEpoch : 21, batch 761\n(Train) Batch 761 Loss : 0.29005366563796997, 맞은 개수 : 117\nEpoch : 21, batch 762\n(Train) Batch 762 Loss : 0.33735114336013794, 맞은 개수 : 117\nEpoch : 21, batch 763\n(Train) Batch 763 Loss : 0.18069498240947723, 맞은 개수 : 122\nEpoch : 21, batch 764\n(Train) Batch 764 Loss : 0.20277583599090576, 맞은 개수 : 121\nEpoch : 21, batch 765\n(Train) Batch 765 Loss : 0.4249112606048584, 맞은 개수 : 113\nEpoch : 21, batch 766\n(Train) Batch 766 Loss : 0.29608216881752014, 맞은 개수 : 117\nEpoch : 21, batch 767\n(Train) Batch 767 Loss : 0.3935507535934448, 맞은 개수 : 113\nEpoch : 21, batch 768\n(Train) Batch 768 Loss : 0.13669182360172272, 맞은 개수 : 124\nEpoch : 21, batch 769\n(Train) Batch 769 Loss : 0.41646072268486023, 맞은 개수 : 116\nEpoch : 21, batch 770\n(Train) Batch 770 Loss : 0.24898746609687805, 맞은 개수 : 118\nEpoch : 21, batch 771\n(Train) Batch 771 Loss : 0.2359093874692917, 맞은 개수 : 119\nEpoch : 21, batch 772\n(Train) Batch 772 Loss : 0.2460281103849411, 맞은 개수 : 122\nEpoch : 21, batch 773\n(Train) Batch 773 Loss : 0.2925603985786438, 맞은 개수 : 119\nEpoch : 21, batch 774\n(Train) Batch 774 Loss : 0.21475966274738312, 맞은 개수 : 119\nEpoch : 21, batch 775\n(Train) Batch 775 Loss : 0.3101752698421478, 맞은 개수 : 115\nEpoch : 21, batch 776\n(Train) Batch 776 Loss : 0.30741095542907715, 맞은 개수 : 116\nEpoch : 21, batch 777\n(Train) Batch 777 Loss : 0.28806039690971375, 맞은 개수 : 113\nEpoch : 21, batch 778\n(Train) Batch 778 Loss : 0.14645834267139435, 맞은 개수 : 122\nEpoch : 21, batch 779\n(Train) Batch 779 Loss : 0.260057270526886, 맞은 개수 : 118\nEpoch : 21, batch 780\n(Train) Batch 780 Loss : 0.3529507517814636, 맞은 개수 : 117\nEpoch : 21, batch 781\n(Train) Batch 781 Loss : 0.3993636965751648, 맞은 개수 : 113\nEpoch : 21, batch 782\n(Train) Batch 782 Loss : 0.38478773832321167, 맞은 개수 : 112\nEpoch : 21, batch 783\n(Train) Batch 783 Loss : 0.22303733229637146, 맞은 개수 : 119\nEpoch : 21, batch 784\n(Train) Batch 784 Loss : 0.3433356285095215, 맞은 개수 : 115\nEpoch : 21, batch 785\n(Train) Batch 785 Loss : 0.3097918927669525, 맞은 개수 : 116\nEpoch : 21, batch 786\n(Train) Batch 786 Loss : 0.14155367016792297, 맞은 개수 : 120\nEpoch : 21, batch 787\n(Train) Batch 787 Loss : 0.44952958822250366, 맞은 개수 : 113\nEpoch : 21, batch 788\n(Train) Batch 788 Loss : 0.29877495765686035, 맞은 개수 : 117\nEpoch : 21, batch 789\n(Train) Batch 789 Loss : 0.2528799772262573, 맞은 개수 : 116\nEpoch : 21, batch 790\n(Train) Batch 790 Loss : 0.23507244884967804, 맞은 개수 : 119\nEpoch : 21, batch 791\n(Train) Batch 791 Loss : 0.3642577528953552, 맞은 개수 : 113\nEpoch : 21, batch 792\n(Train) Batch 792 Loss : 0.37917450070381165, 맞은 개수 : 114\nEpoch : 21, batch 793\n(Train) Batch 793 Loss : 0.24316956102848053, 맞은 개수 : 117\nEpoch : 21, batch 794\n(Train) Batch 794 Loss : 0.1964874416589737, 맞은 개수 : 120\nEpoch : 21, batch 795\n(Train) Batch 795 Loss : 0.19157904386520386, 맞은 개수 : 121\nEpoch : 21, batch 796\n(Train) Batch 796 Loss : 0.16539889574050903, 맞은 개수 : 124\nEpoch : 21, batch 797\n(Train) Batch 797 Loss : 0.2880125343799591, 맞은 개수 : 117\nEpoch : 21, batch 798\n(Train) Batch 798 Loss : 0.5780220031738281, 맞은 개수 : 109\nEpoch : 21, batch 799\n(Train) Batch 799 Loss : 0.22067435085773468, 맞은 개수 : 119\nEpoch : 21, batch 800\n(Train) Batch 800 Loss : 0.22234567999839783, 맞은 개수 : 115\nEpoch : 21, batch 801\n(Train) Batch 801 Loss : 0.21142461895942688, 맞은 개수 : 122\nEpoch : 21, batch 802\n(Train) Batch 802 Loss : 0.2815033495426178, 맞은 개수 : 116\nEpoch : 21, batch 803\n(Train) Batch 803 Loss : 0.3291322886943817, 맞은 개수 : 115\nEpoch : 21, batch 804\n(Train) Batch 804 Loss : 0.23157696425914764, 맞은 개수 : 119\nEpoch : 21, batch 805\n(Train) Batch 805 Loss : 0.23339584469795227, 맞은 개수 : 116\nEpoch : 21, batch 806\n(Train) Batch 806 Loss : 0.45783254504203796, 맞은 개수 : 111\nEpoch : 21, batch 807\n(Train) Batch 807 Loss : 0.16645453870296478, 맞은 개수 : 124\nEpoch : 21, batch 808\n(Train) Batch 808 Loss : 0.2731100618839264, 맞은 개수 : 116\nEpoch : 21, batch 809\n(Train) Batch 809 Loss : 0.17389893531799316, 맞은 개수 : 120\nEpoch : 21, batch 810\n(Train) Batch 810 Loss : 0.3221013844013214, 맞은 개수 : 116\nEpoch : 21, batch 811\n(Train) Batch 811 Loss : 0.4356139004230499, 맞은 개수 : 111\nEpoch : 21, batch 812\n(Train) Batch 812 Loss : 0.294586718082428, 맞은 개수 : 117\nEpoch : 21, batch 813\n(Train) Batch 813 Loss : 0.234480619430542, 맞은 개수 : 118\nEpoch : 21, batch 814\n(Train) Batch 814 Loss : 0.4219757914543152, 맞은 개수 : 115\nEpoch : 21, batch 815\n(Train) Batch 815 Loss : 0.29893434047698975, 맞은 개수 : 119\nEpoch : 21, batch 816\n(Train) Batch 816 Loss : 0.3188176155090332, 맞은 개수 : 117\nEpoch : 21, batch 817\n(Train) Batch 817 Loss : 0.28490447998046875, 맞은 개수 : 119\nEpoch : 21, batch 818\n(Train) Batch 818 Loss : 0.20603784918785095, 맞은 개수 : 121\nEpoch : 21, batch 819\n(Train) Batch 819 Loss : 0.2860732078552246, 맞은 개수 : 119\nEpoch : 21, batch 820\n(Train) Batch 820 Loss : 0.21144206821918488, 맞은 개수 : 121\nEpoch : 21, batch 821\n(Train) Batch 821 Loss : 0.25194060802459717, 맞은 개수 : 114\nEpoch : 21, batch 822\n(Train) Batch 822 Loss : 0.4577002227306366, 맞은 개수 : 111\nEpoch : 21, batch 823\n(Train) Batch 823 Loss : 0.26978498697280884, 맞은 개수 : 120\nEpoch : 21, batch 824\n(Train) Batch 824 Loss : 0.23671135306358337, 맞은 개수 : 122\nEpoch : 21, batch 825\n(Train) Batch 825 Loss : 0.4078851342201233, 맞은 개수 : 113\nEpoch : 21, batch 826\n(Train) Batch 826 Loss : 0.24923062324523926, 맞은 개수 : 114\nEpoch : 21, batch 827\n(Train) Batch 827 Loss : 0.2638361155986786, 맞은 개수 : 116\nEpoch : 21, batch 828\n(Train) Batch 828 Loss : 0.34839263558387756, 맞은 개수 : 117\nEpoch : 21, batch 829\n(Train) Batch 829 Loss : 0.29738470911979675, 맞은 개수 : 117\nEpoch : 21, batch 830\n(Train) Batch 830 Loss : 0.3710018992424011, 맞은 개수 : 114\nEpoch : 21, batch 831\n(Train) Batch 831 Loss : 0.40169188380241394, 맞은 개수 : 113\nEpoch : 21, batch 832\n(Train) Batch 832 Loss : 0.17935743927955627, 맞은 개수 : 121\nEpoch : 21, batch 833\n(Train) Batch 833 Loss : 0.24923193454742432, 맞은 개수 : 118\nEpoch : 21, batch 834\n(Train) Batch 834 Loss : 0.2048168182373047, 맞은 개수 : 121\nEpoch : 21, batch 835\n(Train) Batch 835 Loss : 0.23476529121398926, 맞은 개수 : 119\nEpoch : 21, batch 836\n(Train) Batch 836 Loss : 0.27373865246772766, 맞은 개수 : 119\nEpoch : 21, batch 837\n(Train) Batch 837 Loss : 0.39748141169548035, 맞은 개수 : 114\nEpoch : 21, batch 838\n(Train) Batch 838 Loss : 0.31915226578712463, 맞은 개수 : 118\nEpoch : 21, batch 839\n(Train) Batch 839 Loss : 0.2611960768699646, 맞은 개수 : 115\nEpoch : 21, batch 840\n(Train) Batch 840 Loss : 0.2581121325492859, 맞은 개수 : 118\nEpoch : 21, batch 841\n(Train) Batch 841 Loss : 0.2515624463558197, 맞은 개수 : 118\nEpoch : 21, batch 842\n(Train) Batch 842 Loss : 0.24477429687976837, 맞은 개수 : 123\nEpoch : 21, batch 843\n(Train) Batch 843 Loss : 0.16679145395755768, 맞은 개수 : 122\nEpoch : 21, batch 844\n(Train) Batch 844 Loss : 0.22437570989131927, 맞은 개수 : 118\nEpoch : 21, batch 845\n(Train) Batch 845 Loss : 0.3827860951423645, 맞은 개수 : 117\nEpoch : 21, batch 846\n(Train) Batch 846 Loss : 0.2959071695804596, 맞은 개수 : 117\nEpoch : 21, batch 847\n(Train) Batch 847 Loss : 0.16403181850910187, 맞은 개수 : 121\nEpoch : 21, batch 848\n(Train) Batch 848 Loss : 0.19762305915355682, 맞은 개수 : 119\nEpoch : 21, batch 849\n(Train) Batch 849 Loss : 0.2798371911048889, 맞은 개수 : 118\nEpoch : 21, batch 850\n(Train) Batch 850 Loss : 0.2740797996520996, 맞은 개수 : 117\nEpoch : 21, batch 851\n(Train) Batch 851 Loss : 0.3378385305404663, 맞은 개수 : 115\nEpoch : 21, batch 852\n(Train) Batch 852 Loss : 0.3119443655014038, 맞은 개수 : 115\nEpoch : 21, batch 853\n(Train) Batch 853 Loss : 0.25787457823753357, 맞은 개수 : 117\nEpoch : 21, batch 854\n(Train) Batch 854 Loss : 0.2561390995979309, 맞은 개수 : 114\nEpoch : 21, batch 855\n(Train) Batch 855 Loss : 0.36163651943206787, 맞은 개수 : 113\nEpoch : 21, batch 856\n(Train) Batch 856 Loss : 0.23495540022850037, 맞은 개수 : 118\nEpoch : 21, batch 857\n(Train) Batch 857 Loss : 0.2853795886039734, 맞은 개수 : 117\nEpoch : 21, batch 858\n(Train) Batch 858 Loss : 0.1713724583387375, 맞은 개수 : 120\nEpoch : 21, batch 859\n(Train) Batch 859 Loss : 0.2858138680458069, 맞은 개수 : 117\nEpoch : 21, batch 860\n(Train) Batch 860 Loss : 0.14324401319026947, 맞은 개수 : 122\nEpoch : 21, batch 861\n(Train) Batch 861 Loss : 0.27661362290382385, 맞은 개수 : 116\nEpoch : 21, batch 862\n(Train) Batch 862 Loss : 0.28106266260147095, 맞은 개수 : 115\nEpoch : 21, batch 863\n(Train) Batch 863 Loss : 0.2662731409072876, 맞은 개수 : 115\nEpoch : 21, batch 864\n(Train) Batch 864 Loss : 0.1916084587574005, 맞은 개수 : 118\nEpoch : 21, batch 865\n(Train) Batch 865 Loss : 0.30170196294784546, 맞은 개수 : 119\nEpoch : 21, batch 866\n(Train) Batch 866 Loss : 0.2501973509788513, 맞은 개수 : 119\nEpoch : 21, batch 867\n(Train) Batch 867 Loss : 0.2609782814979553, 맞은 개수 : 118\nEpoch : 21, batch 868\n(Train) Batch 868 Loss : 0.29553258419036865, 맞은 개수 : 114\nEpoch : 21, batch 869\n(Train) Batch 869 Loss : 0.2458600401878357, 맞은 개수 : 119\nEpoch : 21, batch 870\n(Train) Batch 870 Loss : 0.24299952387809753, 맞은 개수 : 117\nEpoch : 21, batch 871\n(Train) Batch 871 Loss : 0.24143001437187195, 맞은 개수 : 117\nEpoch : 21, batch 872\n(Train) Batch 872 Loss : 0.29820916056632996, 맞은 개수 : 115\nEpoch : 21, batch 873\n(Train) Batch 873 Loss : 0.2647554576396942, 맞은 개수 : 115\nEpoch : 21, batch 874\n(Train) Batch 874 Loss : 0.33678919076919556, 맞은 개수 : 115\nEpoch : 21, batch 875\n(Train) Batch 875 Loss : 0.23512619733810425, 맞은 개수 : 118\nEpoch : 21, batch 876\n(Train) Batch 876 Loss : 0.3544773459434509, 맞은 개수 : 115\nEpoch : 21, batch 877\n(Train) Batch 877 Loss : 0.25319603085517883, 맞은 개수 : 117\nEpoch : 21, batch 878\n(Train) Batch 878 Loss : 0.2822883725166321, 맞은 개수 : 118\nEpoch : 21, batch 879\n(Train) Batch 879 Loss : 0.32240352034568787, 맞은 개수 : 115\nEpoch : 21, batch 880\n(Train) Batch 880 Loss : 0.37424376606941223, 맞은 개수 : 117\nEpoch : 21, batch 881\n(Train) Batch 881 Loss : 0.19882014393806458, 맞은 개수 : 120\nEpoch : 21, batch 882\n(Train) Batch 882 Loss : 0.3919268250465393, 맞은 개수 : 111\nEpoch : 21, batch 883\n(Train) Batch 883 Loss : 0.15336892008781433, 맞은 개수 : 120\nEpoch : 21, batch 884\n(Train) Batch 884 Loss : 0.23540098965168, 맞은 개수 : 119\nEpoch : 21, batch 885\n(Train) Batch 885 Loss : 0.2546374201774597, 맞은 개수 : 117\nEpoch : 21, batch 886\n(Train) Batch 886 Loss : 0.1812121719121933, 맞은 개수 : 119\nEpoch : 21, batch 887\n(Train) Batch 887 Loss : 0.28906580805778503, 맞은 개수 : 115\nEpoch : 21, batch 888\n(Train) Batch 888 Loss : 0.2932746410369873, 맞은 개수 : 115\nEpoch : 21, batch 889\n(Train) Batch 889 Loss : 0.3263466954231262, 맞은 개수 : 115\nEpoch : 21, batch 890\n(Train) Batch 890 Loss : 0.23509174585342407, 맞은 개수 : 117\nEpoch : 21, batch 891\n(Train) Batch 891 Loss : 0.3063419461250305, 맞은 개수 : 117\nEpoch : 21, batch 892\n(Train) Batch 892 Loss : 0.29538851976394653, 맞은 개수 : 114\nEpoch : 21, batch 893\n(Train) Batch 893 Loss : 0.14190398156642914, 맞은 개수 : 119\nEpoch : 21, batch 894\n(Train) Batch 894 Loss : 0.2756197154521942, 맞은 개수 : 118\nEpoch : 21, batch 895\n(Train) Batch 895 Loss : 0.17610768973827362, 맞은 개수 : 120\nEpoch : 21, batch 896\n(Train) Batch 896 Loss : 0.2287941724061966, 맞은 개수 : 118\nEpoch : 21, batch 897\n(Train) Batch 897 Loss : 0.30807965993881226, 맞은 개수 : 119\nEpoch : 21, batch 898\n(Train) Batch 898 Loss : 0.24662256240844727, 맞은 개수 : 117\nEpoch : 21, batch 899\n(Train) Batch 899 Loss : 0.21207180619239807, 맞은 개수 : 115\nEpoch : 21, batch 900\n(Train) Batch 900 Loss : 0.2883968949317932, 맞은 개수 : 118\nEpoch : 21, batch 901\n(Train) Batch 901 Loss : 0.2475661337375641, 맞은 개수 : 113\nEpoch : 21, batch 902\n(Train) Batch 902 Loss : 0.24999551475048065, 맞은 개수 : 117\nEpoch : 21, batch 903\n(Train) Batch 903 Loss : 0.15901434421539307, 맞은 개수 : 122\nEpoch : 21, batch 904\n(Train) Batch 904 Loss : 0.3185397684574127, 맞은 개수 : 114\nEpoch : 21, batch 905\n(Train) Batch 905 Loss : 0.16592995822429657, 맞은 개수 : 121\nEpoch : 21, batch 906\n(Train) Batch 906 Loss : 0.283620685338974, 맞은 개수 : 117\nEpoch : 21, batch 907\n(Train) Batch 907 Loss : 0.3171873986721039, 맞은 개수 : 117\nEpoch : 21, batch 908\n(Train) Batch 908 Loss : 0.1993895173072815, 맞은 개수 : 119\nEpoch : 21, batch 909\n(Train) Batch 909 Loss : 0.29647475481033325, 맞은 개수 : 115\nEpoch : 21, batch 910\n(Train) Batch 910 Loss : 0.34231987595558167, 맞은 개수 : 113\nEpoch : 21, batch 911\n(Train) Batch 911 Loss : 0.2830241024494171, 맞은 개수 : 115\nEpoch : 21, batch 912\n(Train) Batch 912 Loss : 0.19341525435447693, 맞은 개수 : 119\nEpoch : 21, batch 913\n(Train) Batch 913 Loss : 0.26749107241630554, 맞은 개수 : 117\nEpoch : 21, batch 914\n(Train) Batch 914 Loss : 0.2343679815530777, 맞은 개수 : 119\nEpoch : 21, batch 915\n(Train) Batch 915 Loss : 0.1831020712852478, 맞은 개수 : 120\nEpoch : 21, batch 916\n(Train) Batch 916 Loss : 0.26673007011413574, 맞은 개수 : 117\nEpoch : 21, batch 917\n(Train) Batch 917 Loss : 0.14570161700248718, 맞은 개수 : 122\nEpoch : 21, batch 918\n(Train) Batch 918 Loss : 0.355538934469223, 맞은 개수 : 117\nEpoch : 21, batch 919\n(Train) Batch 919 Loss : 0.2717444896697998, 맞은 개수 : 112\nEpoch : 21, batch 920\n(Train) Batch 920 Loss : 0.15341439843177795, 맞은 개수 : 120\nEpoch : 21, batch 921\n(Train) Batch 921 Loss : 0.3890424370765686, 맞은 개수 : 110\nEpoch : 21, batch 922\n(Train) Batch 922 Loss : 0.21674475073814392, 맞은 개수 : 118\nEpoch : 21, batch 923\n(Train) Batch 923 Loss : 0.14156854152679443, 맞은 개수 : 123\nEpoch : 21, batch 924\n(Train) Batch 924 Loss : 0.24881257116794586, 맞은 개수 : 118\nEpoch : 21, batch 925\n(Train) Batch 925 Loss : 0.422088086605072, 맞은 개수 : 114\nEpoch : 21, batch 926\n(Train) Batch 926 Loss : 0.2897007167339325, 맞은 개수 : 114\nEpoch : 21, batch 927\n(Train) Batch 927 Loss : 0.32210078835487366, 맞은 개수 : 115\nEpoch : 21, batch 928\n(Train) Batch 928 Loss : 0.42660823464393616, 맞은 개수 : 113\nEpoch : 21, batch 929\n(Train) Batch 929 Loss : 0.3625933527946472, 맞은 개수 : 114\nEpoch : 21, batch 930\n(Train) Batch 930 Loss : 0.24785137176513672, 맞은 개수 : 120\nEpoch : 21, batch 931\n(Train) Batch 931 Loss : 0.1449558138847351, 맞은 개수 : 124\nEpoch : 21, batch 932\n(Train) Batch 932 Loss : 0.35525909066200256, 맞은 개수 : 118\nEpoch : 21, batch 933\n(Train) Batch 933 Loss : 0.32251501083374023, 맞은 개수 : 116\nEpoch : 21, batch 934\n(Train) Batch 934 Loss : 0.367729127407074, 맞은 개수 : 111\nEpoch : 21, batch 935\n(Train) Batch 935 Loss : 0.25135818123817444, 맞은 개수 : 117\nEpoch : 21, batch 936\n(Train) Batch 936 Loss : 0.3554196059703827, 맞은 개수 : 117\nEpoch : 21, batch 937\n(Train) Batch 937 Loss : 0.333965003490448, 맞은 개수 : 113\nEpoch : 21, batch 938\n(Train) Batch 938 Loss : 0.24809645116329193, 맞은 개수 : 121\nEpoch : 21, batch 939\n(Train) Batch 939 Loss : 0.4470934271812439, 맞은 개수 : 110\nEpoch : 21, batch 940\n(Train) Batch 940 Loss : 0.34157341718673706, 맞은 개수 : 115\nEpoch : 21, batch 941\n(Train) Batch 941 Loss : 0.22234691679477692, 맞은 개수 : 117\nEpoch : 21, batch 942\n(Train) Batch 942 Loss : 0.20652234554290771, 맞은 개수 : 120\nEpoch : 21, batch 943\n(Train) Batch 943 Loss : 0.345410019159317, 맞은 개수 : 115\nEpoch : 21, batch 944\n(Train) Batch 944 Loss : 0.3407166004180908, 맞은 개수 : 116\nEpoch : 21, batch 945\n(Train) Batch 945 Loss : 0.3436206877231598, 맞은 개수 : 118\nEpoch : 21, batch 946\n(Train) Batch 946 Loss : 0.2580642104148865, 맞은 개수 : 117\nEpoch : 21, batch 947\n(Train) Batch 947 Loss : 0.28239473700523376, 맞은 개수 : 118\nEpoch : 21, batch 948\n(Train) Batch 948 Loss : 0.31333428621292114, 맞은 개수 : 116\nEpoch : 21, batch 949\n(Train) Batch 949 Loss : 0.20097289979457855, 맞은 개수 : 119\nEpoch : 21, batch 950\n(Train) Batch 950 Loss : 0.2835726737976074, 맞은 개수 : 118\nEpoch : 21, batch 951\n(Train) Batch 951 Loss : 0.41316062211990356, 맞은 개수 : 118\nEpoch : 21, batch 952\n(Train) Batch 952 Loss : 0.41907331347465515, 맞은 개수 : 111\nEpoch : 21, batch 953\n(Train) Batch 953 Loss : 0.23661692440509796, 맞은 개수 : 117\nEpoch : 21, batch 954\n(Train) Batch 954 Loss : 0.33271104097366333, 맞은 개수 : 114\nEpoch : 21, batch 955\n(Train) Batch 955 Loss : 0.251669317483902, 맞은 개수 : 119\nEpoch : 21, batch 956\n(Train) Batch 956 Loss : 0.25112199783325195, 맞은 개수 : 115\nEpoch : 21, batch 957\n(Train) Batch 957 Loss : 0.29565876722335815, 맞은 개수 : 117\nEpoch : 21, batch 958\n(Train) Batch 958 Loss : 0.22143296897411346, 맞은 개수 : 116\nEpoch : 21, batch 959\n(Train) Batch 959 Loss : 0.29982346296310425, 맞은 개수 : 117\nEpoch : 21, batch 960\n(Train) Batch 960 Loss : 0.18224307894706726, 맞은 개수 : 122\nEpoch : 21, batch 961\n(Train) Batch 961 Loss : 0.35288292169570923, 맞은 개수 : 119\nEpoch : 21, batch 962\n(Train) Batch 962 Loss : 0.39135682582855225, 맞은 개수 : 114\nEpoch : 21, batch 963\n(Train) Batch 963 Loss : 0.32152292132377625, 맞은 개수 : 113\nEpoch : 21, batch 964\n(Train) Batch 964 Loss : 0.17823883891105652, 맞은 개수 : 125\nEpoch : 21, batch 965\n(Train) Batch 965 Loss : 0.2052009403705597, 맞은 개수 : 121\nEpoch : 21, batch 966\n(Train) Batch 966 Loss : 0.21340221166610718, 맞은 개수 : 122\nEpoch : 21, batch 967\n(Train) Batch 967 Loss : 0.3204386234283447, 맞은 개수 : 115\nEpoch : 21, batch 968\n(Train) Batch 968 Loss : 0.20530757308006287, 맞은 개수 : 120\nEpoch : 21, batch 969\n(Train) Batch 969 Loss : 0.29516154527664185, 맞은 개수 : 116\nEpoch : 21, batch 970\n(Train) Batch 970 Loss : 0.3250330984592438, 맞은 개수 : 112\nEpoch : 21, batch 971\n(Train) Batch 971 Loss : 0.27682965993881226, 맞은 개수 : 118\nEpoch : 21, batch 972\n(Train) Batch 972 Loss : 0.2971396744251251, 맞은 개수 : 115\nEpoch : 21, batch 973\n(Train) Batch 973 Loss : 0.31774914264678955, 맞은 개수 : 117\nEpoch : 21, batch 974\n(Train) Batch 974 Loss : 0.2550230026245117, 맞은 개수 : 114\nEpoch : 21, batch 975\n(Train) Batch 975 Loss : 0.36973443627357483, 맞은 개수 : 115\nEpoch : 21, batch 976\n(Train) Batch 976 Loss : 0.0974004715681076, 맞은 개수 : 124\nEpoch : 21, batch 977\n(Train) Batch 977 Loss : 0.35976871848106384, 맞은 개수 : 117\nEpoch : 21, batch 978\n(Train) Batch 978 Loss : 0.20910722017288208, 맞은 개수 : 119\nEpoch : 21, batch 979\n(Train) Batch 979 Loss : 0.360116571187973, 맞은 개수 : 115\nEpoch : 21, batch 980\n(Train) Batch 980 Loss : 0.24346207082271576, 맞은 개수 : 121\nEpoch : 21, batch 981\n(Train) Batch 981 Loss : 0.39882615208625793, 맞은 개수 : 114\nEpoch : 21, batch 982\n(Train) Batch 982 Loss : 0.28885045647621155, 맞은 개수 : 116\nEpoch : 21, batch 983\n(Train) Batch 983 Loss : 0.361780047416687, 맞은 개수 : 115\nEpoch : 21, batch 984\n(Train) Batch 984 Loss : 0.14209292829036713, 맞은 개수 : 121\nEpoch : 21, batch 985\n(Train) Batch 985 Loss : 0.2242087870836258, 맞은 개수 : 118\nEpoch : 21, batch 986\n(Train) Batch 986 Loss : 0.17170435190200806, 맞은 개수 : 121\nEpoch : 21, batch 987\n(Train) Batch 987 Loss : 0.21582254767417908, 맞은 개수 : 116\nEpoch : 21, batch 988\n(Train) Batch 988 Loss : 0.27145832777023315, 맞은 개수 : 117\nEpoch : 21, batch 989\n(Train) Batch 989 Loss : 0.2649301588535309, 맞은 개수 : 115\nEpoch : 21, batch 990\n(Train) Batch 990 Loss : 0.2197280079126358, 맞은 개수 : 121\nEpoch : 21, batch 991\n(Train) Batch 991 Loss : 0.2759722173213959, 맞은 개수 : 120\nEpoch : 21, batch 992\n(Train) Batch 992 Loss : 0.2510928213596344, 맞은 개수 : 118\nEpoch : 21, batch 993\n(Train) Batch 993 Loss : 0.40639397501945496, 맞은 개수 : 112\nEpoch : 21, batch 994\n(Train) Batch 994 Loss : 0.4274837076663971, 맞은 개수 : 113\nEpoch : 21, batch 995\n(Train) Batch 995 Loss : 0.28525668382644653, 맞은 개수 : 118\nEpoch : 21, batch 996\n(Train) Batch 996 Loss : 0.39261874556541443, 맞은 개수 : 117\nEpoch : 21, batch 997\n(Train) Batch 997 Loss : 0.31563469767570496, 맞은 개수 : 116\nEpoch : 21, batch 998\n(Train) Batch 998 Loss : 0.3995126783847809, 맞은 개수 : 112\nEpoch : 21, batch 999\n(Train) Batch 999 Loss : 0.32498058676719666, 맞은 개수 : 110\nEpoch : 21, batch 1000\n(Train) Batch 1000 Loss : 0.3067612946033478, 맞은 개수 : 117\nEpoch : 21, batch 1001\n(Train) Batch 1001 Loss : 0.32009997963905334, 맞은 개수 : 116\nEpoch : 21, batch 1002\n(Train) Batch 1002 Loss : 0.23988014459609985, 맞은 개수 : 117\nEpoch : 21, batch 1003\n(Train) Batch 1003 Loss : 0.208184614777565, 맞은 개수 : 121\nEpoch : 21, batch 1004\n(Train) Batch 1004 Loss : 0.33306804299354553, 맞은 개수 : 116\nEpoch : 21, batch 1005\n(Train) Batch 1005 Loss : 0.3105365037918091, 맞은 개수 : 115\nEpoch : 21, batch 1006\n(Train) Batch 1006 Loss : 0.18979820609092712, 맞은 개수 : 119\nEpoch : 21, batch 1007\n(Train) Batch 1007 Loss : 0.24479106068611145, 맞은 개수 : 118\nEpoch : 21, batch 1008\n(Train) Batch 1008 Loss : 0.173448845744133, 맞은 개수 : 120\nEpoch : 21, batch 1009\n(Train) Batch 1009 Loss : 0.16329257190227509, 맞은 개수 : 121\nEpoch : 21, batch 1010\n(Train) Batch 1010 Loss : 0.2397461235523224, 맞은 개수 : 120\nEpoch : 21, batch 1011\n(Train) Batch 1011 Loss : 0.29200905561447144, 맞은 개수 : 118\nEpoch : 21, batch 1012\n(Train) Batch 1012 Loss : 0.3701118528842926, 맞은 개수 : 109\nEpoch : 21, batch 1013\n(Train) Batch 1013 Loss : 0.2865234911441803, 맞은 개수 : 119\nEpoch : 21, batch 1014\n(Train) Batch 1014 Loss : 0.307839572429657, 맞은 개수 : 119\nEpoch : 21, batch 1015\n(Train) Batch 1015 Loss : 0.2983953654766083, 맞은 개수 : 113\nEpoch : 21, batch 1016\n(Train) Batch 1016 Loss : 0.2558833658695221, 맞은 개수 : 116\nEpoch : 21, batch 1017\n(Train) Batch 1017 Loss : 0.22865264117717743, 맞은 개수 : 117\nEpoch : 21, batch 1018\n(Train) Batch 1018 Loss : 0.2917855978012085, 맞은 개수 : 115\nEpoch : 21, batch 1019\n(Train) Batch 1019 Loss : 0.2311866134405136, 맞은 개수 : 119\nEpoch : 21, batch 1020\n(Train) Batch 1020 Loss : 0.2457331418991089, 맞은 개수 : 121\nEpoch : 21, batch 1021\n(Train) Batch 1021 Loss : 0.28373071551322937, 맞은 개수 : 119\nEpoch : 21, batch 1022\n(Train) Batch 1022 Loss : 0.1584816426038742, 맞은 개수 : 123\nEpoch : 21, batch 1023\n(Train) Batch 1023 Loss : 0.1977463662624359, 맞은 개수 : 122\nEpoch : 21, batch 1024\n(Train) Batch 1024 Loss : 0.23650002479553223, 맞은 개수 : 119\nEpoch : 21, batch 1025\n(Train) Batch 1025 Loss : 0.24209584295749664, 맞은 개수 : 117\nEpoch : 21, batch 1026\n(Train) Batch 1026 Loss : 0.17954052984714508, 맞은 개수 : 122\nEpoch : 21, batch 1027\n(Train) Batch 1027 Loss : 0.2353360652923584, 맞은 개수 : 120\nEpoch : 21, batch 1028\n(Train) Batch 1028 Loss : 0.3453921377658844, 맞은 개수 : 115\nEpoch : 21, batch 1029\n(Train) Batch 1029 Loss : 0.3028457462787628, 맞은 개수 : 116\nEpoch : 21, batch 1030\n(Train) Batch 1030 Loss : 0.3183094561100006, 맞은 개수 : 114\nEpoch : 21, batch 1031\n(Train) Batch 1031 Loss : 0.3111720383167267, 맞은 개수 : 114\nEpoch : 21, batch 1032\n(Train) Batch 1032 Loss : 0.2771467864513397, 맞은 개수 : 115\nEpoch : 21, batch 1033\n(Train) Batch 1033 Loss : 0.1332325041294098, 맞은 개수 : 123\nEpoch : 21, batch 1034\n(Train) Batch 1034 Loss : 0.2961198687553406, 맞은 개수 : 117\nEpoch : 21, batch 1035\n(Train) Batch 1035 Loss : 0.31954845786094666, 맞은 개수 : 113\nEpoch : 21, batch 1036\n(Train) Batch 1036 Loss : 0.31476983428001404, 맞은 개수 : 116\nEpoch : 21, batch 1037\n(Train) Batch 1037 Loss : 0.2291559875011444, 맞은 개수 : 117\nEpoch : 21, batch 1038\n(Train) Batch 1038 Loss : 0.3481404483318329, 맞은 개수 : 114\nEpoch : 21, batch 1039\n(Train) Batch 1039 Loss : 0.35813215374946594, 맞은 개수 : 115\nEpoch : 21, batch 1040\n(Train) Batch 1040 Loss : 0.2741892635822296, 맞은 개수 : 115\nEpoch : 21, batch 1041\n(Train) Batch 1041 Loss : 0.2028512805700302, 맞은 개수 : 119\nEpoch : 21, batch 1042\n(Train) Batch 1042 Loss : 0.19992338120937347, 맞은 개수 : 119\nEpoch : 21, batch 1043\n(Train) Batch 1043 Loss : 0.21962736546993256, 맞은 개수 : 121\nEpoch : 21, batch 1044\n(Train) Batch 1044 Loss : 0.1103687733411789, 맞은 개수 : 124\nEpoch : 21, batch 1045\n(Train) Batch 1045 Loss : 0.17028573155403137, 맞은 개수 : 119\nEpoch : 21, batch 1046\n(Train) Batch 1046 Loss : 0.2766525447368622, 맞은 개수 : 115\nEpoch : 21, batch 1047\n(Train) Batch 1047 Loss : 0.3551463484764099, 맞은 개수 : 117\nEpoch : 21, batch 1048\n(Train) Batch 1048 Loss : 0.22919495403766632, 맞은 개수 : 120\nEpoch : 21, batch 1049\n(Train) Batch 1049 Loss : 0.28459107875823975, 맞은 개수 : 116\nEpoch : 21, batch 1050\n(Train) Batch 1050 Loss : 0.17377841472625732, 맞은 개수 : 119\nEpoch : 21, batch 1051\n(Train) Batch 1051 Loss : 0.19968047738075256, 맞은 개수 : 124\nEpoch : 21, batch 1052\n(Train) Batch 1052 Loss : 0.35479336977005005, 맞은 개수 : 112\nEpoch : 21, batch 1053\n(Train) Batch 1053 Loss : 0.31426477432250977, 맞은 개수 : 115\nEpoch : 21, batch 1054\n(Train) Batch 1054 Loss : 0.15609276294708252, 맞은 개수 : 122\nEpoch : 21, batch 1055\n(Train) Batch 1055 Loss : 0.19308525323867798, 맞은 개수 : 120\nEpoch : 21, batch 1056\n(Train) Batch 1056 Loss : 0.3398144841194153, 맞은 개수 : 113\nEpoch : 21, batch 1057\n(Train) Batch 1057 Loss : 0.19211193919181824, 맞은 개수 : 118\nEpoch : 21, batch 1058\n(Train) Batch 1058 Loss : 0.5239531397819519, 맞은 개수 : 105\nEpoch : 21, batch 1059\n(Train) Batch 1059 Loss : 0.3390313386917114, 맞은 개수 : 114\nEpoch : 21, batch 1060\n(Train) Batch 1060 Loss : 0.3887872099876404, 맞은 개수 : 114\nEpoch : 21, batch 1061\n(Train) Batch 1061 Loss : 0.22073404490947723, 맞은 개수 : 122\nEpoch : 21, batch 1062\n(Train) Batch 1062 Loss : 0.22715236246585846, 맞은 개수 : 117\nEpoch : 21, batch 1063\n(Train) Batch 1063 Loss : 0.2757596969604492, 맞은 개수 : 117\nEpoch : 21, batch 1064\n(Train) Batch 1064 Loss : 0.42948782444000244, 맞은 개수 : 108\nEpoch : 21, batch 1065\n(Train) Batch 1065 Loss : 0.20756547152996063, 맞은 개수 : 118\nEpoch : 21, batch 1066\n(Train) Batch 1066 Loss : 0.3927649259567261, 맞은 개수 : 115\nEpoch : 21, batch 1067\n(Train) Batch 1067 Loss : 0.36787837743759155, 맞은 개수 : 114\nEpoch : 21, batch 1068\n(Train) Batch 1068 Loss : 0.33813297748565674, 맞은 개수 : 115\nEpoch : 21, batch 1069\n(Train) Batch 1069 Loss : 0.30509519577026367, 맞은 개수 : 115\nEpoch : 21, batch 1070\n(Train) Batch 1070 Loss : 0.47725826501846313, 맞은 개수 : 111\nEpoch : 21, batch 1071\n(Train) Batch 1071 Loss : 0.4487418830394745, 맞은 개수 : 111\nEpoch : 21, batch 1072\n(Train) Batch 1072 Loss : 0.20632103085517883, 맞은 개수 : 119\nEpoch : 21, batch 1073\n(Train) Batch 1073 Loss : 0.3631438612937927, 맞은 개수 : 115\nEpoch : 21, batch 1074\n(Train) Batch 1074 Loss : 0.35221219062805176, 맞은 개수 : 121\nEpoch : 21, batch 1075\n(Train) Batch 1075 Loss : 0.29716840386390686, 맞은 개수 : 116\nEpoch : 21, batch 1076\n(Train) Batch 1076 Loss : 0.4547369182109833, 맞은 개수 : 113\nEpoch : 21, batch 1077\n(Train) Batch 1077 Loss : 0.21990624070167542, 맞은 개수 : 120\nEpoch : 21, batch 1078\n(Train) Batch 1078 Loss : 0.23873759806156158, 맞은 개수 : 120\nEpoch : 21, batch 1079\n(Train) Batch 1079 Loss : 0.5516682863235474, 맞은 개수 : 110\nEpoch : 21, batch 1080\n(Train) Batch 1080 Loss : 0.4467069208621979, 맞은 개수 : 114\nEpoch : 21, batch 1081\n(Train) Batch 1081 Loss : 0.27348247170448303, 맞은 개수 : 117\nEpoch : 21, batch 1082\n(Train) Batch 1082 Loss : 0.20406877994537354, 맞은 개수 : 115\nEpoch : 21, batch 1083\n(Train) Batch 1083 Loss : 0.19877277314662933, 맞은 개수 : 121\nEpoch : 21, batch 1084\n(Train) Batch 1084 Loss : 0.3092186748981476, 맞은 개수 : 114\nEpoch : 21, batch 1085\n(Train) Batch 1085 Loss : 0.305514931678772, 맞은 개수 : 116\nEpoch : 21, batch 1086\n(Train) Batch 1086 Loss : 0.38369888067245483, 맞은 개수 : 110\nEpoch : 21, batch 1087\n(Train) Batch 1087 Loss : 0.36156710982322693, 맞은 개수 : 112\nEpoch : 21, batch 1088\n(Train) Batch 1088 Loss : 0.3547564446926117, 맞은 개수 : 116\nEpoch : 21, batch 1089\n(Train) Batch 1089 Loss : 0.27133581042289734, 맞은 개수 : 118\nEpoch : 21, batch 1090\n(Train) Batch 1090 Loss : 0.2961914539337158, 맞은 개수 : 118\nEpoch : 21, batch 1091\n(Train) Batch 1091 Loss : 0.383280873298645, 맞은 개수 : 111\nEpoch : 21, batch 1092\n(Train) Batch 1092 Loss : 0.36658185720443726, 맞은 개수 : 114\nEpoch : 21, batch 1093\n(Train) Batch 1093 Loss : 0.2657169699668884, 맞은 개수 : 115\nEpoch : 21, batch 1094\n(Train) Batch 1094 Loss : 0.2378353327512741, 맞은 개수 : 119\nEpoch : 21, batch 1095\n(Train) Batch 1095 Loss : 0.39126256108283997, 맞은 개수 : 113\nEpoch : 21, batch 1096\n(Train) Batch 1096 Loss : 0.27380481362342834, 맞은 개수 : 118\nEpoch : 21, batch 1097\n(Train) Batch 1097 Loss : 0.19900661706924438, 맞은 개수 : 120\nEpoch : 21, batch 1098\n(Train) Batch 1098 Loss : 0.15284304320812225, 맞은 개수 : 121\nEpoch : 21, batch 1099\n(Train) Batch 1099 Loss : 0.23592932522296906, 맞은 개수 : 119\nEpoch : 21, batch 1100\n(Train) Batch 1100 Loss : 0.2965543866157532, 맞은 개수 : 119\nEpoch : 21, batch 1101\n(Train) Batch 1101 Loss : 0.32486072182655334, 맞은 개수 : 118\nEpoch : 21, batch 1102\n(Train) Batch 1102 Loss : 0.3730199635028839, 맞은 개수 : 115\nEpoch : 21, batch 1103\n(Train) Batch 1103 Loss : 0.30205923318862915, 맞은 개수 : 116\nEpoch : 21, batch 1104\n(Train) Batch 1104 Loss : 0.20134921371936798, 맞은 개수 : 124\nEpoch : 21, batch 1105\n(Train) Batch 1105 Loss : 0.34571367502212524, 맞은 개수 : 116\nEpoch : 21, batch 1106\n(Train) Batch 1106 Loss : 0.14973068237304688, 맞은 개수 : 116\nEpoch : 21, batch 1107\n(Train) Batch 1107 Loss : 0.22489628195762634, 맞은 개수 : 116\nEpoch : 21, batch 1108\n(Train) Batch 1108 Loss : 0.19460418820381165, 맞은 개수 : 122\nEpoch : 21, batch 1109\n(Train) Batch 1109 Loss : 0.18655624985694885, 맞은 개수 : 122\nEpoch : 21, batch 1110\n(Train) Batch 1110 Loss : 0.28011706471443176, 맞은 개수 : 117\nEpoch : 21, batch 1111\n(Train) Batch 1111 Loss : 0.42847004532814026, 맞은 개수 : 114\nEpoch : 21, batch 1112\n(Train) Batch 1112 Loss : 0.2688639163970947, 맞은 개수 : 118\nEpoch : 21, batch 1113\n(Train) Batch 1113 Loss : 0.07041500508785248, 맞은 개수 : 126\nEpoch : 21, batch 1114\n(Train) Batch 1114 Loss : 0.24833790957927704, 맞은 개수 : 116\nEpoch : 21, batch 1115\n(Train) Batch 1115 Loss : 0.35118961334228516, 맞은 개수 : 117\nEpoch : 21, batch 1116\n(Train) Batch 1116 Loss : 0.35202091932296753, 맞은 개수 : 113\nEpoch : 21, batch 1117\n(Train) Batch 1117 Loss : 0.25274568796157837, 맞은 개수 : 117\nEpoch : 21, batch 1118\n(Train) Batch 1118 Loss : 0.21985524892807007, 맞은 개수 : 120\nEpoch : 21, batch 1119\n(Train) Batch 1119 Loss : 0.40681758522987366, 맞은 개수 : 111\nEpoch : 21, batch 1120\n(Train) Batch 1120 Loss : 0.3673078417778015, 맞은 개수 : 111\nEpoch : 21, batch 1121\n(Train) Batch 1121 Loss : 0.20507097244262695, 맞은 개수 : 120\nEpoch : 21, batch 1122\n(Train) Batch 1122 Loss : 0.17539310455322266, 맞은 개수 : 121\nEpoch : 21, batch 1123\n(Train) Batch 1123 Loss : 0.32479363679885864, 맞은 개수 : 115\nEpoch : 21, batch 1124\n(Train) Batch 1124 Loss : 0.3492134213447571, 맞은 개수 : 109\nEpoch : 21, batch 1125\n(Train) Batch 1125 Loss : 0.4399753212928772, 맞은 개수 : 109\nEpoch : 21, batch 1126\n(Train) Batch 1126 Loss : 0.2517697811126709, 맞은 개수 : 116\nEpoch : 21, batch 1127\n(Train) Batch 1127 Loss : 0.4685826301574707, 맞은 개수 : 111\nEpoch : 21, batch 1128\n(Train) Batch 1128 Loss : 0.2871758043766022, 맞은 개수 : 118\nEpoch : 21, batch 1129\n(Train) Batch 1129 Loss : 0.37098750472068787, 맞은 개수 : 115\nEpoch : 21, batch 1130\n(Train) Batch 1130 Loss : 0.3363799750804901, 맞은 개수 : 113\nEpoch : 21, batch 1131\n(Train) Batch 1131 Loss : 0.3505188226699829, 맞은 개수 : 115\nEpoch : 21, batch 1132\n(Train) Batch 1132 Loss : 0.24334821105003357, 맞은 개수 : 118\nEpoch : 21, batch 1133\n(Train) Batch 1133 Loss : 0.2993175983428955, 맞은 개수 : 118\nEpoch : 21, batch 1134\n(Train) Batch 1134 Loss : 0.2213440090417862, 맞은 개수 : 117\nEpoch : 21, batch 1135\n(Train) Batch 1135 Loss : 0.2752155661582947, 맞은 개수 : 114\nEpoch : 21, batch 1136\n(Train) Batch 1136 Loss : 0.33593305945396423, 맞은 개수 : 113\nEpoch : 21, batch 1137\n(Train) Batch 1137 Loss : 0.34805917739868164, 맞은 개수 : 113\nEpoch : 21, batch 1138\n(Train) Batch 1138 Loss : 0.3345901370048523, 맞은 개수 : 110\nEpoch : 21, batch 1139\n(Train) Batch 1139 Loss : 0.20047059655189514, 맞은 개수 : 122\nEpoch : 21, batch 1140\n(Train) Batch 1140 Loss : 0.2554229497909546, 맞은 개수 : 119\nEpoch : 21, batch 1141\n(Train) Batch 1141 Loss : 0.22402401268482208, 맞은 개수 : 120\nEpoch : 21, batch 1142\n(Train) Batch 1142 Loss : 0.3370307683944702, 맞은 개수 : 116\nEpoch : 21, batch 1143\n(Train) Batch 1143 Loss : 0.34364205598831177, 맞은 개수 : 115\nEpoch : 21, batch 1144\n(Train) Batch 1144 Loss : 0.2317098081111908, 맞은 개수 : 120\nEpoch : 21, batch 1145\n(Train) Batch 1145 Loss : 0.1507619321346283, 맞은 개수 : 120\nEpoch : 21, batch 1146\n(Train) Batch 1146 Loss : 0.1574699729681015, 맞은 개수 : 120\nEpoch : 21, batch 1147\n(Train) Batch 1147 Loss : 0.3676531910896301, 맞은 개수 : 111\nEpoch : 21, batch 1148\n(Train) Batch 1148 Loss : 0.28946179151535034, 맞은 개수 : 117\nEpoch : 21, batch 1149\n(Train) Batch 1149 Loss : 0.237364262342453, 맞은 개수 : 118\nEpoch : 21, batch 1150\n(Train) Batch 1150 Loss : 0.2970752716064453, 맞은 개수 : 116\nEpoch : 21, batch 1151\n(Train) Batch 1151 Loss : 0.26974156498908997, 맞은 개수 : 117\nEpoch : 21, batch 1152\n(Train) Batch 1152 Loss : 0.26657482981681824, 맞은 개수 : 119\nEpoch : 21, batch 1153\n(Train) Batch 1153 Loss : 0.3001953363418579, 맞은 개수 : 118\nEpoch : 21, batch 1154\n(Train) Batch 1154 Loss : 0.2693384885787964, 맞은 개수 : 119\nEpoch : 21, batch 1155\n(Train) Batch 1155 Loss : 0.4011193811893463, 맞은 개수 : 115\nEpoch : 21, batch 1156\n(Train) Batch 1156 Loss : 0.15368911623954773, 맞은 개수 : 121\nEpoch : 21, batch 1157\n(Train) Batch 1157 Loss : 0.2502811551094055, 맞은 개수 : 121\nEpoch : 21, batch 1158\n(Train) Batch 1158 Loss : 0.244384765625, 맞은 개수 : 121\nEpoch : 21, batch 1159\n(Train) Batch 1159 Loss : 0.2829323709011078, 맞은 개수 : 114\nEpoch : 21, batch 1160\n(Train) Batch 1160 Loss : 0.3454725444316864, 맞은 개수 : 116\nEpoch : 21, batch 1161\n(Train) Batch 1161 Loss : 0.26057326793670654, 맞은 개수 : 117\nEpoch : 21, batch 1162\n(Train) Batch 1162 Loss : 0.27846360206604004, 맞은 개수 : 118\nEpoch : 21, batch 1163\n(Train) Batch 1163 Loss : 0.3248472213745117, 맞은 개수 : 114\nEpoch : 21, batch 1164\n(Train) Batch 1164 Loss : 0.3840484917163849, 맞은 개수 : 116\nEpoch : 21, batch 1165\n(Train) Batch 1165 Loss : 0.3104545474052429, 맞은 개수 : 110\nEpoch : 21, batch 1166\n(Train) Batch 1166 Loss : 0.26564332842826843, 맞은 개수 : 119\nEpoch : 21, batch 1167\n(Train) Batch 1167 Loss : 0.24693357944488525, 맞은 개수 : 117\nEpoch : 21, batch 1168\n(Train) Batch 1168 Loss : 0.3060416281223297, 맞은 개수 : 114\nEpoch : 21, batch 1169\n(Train) Batch 1169 Loss : 0.29393497109413147, 맞은 개수 : 112\nEpoch : 21, batch 1170\n(Train) Batch 1170 Loss : 0.22979961335659027, 맞은 개수 : 118\nEpoch : 21, batch 1171\n(Train) Batch 1171 Loss : 0.38263192772865295, 맞은 개수 : 112\nEpoch : 21, batch 1172\n(Train) Batch 1172 Loss : 0.38620784878730774, 맞은 개수 : 109\nEpoch : 21, batch 1173\n(Train) Batch 1173 Loss : 0.387727290391922, 맞은 개수 : 113\nEpoch : 21, batch 1174\n(Train) Batch 1174 Loss : 0.27204856276512146, 맞은 개수 : 114\nEpoch : 21, batch 1175\n(Train) Batch 1175 Loss : 0.2987787425518036, 맞은 개수 : 119\nEpoch : 21, batch 1176\n(Train) Batch 1176 Loss : 0.39050057530403137, 맞은 개수 : 115\nEpoch : 21, batch 1177\n(Train) Batch 1177 Loss : 0.35997891426086426, 맞은 개수 : 115\nEpoch : 21, batch 1178\n(Train) Batch 1178 Loss : 0.15285563468933105, 맞은 개수 : 123\nEpoch : 21, batch 1179\n(Train) Batch 1179 Loss : 0.26367634534835815, 맞은 개수 : 119\nEpoch : 21, batch 1180\n(Train) Batch 1180 Loss : 0.2573891878128052, 맞은 개수 : 118\nEpoch : 21, batch 1181\n(Train) Batch 1181 Loss : 0.2009512484073639, 맞은 개수 : 117\nEpoch : 21, batch 1182\n(Train) Batch 1182 Loss : 0.18695542216300964, 맞은 개수 : 119\nEpoch : 21, batch 1183\n(Train) Batch 1183 Loss : 0.29396551847457886, 맞은 개수 : 114\nEpoch : 21, batch 1184\n(Train) Batch 1184 Loss : 0.32225674390792847, 맞은 개수 : 114\nEpoch : 21, batch 1185\n(Train) Batch 1185 Loss : 0.23736132681369781, 맞은 개수 : 119\nEpoch : 21, batch 1186\n(Train) Batch 1186 Loss : 0.34106019139289856, 맞은 개수 : 112\nEpoch : 21, batch 1187\n(Train) Batch 1187 Loss : 0.22431230545043945, 맞은 개수 : 118\nEpoch : 21, batch 1188\n(Train) Batch 1188 Loss : 0.46477964520454407, 맞은 개수 : 116\nEpoch : 21, batch 1189\n(Train) Batch 1189 Loss : 0.28467923402786255, 맞은 개수 : 113\nEpoch : 21, batch 1190\n(Train) Batch 1190 Loss : 0.24962790310382843, 맞은 개수 : 119\nEpoch : 21, batch 1191\n(Train) Batch 1191 Loss : 0.26515820622444153, 맞은 개수 : 117\nEpoch : 21, batch 1192\n(Train) Batch 1192 Loss : 0.24420076608657837, 맞은 개수 : 116\nEpoch : 21, batch 1193\n(Train) Batch 1193 Loss : 0.2608391344547272, 맞은 개수 : 119\nEpoch : 21, batch 1194\n(Train) Batch 1194 Loss : 0.260040283203125, 맞은 개수 : 118\nEpoch : 21, batch 1195\n(Train) Batch 1195 Loss : 0.4819183945655823, 맞은 개수 : 112\nEpoch : 21, batch 1196\n(Train) Batch 1196 Loss : 0.36228853464126587, 맞은 개수 : 115\nEpoch : 21, batch 1197\n(Train) Batch 1197 Loss : 0.4480558931827545, 맞은 개수 : 114\nEpoch : 21, batch 1198\n(Train) Batch 1198 Loss : 0.15943965315818787, 맞은 개수 : 123\nEpoch : 21, batch 1199\n(Train) Batch 1199 Loss : 0.29749158024787903, 맞은 개수 : 114\nEpoch : 21, batch 1200\n(Train) Batch 1200 Loss : 0.5250008702278137, 맞은 개수 : 108\nEpoch : 21, batch 1201\n(Train) Batch 1201 Loss : 0.31161996722221375, 맞은 개수 : 118\nEpoch : 21, batch 1202\n(Train) Batch 1202 Loss : 0.2514675557613373, 맞은 개수 : 119\nEpoch : 21, batch 1203\n(Train) Batch 1203 Loss : 0.4036989212036133, 맞은 개수 : 111\nEpoch : 21, batch 1204\n(Train) Batch 1204 Loss : 0.43907496333122253, 맞은 개수 : 112\nEpoch : 21, batch 1205\n(Train) Batch 1205 Loss : 0.3145922124385834, 맞은 개수 : 116\nEpoch : 21, batch 1206\n(Train) Batch 1206 Loss : 0.3632509112358093, 맞은 개수 : 115\nEpoch : 21, batch 1207\n(Train) Batch 1207 Loss : 0.19770751893520355, 맞은 개수 : 122\nEpoch : 21, batch 1208\n(Train) Batch 1208 Loss : 0.5173219442367554, 맞은 개수 : 109\nEpoch : 21, batch 1209\n(Train) Batch 1209 Loss : 0.2925400137901306, 맞은 개수 : 117\nEpoch : 21, batch 1210\n(Train) Batch 1210 Loss : 0.23186922073364258, 맞은 개수 : 118\nEpoch : 21, batch 1211\n(Train) Batch 1211 Loss : 0.3608776926994324, 맞은 개수 : 112\nEpoch : 21, batch 1212\n(Train) Batch 1212 Loss : 0.3615962266921997, 맞은 개수 : 112\nEpoch : 21, batch 1213\n(Train) Batch 1213 Loss : 0.3603881001472473, 맞은 개수 : 114\nEpoch : 21, batch 1214\n(Train) Batch 1214 Loss : 0.40601083636283875, 맞은 개수 : 112\nEpoch : 21, batch 1215\n(Train) Batch 1215 Loss : 0.28382882475852966, 맞은 개수 : 114\nEpoch : 21, batch 1216\n(Train) Batch 1216 Loss : 0.3191549479961395, 맞은 개수 : 114\nEpoch : 21, batch 1217\n(Train) Batch 1217 Loss : 0.27800703048706055, 맞은 개수 : 117\nEpoch : 21, batch 1218\n(Train) Batch 1218 Loss : 0.17951449751853943, 맞은 개수 : 119\nEpoch : 21, batch 1219\n(Train) Batch 1219 Loss : 0.2699469327926636, 맞은 개수 : 119\nEpoch : 21, batch 1220\n(Train) Batch 1220 Loss : 0.18141724169254303, 맞은 개수 : 116\nEpoch : 21, batch 1221\n(Train) Batch 1221 Loss : 0.20419222116470337, 맞은 개수 : 118\nEpoch : 21, batch 1222\n(Train) Batch 1222 Loss : 0.4919954836368561, 맞은 개수 : 112\nEpoch : 21, batch 1223\n(Train) Batch 1223 Loss : 0.2551939785480499, 맞은 개수 : 120\nEpoch : 21, batch 1224\n(Train) Batch 1224 Loss : 0.10989638417959213, 맞은 개수 : 126\nEpoch : 21, batch 1225\n(Train) Batch 1225 Loss : 0.29033857583999634, 맞은 개수 : 119\nEpoch : 21, batch 1226\n(Train) Batch 1226 Loss : 0.24944686889648438, 맞은 개수 : 114\nEpoch : 21, batch 1227\n(Train) Batch 1227 Loss : 0.18060553073883057, 맞은 개수 : 121\nEpoch : 21, batch 1228\n(Train) Batch 1228 Loss : 0.25502821803092957, 맞은 개수 : 117\nEpoch : 21, batch 1229\n(Train) Batch 1229 Loss : 0.18163414299488068, 맞은 개수 : 120\nEpoch : 21, batch 1230\n(Train) Batch 1230 Loss : 0.25428372621536255, 맞은 개수 : 116\nEpoch : 21, batch 1231\n(Train) Batch 1231 Loss : 0.09027799218893051, 맞은 개수 : 125\nEpoch : 21, batch 1232\n(Train) Batch 1232 Loss : 0.25022560358047485, 맞은 개수 : 116\nEpoch : 21, batch 1233\n(Train) Batch 1233 Loss : 0.29749932885169983, 맞은 개수 : 116\nEpoch : 21, batch 1234\n(Train) Batch 1234 Loss : 0.29073798656463623, 맞은 개수 : 115\nEpoch : 21, batch 1235\n(Train) Batch 1235 Loss : 0.24765197932720184, 맞은 개수 : 119\nEpoch : 21, batch 1236\n(Train) Batch 1236 Loss : 0.29241034388542175, 맞은 개수 : 114\nEpoch : 21, batch 1237\n(Train) Batch 1237 Loss : 0.22801394760608673, 맞은 개수 : 117\nEpoch : 21, batch 1238\n(Train) Batch 1238 Loss : 0.26835429668426514, 맞은 개수 : 117\nEpoch : 21, batch 1239\n(Train) Batch 1239 Loss : 0.15552474558353424, 맞은 개수 : 122\nEpoch : 21, batch 1240\n(Train) Batch 1240 Loss : 0.21681702136993408, 맞은 개수 : 118\nEpoch : 21, batch 1241\n(Train) Batch 1241 Loss : 0.37622809410095215, 맞은 개수 : 117\nEpoch : 21, batch 1242\n(Train) Batch 1242 Loss : 0.3309323489665985, 맞은 개수 : 114\nEpoch : 21, batch 1243\n(Train) Batch 1243 Loss : 0.303352415561676, 맞은 개수 : 116\nEpoch : 21, batch 1244\n(Train) Batch 1244 Loss : 0.19210942089557648, 맞은 개수 : 122\nEpoch : 21, batch 1245\n(Train) Batch 1245 Loss : 0.16742587089538574, 맞은 개수 : 121\nEpoch : 21, batch 1246\n(Train) Batch 1246 Loss : 0.23010921478271484, 맞은 개수 : 119\nEpoch : 21, batch 1247\n(Train) Batch 1247 Loss : 0.24253351986408234, 맞은 개수 : 118\nEpoch : 21, batch 1248\n(Train) Batch 1248 Loss : 0.24426278471946716, 맞은 개수 : 115\nEpoch : 21, batch 1249\n(Train) Batch 1249 Loss : 0.21709787845611572, 맞은 개수 : 119\nEpoch : 21, batch 1250\n(Train) Batch 1250 Loss : 0.37099483609199524, 맞은 개수 : 116\nEpoch : 21, batch 1251\n(Train) Batch 1251 Loss : 0.29398706555366516, 맞은 개수 : 114\nEpoch : 21, batch 1252\n(Train) Batch 1252 Loss : 0.2138722836971283, 맞은 개수 : 119\nEpoch : 21, batch 1253\n(Train) Batch 1253 Loss : 0.16682322323322296, 맞은 개수 : 124\nEpoch : 21, batch 1254\n(Train) Batch 1254 Loss : 0.19591574370861053, 맞은 개수 : 117\nEpoch : 21, batch 1255\n(Train) Batch 1255 Loss : 0.31151244044303894, 맞은 개수 : 116\nEpoch : 21, batch 1256\n(Train) Batch 1256 Loss : 0.21755938231945038, 맞은 개수 : 121\nEpoch : 21, batch 1257\n(Train) Batch 1257 Loss : 0.27926990389823914, 맞은 개수 : 119\nEpoch : 21, batch 1258\n(Train) Batch 1258 Loss : 0.2568438947200775, 맞은 개수 : 119\nEpoch : 21, batch 1259\n(Train) Batch 1259 Loss : 0.2709552049636841, 맞은 개수 : 121\nEpoch : 21, batch 1260\n(Train) Batch 1260 Loss : 0.28958660364151, 맞은 개수 : 117\nEpoch : 21, batch 1261\n(Train) Batch 1261 Loss : 0.25746479630470276, 맞은 개수 : 114\nEpoch : 21, batch 1262\n(Train) Batch 1262 Loss : 0.2976848781108856, 맞은 개수 : 113\nEpoch : 21, batch 1263\n(Train) Batch 1263 Loss : 0.34027203917503357, 맞은 개수 : 115\nEpoch : 21, batch 1264\n(Train) Batch 1264 Loss : 0.34363502264022827, 맞은 개수 : 113\nEpoch : 21, batch 1265\n(Train) Batch 1265 Loss : 0.3669399917125702, 맞은 개수 : 118\nEpoch : 21, batch 1266\n(Train) Batch 1266 Loss : 0.2885477840900421, 맞은 개수 : 115\nEpoch : 21, batch 1267\n(Train) Batch 1267 Loss : 0.2191563844680786, 맞은 개수 : 121\nEpoch : 21, batch 1268\n(Train) Batch 1268 Loss : 0.32205483317375183, 맞은 개수 : 114\nEpoch : 21, batch 1269\n(Train) Batch 1269 Loss : 0.34733846783638, 맞은 개수 : 118\nEpoch : 21, batch 1270\n(Train) Batch 1270 Loss : 0.30803820490837097, 맞은 개수 : 115\nEpoch : 21, batch 1271\n(Train) Batch 1271 Loss : 0.18353930115699768, 맞은 개수 : 120\nEpoch : 21, batch 1272\n(Train) Batch 1272 Loss : 0.2531641721725464, 맞은 개수 : 116\nEpoch : 21, batch 1273\n(Train) Batch 1273 Loss : 0.1953222006559372, 맞은 개수 : 122\nEpoch : 21, batch 1274\n(Train) Batch 1274 Loss : 0.1867978274822235, 맞은 개수 : 118\nEpoch : 21, batch 1275\n(Train) Batch 1275 Loss : 0.23696959018707275, 맞은 개수 : 119\nEpoch : 21, batch 1276\n(Train) Batch 1276 Loss : 0.40532708168029785, 맞은 개수 : 113\nEpoch : 21, batch 1277\n(Train) Batch 1277 Loss : 0.24185018241405487, 맞은 개수 : 116\nEpoch : 21, batch 1278\n(Train) Batch 1278 Loss : 0.18335767090320587, 맞은 개수 : 120\nEpoch : 21, batch 1279\n(Train) Batch 1279 Loss : 0.38012468814849854, 맞은 개수 : 112\nEpoch : 21, batch 1280\n(Train) Batch 1280 Loss : 0.208634614944458, 맞은 개수 : 121\nEpoch : 21, batch 1281\n(Train) Batch 1281 Loss : 0.35319316387176514, 맞은 개수 : 114\nEpoch : 21, batch 1282\n(Train) Batch 1282 Loss : 0.31414270401000977, 맞은 개수 : 115\nEpoch : 21, batch 1283\n(Train) Batch 1283 Loss : 0.19961059093475342, 맞은 개수 : 119\nEpoch : 21, batch 1284\n(Train) Batch 1284 Loss : 0.3109976053237915, 맞은 개수 : 117\nEpoch : 21, batch 1285\n(Train) Batch 1285 Loss : 0.26998773217201233, 맞은 개수 : 118\nEpoch : 21, batch 1286\n(Train) Batch 1286 Loss : 0.3748062252998352, 맞은 개수 : 112\nEpoch : 21, batch 1287\n(Train) Batch 1287 Loss : 0.3656885325908661, 맞은 개수 : 118\nEpoch : 21, batch 1288\n(Train) Batch 1288 Loss : 0.2573027014732361, 맞은 개수 : 122\nEpoch : 21, batch 1289\n(Train) Batch 1289 Loss : 0.28888195753097534, 맞은 개수 : 118\nEpoch : 21, batch 1290\n(Train) Batch 1290 Loss : 0.4158836901187897, 맞은 개수 : 113\nEpoch : 21, batch 1291\n(Train) Batch 1291 Loss : 0.43228474259376526, 맞은 개수 : 114\nEpoch : 21, batch 1292\n(Train) Batch 1292 Loss : 0.24360835552215576, 맞은 개수 : 116\nEpoch : 21, batch 1293\n(Train) Batch 1293 Loss : 0.32177969813346863, 맞은 개수 : 114\nEpoch : 21, batch 1294\n(Train) Batch 1294 Loss : 0.29616668820381165, 맞은 개수 : 118\nEpoch : 21, batch 1295\n(Train) Batch 1295 Loss : 0.29763826727867126, 맞은 개수 : 114\nEpoch : 21, batch 1296\n(Train) Batch 1296 Loss : 0.3106665313243866, 맞은 개수 : 117\nEpoch : 21, batch 1297\n(Train) Batch 1297 Loss : 0.2923893332481384, 맞은 개수 : 118\nEpoch : 21, batch 1298\n(Train) Batch 1298 Loss : 0.16920292377471924, 맞은 개수 : 122\nEpoch : 21, batch 1299\n(Train) Batch 1299 Loss : 0.24002836644649506, 맞은 개수 : 116\nEpoch : 21, batch 1300\n(Train) Batch 1300 Loss : 0.3596879839897156, 맞은 개수 : 116\nEpoch : 21, batch 1301\n(Train) Batch 1301 Loss : 0.18961454927921295, 맞은 개수 : 118\nEpoch : 21, batch 1302\n(Train) Batch 1302 Loss : 0.35318148136138916, 맞은 개수 : 115\nEpoch : 21, batch 1303\n(Train) Batch 1303 Loss : 0.271837443113327, 맞은 개수 : 115\nEpoch : 21, batch 1304\n(Train) Batch 1304 Loss : 0.2890906035900116, 맞은 개수 : 114\nEpoch : 21, batch 1305\n(Train) Batch 1305 Loss : 0.2590496242046356, 맞은 개수 : 117\nEpoch : 21, batch 1306\n(Train) Batch 1306 Loss : 0.3760508596897125, 맞은 개수 : 110\nEpoch : 21, batch 1307\n(Train) Batch 1307 Loss : 0.24003185331821442, 맞은 개수 : 118\nEpoch : 21, batch 1308\n(Train) Batch 1308 Loss : 0.2144191861152649, 맞은 개수 : 119\nEpoch : 21, batch 1309\n(Train) Batch 1309 Loss : 0.34496429562568665, 맞은 개수 : 113\nEpoch : 21, batch 1310\n(Train) Batch 1310 Loss : 0.2663716971874237, 맞은 개수 : 114\nEpoch : 21, batch 1311\n(Train) Batch 1311 Loss : 0.28516054153442383, 맞은 개수 : 117\nEpoch : 21, batch 1312\n(Train) Batch 1312 Loss : 0.19983206689357758, 맞은 개수 : 121\nEpoch : 21, batch 1313\n(Train) Batch 1313 Loss : 0.2949133515357971, 맞은 개수 : 118\nEpoch : 21, batch 1314\n(Train) Batch 1314 Loss : 0.33737367391586304, 맞은 개수 : 115\nEpoch : 21, batch 1315\n(Train) Batch 1315 Loss : 0.34446626901626587, 맞은 개수 : 115\nEpoch : 21, batch 1316\n(Train) Batch 1316 Loss : 0.24775484204292297, 맞은 개수 : 115\nEpoch : 21, batch 1317\n(Train) Batch 1317 Loss : 0.32282787561416626, 맞은 개수 : 116\nEpoch : 21, batch 1318\n(Train) Batch 1318 Loss : 0.28894221782684326, 맞은 개수 : 116\nEpoch : 21, batch 1319\n(Train) Batch 1319 Loss : 0.26268813014030457, 맞은 개수 : 115\nEpoch : 21, batch 1320\n(Train) Batch 1320 Loss : 0.3677614629268646, 맞은 개수 : 114\nEpoch : 21, batch 1321\n(Train) Batch 1321 Loss : 0.2520250976085663, 맞은 개수 : 117\nEpoch : 21, batch 1322\n(Train) Batch 1322 Loss : 0.35350170731544495, 맞은 개수 : 115\nEpoch : 21, batch 1323\n(Train) Batch 1323 Loss : 0.3170327842235565, 맞은 개수 : 114\nEpoch : 21, batch 1324\n(Train) Batch 1324 Loss : 0.41633376479148865, 맞은 개수 : 116\nEpoch : 21, batch 1325\n(Train) Batch 1325 Loss : 0.42842718958854675, 맞은 개수 : 112\nEpoch : 21, batch 1326\n(Train) Batch 1326 Loss : 0.34580880403518677, 맞은 개수 : 110\nEpoch : 21, batch 1327\n(Train) Batch 1327 Loss : 0.23111394047737122, 맞은 개수 : 117\nEpoch : 21, batch 1328\n(Train) Batch 1328 Loss : 0.3011825978755951, 맞은 개수 : 110\nEpoch : 21, batch 1329\n(Train) Batch 1329 Loss : 0.2562515437602997, 맞은 개수 : 120\nEpoch : 21, batch 1330\n(Train) Batch 1330 Loss : 0.348486065864563, 맞은 개수 : 117\nEpoch : 21, batch 1331\n(Train) Batch 1331 Loss : 0.3928961157798767, 맞은 개수 : 114\nEpoch : 21, batch 1332\n(Train) Batch 1332 Loss : 0.42721474170684814, 맞은 개수 : 113\nEpoch : 21, batch 1333\n(Train) Batch 1333 Loss : 0.30751898884773254, 맞은 개수 : 115\nEpoch : 21, batch 1334\n(Train) Batch 1334 Loss : 0.2290443331003189, 맞은 개수 : 116\nEpoch : 21, batch 1335\n(Train) Batch 1335 Loss : 0.33384108543395996, 맞은 개수 : 118\nEpoch : 21, batch 1336\n(Train) Batch 1336 Loss : 0.1965683102607727, 맞은 개수 : 122\nEpoch : 21, batch 1337\n(Train) Batch 1337 Loss : 0.26125267148017883, 맞은 개수 : 116\nEpoch : 21, batch 1338\n(Train) Batch 1338 Loss : 0.31078338623046875, 맞은 개수 : 115\nEpoch : 21, batch 1339\n(Train) Batch 1339 Loss : 0.4355577528476715, 맞은 개수 : 109\nEpoch : 21, batch 1340\n(Train) Batch 1340 Loss : 0.15514230728149414, 맞은 개수 : 120\nEpoch : 21, batch 1341\n(Train) Batch 1341 Loss : 0.2464183270931244, 맞은 개수 : 117\nEpoch : 21, batch 1342\n(Train) Batch 1342 Loss : 0.3418298065662384, 맞은 개수 : 114\nEpoch : 21, batch 1343\n(Train) Batch 1343 Loss : 0.32156437635421753, 맞은 개수 : 114\nEpoch : 21, batch 1344\n(Train) Batch 1344 Loss : 0.3468315005302429, 맞은 개수 : 113\nEpoch : 21, batch 1345\n(Train) Batch 1345 Loss : 0.30018049478530884, 맞은 개수 : 117\nEpoch : 21, batch 1346\n(Train) Batch 1346 Loss : 0.27101296186447144, 맞은 개수 : 118\nEpoch : 21, batch 1347\n(Train) Batch 1347 Loss : 0.3242837190628052, 맞은 개수 : 118\nEpoch : 21, batch 1348\n(Train) Batch 1348 Loss : 0.25547608733177185, 맞은 개수 : 118\nEpoch : 21, batch 1349\n(Train) Batch 1349 Loss : 0.45222705602645874, 맞은 개수 : 110\nEpoch : 21, batch 1350\n(Train) Batch 1350 Loss : 0.23304133117198944, 맞은 개수 : 120\nEpoch : 21, batch 1351\n(Train) Batch 1351 Loss : 0.29925233125686646, 맞은 개수 : 120\nEpoch : 21, batch 1352\n(Train) Batch 1352 Loss : 0.2741970717906952, 맞은 개수 : 117\nEpoch : 21, batch 1353\n(Train) Batch 1353 Loss : 0.3281203508377075, 맞은 개수 : 114\nEpoch : 21, batch 1354\n(Train) Batch 1354 Loss : 0.306892454624176, 맞은 개수 : 118\nEpoch : 21, batch 1355\n(Train) Batch 1355 Loss : 0.34955236315727234, 맞은 개수 : 116\nEpoch : 21, batch 1356\n(Train) Batch 1356 Loss : 0.1773342490196228, 맞은 개수 : 120\nEpoch : 21, batch 1357\n(Train) Batch 1357 Loss : 0.22086896002292633, 맞은 개수 : 120\nEpoch : 21, batch 1358\n(Train) Batch 1358 Loss : 0.14094054698944092, 맞은 개수 : 122\nEpoch : 21, batch 1359\n(Train) Batch 1359 Loss : 0.2819451093673706, 맞은 개수 : 120\nEpoch : 21, batch 1360\n(Train) Batch 1360 Loss : 0.16847842931747437, 맞은 개수 : 120\nEpoch : 21, batch 1361\n(Train) Batch 1361 Loss : 0.22416286170482635, 맞은 개수 : 121\nEpoch : 21, batch 1362\n(Train) Batch 1362 Loss : 0.23411676287651062, 맞은 개수 : 119\nEpoch : 21, batch 1363\n(Train) Batch 1363 Loss : 0.26782265305519104, 맞은 개수 : 120\nEpoch : 21, batch 1364\n(Train) Batch 1364 Loss : 0.3071238398551941, 맞은 개수 : 118\nEpoch : 21, batch 1365\n(Train) Batch 1365 Loss : 0.2814543545246124, 맞은 개수 : 117\nEpoch : 21, batch 1366\n(Train) Batch 1366 Loss : 0.4144425094127655, 맞은 개수 : 112\nEpoch : 21, batch 1367\n(Train) Batch 1367 Loss : 0.2052665501832962, 맞은 개수 : 118\nEpoch : 21, batch 1368\n(Train) Batch 1368 Loss : 0.24253055453300476, 맞은 개수 : 117\nEpoch : 21, batch 1369\n(Train) Batch 1369 Loss : 0.23593194782733917, 맞은 개수 : 116\nEpoch : 21, batch 1370\n(Train) Batch 1370 Loss : 0.3580823838710785, 맞은 개수 : 115\nEpoch : 21, batch 1371\n(Train) Batch 1371 Loss : 0.29396170377731323, 맞은 개수 : 115\nEpoch : 21, batch 1372\n(Train) Batch 1372 Loss : 0.30025607347488403, 맞은 개수 : 117\nEpoch : 21, batch 1373\n(Train) Batch 1373 Loss : 0.16172346472740173, 맞은 개수 : 119\nEpoch : 21, batch 1374\n(Train) Batch 1374 Loss : 0.30755332112312317, 맞은 개수 : 117\nEpoch : 21, batch 1375\n(Train) Batch 1375 Loss : 0.2675316035747528, 맞은 개수 : 117\nEpoch : 21, batch 1376\n(Train) Batch 1376 Loss : 0.4034886658191681, 맞은 개수 : 114\nEpoch : 21, batch 1377\n(Train) Batch 1377 Loss : 0.34795045852661133, 맞은 개수 : 116\nEpoch : 21, batch 1378\n(Train) Batch 1378 Loss : 0.2926636040210724, 맞은 개수 : 116\nEpoch : 21, batch 1379\n(Train) Batch 1379 Loss : 0.32975420355796814, 맞은 개수 : 117\nEpoch : 21, batch 1380\n(Train) Batch 1380 Loss : 0.257300466299057, 맞은 개수 : 115\nEpoch : 21, batch 1381\n(Train) Batch 1381 Loss : 0.2355109006166458, 맞은 개수 : 115\nEpoch : 21, batch 1382\n(Train) Batch 1382 Loss : 0.3053531050682068, 맞은 개수 : 119\nEpoch : 21, batch 1383\n(Train) Batch 1383 Loss : 0.15870866179466248, 맞은 개수 : 121\nEpoch : 21, batch 1384\n(Train) Batch 1384 Loss : 0.36021965742111206, 맞은 개수 : 114\nEpoch : 21, batch 1385\n(Train) Batch 1385 Loss : 0.2374456524848938, 맞은 개수 : 120\nEpoch : 21, batch 1386\n(Train) Batch 1386 Loss : 0.46555808186531067, 맞은 개수 : 111\nEpoch : 21, batch 1387\n(Train) Batch 1387 Loss : 0.41557589173316956, 맞은 개수 : 114\nEpoch : 21, batch 1388\n(Train) Batch 1388 Loss : 0.45541220903396606, 맞은 개수 : 110\nEpoch : 21, batch 1389\n(Train) Batch 1389 Loss : 0.2204277068376541, 맞은 개수 : 120\nEpoch : 21, batch 1390\n(Train) Batch 1390 Loss : 0.4848617613315582, 맞은 개수 : 113\nEpoch : 21, batch 1391\n(Train) Batch 1391 Loss : 0.2843640446662903, 맞은 개수 : 113\nEpoch : 21, batch 1392\n(Train) Batch 1392 Loss : 0.5197131633758545, 맞은 개수 : 115\nEpoch : 21, batch 1393\n(Train) Batch 1393 Loss : 0.47844624519348145, 맞은 개수 : 113\nEpoch : 21, batch 1394\n(Train) Batch 1394 Loss : 0.11917492747306824, 맞은 개수 : 125\nEpoch : 21, batch 1395\n(Train) Batch 1395 Loss : 0.30861014127731323, 맞은 개수 : 117\nEpoch : 21, batch 1396\n(Train) Batch 1396 Loss : 0.23054128885269165, 맞은 개수 : 117\nEpoch : 21, batch 1397\n(Train) Batch 1397 Loss : 0.28302544355392456, 맞은 개수 : 118\nEpoch : 21, batch 1398\n(Train) Batch 1398 Loss : 0.35353511571884155, 맞은 개수 : 113\nEpoch : 21, batch 1399\n(Train) Batch 1399 Loss : 0.32881754636764526, 맞은 개수 : 112\nEpoch : 21, batch 1400\n(Train) Batch 1400 Loss : 0.19655071198940277, 맞은 개수 : 118\nEpoch : 21, batch 1401\n(Train) Batch 1401 Loss : 0.2593752443790436, 맞은 개수 : 113\nEpoch : 21, batch 1402\n(Train) Batch 1402 Loss : 0.18073920905590057, 맞은 개수 : 119\nEpoch : 21, batch 1403\n(Train) Batch 1403 Loss : 0.25680628418922424, 맞은 개수 : 117\nEpoch : 21, batch 1404\n(Train) Batch 1404 Loss : 0.19217494130134583, 맞은 개수 : 121\nEpoch : 21, batch 1405\n(Train) Batch 1405 Loss : 0.2798047661781311, 맞은 개수 : 119\nEpoch : 21, batch 1406\n(Train) Batch 1406 Loss : 0.2839561700820923, 맞은 개수 : 115\nEpoch : 21, batch 1407\n(Train) Batch 1407 Loss : 0.23753254115581512, 맞은 개수 : 116\nEpoch : 21, batch 1408\n(Train) Batch 1408 Loss : 0.28695008158683777, 맞은 개수 : 118\nEpoch : 21, batch 1409\n(Train) Batch 1409 Loss : 0.37977516651153564, 맞은 개수 : 116\nEpoch : 21, batch 1410\n(Train) Batch 1410 Loss : 0.38617071509361267, 맞은 개수 : 115\nEpoch : 21, batch 1411\n(Train) Batch 1411 Loss : 0.41338202357292175, 맞은 개수 : 114\nEpoch : 21, batch 1412\n(Train) Batch 1412 Loss : 0.29017457365989685, 맞은 개수 : 119\nEpoch : 21, batch 1413\n(Train) Batch 1413 Loss : 0.37868499755859375, 맞은 개수 : 113\nEpoch : 21, batch 1414\n(Train) Batch 1414 Loss : 0.24765460193157196, 맞은 개수 : 118\nEpoch : 21, batch 1415\n(Train) Batch 1415 Loss : 0.1907568722963333, 맞은 개수 : 118\nEpoch : 21, batch 1416\n(Train) Batch 1416 Loss : 0.20880289375782013, 맞은 개수 : 119\nEpoch : 21, batch 1417\n(Train) Batch 1417 Loss : 0.35042357444763184, 맞은 개수 : 112\nEpoch : 21, batch 1418\n(Train) Batch 1418 Loss : 0.22035443782806396, 맞은 개수 : 119\nEpoch : 21, batch 1419\n(Train) Batch 1419 Loss : 0.3492809534072876, 맞은 개수 : 117\nEpoch : 21, batch 1420\n(Train) Batch 1420 Loss : 0.3387828469276428, 맞은 개수 : 115\nEpoch : 21, batch 1421\n(Train) Batch 1421 Loss : 0.33748239278793335, 맞은 개수 : 116\nEpoch : 21, batch 1422\n(Train) Batch 1422 Loss : 0.2995629608631134, 맞은 개수 : 119\nEpoch : 21, batch 1423\n(Train) Batch 1423 Loss : 0.3185691833496094, 맞은 개수 : 116\nEpoch : 21, batch 1424\n(Train) Batch 1424 Loss : 0.27031803131103516, 맞은 개수 : 118\nEpoch : 21, batch 1425\n(Train) Batch 1425 Loss : 0.2792951166629791, 맞은 개수 : 116\nEpoch : 21, batch 1426\n(Train) Batch 1426 Loss : 0.30887311697006226, 맞은 개수 : 115\nEpoch : 21, batch 1427\n(Train) Batch 1427 Loss : 0.19912667572498322, 맞은 개수 : 117\nEpoch : 21, batch 1428\n(Train) Batch 1428 Loss : 0.4137974977493286, 맞은 개수 : 110\nEpoch : 21, batch 1429\n(Train) Batch 1429 Loss : 0.2874486744403839, 맞은 개수 : 116\nEpoch : 21, batch 1430\n(Train) Batch 1430 Loss : 0.5171018242835999, 맞은 개수 : 110\nEpoch : 21, batch 1431\n(Train) Batch 1431 Loss : 0.38184159994125366, 맞은 개수 : 113\nEpoch : 21, batch 1432\n(Train) Batch 1432 Loss : 0.1968442052602768, 맞은 개수 : 116\nEpoch : 21, batch 1433\n(Train) Batch 1433 Loss : 0.2375478297472, 맞은 개수 : 118\nEpoch : 21, batch 1434\n(Train) Batch 1434 Loss : 0.28445741534233093, 맞은 개수 : 116\nEpoch : 21, batch 1435\n(Train) Batch 1435 Loss : 0.45100733637809753, 맞은 개수 : 108\nEpoch : 21, batch 1436\n(Train) Batch 1436 Loss : 0.35028019547462463, 맞은 개수 : 116\nEpoch : 21, batch 1437\n(Train) Batch 1437 Loss : 0.24151664972305298, 맞은 개수 : 118\nEpoch : 21, batch 1438\n(Train) Batch 1438 Loss : 0.20400159060955048, 맞은 개수 : 122\nEpoch : 21, batch 1439\n(Train) Batch 1439 Loss : 0.3958747386932373, 맞은 개수 : 112\nEpoch : 21, batch 1440\n(Train) Batch 1440 Loss : 0.2606522738933563, 맞은 개수 : 118\nEpoch : 21, batch 1441\n(Train) Batch 1441 Loss : 0.27856603264808655, 맞은 개수 : 116\nEpoch : 21, batch 1442\n(Train) Batch 1442 Loss : 0.28678956627845764, 맞은 개수 : 115\nEpoch : 21, batch 1443\n(Train) Batch 1443 Loss : 0.2728860378265381, 맞은 개수 : 119\nEpoch : 21, batch 1444\n(Train) Batch 1444 Loss : 0.2746431231498718, 맞은 개수 : 117\nEpoch : 21, batch 1445\n(Train) Batch 1445 Loss : 0.22588340938091278, 맞은 개수 : 120\nEpoch : 21, batch 1446\n(Train) Batch 1446 Loss : 0.35065141320228577, 맞은 개수 : 115\nEpoch : 21, batch 1447\n(Train) Batch 1447 Loss : 0.32517385482788086, 맞은 개수 : 111\nEpoch : 21, batch 1448\n(Train) Batch 1448 Loss : 0.3161200284957886, 맞은 개수 : 116\nEpoch : 21, batch 1449\n(Train) Batch 1449 Loss : 0.3851225674152374, 맞은 개수 : 112\nEpoch : 21, batch 1450\n(Train) Batch 1450 Loss : 0.3189608156681061, 맞은 개수 : 112\nEpoch : 21, batch 1451\n(Train) Batch 1451 Loss : 0.23318932950496674, 맞은 개수 : 120\nEpoch : 21, batch 1452\n(Train) Batch 1452 Loss : 0.2824317514896393, 맞은 개수 : 120\nEpoch : 21, batch 1453\n(Train) Batch 1453 Loss : 0.39628899097442627, 맞은 개수 : 117\nEpoch : 21, batch 1454\n(Train) Batch 1454 Loss : 0.3865060806274414, 맞은 개수 : 112\nEpoch : 21, batch 1455\n(Train) Batch 1455 Loss : 0.4590195119380951, 맞은 개수 : 114\nEpoch : 21, batch 1456\n(Train) Batch 1456 Loss : 0.43019944429397583, 맞은 개수 : 112\nEpoch : 21, batch 1457\n(Train) Batch 1457 Loss : 0.32582592964172363, 맞은 개수 : 115\nEpoch : 21, batch 1458\n(Train) Batch 1458 Loss : 0.274366557598114, 맞은 개수 : 116\nEpoch : 21, batch 1459\n(Train) Batch 1459 Loss : 0.24845436215400696, 맞은 개수 : 119\nEpoch : 21, batch 1460\n(Train) Batch 1460 Loss : 0.32008424401283264, 맞은 개수 : 116\nEpoch : 21, batch 1461\n(Train) Batch 1461 Loss : 0.3098926246166229, 맞은 개수 : 119\nEpoch : 21, batch 1462\n(Train) Batch 1462 Loss : 0.3582051396369934, 맞은 개수 : 115\nEpoch : 21, batch 1463\n(Train) Batch 1463 Loss : 0.2263190746307373, 맞은 개수 : 117\nEpoch : 21, batch 1464\n(Train) Batch 1464 Loss : 0.2926892936229706, 맞은 개수 : 117\nEpoch : 21, batch 1465\n(Train) Batch 1465 Loss : 0.28649911284446716, 맞은 개수 : 115\nEpoch : 21, batch 1466\n(Train) Batch 1466 Loss : 0.354900062084198, 맞은 개수 : 114\nEpoch : 21, batch 1467\n(Train) Batch 1467 Loss : 0.22655680775642395, 맞은 개수 : 118\nEpoch : 21, batch 1468\n(Train) Batch 1468 Loss : 0.3789812922477722, 맞은 개수 : 113\nEpoch : 21, batch 1469\n(Train) Batch 1469 Loss : 0.42631834745407104, 맞은 개수 : 115\nEpoch : 21, batch 1470\n(Train) Batch 1470 Loss : 0.2717050313949585, 맞은 개수 : 120\nEpoch : 21, batch 1471\n(Train) Batch 1471 Loss : 0.17511606216430664, 맞은 개수 : 122\nEpoch : 21, batch 1472\n(Train) Batch 1472 Loss : 0.2522352635860443, 맞은 개수 : 119\nEpoch : 21, batch 1473\n(Train) Batch 1473 Loss : 0.4262445867061615, 맞은 개수 : 112\nEpoch : 21, batch 1474\n(Train) Batch 1474 Loss : 0.427727609872818, 맞은 개수 : 109\nEpoch : 21, batch 1475\n(Train) Batch 1475 Loss : 0.3319065570831299, 맞은 개수 : 115\nEpoch : 21, batch 1476\n(Train) Batch 1476 Loss : 0.33889999985694885, 맞은 개수 : 112\nEpoch : 21, batch 1477\n(Train) Batch 1477 Loss : 0.21430495381355286, 맞은 개수 : 120\nEpoch : 21, batch 1478\n(Train) Batch 1478 Loss : 0.5398494601249695, 맞은 개수 : 114\nEpoch : 21, batch 1479\n(Train) Batch 1479 Loss : 0.382988840341568, 맞은 개수 : 116\nEpoch : 21, batch 1480\n(Train) Batch 1480 Loss : 0.24982070922851562, 맞은 개수 : 121\nEpoch : 21, batch 1481\n(Train) Batch 1481 Loss : 0.2769792377948761, 맞은 개수 : 115\nEpoch : 21, batch 1482\n(Train) Batch 1482 Loss : 0.3726617991924286, 맞은 개수 : 116\nEpoch : 21, batch 1483\n(Train) Batch 1483 Loss : 0.41651687026023865, 맞은 개수 : 113\nEpoch : 21, batch 1484\n(Train) Batch 1484 Loss : 0.2616152763366699, 맞은 개수 : 116\nEpoch : 21, batch 1485\n(Train) Batch 1485 Loss : 0.16047155857086182, 맞은 개수 : 123\nEpoch : 21, batch 1486\n(Train) Batch 1486 Loss : 0.2690041661262512, 맞은 개수 : 116\nEpoch : 21, batch 1487\n(Train) Batch 1487 Loss : 0.24254447221755981, 맞은 개수 : 118\nEpoch : 21, batch 1488\n(Train) Batch 1488 Loss : 0.2982636094093323, 맞은 개수 : 118\nEpoch : 21, batch 1489\n(Train) Batch 1489 Loss : 0.2917740046977997, 맞은 개수 : 114\nEpoch : 21, batch 1490\n(Train) Batch 1490 Loss : 0.2444991022348404, 맞은 개수 : 118\nEpoch : 21, batch 1491\n(Train) Batch 1491 Loss : 0.20561473071575165, 맞은 개수 : 122\nEpoch : 21, batch 1492\n(Train) Batch 1492 Loss : 0.24959653615951538, 맞은 개수 : 117\nEpoch : 21, batch 1493\n(Train) Batch 1493 Loss : 0.3211114704608917, 맞은 개수 : 115\nEpoch : 21, batch 1494\n(Train) Batch 1494 Loss : 0.2914619445800781, 맞은 개수 : 114\nEpoch : 21, batch 1495\n(Train) Batch 1495 Loss : 0.32419249415397644, 맞은 개수 : 116\nEpoch : 21, batch 1496\n(Train) Batch 1496 Loss : 0.21878081560134888, 맞은 개수 : 120\nEpoch : 21, batch 1497\n(Train) Batch 1497 Loss : 0.33961862325668335, 맞은 개수 : 119\nEpoch : 21, batch 1498\n(Train) Batch 1498 Loss : 0.3536829948425293, 맞은 개수 : 114\nEpoch : 21, batch 1499\n(Train) Batch 1499 Loss : 0.29692670702934265, 맞은 개수 : 114\nEpoch : 21, batch 1500\n(Train) Batch 1500 Loss : 0.26592516899108887, 맞은 개수 : 118\nEpoch : 21, batch 1501\n(Train) Batch 1501 Loss : 0.27451780438423157, 맞은 개수 : 115\nEpoch : 21, batch 1502\n(Train) Batch 1502 Loss : 0.3642564117908478, 맞은 개수 : 117\nEpoch : 21, batch 1503\n(Train) Batch 1503 Loss : 0.35032349824905396, 맞은 개수 : 116\nEpoch : 21, batch 1504\n(Train) Batch 1504 Loss : 0.31134724617004395, 맞은 개수 : 115\nEpoch : 21, batch 1505\n(Train) Batch 1505 Loss : 0.342685729265213, 맞은 개수 : 110\nEpoch : 21, batch 1506\n(Train) Batch 1506 Loss : 0.19304785132408142, 맞은 개수 : 122\nEpoch : 21, batch 1507\n(Train) Batch 1507 Loss : 0.2515586316585541, 맞은 개수 : 115\nEpoch : 21, batch 1508\n(Train) Batch 1508 Loss : 0.3901975750923157, 맞은 개수 : 114\nEpoch : 21, batch 1509\n(Train) Batch 1509 Loss : 0.3077943027019501, 맞은 개수 : 116\nEpoch : 21, batch 1510\n(Train) Batch 1510 Loss : 0.24957908689975739, 맞은 개수 : 114\nEpoch : 21, batch 1511\n(Train) Batch 1511 Loss : 0.2132013589143753, 맞은 개수 : 116\nEpoch : 21, batch 1512\n(Train) Batch 1512 Loss : 0.17619691789150238, 맞은 개수 : 123\nEpoch : 21, batch 1513\n(Train) Batch 1513 Loss : 0.32510197162628174, 맞은 개수 : 116\nEpoch : 21, batch 1514\n(Train) Batch 1514 Loss : 0.4583181142807007, 맞은 개수 : 112\nEpoch : 21, batch 1515\n(Train) Batch 1515 Loss : 0.2921997904777527, 맞은 개수 : 113\nEpoch : 21, batch 1516\n(Train) Batch 1516 Loss : 0.20290470123291016, 맞은 개수 : 119\nEpoch : 21, batch 1517\n(Train) Batch 1517 Loss : 0.24995234608650208, 맞은 개수 : 120\nEpoch : 21, batch 1518\n(Train) Batch 1518 Loss : 0.24688906967639923, 맞은 개수 : 117\nEpoch : 21, batch 1519\n(Train) Batch 1519 Loss : 0.2984221875667572, 맞은 개수 : 117\nEpoch : 21, batch 1520\n(Train) Batch 1520 Loss : 0.3519921600818634, 맞은 개수 : 116\nEpoch : 21, batch 1521\n(Train) Batch 1521 Loss : 0.24217739701271057, 맞은 개수 : 117\nEpoch : 21, batch 1522\n(Train) Batch 1522 Loss : 0.21023160219192505, 맞은 개수 : 119\nEpoch : 21, batch 1523\n(Train) Batch 1523 Loss : 0.3450690507888794, 맞은 개수 : 113\nEpoch : 21, batch 1524\n(Train) Batch 1524 Loss : 0.37396129965782166, 맞은 개수 : 116\nEpoch : 21, batch 1525\n(Train) Batch 1525 Loss : 0.32644832134246826, 맞은 개수 : 115\nEpoch : 21, batch 1526\n(Train) Batch 1526 Loss : 0.20573872327804565, 맞은 개수 : 121\nEpoch : 21, batch 1527\n(Train) Batch 1527 Loss : 0.1639433056116104, 맞은 개수 : 121\nEpoch : 21, batch 1528\n(Train) Batch 1528 Loss : 0.1902018040418625, 맞은 개수 : 123\nEpoch : 21, batch 1529\n(Train) Batch 1529 Loss : 0.36573395133018494, 맞은 개수 : 114\nEpoch : 21, batch 1530\n(Train) Batch 1530 Loss : 0.23035402595996857, 맞은 개수 : 118\nEpoch : 21, batch 1531\n(Train) Batch 1531 Loss : 0.3200315237045288, 맞은 개수 : 114\nEpoch : 21, batch 1532\n(Train) Batch 1532 Loss : 0.23863905668258667, 맞은 개수 : 118\nEpoch : 21, batch 1533\n(Train) Batch 1533 Loss : 0.33409157395362854, 맞은 개수 : 116\nEpoch : 21, batch 1534\n(Train) Batch 1534 Loss : 0.174250528216362, 맞은 개수 : 123\nEpoch : 21, batch 1535\n(Train) Batch 1535 Loss : 0.2452893704175949, 맞은 개수 : 115\nEpoch : 21, batch 1536\n(Train) Batch 1536 Loss : 0.2508767247200012, 맞은 개수 : 120\nEpoch : 21, batch 1537\n(Train) Batch 1537 Loss : 0.4497438073158264, 맞은 개수 : 115\nEpoch : 21, batch 1538\n(Train) Batch 1538 Loss : 0.31067100167274475, 맞은 개수 : 115\nEpoch : 21, batch 1539\n(Train) Batch 1539 Loss : 0.5064995884895325, 맞은 개수 : 111\nEpoch : 21, batch 1540\n(Train) Batch 1540 Loss : 0.3111889064311981, 맞은 개수 : 115\nEpoch : 21, batch 1541\n(Train) Batch 1541 Loss : 0.34574180841445923, 맞은 개수 : 116\nEpoch : 21, batch 1542\n(Train) Batch 1542 Loss : 0.247414693236351, 맞은 개수 : 117\nEpoch : 21, batch 1543\n(Train) Batch 1543 Loss : 0.42295974493026733, 맞은 개수 : 116\nEpoch : 21, batch 1544\n(Train) Batch 1544 Loss : 0.40217554569244385, 맞은 개수 : 110\nEpoch : 21, batch 1545\n(Train) Batch 1545 Loss : 0.32223939895629883, 맞은 개수 : 116\nEpoch : 21, batch 1546\n(Train) Batch 1546 Loss : 0.2927168607711792, 맞은 개수 : 114\nEpoch : 21, batch 1547\n(Train) Batch 1547 Loss : 0.2746124863624573, 맞은 개수 : 114\nEpoch : 21, batch 1548\n(Train) Batch 1548 Loss : 0.3330969512462616, 맞은 개수 : 118\nEpoch : 21, batch 1549\n(Train) Batch 1549 Loss : 0.27824169397354126, 맞은 개수 : 116\nEpoch : 21, batch 1550\n(Train) Batch 1550 Loss : 0.19352301955223083, 맞은 개수 : 121\nEpoch : 21, batch 1551\n(Train) Batch 1551 Loss : 0.3645031452178955, 맞은 개수 : 114\nEpoch : 21, batch 1552\n(Train) Batch 1552 Loss : 0.3749561607837677, 맞은 개수 : 114\nEpoch : 21, batch 1553\n(Train) Batch 1553 Loss : 0.506191611289978, 맞은 개수 : 113\nEpoch : 21, batch 1554\n(Train) Batch 1554 Loss : 0.44141829013824463, 맞은 개수 : 112\nEpoch : 21, batch 1555\n(Train) Batch 1555 Loss : 0.21368630230426788, 맞은 개수 : 118\nEpoch : 21, batch 1556\n(Train) Batch 1556 Loss : 0.39581558108329773, 맞은 개수 : 115\nEpoch : 21, batch 1557\n(Train) Batch 1557 Loss : 0.3319558799266815, 맞은 개수 : 116\nEpoch : 21, batch 1558\n(Train) Batch 1558 Loss : 0.35179007053375244, 맞은 개수 : 116\nEpoch : 21, batch 1559\n(Train) Batch 1559 Loss : 0.34485629200935364, 맞은 개수 : 117\nEpoch : 21, batch 1560\n(Train) Batch 1560 Loss : 0.23058167099952698, 맞은 개수 : 120\nEpoch : 21, batch 1561\n(Train) Batch 1561 Loss : 0.33677807450294495, 맞은 개수 : 116\nEpoch : 21, batch 1562\n(Train) Batch 1562 Loss : 0.18178647756576538, 맞은 개수 : 60\nepoch 21 Loss/Train :0.2744756396514288 \nepoch 21 Accuracy/Train : 0.915575\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  0   0   0   0 114   0 148   0   0   0   0   0   0   0  13 186   0   0\n   0 196  17  36   1   0   0   0   0   0  39   0   0   0], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n(VAL) Batch 0 Loss : 1.2774443626403809, accuracy: 0.71875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  0   0   0  18  69 184   0   0   0   0   0   0  42   0   0  47   0  36\n   7   1   1   1  40   1   1  19   1   1  21   1   1   1], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n(VAL) Batch 1 Loss : 1.5667158365249634, accuracy: 0.6875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  1   1   1   1   1   1 132 154  10   1   1   1   1   1   1   1   1   1\n   1   1   1  10   1   1   1   1   1   1   1  40   1   1], 정답 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n(VAL) Batch 2 Loss : 0.6397421956062317, accuracy: 0.84375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  1   1   1  10  16   2   2   2  16  43   1   2   2   2  37   2   2 160\n  43  15  50   2   4   2  40   2 173   1   3  38  39   3], 정답 [1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n(VAL) Batch 3 Loss : 2.529486894607544, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 93 199   2   2   2 178   3 198 155   2  15   1   2   2  14  85   2 178\n  15  47   3   2   2   3   3  17   5 191  32  16   5  39], 정답 [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3]\n(VAL) Batch 4 Loss : 2.8436825275421143, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 39  44   4   1 178   3  43 122   5   3 192 174   3   1   6  61   6  39\n 104   2   7  10  89  17  44   3   5  90   3 191   2   3], 정답 [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n(VAL) Batch 5 Loss : 3.519378662109375, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  3   3 174 199   0   1  15  56 132  81   4 107   2   4 197   4  39  81\n  56   4  12  10   2  23   4 154  46  96  61   4  16   4], 정답 [3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n(VAL) Batch 6 Loss : 3.732609510421753, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 16  10   4   4  23   2 174 195 197   2  46   2  95   4  46   4 152  50\n 108  37  78  97   4   1  46   4   9  41 153  41  24  14], 정답 [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5]\n(VAL) Batch 7 Loss : 4.5101213455200195, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [136   5  10  84  12  30   1   6   3 181 107 125 111  46  32  15  25  11\n  35 118  99  32   3  35  81  25  27  68  10   2   2  39], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n(VAL) Batch 8 Loss : 5.275069236755371, accuracy: 0.03125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 99   7   7  19 196 103  41  18   5 105   5   2 181   6  85   4   6   6\n   6   6   6   6 105  14 190   6   6   6 195   6 105 195], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\n(VAL) Batch 9 Loss : 2.85115909576416, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  6  19   6   6   6  35  15   6   6   6   6   6  94   6   6  49  23   3\n  14   9   6   6 162  67  50 188   6   6  85   6  20   7], 정답 [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7]\n(VAL) Batch 10 Loss : 1.8937467336654663, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  7   7   7   7 181  16   7   9  16   6 172  10  13   7   3   8 175  41\n   7   7  14   9  38   7   7  10   7   5  94  10  88 144], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n(VAL) Batch 11 Loss : 3.9026169776916504, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  9   7 192  10   9  10  58   8  17  92   7  42  13  15   7   7   8  38\n  43   8  75  41   8   8   8  69   8  41   8   7 125   8], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n(VAL) Batch 12 Loss : 2.5239899158477783, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  8   8   8   8   8   1  55  37  13   8   8 111   9   8   8  37  41   8\n   8   8   8   7   8   8  37   8   8  95   8   9  37   8], 정답 [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n(VAL) Batch 13 Loss : 1.71216881275177, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  8   1   9  96   9   6   9   9   8   9 199   9   1 122 125 157   7 196\n   8   9   5  46  17  17  10  33   7  47  20   9   9   9], 정답 [8 8 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\n(VAL) Batch 14 Loss : 3.622366428375244, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [111  38  33  66   8 189   9 196   7   9   7  56  33   9 119   9   7  18\n  41  82  36  10   7 166  10  10  44   7  10  41 106   1], 정답 [ 9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9 10 10 10 10\n 10 10 10 10 10 10 10 10]\n(VAL) Batch 15 Loss : 3.952890634536743, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  7 104  10  10   6  20  10  36 190  41 117  41  37  40  10 179  41 152\n  38  52  10  10  44  10  16   7  38 159  37 196  88  10], 정답 [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n 10 10 10 10 10 10 10 10]\n(VAL) Batch 16 Loss : 3.726637840270996, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 41 175 101  10  10   7  11  11  87  48  11  20  11  51  43  72  14 165\n  11  22  57  11  21  91   5  11  71  20  32  20  11  11], 정답 [10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n 11 11 11 11 11 11 11 11]\n(VAL) Batch 17 Loss : 2.671782970428467, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 11  11  55  91  20  11  34  11 123  96  78  11 134  37  11  11  11  55\n  50 123  11  11 122  11  12  12  12  27  26  12  12  12], 정답 [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n 12 12 12 12 12 12 12 12]\n(VAL) Batch 18 Loss : 2.960540294647217, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 31  19 190  16  12  12 179  12  12  48  12 105  12  71  12  12  48  54\n  17  12  76  12  12  30  12 184  11  12  12  12  12  49], 정답 [12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n 12 12 12 12 12 12 12 12]\n(VAL) Batch 19 Loss : 2.3811583518981934, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 12  18  35  12  35  34   5  12 136  96  13  46  13  13  78  13  13  13\n  13 196 187 178  13  23  13  12  13  13  13  13  13 196], 정답 [12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13 13 13 13 13\n 13 13 13 13 13 13 13 13]\n(VAL) Batch 20 Loss : 2.4220149517059326, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 13  13  13  13  13  13   7  13  92  23  13 183  23  13  13  13  13  13\n  17 148  13  13  13  23  13  13  13  13  88  14  14  14], 정답 [13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13\n 13 13 13 13 14 14 14 14]\n(VAL) Batch 21 Loss : 1.2177757024765015, accuracy: 0.71875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 14  14  14 196  14   0 196  14 195  14  14 183  23  15  14 196  14   6\n  46  39 183 196  46  14  14  23 196  17  14  14  30  62], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n 14 14 14 14 14 14 14 14]\n(VAL) Batch 22 Loss : 2.5397284030914307, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 14   6 134  14  14  14  14 196  14 196 169  14 196  14  15 181 192  15\n 103  48  15   2  10  15  47 193   7  15 122 185   6  81], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15\n 15 15 15 15 15 15 15 15]\n(VAL) Batch 23 Loss : 3.3233585357666016, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 41   1 181  15  15  17  10  29 199  35  23  17   1 168  15   2  55  15\n  60  15  15  15  15   7  16   1  37 105 186  17 183  37], 정답 [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n 15 15 15 15 15 15 15 15]\n(VAL) Batch 24 Loss : 4.303423881530762, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 16  16  45 122  41  16  44 102  16  17  93  92  85  16   2  16  16  20\n  54  56  17  36 185  39 192  36  10 179  83  94  16 180], 정답 [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n 16 16 16 16 16 16 16 16]\n(VAL) Batch 25 Loss : 4.472617149353027, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 17  16 185 174  16 198   6 190   5 156  15  50  27 185  55 185   3  13\n  17  17  17  19  46   6  17 126  17  17  58  19  17  17], 정답 [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 17 17 17 17 17 17\n 17 17 17 17 17 17 17 17]\n(VAL) Batch 26 Loss : 3.3624699115753174, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 15  17  17  17 182 176  17 105 190 187  17  17  17  19 174  17 110 192\n 190  17 189  17  71  17 189 196  14 185  17  17  17  17], 정답 [17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17\n 17 17 17 17 17 17 17 17]\n(VAL) Batch 27 Loss : 2.3760621547698975, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 17  17 185  17 189 182  41 190  77  18 153 163 143 174 191  18 181  18\n  18  18  18 189  10  18 130  18  18  82  18 189  77 191], 정답 [17 17 17 17 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18\n 18 18 18 18 18 18 18 18]\n(VAL) Batch 28 Loss : 3.364008903503418, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 18  18   7  18  46 181 189  18  18  84 176 168 186  18 180  18 181 143\n 168  18  59 150  19  91 196  19   5  26  19  19  27  19], 정답 [18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 19 19\n 19 19 19 19 19 19 19 19]\n(VAL) Batch 29 Loss : 2.8435399532318115, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  9  19 191 196   1 190  19  19  19 191  46  19  19  19   7  19  41  19\n  19  41  19 195  19 191  19  19 108 191  19  13  41 196], 정답 [19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19\n 19 19 19 19 19 19 19 19]\n(VAL) Batch 30 Loss : 3.224008560180664, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 19  18  19  19  19  18  19 196 183  20  20  20  20  20  11  20  20  20\n  20  55  11  22  43  11  46  84  55  20 152 168  20   9], 정답 [19 19 19 19 19 19 19 19 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20\n 20 20 20 20 20 20 20 20]\n(VAL) Batch 31 Loss : 2.7485289573669434, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 54 114 101 139 174  37  20  20  56  20  20  27 122  55  20  20  11  10\n  16  34   8  20  20  20 122  16  21  35  21  21  55  21], 정답 [20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20\n 20 20 21 21 21 21 21 21]\n(VAL) Batch 32 Loss : 2.196753978729248, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  7 128  21 198 148  21 164  21  60  21  21  21  21  21 105  21 104  23\n  60  21 100  21  41  21  21  21  21  21  21  21  84  21], 정답 [21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21\n 21 21 21 21 21 21 21 21]\n(VAL) Batch 33 Loss : 2.4341282844543457, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 20  21  20  21  31  29  21  21  21  21  67  21  12  22  21  55 137  22\n  22  22  11  95  11  22  22  22  22  22  20  22  22  22], 정답 [21 21 21 21 21 21 21 21 21 21 21 21 22 22 22 22 22 22 22 22 22 22 22 22\n 22 22 22 22 22 22 22 22]\n(VAL) Batch 34 Loss : 2.310434341430664, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 22 103  22  22  22  75 101  21  22 150  22  22  22  22 137  22  95  22\n  22  22  22  22  56 188  11  11  22  22  22  22  23  23], 정답 [22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22\n 22 22 22 22 22 22 23 23]\n(VAL) Batch 35 Loss : 2.386512279510498, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 23  23  23  23  23  23  23  23  23  23  23  23  23 148  23  23  23  23\n  23  23  13  23  23  23  23  23  23  23  55  23  23 185], 정답 [23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n 23 23 23 23 23 23 23 23]\n(VAL) Batch 36 Loss : 0.40638622641563416, accuracy: 0.875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 23  23  23  13  23  23  23  23  23  23  23 196  23  23  23  23  56  24\n  24 125  24 161  55 175  31 134  31  75 156 126 180 123], 정답 [23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 24 24 24 24 24 24 24 24\n 24 24 24 24 24 24 24 24]\n(VAL) Batch 37 Loss : 2.872401237487793, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 47  10  18  31 119 161  24  47  27  90  24  26 175  13  11  31  26  24\n  60  28  69 172  24  28  25  20 127  83  29  48  48  18], 정답 [24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24\n 24 24 24 24 24 24 24 24]\n(VAL) Batch 38 Loss : 5.283474922180176, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 11 195  25  75  25  25 191  25  25  48  25  25  26  25  25 105  25 105\n  25  25  25 192  25  25  25  28  25  25  69  34  20  25], 정답 [24 24 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25\n 25 25 25 25 25 25 25 25]\n(VAL) Batch 39 Loss : 2.0132555961608887, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 48  47  25  25  25 150  25  25   7  25  25  90  24  25  25  25  25  53\n  25 139  26  34  47  27  34  26  26 149  26  69  34 193], 정답 [25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 26 26 26 26\n 26 26 26 26 26 26 26 26]\n(VAL) Batch 40 Loss : 2.168640613555908, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 19  27  33  41  53  50  53  31  26  25  26  12  26  26  24 148  35  33\n  26  31  27  12  26  12  26  34 181  17  74  11  34 110], 정답 [26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26\n 26 26 26 26 26 26 26 26]\n(VAL) Batch 41 Loss : 3.971001386642456, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 26  55  33  26  28  28  27  51  26 123  24  27  50  27  68  28 141 106\n  27 134 181  25  55  24  75  20  24  26  27  27  27 141], 정답 [26 26 26 26 26 26 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27\n 27 27 27 27 27 27 27 27]\n(VAL) Batch 42 Loss : 3.4120402336120605, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 11  28  29 135  20  24  56 164  33  55  22 119 172  57 155  54 168  26\n  77  25  26  27  26  27  27  54 116  98  25  24  71  28], 정답 [27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27\n 28 28 28 28 28 28 28 28]\n(VAL) Batch 43 Loss : 4.693600654602051, accuracy: 0.09375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 27  25 105 181  20 143  28  28  60  28 111  28  28  28 118  53  28  28\n  25 107 143  28  28 100  25  28  59  79  28 162   7  28], 정답 [28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28\n 28 28 28 28 28 28 28 28]\n(VAL) Batch 44 Loss : 4.726308822631836, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 28  28  54  28  28  28  25  28  36  33  29 114   1  27  50  55  24  27\n  20  47   3 107  69  54  84  77  26  47 144  27  50  58], 정답 [28 28 28 28 28 28 28 28 28 28 29 29 29 29 29 29 29 29 29 29 29 29 29 29\n 29 29 29 29 29 29 29 29]\n(VAL) Batch 45 Loss : 4.186412334442139, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 11  48 180  55  41  28 105 111  33  29  26  22  29  25  27  48  54  32\n  31  34  54 118  29  27  27 152  34  29  32  32  35  32], 정답 [29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29\n 29 29 29 29 30 30 30 30]\n(VAL) Batch 46 Loss : 4.85446310043335, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 32   5  76  30  25 106  32  26  98  32   7  30  29  30  30 189  32  31\n  32 193  35  32 150  31 150  10 105  31  32  32  30 103], 정답 [30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30\n 30 30 30 30 30 30 30 30]\n(VAL) Batch 47 Loss : 3.3283302783966064, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 32 142   5  54  30  32 193  41 162   3  32  25  25  71 141  31  32  30\n  31  15  98  31 105  31  31 144  26  31  31  52  32   9], 정답 [30 30 30 30 30 30 30 30 30 30 30 30 30 30 31 31 31 31 31 31 31 31 31 31\n 31 31 31 31 31 31 31 31]\n(VAL) Batch 48 Loss : 3.9375693798065186, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 31  29  25  31  31  35  31  31  17  37  31 137  31  30  32  31  31  31\n  31  31  13  31  47  31 174  32  31  24 174  31   5  29], 정답 [31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31\n 31 31 31 31 31 31 31 31]\n(VAL) Batch 49 Loss : 2.853905439376831, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 32  32 150  46  78 172  32  55 147  89  32  56  27  30 161  32  32  37\n   4  32  32   4 165  48 182  91  25 150  84  34  17  39], 정답 [32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32\n 32 32 32 32 32 32 32 32]\n(VAL) Batch 50 Loss : 4.007228851318359, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [105  56  62  32  35 125  92  79 181 136  30  73  32  22  55  32 160  24\n  99  34  33  48  53  33  33  34  33  34  34 125  19  32], 정답 [32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 33 33 33 33 33 33\n 33 33 33 33 33 33 33 33]\n(VAL) Batch 51 Loss : 3.690352439880371, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 12  33  51  91  34  12  91  34  10  56 161 122  55  33  91  33  35  33\n  34  31  56  48  34  49  33  32  91  33 195  46  34 195], 정답 [33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33\n 33 33 33 33 33 33 33 33]\n(VAL) Batch 52 Loss : 3.9442031383514404, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 56  91  33  53  34  34  31  34  51  34 162  34  34  23  33 196  26  50\n  34  91 181  34  34  34  34  34  34  34  49  34  28  25], 정답 [33 33 33 33 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34\n 34 34 34 34 34 34 34 34]\n(VAL) Batch 53 Loss : 2.618089437484741, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 34  34  34  34  34  34  35  34  34 136  34  34  34 102  35  34 192   3\n  41  34  26  34  35  35  47 146  15  35  35  35  35 169], 정답 [34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 35 35\n 35 35 35 35 35 35 35 35]\n(VAL) Batch 54 Loss : 2.049285650253296, accuracy: 0.625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 55 185  35  35  14  35  35  35  35  35  35  50  35  84  51  35  35  35\n  35  35  57  35  35  35  35 162  35  35  35   7  34  25], 정답 [35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35\n 35 35 35 35 35 35 35 35]\n(VAL) Batch 55 Loss : 1.7582447528839111, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 50  35  35  35  25  35  54  35  36  36  36  36  36  36  37  44  36 189\n 188  38  36  39  36 122 199  36  41  37  36  36  36  36], 정답 [35 35 35 35 35 35 35 35 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n 36 36 36 36 36 36 36 36]\n(VAL) Batch 56 Loss : 1.6834633350372314, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  8  36  36  37  77  36  36 194  36  36  36  36 145  38  36 135  36  36\n  36  36  31  36  36  36  36  36 139  10   8   1  37  24], 정답 [36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n 36 36 37 37 37 37 37 37]\n(VAL) Batch 57 Loss : 2.9135851860046387, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 42 122  37  37   9  37 185  37  37  38  37  37  16  38  37 197  37  90\n  37  37  41 184   8   4  38  45  66  38  37  37  38  37], 정답 [37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37\n 37 37 37 37 37 37 37 37]\n(VAL) Batch 58 Loss : 2.353278398513794, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 37   8  41  37  37  37  37  37  37  26  37  20 183  36  38  38  38  38\n  38  41  38 181  38 158  38  38  38  38  38  38  37  38], 정답 [37 37 37 37 37 37 37 37 37 37 37 37 38 38 38 38 38 38 38 38 38 38 38 38\n 38 38 38 38 38 38 38 38]\n(VAL) Batch 59 Loss : 1.3601704835891724, accuracy: 0.6875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [178  36  36 122  38  38  43  38  38  38  44  38   7  38  47 189  38  38\n  38  38  68  38 187  38  38 189  39  44  45  64  37  37], 정답 [38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38\n 38 38 38 38 38 38 39 39]\n(VAL) Batch 60 Loss : 2.771660327911377, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 43 149   1  82  39  42 116  39  37  42  38  39 178  39 106  36  39 188\n   3  40  40  36 100  39  40  39  39  42  40  42  39  39], 정답 [39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39\n 39 39 39 39 39 39 39 39]\n(VAL) Batch 61 Loss : 3.457930326461792, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 42  42  39  42  39  40  39  39  42  37  43 198 184  39  42   8  43 135\n  41  45  40 156 125  42  39  40 137  23  87  40  39  40], 정답 [39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 40 40 40 40 40 40 40 40\n 40 40 40 40 40 40 40 40]\n(VAL) Batch 62 Loss : 3.711773157119751, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [109 122 199  82 179  40 159  40  64   5  42 159 186  40  43  42  40  39\n  40  42   8 117  10  40  40  40  39 184 199  54 109 109], 정답 [40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n 40 40 40 40 40 40 40 40]\n(VAL) Batch 63 Loss : 3.6914241313934326, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [118 185  41  36   8  41   8 101  41  41   1  87 162  41 177  41  95  41\n  15  85  41  41  37   8 182  36  41  41 148   1  41  41], 정답 [40 40 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n 41 41 41 41 41 41 41 41]\n(VAL) Batch 64 Loss : 3.3358330726623535, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 35  37 122  55  37  10  87  44 198  37  37  89 144  41   9  41  10  17\n  43   9  42  42  43  41  10  75  15  40  37 185  38  42], 정답 [41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 42 42 42 42\n 42 42 42 42 42 42 42 42]\n(VAL) Batch 65 Loss : 4.624843120574951, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 42 188  42 104  42  39 144  42  55  42  42  42  42 199  16 130  42  42\n   2  42 160  42  42  42  40  87  22  42  42  42  42  37], 정답 [42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42\n 42 42 42 42 42 42 42 42]\n(VAL) Batch 66 Loss : 2.5742671489715576, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 42  42   8  42  42  42  43   1  43  39  12  39 125  43  43  43  43  43\n  41  44  42  89  43  43  42  43   0 178  40 127  43  15], 정답 [42 42 42 42 42 42 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43\n 43 43 43 43 43 43 43 43]\n(VAL) Batch 67 Loss : 2.921304225921631, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 43   3  40  43 184  43 184  41  43  44  37  43 187  36  77 125  43 188\n 187  43  40  43  39  41  44  44  44 152  44  44  44  44], 정답 [43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43\n 44 44 44 44 44 44 44 44]\n(VAL) Batch 68 Loss : 3.019519329071045, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 44  44  44  44  44  44  44  44  44  44  44  44  44  44  44  44  44  44\n  44  44  37 199  44  44  44  44  39 199  44  44  44  44], 정답 [44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44\n 44 44 44 44 44 44 44 44]\n(VAL) Batch 69 Loss : 0.5939478874206543, accuracy: 0.875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 44  44  13  44  44  44 101  44  44   1 182  45 188  45  45  45  45 188\n  45  45  45  45  45  45  45  45  45  45  45  45   7  45], 정답 [44 44 44 44 44 44 44 44 44 44 45 45 45 45 45 45 45 45 45 45 45 45 45 45\n 45 45 45 45 45 45 45 45]\n(VAL) Batch 70 Loss : 1.2037488222122192, accuracy: 0.78125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [179  45  45  45  45  36  45  45 187  45  45  45  45 176 168  42  38  45\n  45 188  45  45  45  45  45  45  45  45  14 148 196 196], 정답 [45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45\n 45 45 45 45 46 46 46 46]\n(VAL) Batch 71 Loss : 1.5729036331176758, accuracy: 0.625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 46  18   2  46  46  46  46  46  46 199 194  46  46 196  17  32  46 152\n  13  46  46  46  46  14  16  48  46  46 196  46  46  16], 정답 [46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46 46\n 46 46 46 46 46 46 46 46]\n(VAL) Batch 72 Loss : 2.2253475189208984, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [196  46  46  46  88  14   7  46 158  14  46  46   3  46  31 174 129  58\n  47  31  47 185  47  47  47  47  47  48  47  58  51  47], 정답 [46 46 46 46 46 46 46 46 46 46 46 46 46 46 47 47 47 47 47 47 47 47 47 47\n 47 47 47 47 47 47 47 47]\n(VAL) Batch 73 Loss : 2.2890021800994873, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 47  41  48 180  47  30 186  47 190  47  47  47   7  31 104 181  31  24\n 177  47  55 179  47 190 161  47  47 126  24  17 130 192], 정답 [47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47\n 47 47 47 47 47 47 47 47]\n(VAL) Batch 74 Loss : 3.359988212585449, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 48 114  48  35  48  55 142 181  35  49 162  50  30  55  55  82 114  82\n 119  35 161  11  48  50  29  35 195  50  48  48 176  23], 정답 [48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48\n 48 48 48 48 48 48 48 48]\n(VAL) Batch 75 Loss : 4.380188465118408, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [104  48 100  48  94  48  55  48  48  32   4  50 133  48  91  91 162  48\n  76  57  66  55  50  49 164  49  96 171  22 120  53 159], 정답 [48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 49 49 49 49 49 49\n 49 49 49 49 49 49 49 49]\n(VAL) Batch 76 Loss : 4.09075403213501, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 49  49  49  55  50  49  55  84  35  53  49  12  49 112  54  32  50 170\n  27  49  55  34 111 105 194 196  27  52 145  55  50  49], 정답 [49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49\n 49 49 49 49 49 49 49 49]\n(VAL) Batch 77 Loss : 3.315587282180786, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 27  51  65  49  50  50  50  50  50  51 196 105  50  35  50  49  50  50\n  50  50 138  50  50   6  35  49  50  54  50  50  50  50], 정답 [49 49 49 49 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n 50 50 50 50 50 50 50 50]\n(VAL) Batch 78 Loss : 1.5684940814971924, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 49  50 195  50  50  50  50  50  50  50  50  50 170  50  20  50  50  50\n  50  91  50  50  51  33  48  53  34  51  33  51 196  53], 정답 [50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 51 51\n 51 51 51 51 51 51 51 51]\n(VAL) Batch 79 Loss : 1.5377039909362793, accuracy: 0.625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 12  48  27 195  51  28   4   4  51  51 195  58  51  35  53  34  51  51\n 197  56  51 105  50  51  51  20  46  15  16  33  20  34], 정답 [51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51\n 51 51 51 51 51 51 51 51]\n(VAL) Batch 80 Loss : 3.896559953689575, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [194  20  50  19  51  51  50  91 136  84  52 192  52 193 122  46  28  52\n  52  52  53  52 105  52  52  52  52  52  69 195  52  51], 정답 [51 51 51 51 51 51 51 51 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52\n 52 52 52 52 52 52 52 52]\n(VAL) Batch 81 Loss : 2.6207432746887207, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 52 114  52  34  52  52 105  52  52  52  52  82  52  33 198  52  42  95\n   7  52 185  52  52  91  49  52  34 194  52  68  57  53], 정답 [52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52\n 52 52 53 53 53 53 53 53]\n(VAL) Batch 82 Loss : 2.9496867656707764, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 53 194  53  53  55 193 143  53 158  53 164  51  71  48  53  53  66  52\n  53  34  57  26 143 163  53   7  53  53  53  57  99  53], 정답 [53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53\n 53 53 53 53 53 53 53 53]\n(VAL) Batch 83 Loss : 3.3348376750946045, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 26  53  53 107  53  53  50  53 105  35  53  53  36  54  54 169  54  57\n  54  25  54 162  55  46  54  54  54  54 126  28  54  54], 정답 [53 53 53 53 53 53 53 53 53 53 53 53 54 54 54 54 54 54 54 54 54 54 54 54\n 54 54 54 54 54 54 54 54]\n(VAL) Batch 84 Loss : 2.4829163551330566, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 54  54  47  23 171  55   2 199  75  54 141  58  54  54  55 120  54  55\n  75  56  81  25  48  54  28  50  55  54  54  54  55  55], 정답 [54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54\n 54 54 54 54 54 54 55 55]\n(VAL) Batch 85 Loss : 3.7677009105682373, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 55  55  55  55 114  55 155  55  55  55  55 109  33  55  55  55  55  55\n  63  55  58  55  55  55  55   2  48  55 138  55  12 105], 정답 [55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55\n 55 55 55 55 55 55 55 55]\n(VAL) Batch 86 Loss : 1.5880982875823975, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 55 146  55  55  27  97  55  50  55  29  28  55 121  55  55  55  34  55\n  55  56 198  48  49  55  56  55  56  56  16  46 196  56], 정답 [55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 56 56 56 56 56 56 56 56\n 56 56 56 56 56 56 56 56]\n(VAL) Batch 87 Loss : 3.0957953929901123, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 55  12  25  55  23  17  56  14  34  48  50  55  55 157  56  48  48  55\n  17  12  57  55  15 124  56  56  56  91  47  51  56  34], 정답 [56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56\n 56 56 56 56 56 56 56 56]\n(VAL) Batch 88 Loss : 4.107612609863281, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 55  56 119  48  35  50  35  49 170   6  49  57  57  53 105   9  57 108\n  57 162  57  57  49  61 169  57  57  57  57  57  57  57], 정답 [56 56 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57\n 57 57 57 57 57 57 57 57]\n(VAL) Batch 89 Loss : 3.255528450012207, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 48 162  57  57  57  57  57  57  57 129  57  49  58  57  57  57  35  49\n  71  51 111  58  58  35  58  58  58  67  58 196  26  58], 정답 [57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 58 58 58 58\n 58 58 58 58 58 58 58 58]\n(VAL) Batch 90 Loss : 2.291977643966675, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 58 111  56  58  58  58  58  24  58  58  58  58  55  50  35 142  54  58\n  58  55  50  58 174  58  58  58  47  58  25  58  58  58], 정답 [58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58\n 58 58 58 58 58 58 58 58]\n(VAL) Batch 91 Loss : 1.9128776788711548, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 58 191  58  58  58  58 146  59  59 106 193  59 145 148  59 130  88 159\n  41  59   0  59 181  59 188 116 159  59  81  59 146 104], 정답 [58 58 58 58 58 58 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59\n 59 59 59 59 59 59 59 59]\n(VAL) Batch 92 Loss : 3.160357713699341, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [171  59  98 191 156  93 135 145 147 126 167  59  82  59  59  59  71  59\n  59 111  70 123  59  93 138  60  60 150  60  60  60 168], 정답 [59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59\n 60 60 60 60 60 60 60 60]\n(VAL) Batch 93 Loss : 3.262444019317627, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [139  60  60 170 143  54  60  60  60  60  60  77  60  60  60 143 105  60\n  60  60 155  60  60  60  60  60 106  60  77  60  60 105], 정답 [60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60\n 60 60 60 60 60 60 60 60]\n(VAL) Batch 94 Loss : 2.1504034996032715, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 63  65  60  60  60  99  60  60  18  60  61 152 126  61  97  61  61  59\n  62  61  61  61  61 126  61 126 103  61  61  93  61 162], 정답 [60 60 60 60 60 60 60 60 60 60 61 61 61 61 61 61 61 61 61 61 61 61 61 61\n 61 61 61 61 61 61 61 61]\n(VAL) Batch 95 Loss : 1.8541195392608643, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [111  61  61 108 176  61 126 146 126  61  61  61 126  61  61  67  61  68\n 146  61  61 191  34  61  61  61  61  61  90  17  62 176], 정답 [61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61\n 61 61 61 61 62 62 62 62]\n(VAL) Batch 96 Loss : 2.577364444732666, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [134  92 149 181  90 134  64 120 130  62 149 113 155  62 134 112  89  61\n 146  72 146  90  90 151  62 189  62  62 134 156 134  90], 정답 [62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62\n 62 62 62 62 62 62 62 62]\n(VAL) Batch 97 Loss : 3.8089852333068848, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 62  62 134 158  77 149 131 142 190  62  44  85  62 151 139 100  86 160\n  63  99  63  90 145  63 120 138 189 107  63  87  92  90], 정답 [62 62 62 62 62 62 62 62 62 62 62 62 62 62 63 63 63 63 63 63 63 63 63 63\n 63 63 63 63 63 63 63 63]\n(VAL) Batch 98 Loss : 3.9341845512390137, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 63  63  90  63  63  21 179 169  41  92  60 154 155  63  63 138 101  60\n 127  63  63  87 103 154  94 143 105   9 133 173 181 165], 정답 [63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63 63\n 63 63 63 63 63 63 63 63]\n(VAL) Batch 99 Loss : 3.7176151275634766, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [180 142  40 154 105 151 169 160 151  64 194 117  64  84  81  86  72  64\n  27 137 104  64   6 157  26 163 137  64  64 136  96 151], 정답 [64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n 64 64 64 64 64 64 64 64]\n(VAL) Batch 100 Loss : 4.595167636871338, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 42 127  10  36 160  87 111 124   8 126   9 113  77  82  98  61  19 113\n 146 165  65  82  82 120 146  12  65 193 110  30 105  65], 정답 [64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 65 65 65 65 65 65\n 65 65 65 65 65 65 65 65]\n(VAL) Batch 101 Loss : 5.826810836791992, accuracy: 0.09375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 65 108  59  63 108 128 146 146  87 139 108  65  65 104  86  74  18  84\n 140 146  65  60  98 145 136 135 112 127 136 102  65  65], 정답 [65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65\n 65 65 65 65 65 65 65 65]\n(VAL) Batch 102 Loss : 4.28725004196167, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [113 159 146  55 170  50  66  66  75  48  69 196 164  66 154 170 102  66\n 164  66 103  66  66  92  66   9 170  66  50  66  66  66], 정답 [65 65 65 65 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66\n 66 66 66 66 66 66 66 66]\n(VAL) Batch 103 Loss : 3.6576826572418213, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 66 154 129  50 170 103 175  66 128  66 124  66  87 165  71 170  66  66\n  66 164  66 170  67  85  61 193  67 127  67  75 117 160], 정답 [66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 66 67 67\n 67 67 67 67 67 67 67 67]\n(VAL) Batch 104 Loss : 4.0306830406188965, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [135 120  85  67 172  59 112  80  67  30  81 153  74  67 167 190  67 109\n  89 108 160 134 140  82  65  67  48  63  87 153 167  14], 정답 [67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67 67\n 67 67 67 67 67 67 67 67]\n(VAL) Batch 105 Loss : 5.613801002502441, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 15  78 160  67 130  67 172 145  68  68 145  68 159  68  68  68  68  68\n 171 171  68 171  68  68  68 146  68  68 120  68  32 189], 정답 [67 67 67 67 67 67 67 67 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68\n 68 68 68 68 68 68 68 68]\n(VAL) Batch 106 Loss : 3.2502377033233643, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 60  77 146  61 171  68 112 100  77  68  68  69  68 171  68  68 171 119\n  68  68  68 193  68  68 127  68 193  69  69  55  69  69], 정답 [68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68\n 68 68 69 69 69 69 69 69]\n(VAL) Batch 107 Loss : 2.398104667663574, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 69  69 120  83  26 116  90 156  69  69  13  69 147  69  20  76 110  11\n  69 148 158  97  69  86  94 119  18 125  69  69  53  99], 정답 [69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69 69\n 69 69 69 69 69 69 69 69]\n(VAL) Batch 108 Loss : 4.201005458831787, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [159  69 145  94 174 169  69  69  71 169  23  88  94 194  70 111  70 133\n  70 133  70  70  84  94  94  94  70 160 153 133 133  94], 정답 [69 69 69 69 69 69 69 69 69 69 69 69 70 70 70 70 70 70 70 70 70 70 70 70\n 70 70 70 70 70 70 70 70]\n(VAL) Batch 109 Loss : 3.9004456996917725, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 94 116  70  70  94  94  70  70 133  70  70  70 153 133  94  94  71 133\n 141  94 133  87  81 133  71  82  87  70  94 121  71  71], 정답 [70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70\n 70 70 70 70 70 70 71 71]\n(VAL) Batch 110 Loss : 3.7697157859802246, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 71 101 138  71  71  71  71 173  71  71  71  71  71  71 132 154  71  71\n  71 197  71  71 101  71  71  71  71  71 173  71  71  71], 정답 [71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71\n 71 71 71 71 71 71 71 71]\n(VAL) Batch 111 Loss : 0.7606103420257568, accuracy: 0.75\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 71  71 197  91  71  71  71 124  71 198  71  71  71  34  71  71  72  13\n  83 183  72  64  72  23 100 189  72 125 136 130  68 135], 정답 [71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71 72 72 72 72 72 72 72 72\n 72 72 72 72 72 72 72 72]\n(VAL) Batch 112 Loss : 3.4311468601226807, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 86  90 180 141 149 135  72  72  72  72  72 128 117  72  72 155  54 141\n 167 193 171  31  90 108 101  86 186 130 110  72  69  72], 정답 [72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72\n 72 72 72 72 72 72 72 72]\n(VAL) Batch 113 Loss : 4.416441917419434, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 72  90 135 131  73  13  13  21  73 110 135 135 112 135  73  90  73 135\n  73  89  73 132 158 184  59 138 169  73 135 159  59 135], 정답 [72 72 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73\n 73 73 73 73 73 73 73 73]\n(VAL) Batch 114 Loss : 3.946528673171997, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 73 128 180  62  13 120 153  73  73  73   5 163 122 112  90  65 146  65\n  32  73  74  74 158 158  47 158 106  74 158 156 182  74], 정답 [73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 73 74 74 74 74\n 74 74 74 74 74 74 74 74]\n(VAL) Batch 115 Loss : 4.405106544494629, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 74  74 158 171 180 148 156  74  69 105  74  74 158 158 156  74 158 158\n  99 144 158 181  74  68 179  74  68  74  74 158 102  99], 정답 [74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74\n 74 74 74 74 74 74 74 74]\n(VAL) Batch 116 Loss : 2.865929126739502, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 74  74  80 181  69 123  75  77  21 144 105  75  67  75 189 124 173  75\n 139 180 121   1 106 170 156 120 153 133  75  71  75  75], 정답 [74 74 74 74 74 74 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75\n 75 75 75 75 75 75 75 75]\n(VAL) Batch 117 Loss : 4.093091011047363, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [177  47  16 197  70  73 158  81  75   2 155   5 106  24  98 105  75  64\n 101  71  55  75 164  75 178   6  57 170 114 127 106 155], 정답 [75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75\n 76 76 76 76 76 76 76 76]\n(VAL) Batch 118 Loss : 4.398593425750732, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 76  88  91  76 134 110  76  67   3  20  53 146 172  66  76 150 127 146\n  17 129  76  16 108  76 196  95 127  55 162 106 199 103], 정답 [76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76 76\n 76 76 76 76 76 76 76 76]\n(VAL) Batch 119 Loss : 4.365123271942139, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [165  54 137  76 122 195  61  75  76  71 179  77 167  60 169 106 123 112\n 123  25 125 138  77  72 135 156 116 123  55  77 102 118], 정답 [76 76 76 76 76 76 76 76 76 76 77 77 77 77 77 77 77 77 77 77 77 77 77 77\n 77 77 77 77 77 77 77 77]\n(VAL) Batch 120 Loss : 4.335616588592529, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [113 132  48  41  25  60 121  77  77 123 116  93 164  77 151  21  77  83\n 126 151 146 120 104 158 181 101 144  99  78  12  78  78], 정답 [77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77\n 77 77 77 77 78 78 78 78]\n(VAL) Batch 121 Loss : 4.655580520629883, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 78  78 195  78  78 176  63  78 146  78  78 117  78  78 164  78  78  78\n  78   6  78  78  78  86  76  78  78  78 146  78  78  78], 정답 [78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78\n 78 78 78 78 78 78 78 78]\n(VAL) Batch 122 Loss : 2.0130391120910645, accuracy: 0.6875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 78  65 155  85  78  78  78  86  78  78  47  78  78  78 132 104 138 135\n 155 107 109 168  24 120 127  10  37 160  79 108 109   7], 정답 [78 78 78 78 78 78 78 78 78 78 78 78 78 78 79 79 79 79 79 79 79 79 79 79\n 79 79 79 79 79 79 79 79]\n(VAL) Batch 123 Loss : 4.200724124908447, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [141 100 151   0  79  33  79 124 110 113 157  82   6 160  79  46 111 122\n  79 162  40  79 157 129 144 189 132 100  79 113 169  79], 정답 [79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79\n 79 79 79 79 79 79 79 79]\n(VAL) Batch 124 Loss : 5.137033462524414, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [164 148 128 160  72  31  96  94 122  80  80  58 160 100  82  80  75 188\n 187 172 177 128  11 136 158  80 104 157 116  92 112 194], 정답 [80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80\n 80 80 80 80 80 80 80 80]\n(VAL) Batch 125 Loss : 5.115034580230713, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 78  25 184 140 122 146  80  80  36  69  72  72  55 172 130  55 160  25\n  81  98  81  81 133 156  81  81  81 110  81  81  81  84], 정답 [80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 81 81 81 81 81 81\n 81 81 81 81 81 81 81 81]\n(VAL) Batch 126 Loss : 3.992351531982422, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 81 194  81  81  81  81  67  81  81  81  81  81  81 185 133  81  81  96\n  81  81  81 121  81  81  81 133 134 123  81  81  81  81], 정답 [81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81\n 81 81 81 81 81 81 81 81]\n(VAL) Batch 127 Loss : 1.9495104551315308, accuracy: 0.71875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 81  81  81 142 189 190  82  82  82  82  82  82  99  82  82  82  82  82\n 108 191 181  82  82  89  61 146  99 142  82  82  82  82], 정답 [81 81 81 81 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82\n 82 82 82 82 82 82 82 82]\n(VAL) Batch 128 Loss : 2.2536656856536865, accuracy: 0.625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 82 146 190  82  82  82  68 191 181  82 120  82  82  83  61 146  82  82\n  19  82 192  82  18 189  18  27 189  83  72 172 113  90], 정답 [82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 83 83\n 83 83 83 83 83 83 83 83]\n(VAL) Batch 129 Loss : 3.251361608505249, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 73 100  88  93  18  83 109 112  62  83 193 189 159 180 123  77  83  83\n 186 123 123  83 104  99  83  83 161  83  79 160 182 163], 정답 [83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83\n 83 83 83 83 83 83 83 83]\n(VAL) Batch 130 Loss : 4.055525779724121, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 83  97  83 123   0  83  73 188 147  84   7 110 101 145  78  84 196  75\n 194  27 194   4  84  84  84 124  84  95 153  81 195 117], 정답 [83 83 83 83 83 83 83 83 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84\n 84 84 84 84 84 84 84 84]\n(VAL) Batch 131 Loss : 4.04334020614624, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [111  84  84  84  49 111 108 124  81 154  84  25 119   5 137  32  84 137\n  55  94  84  84 148  25  84 144  82  55  14 105  85 123], 정답 [84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84\n 84 84 85 85 85 85 85 85]\n(VAL) Batch 132 Loss : 3.706092596054077, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 85 105  85 108 131  85  85  21  63  90   6  85  85 123 134  12 134  85\n 134  62 119 169 186 105 134  85  85  67 105 134  90  85], 정답 [85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85\n 85 85 85 85 85 85 85 85]\n(VAL) Batch 133 Loss : 2.924055576324463, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 56 119  85  85 149  85 120  63  85  85  85   9 146  86 110 100  87 108\n  62  86  86 126  86  86  86  86  86  86 106 174  86 146], 정답 [85 85 85 85 85 85 85 85 85 85 85 85 86 86 86 86 86 86 86 86 86 86 86 86\n 86 86 86 86 86 86 86 86]\n(VAL) Batch 134 Loss : 3.2748682498931885, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 87  99 128  80  86  86 128  86 167  86  60  86 128  73  86  86  72  86\n 128 108  86  86  86 128  86 110 138 128  86  86  97  89], 정답 [86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86\n 86 86 86 86 86 86 87 87]\n(VAL) Batch 135 Loss : 2.2758853435516357, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 87 122  87  85  87 121 126  87  87 127  90  92  87  75  87 136 121  87\n  68  87  87 155  87  75 133  87  87 133  97  47  70 174], 정답 [87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87\n 87 87 87 87 87 87 87 87]\n(VAL) Batch 136 Loss : 2.805229902267456, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [150 171  87  87  71  89 136  87  92  63 109  87  87 172 147  87  42 148\n 168  78  88 156 174  21 143 122 104   1  47  59  88 173], 정답 [87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 87 88 88 88 88 88 88 88 88\n 88 88 88 88 88 88 88 88]\n(VAL) Batch 137 Loss : 4.230700969696045, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 40  10  10  88 105 161 179 153  43  14 152 181 156  95 153 109 141  43\n  72 107  92  88  88 127  40 162  54 125  95  22  88 197], 정답 [88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88 88\n 88 88 88 88 88 88 88 88]\n(VAL) Batch 138 Loss : 4.795792102813721, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [156  16  89  89 178  89  89  89  98  63  89 181 151 145  78 119  93 125\n  13 193  61  89 190  89 140 110  89  83  64  89 162  34], 정답 [88 88 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89\n 89 89 89 89 89 89 89 89]\n(VAL) Batch 139 Loss : 4.456204891204834, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 89 137  70  89  89  71 167 193  89  89  80  10   4  89  86  89  89  89\n  63  61 100 184  90  90 189  90  62  90  90  90  90  90], 정답 [89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 89 90 90 90 90\n 90 90 90 90 90 90 90 90]\n(VAL) Batch 140 Loss : 2.591701030731201, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [144  90  90  90  90 134  73 130  90 161 105  90 149 149  90  18  78  90\n 134  90  90  90  62 135 117  90 169 161  90 168  90  59], 정답 [90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90\n 90 90 90 90 90 90 90 90]\n(VAL) Batch 141 Loss : 2.412228584289551, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [131  90 191 149  60 149 174  91  91  91   8  91  91  91  91  91  30 162\n  70  96  91  91  91 195  57  91  91  46 123  91  59  91], 정답 [90 90 90 90 90 90 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91\n 91 91 91 91 91 91 91 91]\n(VAL) Batch 142 Loss : 2.6432137489318848, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 61  91  91  91 162  91  91  91  91 106  91  91  91   6  56  17  91 142\n  56  91  78 169   7  91 174  99  87 156 181   9 106  97], 정답 [91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91\n 92 92 92 92 92 92 92 92]\n(VAL) Batch 143 Loss : 3.165449857711792, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [126 145  92  92 110  59  87 184  67 171  71 134  72 147 130  86 188 126\n 104  89  40 146  92  92  92  58  89  31 198  86 149 188], 정답 [92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92 92\n 92 92 92 92 92 92 92 92]\n(VAL) Batch 144 Loss : 3.9747931957244873, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [157  87 100  64  92 103  92  64  92  92 176 134  93   4  93  93  82 196\n  93 146  93 115  93  93  82 145  93  93   7  93  93 171], 정답 [92 92 92 92 92 92 92 92 92 92 93 93 93 93 93 93 93 93 93 93 93 93 93 93\n 93 93 93 93 93 93 93 93]\n(VAL) Batch 145 Loss : 2.1527695655822754, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 82 150  93  82  97 164 178  93  93 176   0  82  93 191  59 181 181 191\n  61 146  93   7 176  93  59  93 176 150  94  70  63  69], 정답 [93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93 93\n 93 93 93 93 94 94 94 94]\n(VAL) Batch 146 Loss : 3.5679850578308105, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 70 153  94  94 107  70  70  94  94  94  94  70  94 153  94 145  70  94\n  94 166 153  94  94 116  94 116 133  70 153 153  94  94], 정답 [94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94\n 94 94 94 94 94 94 94 94]\n(VAL) Batch 147 Loss : 1.8975515365600586, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [155  87  94  94  35 121  70  55 116  94  94  94  70 153 146  95  95 116\n 197 132 132  95 125 119 194 154  95 108  95 137  78  65], 정답 [94 94 94 94 94 94 94 94 94 94 94 94 94 94 95 95 95 95 95 95 95 95 95 95\n 95 95 95 95 95 95 95 95]\n(VAL) Batch 148 Loss : 3.339733600616455, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [132  71 173  71  95 198 164  95  95  95 132  71  98 112  63  82 145 194\n 132 166 124 144  95 154  95  95  83 146  95  95  95  95], 정답 [95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95 95\n 95 95 95 95 95 95 95 95]\n(VAL) Batch 149 Loss : 3.7249915599823, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [150  86 108  96  96  96 197 170  96  96  96  96 198 195 115  96 198  84\n  96 170  96 194  96 170  96  96  55  75  96  31  84  96], 정답 [96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96\n 96 96 96 96 96 96 96 96]\n(VAL) Batch 150 Loss : 2.0598604679107666, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [157 128  96 197  41  96 170 170  66 162  96 197 170  94  96  84 154  96\n 113 146 163 146  87 167 185  81 146  97  97  86 121 140], 정답 [96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 97 97 97 97 97 97\n 97 97 97 97 97 97 97 97]\n(VAL) Batch 151 Loss : 4.068903923034668, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [166 145  97  97 133  97 149  97  72  97  97  97  97 111  87  97  97  97\n  97  97 191  97  55 153  89 100  65  97  98  98  97 100], 정답 [97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97\n 97 97 97 97 97 97 97 97]\n(VAL) Batch 152 Loss : 2.9015188217163086, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [107 138 181  97  98  83  79  98  98  72 142  98  98  98  97  98 111  56\n  70  98 192 136  98 167  98  80  98 142  86 146  98  98], 정답 [97 97 97 97 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98\n 98 98 98 98 98 98 98 98]\n(VAL) Batch 153 Loss : 3.148777961730957, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 98  97 111  98  98  98  98 142  65  98  97 111 126  98  97  98  98  97\n 177  98  98  98  99  99 127 102 114 181  87  99  39 125], 정답 [98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 98 99 99\n 99 99 99 99 99 99 99 99]\n(VAL) Batch 154 Loss : 2.9946677684783936, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [106  84  99 111 169 189 144 125 122 161  88 126  99 175 159 131 146  98\n 143 122 120 124  84 111 174 119  94  13  99 132 163 193], 정답 [99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99\n 99 99 99 99 99 99 99 99]\n(VAL) Batch 155 Loss : 4.284571170806885, accuracy: 0.09375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 99  99 181 188  59 106 153  68 100 158 106  60 100 127 149 100 120  49\n 160 125  94  86 193   9 156 144 131 195 100 100  69 136], 정답 [ 99  99  99  99  99  99  99  99 100 100 100 100 100 100 100 100 100 100\n 100 100 100 100 100 100 100 100 100 100 100 100 100 100]\n(VAL) Batch 156 Loss : 4.020201683044434, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 77 100  68 127 100 111 105  72  68 156 100  72 117 120 137 100 156 100\n 174 100 122 108 187 138 100 138 170 101 129  66 101 103], 정답 [100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100\n 100 100 100 100 100 100 100 100 101 101 101 101 101 101]\n(VAL) Batch 157 Loss : 3.661560297012329, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [132 101  71  66 101 101 101 101 101 101 124  11  71 101 101 101 115  66\n 101 101 165 101 132 139 101 101 101 101 146 101 101 101], 정답 [101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n 101 101 101 101 101 101 101 101 101 101 101 101 101 101]\n(VAL) Batch 158 Loss : 1.4164848327636719, accuracy: 0.625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [101 101 166 137 101 101 101 101 101  13 198 132 145  90 129 102 174 160\n 102  95 113   4 121 102  44  97 129 157 188  66 102 148], 정답 [101 101 101 101 101 101 101 101 101 101 101 101 102 102 102 102 102 102\n 102 102 102 102 102 102 102 102 102 102 102 102 102 102]\n(VAL) Batch 159 Loss : 4.694321155548096, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 13 102  83 102  55 108 146 124  83 180 102 102  96 109 127 157  85 111\n 190 102   6 102 137 105  61 102 165  84  90  75 103 103], 정답 [102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102\n 102 102 102 102 102 102 102 102 102 102 102 102 103 103]\n(VAL) Batch 160 Loss : 3.7626893520355225, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 50 103 103 103 103 103  94 103 103 103 164 103 103 103 103 103 103 164\n  87 116 166   5 103 103 103   4 103 103 103  98 103 103], 정답 [103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103\n 103 103 103 103 103 103 103 103 103 103 103 103 103 103]\n(VAL) Batch 161 Loss : 1.2564970254898071, accuracy: 0.6875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [103 103 103  87 103 103 103 103 197 165 103 173 164 103 103 103 104 191\n 104 117 174 190 104 192 193 104 177  95 125  98 191 192], 정답 [103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 104 104\n 104 104 104 104 104 104 104 104 104 104 104 104 104 104]\n(VAL) Batch 162 Loss : 2.948742151260376, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 48  82 104 104 174 136 123 104  17 179 104 117 104 104 191 110   0 186\n 191 104 179 141 104 156 182 174 182 104 104 174 144 192], 정답 [104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104\n 104 104 104 104 104 104 104 104 104 104 104 104 104 104]\n(VAL) Batch 163 Loss : 3.3860514163970947, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [104 107 105 119 123   6 105  50 105 105 169 105  55 170 140 106 105 105\n  82 100  60 120 109 169 131 105  77 161 105 120 156 120], 정답 [104 104 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n 105 105 105 105 105 105 105 105 105 105 105 105 105 105]\n(VAL) Batch 164 Loss : 3.3040292263031006, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [123 138 105 151 123  82 111 120  73 105  82 105  55 105  53  28  55  73\n 111 105  92  37 123 106  73 106 106 106  68 106 112  71], 정답 [105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n 105 105 106 106 106 106 106 106 106 106 106 106 106 106]\n(VAL) Batch 165 Loss : 3.3789334297180176, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [106  63 143 119 106 161   5  30  63 141 106 112 105 176 120   5  82 123\n 106  37 172 112 105 106 180  58 181  68 104 106 160  49], 정답 [106 106 106 106 106 106 106 106 106 106 106 106 106 106 106 106 106 106\n 106 106 106 106 106 106 106 106 106 106 106 106 106 106]\n(VAL) Batch 166 Loss : 3.839158296585083, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 99 109 123 123  13 106 107 107  43 107 107 107 111  68 107 107 107 107\n 107   1 114 107 107 107 114 153 114 107 194 107 107 180], 정답 [106 106 106 106 106 106 107 107 107 107 107 107 107 107 107 107 107 107\n 107 107 107 107 107 107 107 107 107 107 107 107 107 107]\n(VAL) Batch 167 Loss : 2.6667063236236572, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [107 107 107 107 107 107 107 116 107 107 107  95  92 107 111 153 107 107\n 153 107 111 107 107  94 108 108 108 108 108 126  75 153], 정답 [107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107\n 107 107 107 107 107 107 108 108 108 108 108 108 108 108]\n(VAL) Batch 168 Loss : 1.2607018947601318, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [108 108 108 108  28 108 108 108 167 108 166 197 108 108  18 108 108 108\n 142 108   5 108 103 108 197 108 108 108 108 108 108 108], 정답 [108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108\n 108 108 108 108 108 108 108 108 108 108 108 108 108 108]\n(VAL) Batch 169 Loss : 1.130702018737793, accuracy: 0.71875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [108 108 145 108 108 108 197 103 108 157 109 109  63 132   5 109  64 114\n 139  18  76 163 109 109  86 106  63  88 109 120 130 180], 정답 [108 108 108 108 108 108 108 108 108 108 109 109 109 109 109 109 109 109\n 109 109 109 109 109 109 109 109 109 109 109 109 109 109]\n(VAL) Batch 170 Loss : 3.164283037185669, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [188 105 189 125 109 189 161  77 150  90 109 111 109 189  45 109 109 109\n  68 122  68  92  60 109 181 109 106 109 144 110  95  87], 정답 [109 109 109 109 109 109 109 109 109 109 109 109 109 109 109 109 109 109\n 109 109 109 109 109 109 109 109 109 109 110 110 110 110]\n(VAL) Batch 171 Loss : 3.5264968872070312, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [130 110 155  86 110 174  99  97 110 110 110 104 142 110 158  28 110  94\n  42 140  86  31 102 110 123  87 127 110 110 166 110  72], 정답 [110 110 110 110 110 110 110 110 110 110 110 110 110 110 110 110 110 110\n 110 110 110 110 110 110 110 110 110 110 110 110 110 110]\n(VAL) Batch 172 Loss : 3.7875254154205322, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [151 141 141 138 110  87 110 113 110  45 110  80 100 110 148  57 111 111\n 111 111 111 111 111 111 108 111 111 119  97  77 165  98], 정답 [110 110 110 110 110 110 110 110 110 110 110 110 110 110 111 111 111 111\n 111 111 111 111 111 111 111 111 111 111 111 111 111 111]\n(VAL) Batch 173 Loss : 2.8450493812561035, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [116  97 111 111 111 111 111  67 111 111 111 111 111 129 116 111 111 111\n  65 111 111 108 111  84 139 111 129 119  11 111 111 111], 정답 [111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111\n 111 111 111 111 111 111 111 111 111 111 111 111 111 111]\n(VAL) Batch 174 Loss : 1.1350152492523193, accuracy: 0.625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [112 112  90 112 191 180 112 165 144  68 112 150  62 105 105 112  60 199\n 161   9 112 112 120  60  68  60  79 112 112 196 105 175], 정답 [112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112\n 112 112 112 112 112 112 112 112 112 112 112 112 112 112]\n(VAL) Batch 175 Loss : 3.704709529876709, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 68 163 176 169  18 112 156 119 114 112 180  60 112 106 105  99  63 134\n 142 186 141 151 144 122  83 142  87  41  39  72 138 169], 정답 [112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112 112\n 113 113 113 113 113 113 113 113 113 113 113 113 113 113]\n(VAL) Batch 176 Loss : 4.196377754211426, accuracy: 0.09375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 98 113  64 113 113 123 113 113 113  21  41 113  98  76 147  82  76 113\n 155 113 138 113 113  83 193 113 173 189  45 113   6 180], 정답 [113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113\n 113 113 113 113 113 113 113 113 113 113 113 113 113 113]\n(VAL) Batch 177 Loss : 3.5477070808410645, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [104  31  61 172 118 114 153  82  20 114 114 114 164 114  90 114 114  59\n 150 114   1  45 114 114 185  18 153 164  94 114  59 114], 정답 [113 113 113 113 114 114 114 114 114 114 114 114 114 114 114 114 114 114\n 114 114 114 114 114 114 114 114 114 114 114 114 114 114]\n(VAL) Batch 178 Loss : 3.8401029109954834, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [143 114 118 111  49 114 164 114 142  59 114 114 164  82  42 114 189 165\n  68 114  92  78 166 115  35 115 115 198 115 115 115 155], 정답 [114 114 114 114 114 114 114 114 114 114 114 114 114 114 114 114 114 114\n 114 114 114 114 115 115 115 115 115 115 115 115 115 115]\n(VAL) Batch 179 Loss : 3.4037535190582275, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [115 115 115 115 115 115 115 164 115 115 115 115 115 115 115 170 115 115\n 145  95 115 115 107 148 115 198 145 115 108 115 115 115], 정답 [115 115 115 115 115 115 115 115 115 115 115 115 115 115 115 115 115 115\n 115 115 115 115 115 115 115 115 115 115 115 115 115 115]\n(VAL) Batch 180 Loss : 1.900444746017456, accuracy: 0.71875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [115  18 115 115 166 115 115 115 116 116 110 116 116 137 116 163  87 104\n 170 116 116  60 116  94 116 150 121 116 156 125 104 154], 정답 [115 115 115 115 115 115 115 115 116 116 116 116 116 116 116 116 116 116\n 116 116 116 116 116 116 116 116 116 116 116 116 116 116]\n(VAL) Batch 181 Loss : 2.1579363346099854, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 93 116 116 116  70  59 133 116 137 116 116 116 133 116 153  81 116 116\n 111 116 145  81 116 153 116  94 187  77 117  68 179 117], 정답 [116 116 116 116 116 116 116 116 116 116 116 116 116 116 116 116 116 116\n 116 116 116 116 116 116 116 116 117 117 117 117 117 117]\n(VAL) Batch 182 Loss : 2.66823148727417, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  7 123 117 117 117 117  19 117 174 149 117 173 117 117 155 117 172 139\n 117 117 117   7  65 117  85  79 119 117 106 155 155 100], 정답 [117 117 117 117 117 117 117 117 117 117 117 117 117 117 117 117 117 117\n 117 117 117 117 117 117 117 117 117 117 117 117 117 117]\n(VAL) Batch 183 Loss : 3.7241220474243164, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [132   1 155 117  21 117  75 181 141 155  20 155 101 170 118 170 118 118\n 118 118 118 118 132 118 118 118 114  50 118  82 118 172], 정답 [117 117 117 117 117 117 117 117 117 117 117 117 118 118 118 118 118 118\n 118 118 118 118 118 118 118 118 118 118 118 118 118 118]\n(VAL) Batch 184 Loss : 2.9926538467407227, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [112 132 118 118  62  52 118  65 146 118 118  60 118 118 118 118  44 118\n 118 118 118 118 118 118 118  95  49 118 118   1 119 119], 정답 [118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118\n 118 118 118 118 118 118 118 118 118 118 118 118 119 119]\n(VAL) Batch 185 Loss : 1.658703327178955, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [119 150 111 105 128 119  77 108 119 111 140 108 105 119 105  75  91 119\n 112 119 123 119  60 105 119  60 106 105 127 128 123 119], 정답 [119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119\n 119 119 119 119 119 119 119 119 119 119 119 119 119 119]\n(VAL) Batch 186 Loss : 3.3022329807281494, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [112  60  57 119 126 133 119 119 105 199  74 164 119 124  19  84 132 142\n 142  82 172 120  58 120 100 120 120 131 120  74 138 134], 정답 [119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 119 120 120\n 120 120 120 120 120 120 120 120 120 120 120 120 120 120]\n(VAL) Batch 187 Loss : 4.733682155609131, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [150  99  60  77 120 112 131 120 120 168 149 120  77 125  74 172 144 106\n 120 146  78 120  27 120 123  80 125  63 109 120  68  42], 정답 [120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120\n 120 120 120 120 120 120 120 120 120 120 120 120 120 120]\n(VAL) Batch 188 Loss : 3.726271152496338, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 21 134 133 133 133 121 121  70 121  70  96 121 133 121 145 121 121 166\n 121 121 121 103 140 111 121 133 166 121  70 103  98 145], 정답 [120 120 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n 121 121 121 121 121 121 121 121 121 121 121 121 121 121]\n(VAL) Batch 189 Loss : 3.0258376598358154, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 70 133 116 145 103  70 145  70  81 166 121 103 145 121 121 121 121 194\n 133 121  76 122 111 122 159  14  45 121 110 117 162 109], 정답 [121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n 121 121 122 122 122 122 122 122 122 122 122 122 122 122]\n(VAL) Batch 190 Loss : 4.126575469970703, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [122  68 198 122  84  88 101 122  67  36  92  10 104  43  51 109 122  30\n 144 189 100   7 172  10 122  91  26 151 122  87 100 140], 정답 [122 122 122 122 122 122 122 122 122 122 122 122 122 122 122 122 122 122\n 122 122 122 122 122 122 122 122 122 122 122 122 122 122]\n(VAL) Batch 191 Loss : 3.7578284740448, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [131  32  40  32 170  72 123 123  80  68 138  74 118  75  28  77  12  73\n 123  50 123  77 180  63 139 188  60  68  60  82  77 123], 정답 [122 122 122 122 122 122 123 123 123 123 123 123 123 123 123 123 123 123\n 123 123 123 123 123 123 123 123 123 123 123 123 123 123]\n(VAL) Batch 192 Loss : 4.4754509925842285, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [116  57 150 112  77  60 120 180  74  80  60 123 123 123 123 141 123 123\n   5  68 105 123 123 148 124  79 124 124 124 124 124  88], 정답 [123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123\n 123 123 123 123 123 123 124 124 124 124 124 124 124 124]\n(VAL) Batch 193 Loss : 2.7317893505096436, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [131 157 124 124 124 124 124 124 124 124 132  79 124 124 109 124 124 124\n 124 165 124 124 124 103  71 124 124 124 124 102 166 124], 정답 [124 124 124 124 124 124 124 124 124 124 124 124 124 124 124 124 124 124\n 124 124 124 124 124 124 124 124 124 124 124 124 124 124]\n(VAL) Batch 194 Loss : 1.5940686464309692, accuracy: 0.6875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [124 124 124 124 124 124 124 124 122 124 120 123   3  43 159 125 123 150\n  10 112 125  88  88 125 138  99 109  60 123  41  99 125], 정답 [124 124 124 124 124 124 124 124 124 124 125 125 125 125 125 125 125 125\n 125 125 125 125 125 125 125 125 125 125 125 125 125 125]\n(VAL) Batch 195 Loss : 2.319071054458618, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [131  10 125  95 125 152  61 125  88  99  70 188 138 146  88 125 167  60\n  68 132  60 125 116 125 106 180 123   5 126 126 109  61], 정답 [125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125 125\n 125 125 125 125 125 125 125 125 125 125 126 126 126 126]\n(VAL) Batch 196 Loss : 3.481699228286743, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [126 126 126 126 126 126 126 122 126  17  54 126 126 126 129  73 126 129\n  61  61 126  61 126 123  98 126 169  99 126 113  59 126], 정답 [126 126 126 126 126 126 126 126 126 126 126 126 126 126 126 126 126 126\n 126 126 126 126 126 126 126 126 126 126 126 126 126 126]\n(VAL) Batch 197 Loss : 2.629754066467285, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 59  77 126 126 160  61 126 102 126 126  61 162 126 126  55 127  92  93\n 127  64 127  72 162   1 127 127 127 127 127  40 127 125], 정답 [126 126 126 126 126 126 126 126 126 126 126 126 126 126 127 127 127 127\n 127 127 127 127 127 127 127 127 127 127 127 127 127 127]\n(VAL) Batch 198 Loss : 2.7035703659057617, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 55 167  60 172 138  63 145  71 105 127  92  87 143 127  87 127 121 168\n  75 104 127  86  86 140 154 172  83 128 168 140 141 104], 정답 [127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127\n 127 127 127 127 127 127 127 127 127 127 127 127 127 127]\n(VAL) Batch 199 Loss : 4.276113986968994, accuracy: 0.125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [100  85 155 145 175 128 128 116  86 128 128 126 128  68 128 124  60 128\n 128  96 128  75  86  86  86 127 164  55  60  72 105 130], 정답 [128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n 128 128 128 128 128 128 128 128 128 128 128 128 128 128]\n(VAL) Batch 200 Loss : 3.749898672103882, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [128 110 138 146 127 128 128 128 167 128 128  86 167  86 128 140 128 128\n 129 176 129 129 178  55 108 164 129 129 129 129 129 129], 정답 [128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128\n 129 129 129 129 129 129 129 129 129 129 129 129 129 129]\n(VAL) Batch 201 Loss : 1.8936887979507446, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [112  60  86  92 129 129  71 129  72 129 113 172 108 170 129  65 129 129\n 134 129 129 127 105 146 126 129 129 145 129 129 129  71], 정답 [129 129 129 129 129 129 129 129 129 129 129 129 129 129 129 129 129 129\n 129 129 129 129 129 129 129 129 129 129 129 129 129 129]\n(VAL) Batch 202 Loss : 3.1951801776885986, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [146 129 126 129  99  90 130  83  59 130  99  25 130 130 128 130  73 130\n  23 193 130 189 130 121  21  17 184 130 130 182  69  80], 정답 [129 129 129 129 130 130 130 130 130 130 130 130 130 130 130 130 130 130\n 130 130 130 130 130 130 130 130 130 130 130 130 130 130]\n(VAL) Batch 203 Loss : 3.569077253341675, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 99 130 130 130 146 130  36  90 130 107 188 130 147 138 130   0 130 130\n 180  84 130 193 138 109 159  71  72 175  86  97  68 145], 정답 [130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130\n 130 130 130 130 131 131 131 131 131 131 131 131 131 131]\n(VAL) Batch 204 Loss : 4.47308349609375, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 77 123  77 152 131 149  69  86 132 125 185 176  36 138 138  77  27 189\n  99 183 123 167 178 152 116  71 131  98  69 147  83 147], 정답 [131 131 131 131 131 131 131 131 131 131 131 131 131 131 131 131 131 131\n 131 131 131 131 131 131 131 131 131 131 131 131 131 131]\n(VAL) Batch 205 Loss : 5.859655857086182, accuracy: 0.0625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [106  98 141 122 137 100  65  89  19 101 124 125 101 108 132  98 101   6\n 127 109  71 101 132 128  71 154 124  71  71 131 122 132], 정답 [131 131 131 131 131 131 131 131 132 132 132 132 132 132 132 132 132 132\n 132 132 132 132 132 132 132 132 132 132 132 132 132 132]\n(VAL) Batch 206 Loss : 5.03107213973999, accuracy: 0.09375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [132  60 122  60 156 134 124 101 126 172 132  68   7 163 137 165  94 180\n 105 124 101 172 185 132 129  83 133 133 133 133 133 133], 정답 [132 132 132 132 132 132 132 132 132 132 132 132 132 132 132 132 132 132\n 132 132 132 132 132 132 132 132 133 133 133 133 133 133]\n(VAL) Batch 207 Loss : 4.824151992797852, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [133 133 133 133 170 133 133  97 133 133 133 133 133 166 133 133 133 153\n 133 133 164 133 133 133 133 133 116 133 153  94 133 133], 정답 [133 133 133 133 133 133 133 133 133 133 133 133 133 133 133 133 133 133\n 133 133 133 133 133 133 133 133 133 133 133 133 133 133]\n(VAL) Batch 208 Loss : 1.1845471858978271, accuracy: 0.75\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [133 133 133 133 133 133 133 133 133 146 116 133 149 134  82  99 134 173\n 134 134 118 126 134 134 176  59  62 134 105 163  62 134], 정답 [133 133 133 133 133 133 133 133 133 133 133 133 134 134 134 134 134 134\n 134 134 134 134 134 134 134 134 134 134 134 134 134 134]\n(VAL) Batch 209 Loss : 2.3288681507110596, accuracy: 0.5625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [134 134 169 134 134 134 129 134  90 134 189 134 167 114 149 134  85 134\n 113 134 164 105  14 112  85  80 134  90 134  45 182 138], 정답 [134 134 134 134 134 134 134 134 134 134 134 134 134 134 134 134 134 134\n 134 134 134 134 134 134 134 134 134 134 134 134 135 135]\n(VAL) Batch 210 Loss : 2.958507776260376, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [172 165 108 193  67  85 124 128 135 189 155 186 108 189  90 149  99 176\n  73  59  89 169  93 118 176   8 103 108 128 163  73  90], 정답 [135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135\n 135 135 135 135 135 135 135 135 135 135 135 135 135 135]\n(VAL) Batch 211 Loss : 5.992851257324219, accuracy: 0.03125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [122 134 135 155 180 138 146  98  73 199  73  85 172 122 130 144  12 136\n 189 123 140  98 136 136  75 107 136  83  54  63  27   2], 정답 [135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 136 136\n 136 136 136 136 136 136 136 136 136 136 136 136 136 136]\n(VAL) Batch 212 Loss : 3.839010000228882, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [136   3  92 136  86  84  31  97 123  98 151 120 136 182  28 136 136  53\n 173 136 193 136 136  18 144  77 136  97  83 156 190  64], 정답 [136 136 136 136 136 136 136 136 136 136 136 136 136 136 136 136 136 136\n 136 136 136 136 136 136 136 136 136 136 136 136 136 136]\n(VAL) Batch 213 Loss : 3.9921977519989014, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [136 136 197 116  55 113  35 137 123 124  75 137  81 124  94   0 137 137\n 153 136 137 133 111  92 137 137  94  72  99 118  66  84], 정답 [136 136 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137\n 137 137 137 137 137 137 137 137 137 137 137 137 137 137]\n(VAL) Batch 214 Loss : 3.946564197540283, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [128  36  71  21 146  61   9 124  23 124  87 140  64 137 172  69 132  22\n 137 127 189 172  60 138  90 163 138 138 138 138  86 127], 정답 [137 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137 137\n 137 137 138 138 138 138 138 138 138 138 138 138 138 138]\n(VAL) Batch 215 Loss : 3.9612133502960205, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [110 156  69  86  68 116 138 156 114 149 140 148  68 186  13 112 147  63\n 100 138  73  39 100  32 138  90  86  90  21  72 113 100], 정답 [138 138 138 138 138 138 138 138 138 138 138 138 138 138 138 138 138 138\n 138 138 138 138 138 138 138 138 138 138 138 138 138 138]\n(VAL) Batch 216 Loss : 5.057048320770264, accuracy: 0.09375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [105 132 176 138  83  90  66 194  41 197 122 121  16 117 123  75 139  95\n 104  68 189 106  71 108  11 158 111  88 139 143 178  95], 정답 [138 138 138 138 138 138 139 139 139 139 139 139 139 139 139 139 139 139\n 139 139 139 139 139 139 139 139 139 139 139 139 139 139]\n(VAL) Batch 217 Loss : 5.448957920074463, accuracy: 0.09375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [127 185  87  21 139  79  68  40 197 164 146 167 145  32 105  92 132 139\n  77  83  75 112 139 188  97 140 140  65  93  89 140  86], 정답 [139 139 139 139 139 139 139 139 139 139 139 139 139 139 139 139 139 139\n 139 139 139 139 139 139 140 140 140 140 140 140 140 140]\n(VAL) Batch 218 Loss : 5.027792930603027, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [140 140  86 140 176 140 145 130 140 146 140 136 140 107 128  31 102 134\n 140 169 147  13 140 138 174  59 140 145  86  86  86 169], 정답 [140 140 140 140 140 140 140 140 140 140 140 140 140 140 140 140 140 140\n 140 140 140 140 140 140 140 140 140 140 140 140 140 140]\n(VAL) Batch 219 Loss : 4.268347263336182, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [153 167 140 174 110 140  86 140 140  17 125  81 161  94 141 125 193 193\n 141  59  65 103 130 144 141  75 103 127  61 128  87 141], 정답 [140 140 140 140 140 140 140 140 140 140 141 141 141 141 141 141 141 141\n 141 141 141 141 141 141 141 141 141 141 141 141 141 141]\n(VAL) Batch 220 Loss : 4.349477291107178, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 23  77 103 141 149 149  69 127 141 130 122 110  27 160 141  36  18 160\n  97 110 110  98  87 193 155 141 151 178  27 111  53 142], 정답 [141 141 141 141 141 141 141 141 141 141 141 141 141 141 141 141 141 141\n 141 141 141 141 141 141 141 141 141 141 142 142 142 142]\n(VAL) Batch 221 Loss : 4.735405921936035, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [142  86 142 142 138 157 142 126 125 111 142 142 142 167 185 142  54 142\n 133  63 142 127 103  68  99 142 142  64  86 142 127 102], 정답 [142 142 142 142 142 142 142 142 142 142 142 142 142 142 142 142 142 142\n 142 142 142 142 142 142 142 142 142 142 142 142 142 142]\n(VAL) Batch 222 Loss : 3.360788106918335, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [126 160  98 142  68 115  61 154  82 122 136 151 160 117 143 143 143 143\n  13 143  68  59 143 143 143 143 120 143 143 143 143 143], 정답 [142 142 142 142 142 142 142 142 142 142 142 142 142 142 143 143 143 143\n 143 143 143 143 143 143 143 143 143 143 143 143 143 143]\n(VAL) Batch 223 Loss : 2.888944387435913, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [143 143 143  68 143 143 197 143 143 143 143 143 152  45  80 143 143 143\n 143 143 143 143 143 143 198 143 143 163 143 180 143 123], 정답 [143 143 143 143 143 143 143 143 143 143 143 143 143 143 143 143 143 143\n 143 143 143 143 143 143 143 143 143 143 143 143 143 143]\n(VAL) Batch 224 Loss : 2.315315008163452, accuracy: 0.71875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [144 136 139 158  98  77  85  77 144 117 169 189 112 144  47  83 116  86\n  94 144 159 193 144  87 101 104 172 144 148  79 156  30], 정답 [144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144\n 144 144 144 144 144 144 144 144 144 144 144 144 144 144]\n(VAL) Batch 225 Loss : 4.873634338378906, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [120 100  32 107  68 144 158 144 160 144 144 144 144 110 187  90 185 175\n 145 145  94 145 145 145 145 145 145 145 145 145 145 145], 정답 [144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144 144\n 145 145 145 145 145 145 145 145 145 145 145 145 145 145]\n(VAL) Batch 226 Loss : 3.380852699279785, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [145 145 145 145 145 145 145 145 145  65 128 145 145 145 145 145 145 145\n 145 145 145 145 145 145 145 145 145 107 139 145 145 145], 정답 [145 145 145 145 145 145 145 145 145 145 145 145 145 145 145 145 145 145\n 145 145 145 145 145 145 145 145 145 145 145 145 145 145]\n(VAL) Batch 227 Loss : 0.39575135707855225, accuracy: 0.875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [145 145 145 145 146 145 111 146 146 146 146 146 176 146 146 146 146 146\n 146 146 166 146 146 146 128  96 146 146 167 146 146 116], 정답 [145 145 145 145 146 146 146 146 146 146 146 146 146 146 146 146 146 146\n 146 146 146 146 146 146 146 146 146 146 146 146 146 146]\n(VAL) Batch 228 Loss : 0.9811081290245056, accuracy: 0.75\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 68 146 146  61 146 146 145  86 146  96 128 146  59 165 146 146 146  20\n 189 126 146 146  83 137 112 147 147  81 153 146  95  82], 정답 [146 146 146 146 146 146 146 146 146 146 146 146 146 146 146 146 146 146\n 146 146 146 146 147 147 147 147 147 147 147 147 147 147]\n(VAL) Batch 229 Loss : 3.9076499938964844, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [163 147  98  98 174  97 103 147  94 193 176 147 107  99  99  86 151 108\n  87 147  98 147 108  87 147 147  89 147  97 153 155  97], 정답 [147 147 147 147 147 147 147 147 147 147 147 147 147 147 147 147 147 147\n 147 147 147 147 147 147 147 147 147 147 147 147 147 147]\n(VAL) Batch 230 Loss : 4.205531597137451, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 97 135  87  97 193  84 106 146 148  76 148 122 120 148  74 156 148  69\n 115  80 148 148  36  60 148  13 122  23 148 148 106 158], 정답 [147 147 147 147 147 147 147 147 148 148 148 148 148 148 148 148 148 148\n 148 148 148 148 148 148 148 148 148 148 148 148 148 148]\n(VAL) Batch 231 Loss : 4.3608012199401855, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [148 148  23  19  23  22 106 128 181  14 148 148 196 148  24  17  23  48\n  31 139 159 172 148 196 148 196 149 149 149 107  75 149], 정답 [148 148 148 148 148 148 148 148 148 148 148 148 148 148 148 148 148 148\n 148 148 148 148 148 148 148 148 149 149 149 149 149 149]\n(VAL) Batch 232 Loss : 4.0938897132873535, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 18  90  90 149 149  74  90 175 134 178 128  37  32 149 149 149 145 189\n 149  59 149 149 149 149 140  93  62 149 149 128  66 135], 정답 [149 149 149 149 149 149 149 149 149 149 149 149 149 149 149 149 149 149\n 149 149 149 149 149 149 149 149 149 149 149 149 149 149]\n(VAL) Batch 233 Loss : 3.7944064140319824, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [161  93  16   7 149 193 134  77 149 151 158  91 143  84 190  55 112 123\n 123 191 168 119 179 150 153  65 150 150  82 150 150 150], 정답 [149 149 149 149 149 149 149 149 149 149 149 149 150 150 150 150 150 150\n 150 150 150 150 150 150 150 150 150 150 150 150 150 150]\n(VAL) Batch 234 Loss : 4.604221343994141, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 58 150 127  97 193   5 169 161 150 107 150 150 111 150  92  99  60  82\n 185  82 123  89 136  18 150 150 156  86 106 123 110 137], 정답 [150 150 150 150 150 150 150 150 150 150 150 150 150 150 150 150 150 150\n 150 150 150 150 150 150 150 150 150 150 150 150 151 151]\n(VAL) Batch 235 Loss : 4.359157562255859, accuracy: 0.21875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 30 134  62 151 151 151 151 110 173  82 180  87 151  59  83  97 161  67\n 122 141 151 173 151 151 151  97 169 100 104 126  67 172], 정답 [151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151\n 151 151 151 151 151 151 151 151 151 151 151 151 151 151]\n(VAL) Batch 236 Loss : 4.212260723114014, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [151 172 150 104 151 123 151 140 151 129 121 173 126 151 154  94  14 152\n 168 196 152 152 152 152  46  39 152  14 152  37 172 141], 정답 [151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151 152 152\n 152 152 152 152 152 152 152 152 152 152 152 152 152 152]\n(VAL) Batch 237 Loss : 3.580042839050293, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 78  12 152 152   4 134  35 152  89  64 162  23 152 152  17  89   2  15\n 152 199 152  10 152   8  14 152 152 196 162 152  92 152], 정답 [152 152 152 152 152 152 152 152 152 152 152 152 152 152 152 152 152 152\n 152 152 152 152 152 152 152 152 152 152 152 152 152 152]\n(VAL) Batch 238 Loss : 3.0626556873321533, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [161 152 153 153  70  61 153 153 153 153  94 153  94 121  94  94 153 171\n  70 153  92 116  94  94 133 145 116  94 153  94  94 153], 정답 [152 152 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153\n 153 153 153 153 153 153 153 153 153 153 153 153 153 153]\n(VAL) Batch 239 Loss : 2.1678311824798584, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [153 153  70 145 153  94 153 153  94 153  70 153 153 153 153  94 133 153\n 153 153 154  87 124 154 152 128 124 164 146 165 105 154], 정답 [153 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153 153\n 153 153 154 154 154 154 154 154 154 154 154 154 154 154]\n(VAL) Batch 240 Loss : 3.174929618835449, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [125 152 195 154 170 154 154 154 170 170 170 147 154 132 154  87  75 154\n  59 108  12 198 154 154 129 108 154 154 154  49 167 154], 정답 [154 154 154 154 154 154 154 154 154 154 154 154 154 154 154 154 154 154\n 154 154 154 154 154 154 154 154 154 154 154 154 154 154]\n(VAL) Batch 241 Loss : 3.9924066066741943, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [154  95 198 154 149 154 155 155 155 117 110 155  13 155  87 123 143 132\n 155 123 155 148 155 110  86 155 155 141  38 155 130 141], 정답 [154 154 154 154 154 154 155 155 155 155 155 155 155 155 155 155 155 155\n 155 155 155 155 155 155 155 155 155 155 155 155 155 155]\n(VAL) Batch 242 Loss : 3.4630534648895264, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [155 195 155 117  13 155 193  13 117 141 184 141 155 108 155 111 155 155\n 124  13 155 155 155  69 112 123 156 112  77  50 179  41], 정답 [155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155\n 155 155 155 155 155 155 156 156 156 156 156 156 156 156]\n(VAL) Batch 243 Loss : 3.0922203063964844, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  3 136 156  83 128 156 156 127 156 156 125 156 156 137 156 156   6 159\n 123 183 156 112 105 174 153 144 187  75 156 100 156 156], 정답 [156 156 156 156 156 156 156 156 156 156 156 156 156 156 156 156 156 156\n 156 156 156 156 156 156 156 156 156 156 156 156 156 156]\n(VAL) Batch 244 Loss : 3.2321794033050537, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [156  81 168  22 156 156 156   3 156  25 195 129 166 157 157  84 157 118\n 164 124 132  71 157 157 145   4 145  61 194 124 157 198], 정답 [156 156 156 156 156 156 156 156 156 156 157 157 157 157 157 157 157 157\n 157 157 157 157 157 157 157 157 157 157 157 157 157 157]\n(VAL) Batch 245 Loss : 3.1169419288635254, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [102  71 162 157 197 154 162  54 129 194 108 126 157 101 164 157 157 106\n 157  96 170  96 157 157 157 157 124 127 173 158 158 136], 정답 [157 157 157 157 157 157 157 157 157 157 157 157 157 157 157 157 157 157\n 157 157 157 157 157 157 157 157 157 157 158 158 158 158]\n(VAL) Batch 246 Loss : 3.2087626457214355, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [123 125  82 158 139   9 158  60  82 171  68  75 197  74 158 158 158  53\n  77 105 120 112 158  82 134  80 168 108 158 158 149 166], 정답 [158 158 158 158 158 158 158 158 158 158 158 158 158 158 158 158 158 158\n 158 158 158 158 158 158 158 158 158 158 158 158 158 158]\n(VAL) Batch 247 Loss : 3.902367353439331, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [156 158 158 116  24 120  42 158 125 158  74 117  99 158  13 122 159 159\n 180 159 189 119 189  40 122  16  92 115 156  61 122 125], 정답 [158 158 158 158 158 158 158 158 158 158 158 158 158 158 159 159 159 159\n 159 159 159 159 159 159 159 159 159 159 159 159 159 159]\n(VAL) Batch 248 Loss : 4.07741641998291, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 77  77 164  30 116  21 122 100  11 161 125 120  72 109 181   4  55  70\n 175 149 125 189 159 150 148  49  81 130   7 180  92  32], 정답 [159 159 159 159 159 159 159 159 159 159 159 159 159 159 159 159 159 159\n 159 159 159 159 159 159 159 159 159 159 159 159 159 159]\n(VAL) Batch 249 Loss : 5.440670490264893, accuracy: 0.03125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  3  71 183 153  80 112 160  86 184 172 167 160   7  75 178 160  80  72\n  87 147 160 141 146 160  64   2 117 181 189   6 181 110], 정답 [160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160\n 160 160 160 160 160 160 160 160 160 160 160 160 160 160]\n(VAL) Batch 250 Loss : 4.306654930114746, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [160  10 193 168 119 160  67 180  75 139 160 135 160 160 106 160 192  17\n  34  53  32  83 189  56  45 190 102  65 145 106  54  32], 정답 [160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160\n 161 161 161 161 161 161 161 161 161 161 161 161 161 161]\n(VAL) Batch 251 Loss : 4.614184856414795, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [189 161 189  82 161 161  52 190  34  34 100  83 161 181  90 161  27  31\n 180  31 134 180 161  27  27  31  22  25 171 161 199 161], 정답 [161 161 161 161 161 161 161 161 161 161 161 161 161 161 161 161 161 161\n 161 161 161 161 161 161 161 161 161 161 161 161 161 161]\n(VAL) Batch 252 Loss : 3.1240272521972656, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 21 180 161  24  51 162 162 162  66 174 162 162 162 162 162 162 162 162\n 162 122 164  61 103 162 162 169 114 162 152 129 162  14], 정답 [161 161 161 161 162 162 162 162 162 162 162 162 162 162 162 162 162 162\n 162 162 162 162 162 162 162 162 162 162 162 162 162 162]\n(VAL) Batch 253 Loss : 2.3991689682006836, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [162 162 162 170  94 195 195   4  14  93 162 197 162 162 162 162 162 162\n  66 162  12 126  71  55 177 189 163 116 163  97 163 112], 정답 [162 162 162 162 162 162 162 162 162 162 162 162 162 162 162 162 162 162\n 162 162 162 162 163 163 163 163 163 163 163 163 163 163]\n(VAL) Batch 254 Loss : 2.802600860595703, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [163 163 163 163  65 163  60 163 163  65 163 123 163 163 163 163  17 163\n  13 163 109  83 163 163 135 163 112 104 163 163 171 163], 정답 [163 163 163 163 163 163 163 163 163 163 163 163 163 163 163 163 163 163\n 163 163 163 163 163 163 163 163 163 163 163 163 163 163]\n(VAL) Batch 255 Loss : 2.2678308486938477, accuracy: 0.625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [163  71  60 180 163 118 163 163 164 164 164 107 115 164 164 164 115 164\n  49 164  70 165 164 164 164  49 164 164 164  49 116 164], 정답 [163 163 163 163 163 163 163 163 164 164 164 164 164 164 164 164 164 164\n 164 164 164 164 164 164 164 164 164 164 164 164 164 164]\n(VAL) Batch 256 Loss : 1.8957182168960571, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [164 164 166 164 170 164 164  82 106 107  16  66 111 128  60  18 143 111\n 196 146 111 111 108 164 111 164 170 165 165 171 162 165], 정답 [164 164 164 164 164 164 164 164 164 164 164 164 164 164 164 164 164 164\n 164 164 164 164 164 164 164 164 165 165 165 165 165 165]\n(VAL) Batch 257 Loss : 3.464696168899536, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [102 170 165 165 165  72 165 121 165 102 165 165 165 165 124 165  67 165\n 165 108 165  96 165 167 128 165 165 165 165 165 165 165], 정답 [165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165\n 165 165 165 165 165 165 165 165 165 165 165 165 165 165]\n(VAL) Batch 258 Loss : 1.6519776582717896, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [165 165 165 165 195 165  66  71  84 165 165  66 166 166 166 166 145 166\n 166 166 166  95 166 146 166 166 166 166 166 166 166 166], 정답 [165 165 165 165 165 165 165 165 165 165 165 165 166 166 166 166 166 166\n 166 166 166 166 166 166 166 166 166 166 166 166 166 166]\n(VAL) Batch 259 Loss : 1.2942473888397217, accuracy: 0.75\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [162 103 166 166 166 166 166 166 166 121 166 108 145 166 166 166 166 166\n 133 166 165 166 166 166 166 166 116 166 166 167  21 167], 정답 [166 166 166 166 166 166 166 166 166 166 166 166 166 166 166 166 166 166\n 166 166 166 166 166 166 166 166 166 166 166 166 167 167]\n(VAL) Batch 260 Loss : 1.367240071296692, accuracy: 0.6875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [128  68 167 167  49  87 137 116 128 167 140  85 167  86  92 146 109 127\n 128 167 128 167 167 170  65 167 127 167 128 111 105 171], 정답 [167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 167\n 167 167 167 167 167 167 167 167 167 167 167 167 167 167]\n(VAL) Batch 261 Loss : 3.359255075454712, accuracy: 0.28125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [167 167 167 127 167 139 167 132 167 167 167 163 167 146 105 167 158 126\n 111  60 148 111 111 170 164 154  18  30 137  94  64  31], 정답 [167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 167 168 168\n 168 168 168 168 168 168 168 168 168 168 168 168 168 168]\n(VAL) Batch 262 Loss : 4.517567157745361, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [180 151 116  84 180 127  82  64 149  97  66 125 111 114 172  94 168  84\n 112  70 180  73 168 150  97 172 143 113 168 136  80 153], 정답 [168 168 168 168 168 168 168 168 168 168 168 168 168 168 168 168 168 168\n 168 168 168 168 168 168 168 168 168 168 168 168 168 168]\n(VAL) Batch 263 Loss : 5.421516418457031, accuracy: 0.09375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 66  82 145 126 188  85 169 163 169 169 169 169  61   9  80 169  62 169\n  85 169  98  85  37 169  61 169 169  17  62 169 180  62], 정답 [168 168 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169\n 169 169 169 169 169 169 169 169 169 169 169 169 169 169]\n(VAL) Batch 264 Loss : 4.029829502105713, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [122 128  68  62 168 169 169  82 169 180  85  86  41  82 105  62 169  68\n  61  61 129 145 170 165 170 170 170 162 170 170 170 170], 정답 [169 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169 169\n 169 169 170 170 170 170 170 170 170 170 170 170 170 170]\n(VAL) Batch 265 Loss : 2.930793046951294, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [197 170 108 103 170 194 170 162 170 170 170  42 170 170 170 170 170  69\n 170 194 194 197 170 170 170 170 170 165 170 170 170 170], 정답 [170 170 170 170 170 170 170 170 170 170 170 170 170 170 170 170 170 170\n 170 170 170 170 170 170 170 170 170 170 170 170 170 170]\n(VAL) Batch 266 Loss : 1.6769479513168335, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [170 105 154 170 170  70 171 171 171 171  77 171  64 171  61 167 171 171\n  31  99 171 167  74  68 172 171 168 171 171 171  68  77], 정답 [170 170 170 170 170 170 171 171 171 171 171 171 171 171 171 171 171 171\n 171 171 171 171 171 171 171 171 171 171 171 171 171 171]\n(VAL) Batch 267 Loss : 2.8871397972106934, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 95  68  65 171 118 163 171 171 105 171  28 171 171 123 118 171 171 120\n 171 181  96  68 171 171 111  72 141 127 172 135 106 142], 정답 [171 171 171 171 171 171 171 171 171 171 171 171 171 171 171 171 171 171\n 171 171 171 171 171 171 172 172 172 172 172 172 172 172]\n(VAL) Batch 268 Loss : 3.3442344665527344, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 90 100  62  86 189 100 161 173  92 172 172  85 172 104 172 130  90 144\n 188 172 187 147 160 172 109 183 111 135 160 111  31 187], 정답 [172 172 172 172 172 172 172 172 172 172 172 172 172 172 172 172 172 172\n 172 172 172 172 172 172 172 172 172 172 172 172 172 172]\n(VAL) Batch 269 Loss : 3.556209087371826, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  0  72 172 172 194 177  89  90  32  80 170 173 173 173 173  71 101 173\n 173 173  95 164 173 154 197 173 173 173 173 173  71 173], 정답 [172 172 172 172 172 172 172 172 172 172 173 173 173 173 173 173 173 173\n 173 173 173 173 173 173 173 173 173 173 173 173 173 173]\n(VAL) Batch 270 Loss : 2.5186188220977783, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [173  71 173   0 173 103  71 124 173 157 170 173 173  73 118  71 146 129\n 173 173 173 173 173 173  13 173 173 173  92  87  87 104], 정답 [173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173\n 173 173 173 173 173 173 173 173 173 173 174 174 174 174]\n(VAL) Batch 271 Loss : 2.6743366718292236, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [174 174 182 174   7 183 178 102 177 104 174 174 104  17  69 174 174 178\n 178  46 174 104 174 174 174 104 174 174 174 104 104 104], 정답 [174 174 174 174 174 174 174 174 174 174 174 174 174 174 174 174 174 174\n 174 174 174 174 174 174 174 174 174 174 174 174 174 174]\n(VAL) Batch 272 Loss : 2.3624911308288574, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [191  98 191 174 154 147 139 174 193 174 174 192 143 153 137 193 186  28\n 123  27  98 188 175 178 159 110 175 190 191  88 105  98], 정답 [174 174 174 174 174 174 174 174 174 174 174 174 174 174 175 175 175 175\n 175 175 175 175 175 175 175 175 175 175 175 175 175 175]\n(VAL) Batch 273 Loss : 4.600225925445557, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 73   6 181  90 163  93  99 104 109 124  10  22 128 130 156  68  60 191\n  69 175  61  59 122 109 175 120 174 175 193 100 153 177], 정답 [175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175\n 175 175 175 175 175 175 175 175 175 175 175 175 175 175]\n(VAL) Batch 274 Loss : 5.475642204284668, accuracy: 0.09375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [176 176 176 176 176 176 176 135 176 176  89 119 176 176  93 108 174 176\n 181 176 176  62 176 176 181 176 115 176  97 176 176 176], 정답 [176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176\n 176 176 176 176 176 176 176 176 176 176 176 176 176 176]\n(VAL) Batch 275 Loss : 1.778801441192627, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [176 176  62 176  17 176 176  82 176 164 146  90 176 196 176 169 176  82\n 139 178 177 192  41 177  41 177 136 177  36 177 192 104], 정답 [176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176 176\n 177 177 177 177 177 177 177 177 177 177 177 177 177 177]\n(VAL) Batch 276 Loss : 2.421274185180664, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [177 190 181 177 145 177  62  38  25 177 177 193 181 177 177 177 104 104\n 177  18 177 178 191 191 190 193 177 155 178 177 190 104], 정답 [177 177 177 177 177 177 177 177 177 177 177 177 177 177 177 177 177 177\n 177 177 177 177 177 177 177 177 177 177 177 177 177 177]\n(VAL) Batch 277 Loss : 2.7763009071350098, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [178 177 177 182   2 193 178 177 178 178 178 178 178 178 178 178 178  18\n 178  42 178 191 178 178 182 161 178 178 181 178 183  44], 정답 [177 177 177 177 178 178 178 178 178 178 178 178 178 178 178 178 178 178\n 178 178 178 178 178 178 178 178 178 178 178 178 178 178]\n(VAL) Batch 278 Loss : 2.20115327835083, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  2 174 192 189 178 178 178 182 178 178  44 178 178  38 178 178 182 178\n 175 178 178 178 161 199 182 190 138 130 193 178  14  25], 정답 [178 178 178 178 178 178 178 178 178 178 178 178 178 178 178 178 178 178\n 178 178 178 178 179 179 179 179 179 179 179 179 179 179]\n(VAL) Batch 279 Loss : 2.8244407176971436, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [181 179 189 189 177  83  99 182 160 136 177 181 189  41 153 189 179  86\n 177 125 177 181 179 182 179 190 191 104 179 189 123  68], 정답 [179 179 179 179 179 179 179 179 179 179 179 179 179 179 179 179 179 179\n 179 179 179 179 179 179 179 179 179 179 179 179 179 179]\n(VAL) Batch 280 Loss : 4.460080146789551, accuracy: 0.15625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [181 181  49  32 179  16 182 182 180 141  74 180  60 180 180 161  77 123\n  90  24  60 180  10  13 189 193  68  18 138 112  34 136], 정답 [179 179 179 179 179 179 179 179 180 180 180 180 180 180 180 180 180 180\n 180 180 180 180 180 180 180 180 180 180 180 180 180 180]\n(VAL) Batch 281 Loss : 4.9714035987854, accuracy: 0.1875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 77 163 123  77 180 180 113 180 189 180  60 112  25  74  68 180 114  75\n  85 192  83 144 123 180 123 180 150 181  59 161 181 181], 정답 [180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180\n 180 180 180 180 180 180 180 180 181 181 181 181 181 181]\n(VAL) Batch 282 Loss : 3.7045531272888184, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [181 174 181 181  18 181 192 130 181 175 181 101 177 159 181  47 180 181\n 182 181 147 191 178 181  74 104   7 176  19 181 181 181], 정답 [181 181 181 181 181 181 181 181 181 181 181 181 181 181 181 181 181 181\n 181 181 181 181 181 181 181 181 181 181 181 181 181 181]\n(VAL) Batch 283 Loss : 4.43649959564209, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [181 182 181 181 181 182 133  75 181 181 181 181 182 182  18 191  18 182\n 177 182 182 175 177 182 192 182 177 136 182 190 181 191], 정답 [181 181 181 181 181 181 181 181 181 181 181 181 182 182 182 182 182 182\n 182 182 182 182 182 182 182 182 182 182 182 182 182 182]\n(VAL) Batch 284 Loss : 2.5374348163604736, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [192 104 193 182 182 182 192 192 187 182 182   0 182  38 192 182  42 191\n 192 186 182  36 182 191 190 182 192 182 182 181 183  47], 정답 [182 182 182 182 182 182 182 182 182 182 182 182 182 182 182 182 182 182\n 182 182 182 182 182 182 182 182 182 182 182 182 183 183]\n(VAL) Batch 285 Loss : 4.0395917892456055, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 17 183 182 183 177 182 183 183 134 196 183  45 183 183 183  38  17 183\n 183  43 182 179 182  38 183 183  43 183 183 183 199 196], 정답 [183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 183\n 183 183 183 183 183 183 183 183 183 183 183 183 183 183]\n(VAL) Batch 286 Loss : 2.928649425506592, accuracy: 0.46875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [178 183 104   6 183 183  36 134 183 182 196 182 182 178  14 199  43 199\n 184 184 184 184 184 184 187   0  88 184 184 184 179 189], 정답 [183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 183 184 184\n 184 184 184 184 184 184 184 184 184 184 184 184 184 184]\n(VAL) Batch 287 Loss : 2.8327889442443848, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [184 184 186 187  47 184  45 184 184 184 184 184 184 184 184 189 186 159\n 184 184 138 115 184 186 184 184 184 184 184 184 184 199], 정답 [184 184 184 184 184 184 184 184 184 184 184 184 184 184 184 184 184 184\n 184 184 184 184 184 184 184 184 184 184 184 184 184 184]\n(VAL) Batch 288 Loss : 1.539245367050171, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [184 187 174  12 124 190 185 185  91 185 182  16 159 185 162 185  15  46\n 185  10 185 149 173  58  93 185 185  15   6 185 185 185], 정답 [184 184 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185\n 185 185 185 185 185 185 185 185 185 185 185 185 185 185]\n(VAL) Batch 289 Loss : 3.339930295944214, accuracy: 0.40625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [114 185 185  15 185 185   0 185 185 185 185 185 185 185 143  83 185   3\n 127 185 181 153 186 186 187 192  45 188 189 186 186 193], 정답 [185 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185 185\n 185 185 186 186 186 186 186 186 186 186 186 186 186 186]\n(VAL) Batch 290 Loss : 2.5060508251190186, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 13 186 186 186 186  13 186 180 186 104 186 186 186 188 186 186 187 190\n 186 186 186 153 187 186 186 191 184 188 186 187 186 186], 정답 [186 186 186 186 186 186 186 186 186 186 186 186 186 186 186 186 186 186\n 186 186 186 186 186 186 186 186 186 186 186 186 186 186]\n(VAL) Batch 291 Loss : 2.259004831314087, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [  9 186 188 186 186 187 188 172 192  45 187  83 187 187 187 110 186 104\n 168 187   0 187 188 187 187 186 187 188 187 153 187 181], 정답 [186 186 186 186 186 186 187 187 187 187 187 187 187 187 187 187 187 187\n 187 187 187 187 187 187 187 187 187 187 187 187 187 187]\n(VAL) Batch 292 Loss : 2.153169870376587, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [187 187  42 187 187 187 104 199 184 188 187 178 187  45 193 186 186 187\n 187 187 187 104 187 193 188   1 188 180 178 188 175 188], 정답 [187 187 187 187 187 187 187 187 187 187 187 187 187 187 187 187 187 187\n 187 187 187 187 187 187 188 188 188 188 188 188 188 188]\n(VAL) Batch 293 Loss : 2.4163174629211426, accuracy: 0.5\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 37 188 188 192 186 156  18 188 181 188 188 188  15 187  27 188 188  45\n  45 188 179 188 123  99 189 188 188 113 188 187 181 188], 정답 [188 188 188 188 188 188 188 188 188 188 188 188 188 188 188 188 188 188\n 188 188 188 188 188 188 188 188 188 188 188 188 188 188]\n(VAL) Batch 294 Loss : 3.0663037300109863, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [184  18  79 188 188 135  44 188 188 188 189 189  90 189 189 189 189 189\n 189 189 191 189 189 189 189  90 189  59  46 189 189 189], 정답 [188 188 188 188 188 188 188 188 188 188 189 189 189 189 189 189 189 189\n 189 189 189 189 189 189 189 189 189 189 189 189 189 189]\n(VAL) Batch 295 Loss : 1.6866451501846313, accuracy: 0.6875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [184 189  16 189 189 189 189  42 189 141 189 189 189 189 184 190 189 189\n 189 189 117 189 189 189 185 189 189 189 190 190 190 190], 정답 [189 189 189 189 189 189 189 189 189 189 189 189 189 189 189 189 189 189\n 189 189 189 189 189 189 189 189 189 189 190 190 190 190]\n(VAL) Batch 296 Loss : 1.6150394678115845, accuracy: 0.75\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [190 190 189 177 177 190 189 177  89 190 182 190  88 190 190 190 177 191\n 190 104  41 178 190 192 105 174  14 177 191 191 190 179], 정답 [190 190 190 190 190 190 190 190 190 190 190 190 190 190 190 190 190 190\n 190 190 190 190 190 190 190 190 190 190 190 190 190 190]\n(VAL) Batch 297 Loss : 2.92026948928833, accuracy: 0.34375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [190 190  47 190 191 190 104 190 192 190 188 190 192 136 191 191 181 104\n 191  19 191 191 191 191 191 191 191 190 174 190 191 191], 정답 [190 190 190 190 190 190 190 190 190 190 190 190 190 190 191 191 191 191\n 191 191 191 191 191 191 191 191 191 191 191 191 191 191]\n(VAL) Batch 298 Loss : 2.2572193145751953, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [191 191 177 191 192 145 192 191 191 191 191  18 191 191  68 191 191 191\n 191 191 191 191 104 191 181 191 191 191 191 191 191 191], 정답 [191 191 191 191 191 191 191 191 191 191 191 191 191 191 191 191 191 191\n 191 191 191 191 191 191 191 191 191 191 191 191 191 191]\n(VAL) Batch 299 Loss : 0.9290335178375244, accuracy: 0.75\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [186 177 137  25 192 192  89 192 192 192 192 193 181 192   6 192 192 192\n 182 190 192 192 192 183 182 192 190 192 192 192 193 193], 정답 [192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192\n 192 192 192 192 192 192 192 192 192 192 192 192 192 192]\n(VAL) Batch 300 Loss : 2.021205186843872, accuracy: 0.53125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [192 192   0 181 192 181 191 182 192 192 192 192 178  24 192 192 192 187\n 193 193 193 193 193 181 193 193 193 193 193 193 193 150], 정답 [192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192 192\n 193 193 193 193 193 193 193 193 193 193 193 193 193 193]\n(VAL) Batch 301 Loss : 1.7449272871017456, accuracy: 0.6875\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [193  69 193 193 193 193 193 193 193 193  93 181 193 193 193 123 163 193\n 174 193  98 113 193 193 160 193 174 193 193 193 175 193], 정답 [193 193 193 193 193 193 193 193 193 193 193 193 193 193 193 193 193 193\n 193 193 193 193 193 193 193 193 193 193 193 193 193 193]\n(VAL) Batch 302 Loss : 2.1495578289031982, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [193 113 193 193 194 165  32 194 194 194  31 194 194 194 194 173 195 165\n 194 194 194  39  71  71 194 114 194 194 194  71 194  14], 정답 [193 193 193 193 194 194 194 194 194 194 194 194 194 194 194 194 194 194\n 194 194 194 194 194 194 194 194 194 194 194 194 194 194]\n(VAL) Batch 303 Loss : 1.7895689010620117, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 84 194 197 127 194 194 137  66 117 170 194  69  96 118 194  96  84 194\n 119 127  23 194   8 195  60 198 190  66 197  20 162  96], 정답 [194 194 194 194 194 194 194 194 194 194 194 194 194 194 194 194 194 194\n 194 194 194 194 195 195 195 195 195 195 195 195 195 195]\n(VAL) Batch 304 Loss : 2.9043257236480713, accuracy: 0.25\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 89 126 195 195  10   2  66 194 196 195 195 195 195 197  23 195 157 195\n 195 137  55 198 195 195 194  96  55  66 195  85  71 190], 정답 [195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195\n 195 195 195 195 195 195 195 195 195 195 195 195 195 195]\n(VAL) Batch 305 Loss : 3.5086686611175537, accuracy: 0.375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 96 197 194  56  96 195 195 195 196  23 196 196 196 196 196  46  17 196\n 196  13 196 196 196 183 196 196 196 196  14 196 196 196], 정답 [195 195 195 195 195 195 195 195 196 196 196 196 196 196 196 196 196 196\n 196 196 196 196 196 196 196 196 196 196 196 196 196 196]\n(VAL) Batch 306 Loss : 1.4418116807937622, accuracy: 0.65625\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [196 183  17 196  14 196  17 196  14 196 196 196   3  14 196 196 196  17\n 196 196  19 196 196  10  19  14 194  84 154 107  23 198], 정답 [196 196 196 196 196 196 196 196 196 196 196 196 196 196 196 196 196 196\n 196 196 196 196 196 196 196 196 197 197 197 197 197 197]\n(VAL) Batch 307 Loss : 2.3285224437713623, accuracy: 0.4375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [194  23  96 146 162 198 165 115  96  23  96 197 197  80 154 118 165 146\n 170 197 198 157 153 196 198 170 187  71  51 108 198 154], 정답 [197 197 197 197 197 197 197 197 197 197 197 197 197 197 197 197 197 197\n 197 197 197 197 197 197 197 197 197 197 197 197 197 197]\n(VAL) Batch 308 Loss : 4.174449443817139, accuracy: 0.09375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 51  27  14  27 195 145 196 197 173 102  71 197 197 198 198 158 157 154\n 198 146 198 198 198 194 198  96  69  96  96 198 154  96], 정답 [197 197 197 197 197 197 197 197 197 197 197 197 198 198 198 198 198 198\n 198 198 198 198 198 198 198 198 198 198 198 198 198 198]\n(VAL) Batch 309 Loss : 3.1180083751678467, accuracy: 0.3125\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [198 197 154 198 198 198 198 198 198  66   0 116 136 198 198 124 198 198\n 124 198 198   4 198 158 195 198 198 198 198 197 199 182], 정답 [198 198 198 198 198 198 198 198 198 198 198 198 198 198 198 198 198 198\n 198 198 198 198 198 198 198 198 198 198 198 198 199 199]\n(VAL) Batch 310 Loss : 2.9944047927856445, accuracy: 0.59375\nval - before view : torch.Size([32, 10, 3, 224, 224])\nval after view : torch.Size([320, 3, 224, 224])\nlogits1.shape : torch.Size([320, 200])\nlogits2.shape : torch.Size([32, 10, 200])\nlogits_mean.shape : torch.Size([32, 200])\nVAL : 예측라벨 : [ 36  25 199  15 199 199   5   1 199 180   2 179 148   1 199  69  15  94\n 185   0   6  45 180  38 178 184 179  43 199 199  66 199], 정답 [199 199 199 199 199 199 199 199 199 199 199 199 199 199 199 199 199 199\n 199 199 199 199 199 199 199 199 199 199 199 199 199 199]\n(VAL) Batch 311 Loss : 4.26535177230835, accuracy: 0.25\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [  6 187  13 199   1  16 185  55  15  32 199  73 199 119  43  36], 정답 [199 199 199 199 199 199 199 199 199 199 199 199 199 199 199 199]\n(VAL) Batch 312 Loss : 5.644144058227539, accuracy: 0.1875\nepoch 21 Loss/Validate :3.205622143162706 \nepoch 21 Accuracy/Validate : 0.3974\nEpoch : 22, batch 0\n(Train) Batch 0 Loss : 0.38316652178764343, 맞은 개수 : 113\nEpoch : 22, batch 1\n(Train) Batch 1 Loss : 0.19385264813899994, 맞은 개수 : 121\nEpoch : 22, batch 2\n(Train) Batch 2 Loss : 0.286876916885376, 맞은 개수 : 117\nEpoch : 22, batch 3\n(Train) Batch 3 Loss : 0.3680657148361206, 맞은 개수 : 117\nEpoch : 22, batch 4\n(Train) Batch 4 Loss : 0.2147737294435501, 맞은 개수 : 120\nEpoch : 22, batch 5\n(Train) Batch 5 Loss : 0.2528068721294403, 맞은 개수 : 120\nEpoch : 22, batch 6\n(Train) Batch 6 Loss : 0.2976454496383667, 맞은 개수 : 112\nEpoch : 22, batch 7\n(Train) Batch 7 Loss : 0.2699008285999298, 맞은 개수 : 116\nEpoch : 22, batch 8\n(Train) Batch 8 Loss : 0.2602274417877197, 맞은 개수 : 119\nEpoch : 22, batch 9\n(Train) Batch 9 Loss : 0.22159473598003387, 맞은 개수 : 123\nEpoch : 22, batch 10\n(Train) Batch 10 Loss : 0.28843051195144653, 맞은 개수 : 118\nEpoch : 22, batch 11\n(Train) Batch 11 Loss : 0.21322369575500488, 맞은 개수 : 121\nEpoch : 22, batch 12\n(Train) Batch 12 Loss : 0.13899032771587372, 맞은 개수 : 124\nEpoch : 22, batch 13\n(Train) Batch 13 Loss : 0.22245089709758759, 맞은 개수 : 120\nEpoch : 22, batch 14\n(Train) Batch 14 Loss : 0.28947019577026367, 맞은 개수 : 119\nEpoch : 22, batch 15\n(Train) Batch 15 Loss : 0.09526215493679047, 맞은 개수 : 125\nEpoch : 22, batch 16\n(Train) Batch 16 Loss : 0.12564978003501892, 맞은 개수 : 124\nEpoch : 22, batch 17\n(Train) Batch 17 Loss : 0.3239584267139435, 맞은 개수 : 116\n","output_type":"stream"},{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m epoch_val_true_prediction= \u001b[32m0\u001b[39m\n\u001b[32m     24\u001b[39m epoch_val_total = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_label\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m  \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 총 batch가 200번 반복됨 , 에포크 5니까 총 1000번 반복됨.\u001b[39;49;00m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataset.py:346\u001b[39m, in \u001b[36mConcatDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    345\u001b[39m     sample_idx = idx - \u001b[38;5;28mself\u001b[39m.cumulative_sizes[dataset_idx - \u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torchvision/datasets/folder.py:247\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    245\u001b[39m sample = \u001b[38;5;28mself\u001b[39m.loader(path)\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    249\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torchvision/transforms/transforms.py:277\u001b[39m, in \u001b[36mNormalize.forward\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torchvision/transforms/functional.py:350\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torchvision/transforms/_functional_tensor.py:917\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    912\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    913\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    914\u001b[39m     )\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m     tensor = \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m dtype = tensor.dtype\n\u001b[32m    920\u001b[39m mean = torch.as_tensor(mean, dtype=dtype, device=tensor.device)\n","\u001b[31mKeyboardInterrupt\u001b[39m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"# 시각화 해보기 \nplt.plot(epoch_train_losses, label='Train')\nplt.plot(epoch_val_losses,label = 'Val')\nplt.xlabel('Epoch')\nplt.ylabel(\"Train Loss\")\nplt.title('Train Loss')\nplt.legend()\nplt.grid(True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:39:51.759068Z","iopub.execute_input":"2025-12-01T13:39:51.759307Z","iopub.status.idle":"2025-12-01T13:39:51.876204Z","shell.execute_reply.started":"2025-12-01T13:39:51.759290Z","shell.execute_reply":"2025-12-01T13:39:51.875454Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"wrong_denom = len(small_val_loader) # 8\ncorrect_denom = len(val_dataloader) # 79\ncorrect_denom\n\ncorrection_factor = wrong_denom / correct_denom\n\nepoch_val_losses = [loss * correction_factor for loss in epoch_val_losses]\n\nepoch_val_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T06:57:29.279260Z","iopub.execute_input":"2025-11-28T06:57:29.279536Z","iopub.status.idle":"2025-11-28T06:57:29.285279Z","shell.execute_reply.started":"2025-11-28T06:57:29.279514Z","shell.execute_reply":"2025-11-28T06:57:29.284675Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(epoch_train_losses,label ='Train Loss')\nplt.plot(epoch_val_losses, label = 'Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:39:46.810866Z","iopub.execute_input":"2025-12-01T13:39:46.811136Z","iopub.status.idle":"2025-12-01T13:39:46.913435Z","shell.execute_reply.started":"2025-12-01T13:39:46.811119Z","shell.execute_reply":"2025-12-01T13:39:46.912647Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"plt.plot(epoch_train_accuracies, label = 'Train Accuracy')\nplt.plot(epoch_val_accuracies, label = 'Val Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel(\"Train Accuracy\")\nplt.legend()\nplt.grid(True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:30:45.684037Z","iopub.execute_input":"2025-12-01T11:30:45.684594Z","iopub.status.idle":"2025-12-01T11:30:45.854772Z","shell.execute_reply.started":"2025-12-01T11:30:45.684569Z","shell.execute_reply":"2025-12-01T11:30:45.854157Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"final_acc = epoch_train_accuracies[-1]\n\nfinal_loss = epoch_train_losses[-1]\n\nfinal_val_acc = epoch_val_accuracies[-1]\nfinal_val_loss = epoch_val_losses[-1]\n\nmax_train_acc = max(epoch_train_accuracies)\n\nprint(f'최종 학습 정확도 : {final_acc*100}')\nprint(f'최대 학습 정확도 : {max_train_acc*100}')\nprint(f'최종 검증 정확도 : {final_val_acc * 100}')\nprint(f'최대 검증 정확도 : {max(epoch_val_accuracies) * 100}')\n\n\nprint(f'최종 학습 Loss : {final_loss}')\nprint(f'최종 검증 loss : {final_val_loss}')\nprint(f'최소 학습 Loss : {min(epoch_train_losses)}')\nprint(f'최소 검증 Loss : {min(epoch_val_losses)}')\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:40:02.541835Z","iopub.execute_input":"2025-12-01T13:40:02.542160Z","iopub.status.idle":"2025-12-01T13:40:02.546514Z","shell.execute_reply.started":"2025-12-01T13:40:02.542142Z","shell.execute_reply":"2025-12-01T13:40:02.545782Z"}},"outputs":[{"name":"stdout","text":"최종 학습 정확도 : 91.5575\n최대 학습 정확도 : 91.5575\n최종 검증 정확도 : 39.739999999999995\n최대 검증 정확도 : 39.83\n최종 학습 Loss : 0.2744756396514288\n최종 검증 loss : 3.205622143162706\n최소 학습 Loss : 0.2744756396514288\n최소 검증 Loss : 3.171726841991321\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# 1. 앞 1000개만 쓰는 subset 정의\nnum_small = 1000\nsmall_indices = list(range(num_small))\nsmall_train_dataset = Subset(train_dataset, small_indices)\n\n# 2. 작은 dataloader\nsmall_train_loader = DataLoader(\n    small_train_dataset,\n    batch_size=64,\n    shuffle=True,\n)\n\nsmall_val_dataset = Subset(val_dataset, small_indices)\n\n# 2. 작은 dataloader\nsmall_val_loader = DataLoader(\n    small_val_dataset,\n    batch_size=16,\n    shuffle=False,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T06:57:33.137725Z","iopub.execute_input":"2025-12-01T06:57:33.137967Z","iopub.status.idle":"2025-12-01T06:57:33.142755Z","shell.execute_reply.started":"2025-12-01T06:57:33.137949Z","shell.execute_reply":"2025-12-01T06:57:33.142098Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from collections import Counter\nimport numpy as np\n\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for x, y in small_train_loader:\n        x = x.to(device)\n        logits = model(x)\n        preds = logits.argmax(1).cpu().numpy()\n        all_preds.extend(list(preds))\n        all_labels.extend(list(y.numpy()))\n\nprint(\"pred label dist:\", Counter(all_preds))\nprint(\"true label dist:\", Counter(all_labels))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T05:42:29.560027Z","iopub.execute_input":"2025-12-01T05:42:29.560306Z","iopub.status.idle":"2025-12-01T05:42:31.898545Z","shell.execute_reply.started":"2025-12-01T05:42:29.560286Z","shell.execute_reply":"2025-12-01T05:42:31.897900Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"pred label dist: Counter({1: 1000})\ntrue label dist: Counter({1: 500, 0: 500})\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# 3. 3~5 epoch만 빠르게 돌려보기\nepoch_losses = []\nepoch_accuracies = []\nfor epoch in range(1):\n    epoch_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for imgs, labels in small_train_loader:\n        model.train()\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logits = model(imgs)\n        loss = loss_fn(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item() * imgs.size(0)\n        preds = logits.argmax(1)\n\n        print(f'preds : {preds}')\n        correct += (preds == labels).sum().item()\n        total += imgs.size(0)\n\n    with torch.no_grad():\n        for img,labels in small_val_loader:\n            model.eval()\n            # validate \n            # print(f'val img.shape : {img.shape }')\n            # B, ncrops, C, H, W = images.shape\n            # img = img.view(-1,C, H, W )\n            val_loss , val_true_prediction = validate(model, img,labels)\n            print(f'val loss :{val_loss}, val_accuracy : {val_true_prediction/labels.shape[0]}')\n        \n    print(f'[small] epoch {epoch} loss: {epoch_loss/total:.3f}, acc: {correct/total:.3f}')\n    epoch_losses.append(epoch_loss/total)\n    epoch_accuracies.append(correct/total)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T07:12:38.646697Z","iopub.execute_input":"2025-12-01T07:12:38.647212Z","iopub.status.idle":"2025-12-01T07:12:55.512046Z","shell.execute_reply.started":"2025-12-01T07:12:38.647187Z","shell.execute_reply":"2025-12-01T07:12:55.511342Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"preds : tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0], device='cuda:0')\npreds : tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\npreds : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\npreds : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\npreds : tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1], device='cuda:0')\npreds : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\npreds : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\npreds : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\npreds : tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1], device='cuda:0')\npreds : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0], device='cuda:0')\npreds : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\npreds : tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\npreds : tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')\npreds : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\npreds : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], device='cuda:0')\npreds : tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nval loss :1.041295051574707, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nval loss :1.041256070137024, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nval loss :1.041247010231018, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\nval loss :0.5117075443267822, val_accuracy : 0.875\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\nval loss :0.4355534017086029, val_accuracy : 1.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\nval loss :0.43556177616119385, val_accuracy : 1.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2]\nval loss :15.174603462219238, val_accuracy : 0.25\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\nval loss :20.088008880615234, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\nval loss :20.08845329284668, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3]\nval loss :19.24912452697754, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\nval loss :18.744630813598633, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\nval loss :18.745601654052734, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4]\nval loss :18.84774398803711, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\nval loss :18.94643783569336, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\nval loss :18.948753356933594, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5]\nval loss :18.565109252929688, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\nval loss :17.926576614379883, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\nval loss :17.927223205566406, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6]\nval loss :18.19649887084961, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\nval loss :19.001455307006836, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\nval loss :19.001178741455078, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7]\nval loss :19.095983505249023, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\nval loss :19.764249801635742, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\nval loss :19.763813018798828, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\nval loss :19.765087127685547, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\nval loss :21.0161075592041, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\nval loss :21.017255783081055, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\nval loss :21.016082763671875, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [8 8 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\nval loss :19.663206100463867, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\nval loss :19.469892501831055, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\nval loss :19.46919822692871, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [ 9  9  9  9 10 10 10 10 10 10 10 10 10 10 10 10]\nval loss :19.414203643798828, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\nval loss :19.39652442932129, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\nval loss :19.395498275756836, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11]\nval loss :19.083255767822266, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\nval loss :18.89910125732422, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\nval loss :18.897415161132812, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [11 11 11 11 11 11 11 11 12 12 12 12 12 12 12 12]\nval loss :19.019756317138672, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12]\nval loss :19.142505645751953, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12]\nval loss :19.141294479370117, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13]\nval loss :19.224246978759766, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13]\nval loss :19.35984992980957, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13]\nval loss :19.3610897064209, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [13 13 13 13 13 13 13 13 13 13 13 13 14 14 14 14]\nval loss :19.60513687133789, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]\nval loss :20.340238571166992, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]\nval loss :20.340391159057617, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [14 14 14 14 14 14 14 14 14 14 14 14 14 14 15 15]\nval loss :20.22122573852539, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15]\nval loss :19.393810272216797, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15]\nval loss :19.395092010498047, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15]\nval loss :19.39630699157715, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16]\nval loss :18.585102081298828, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16]\nval loss :18.58470916748047, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16]\nval loss :18.58475685119629, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [16 16 17 17 17 17 17 17 17 17 17 17 17 17 17 17]\nval loss :18.722476959228516, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17]\nval loss :18.74221420288086, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17]\nval loss :18.74142074584961, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [17 17 17 17 18 18 18 18 18 18 18 18 18 18 18 18]\nval loss :19.111194610595703, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18]\nval loss :19.235937118530273, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18]\nval loss :19.236392974853516, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [18 18 18 18 18 18 19 19 19 19 19 19 19 19 19 19]\nval loss :19.556509017944336, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19]\nval loss :19.747474670410156, val_accuracy : 0.0\nval - before view : torch.Size([16, 10, 3, 224, 224])\nval after view : torch.Size([160, 3, 224, 224])\nlogits1.shape : torch.Size([160, 200])\nlogits2.shape : torch.Size([16, 10, 200])\nlogits_mean.shape : torch.Size([16, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], 정답 [19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19]\nval loss :19.747417449951172, val_accuracy : 0.0\nval - before view : torch.Size([8, 10, 3, 224, 224])\nval after view : torch.Size([80, 3, 224, 224])\nlogits1.shape : torch.Size([80, 200])\nlogits2.shape : torch.Size([8, 10, 200])\nlogits_mean.shape : torch.Size([8, 200])\nVAL : 예측라벨 : [1 1 1 1 1 1 1 1], 정답 [19 19 19 19 19 19 19 19]\nval loss :19.747821807861328, val_accuracy : 0.0\n[small] epoch 0 loss: 1.124, acc: 0.501\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"plt.plot(epoch_losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title(\"Adam + lr : 1e-4\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:21:50.280060Z","iopub.status.idle":"2025-11-30T13:21:50.280361Z","shell.execute_reply.started":"2025-11-30T13:21:50.280191Z","shell.execute_reply":"2025-11-30T13:21:50.280207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(epoch_accuracies)\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title(\"Adam + lr : 1e-4\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T13:49:36.555935Z","iopub.execute_input":"2025-11-27T13:49:36.556736Z","iopub.status.idle":"2025-11-27T13:49:36.738311Z","shell.execute_reply.started":"2025-11-27T13:49:36.556708Z","shell.execute_reply":"2025-11-27T13:49:36.737547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for imgs, labels in small_train_loader:\n    print(np.unique(labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T13:25:20.531133Z","iopub.execute_input":"2025-11-27T13:25:20.531772Z","iopub.status.idle":"2025-11-27T13:25:22.378769Z","shell.execute_reply.started":"2025-11-27T13:25:20.531748Z","shell.execute_reply":"2025-11-27T13:25:22.377779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epoch_train_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:42:10.845332Z","iopub.execute_input":"2025-12-01T13:42:10.845575Z","iopub.status.idle":"2025-12-01T13:42:10.849120Z","shell.execute_reply.started":"2025-12-01T13:42:10.845558Z","shell.execute_reply":"2025-12-01T13:42:10.848322Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[0.2950662958053771, 0.2744756396514288]"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"x0, y0 = train_dataset[0]      # #train_dataset[0] # 레이블 0인, 이미지들 \n\nx0 = x0.unsqueeze(0).to(device)\ny0 = torch.tensor([0], device=device)\nmodel.to(device)\nfor step in range(200):\n    optimizer.zero_grad()\n    logits = model(x0)\n    loss = loss_fn(logits, y0)\n    loss.backward()\n    optimizer.step()\n\n    pred = logits.argmax(1).item()\n    if step % 20 == 0:\n        print(step, \"loss:\", loss.item(), \"pred:\", pred)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T13:19:16.417314Z","iopub.execute_input":"2025-11-27T13:19:16.417675Z","iopub.status.idle":"2025-11-27T13:19:18.037418Z","shell.execute_reply.started":"2025-11-27T13:19:16.417643Z","shell.execute_reply":"2025-11-27T13:19:18.036753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images, labels = next(iter(train_dataloader))\nprint(\"batch label dist:\", np.bincount(labels.numpy()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T12:44:51.811219Z","iopub.execute_input":"2025-11-27T12:44:51.811610Z","iopub.status.idle":"2025-11-27T12:44:52.438935Z","shell.execute_reply.started":"2025-11-27T12:44:51.811576Z","shell.execute_reply":"2025-11-27T12:44:52.437946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nalex = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\nalex.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T12:46:35.202285Z","iopub.execute_input":"2025-11-27T12:46:35.203083Z","iopub.status.idle":"2025-11-27T12:46:35.957954Z","shell.execute_reply.started":"2025-11-27T12:46:35.203016Z","shell.execute_reply":"2025-11-27T12:46:35.957211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"alex.classifier[6].out_features = 200\n\nalex","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T12:48:30.092725Z","iopub.execute_input":"2025-11-27T12:48:30.093037Z","iopub.status.idle":"2025-11-27T12:48:30.099022Z","shell.execute_reply.started":"2025-11-27T12:48:30.093016Z","shell.execute_reply":"2025-11-27T12:48:30.098307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(val_dataloader.batch_size) # batch의 크기 33  , batch_size = 120\nlen(train_dataloader) # batch_size = 120 \n\nepoch_val_accuracies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:42:33.566609Z","iopub.execute_input":"2025-12-01T13:42:33.566856Z","iopub.status.idle":"2025-12-01T13:42:33.570870Z","shell.execute_reply.started":"2025-12-01T13:42:33.566831Z","shell.execute_reply":"2025-12-01T13:42:33.570180Z"}},"outputs":[{"name":"stdout","text":"32\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"[np.float64(0.3983), np.float64(0.3974)]"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"torch.save(\n    {'epoch' : 21,\n    'model_state_dict':model.state_dict(),\n    'optimizer' : optimizer.state_dict(),\n    'train_loss': epoch_train_losses,\n    'val_loss': epoch_val_losses,\n     'train_accuracy' : epoch_train_accuracies,\n     'val_accuracy' : epoch_val_accuracies\n    }\n   ,'./alexnet_ckpt.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:43:01.461326Z","iopub.execute_input":"2025-12-01T13:43:01.461608Z","iopub.status.idle":"2025-12-01T13:43:02.384537Z","shell.execute_reply.started":"2025-12-01T13:43:01.461578Z","shell.execute_reply":"2025-12-01T13:43:02.383523Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from IPython.display import FileLink\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint = torch.load('./alexnet_ckpt.pth',map_location = torch.device('cpu'))\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer'])\nstart_epoch = checkpoint['epoch']+1\nepoch_train_losses = checkpoint['train_loss']\nepoch_val_losses = checkpoint['val_loss']\nepoch_train_accuracies = checkpoint['train_accuracy']\nepoch_val_accuracies = checkpoint['val_accuracy']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:42:56.127547Z","iopub.execute_input":"2025-12-01T13:42:56.127806Z","iopub.status.idle":"2025-12-01T13:42:56.421423Z","shell.execute_reply.started":"2025-12-01T13:42:56.127786Z","shell.execute_reply":"2025-12-01T13:42:56.420284Z"}},"outputs":[{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./alexnet_ckpt.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m model.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      3\u001b[39m optimizer.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m'\u001b[39m])\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/serialization.py:1529\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1521\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1522\u001b[39m                     opened_zipfile,\n\u001b[32m   1523\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1526\u001b[39m                     **pickle_load_args,\n\u001b[32m   1527\u001b[39m                 )\n\u001b[32m   1528\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1530\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1531\u001b[39m             opened_zipfile,\n\u001b[32m   1532\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1535\u001b[39m             **pickle_load_args,\n\u001b[32m   1536\u001b[39m         )\n\u001b[32m   1537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n","\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy._core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."],"ename":"UnpicklingError","evalue":"Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy._core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T12:04:51.689828Z","iopub.execute_input":"2025-12-01T12:04:51.690125Z","iopub.status.idle":"2025-12-01T12:04:51.693863Z","shell.execute_reply.started":"2025-12-01T12:04:51.690104Z","shell.execute_reply":"2025-12-01T12:04:51.693105Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"97.30347652733326"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"model2 =  Alexnet()\nmodel2.load_state_dict(torch.load('./model', weights_only=True))\nmodel2","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}