{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install tensorboardX"
      ],
      "metadata": {
        "id": "K3kLCKsTBvKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIvcvaX21PCb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import torchvision.transforms as T\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 정의하기\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.c1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "        self.s2 = nn.AvgPool2d(2,2)\n",
        "        self.c3  = nn.Conv2d(in_channels= 6, out_channels= 16 , kernel_size=5)\n",
        "        self.s4 = nn.AvgPool2d(2,2)\n",
        "        self.c5 = nn.Conv2d(in_channels= 16, out_channels= 120, kernel_size=5)\n",
        "\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "             nn.Linear(in_features= 120, out_features=84),\n",
        "            nn.Tanh(), # 원래 FC layer에서도 tanh를 적용해줘야하나?\n",
        "            nn.Linear(in_features=84, out_features=10)\n",
        "        )\n",
        "\n",
        "        # 여기서 어떻게 84 -> 10개로 줄엿지?\n",
        "\n",
        "    def forward(self,x):\n",
        "        # 입력 데이터는 28\n",
        "\n",
        "        # C1에 입력 데이터 넘겨주기\n",
        "        # C1에서 나온 데이터에 weight를 곱하고, bias를 더해준다. -> 이는 nn.Conv2d 내부에서 적용되어서 결과가 나온다\n",
        "        # tanh 함수 적용하기\n",
        "        result = F.tanh(self.c1(x))\n",
        "        result = self.s2(result)\n",
        "\n",
        "        # pooling 레이어에 Dropout 적용 후 C3 입력으로 넣어줌\n",
        "       # result = F.dropout(result,p=0.3) # p : default value = 0.5 , 0.3\n",
        "        result = F.tanh(self.c3(result))\n",
        "\n",
        "        result = self.s4(result)\n",
        "\n",
        "        result = self.c5(result)\n",
        "        result = torch.flatten(result,1)\n",
        "\n",
        "        logits = self.classifier(result)\n",
        "\n",
        "        probs = F.softmax(logits,dim=1) # softmax를 통해 결과를 확률값으로 변환해준다.\n",
        "\n",
        "        return logits, probs"
      ],
      "metadata": {
        "id": "a8_WaSoS1o4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ToTensor()를 통해 PIL Image를 FloatTensor로 변경하고 이미지 값들이 0과 1사이에 위치하도록 조정한다.\n",
        "# size를 32 x 32로 변경한다.\n",
        "# 입력 픽셀 값들은 background는 -0.1로 foreground는 1.175로 정규화된다\n",
        "\n",
        "transform = T.Compose([\n",
        "            T.ToTensor(),\n",
        "            T.Resize(32),\n",
        "            T.Normalize(mean=0, std=1)\n",
        "             ]\n",
        "        )\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data', # 데이터셋을 저장할 경로\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform= transform\n",
        ")\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train = False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "#image , label = train_dataset[0]\n",
        "\n",
        "#print(image.shape) # 원본 : torch.Size([1, 28, 28]) -> resize 를 통해 torch.Size([1, 32, 32])로 수정\n",
        "#label # 5\n",
        "\n",
        "#plt.imshow(image.squeeze(0),cmap='gray')"
      ],
      "metadata": {
        "id": "ebaEyi_91oz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델에 데이터를 넘겨주기 위해 Dataset과 DataLoader를 구성해보자 .\n",
        "# MNIST 데이터 자체가 Dataset이므로 우리는 DataLoader만 구성하면 된다.\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,batch_size = 128,shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 128, shuffle= True)"
      ],
      "metadata": {
        "id": "ac7Z856pG5NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "import tensorboard\n",
        "import datetime\n",
        "\n",
        "# Set the log directory\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# Start TensorBoard within the notebook\n",
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "collapsed": true,
        "id": "R8Gmb-REnbMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorboardX import SummaryWriter\n",
        "writer = SummaryWriter(log_dir)"
      ],
      "metadata": {
        "id": "oZY37Gs77gnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_loss(loss,epoch):\n",
        "    plt.plot(epoch, loss)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hbPHvaWxPdqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data_loader):\n",
        "\n",
        "    running_loss = 0\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for data, target in data_loader:\n",
        "\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 순전파\n",
        "        logits, prediction = model(data)\n",
        "\n",
        "        # Removed .to(torch.float32) as CrossEntropyLoss expects long type\n",
        "        loss = criterion(logits, target.to(torch.long) )\n",
        "        running_loss += loss.item() * target.size(0)\n",
        "        # 역전파\n",
        "        loss.backward() # 기울기 누적\n",
        "        optimizer.step() # weight, bias 업데이트\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader.dataset)\n",
        "    return model, epoch_loss"
      ],
      "metadata": {
        "id": "h3rd0zzDcM_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 정확도 평가 함수\n",
        "def get_accuracy(model, data_loader):\n",
        "\n",
        "    n = 0\n",
        "    true_label = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            logits ,probs = lenet(data)\n",
        "\n",
        "            probs = probs.squeeze() # 1차원 제거해주기\n",
        "\n",
        "            max_prob, label = torch.max(probs,1)\n",
        "\n",
        "            n += target.size(0)\n",
        "            true_label += (target == label).sum() # 맞은 예측의 개수\n",
        "        accuracy = true_label.float() / n\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "LJ-lMbUrWOyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "lenet = LeNet()\n",
        "\n",
        "lenet = lenet.to(device)\n",
        "\n",
        "# nn.MSELoss() 대신  CrossEntropyLoss 사용\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(lenet.parameters(), lr=0.0005)\n",
        "\n",
        "\n",
        "# 모델을 학습 시켜보자\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "lenet.train()\n",
        "for epoch in range(epochs):\n",
        "    lenet ,loss = train_model(lenet, train_dataloader)\n",
        "    show_loss(loss, epoch)\n",
        "    accuracy = get_accuracy(lenet, train_dataloader)\n",
        "\n",
        "    print(f'epoch {epoch} loss : {loss}')\n",
        "    # print(f'epoch {epoch} loss : {np.mean(train_losses)}')\n",
        "    writer.add_scalar(\"Loss/train\", loss, epoch)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "D6WXZOiftfLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer.flush()"
      ],
      "metadata": {
        "id": "ehV-Gs268uwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "nhZdfvF4oB4q",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RuntimeError: mat1 and mat2 shapes cannot be multiplied (7680x1 and 120x84)\n",
        "- 7680 : 64(배치 크기) * 120 (입력 크기)\n",
        "- 120 * 84 : 입력 크기, 출력 크기"
      ],
      "metadata": {
        "id": "N5XWZy9E3jng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in lenet.state_dict():\n",
        "    print(param_tensor, \"\\t\", lenet.state_dict()[param_tensor].size())\n",
        "\n",
        "#torch.save(lenet, './model')"
      ],
      "metadata": {
        "id": "HG38Vk1ctnxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lenet = torch.load('model',weights_only=True)\n",
        "lenet.eval()\n",
        "with torch.no_grad():\n",
        "    accuracy = get_accuracy(lenet, test_dataloader) * 100\n",
        "print(f'Accuracy: {accuracy:.2f}%')\n"
      ],
      "metadata": {
        "id": "Xt0Y9PO6Dywt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EckxXEi05ens"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}